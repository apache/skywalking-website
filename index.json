[{"body":"Advanced deployment OAP servers communicate with each other in a cluster environment. In the cluster mode, you could run in different roles.\n Mixed(default) Receiver Aggregator  Sometimes users may wish to deploy cluster nodes with a clearly defined role. They could then use this function.\nMixed By default, the OAP is responsible for:\n Receiving agent traces or metrics. L1 aggregation Internal communication (sending/receiving) L2 aggregation Persistence Alarm  Receiver The OAP is responsible for:\n Receiving agent traces or metrics. L1 aggregation Internal communication (sending)  Aggregator The OAP is responsible for:\n Internal communication(receive) L2 aggregation Persistence Alarm   These roles are designed for complex deployment requirements on security and network policy.\nKubernetes If you are using our native Kubernetes coordinator, the labelSelector setting is used for Aggregator role selection rules. Choose the right OAP deployment based on your needs.\n","excerpt":"Advanced deployment OAP servers communicate with each other in a cluster environment. In the cluster …","ref":"/docs/main/latest/en/setup/backend/advanced-deployment/","title":"Advanced deployment"},{"body":"Alarm Alarm core is driven by a collection of rules, which are defined in config/alarm-settings.yml. There are three parts in alarm rule definition.\n Alarm rules. They define how metrics alarm should be triggered and what conditions should be considered. Webhooks. The list of web service endpoints, which should be called after the alarm is triggered. gRPCHook. The host and port of the remote gRPC method, which should be called after the alarm is triggered.  Entity name Defines the relation between scope and entity name.\n Service: Service name Instance: {Instance name} of {Service name} Endpoint: {Endpoint name} in {Service name} Database: Database service name Service Relation: {Source service name} to {Dest service name} Instance Relation: {Source instance name} of {Source service name} to {Dest instance name} of {Dest service name} Endpoint Relation: {Source endpoint name} in {Source Service name} to {Dest endpoint name} in {Dest service name}  Rules There are two types of rules: individual rules and composite rules. A composite rule is a combination of individual rules.\nIndividual rules An alarm rule is made up of the following elements:\n Rule name. A unique name shown in the alarm message. It must end with _rule. Metrics name. This is also the metrics name in the OAL script. Only long, double, int types are supported. See the list of all potential metrics name. Events can be also configured as the source of alarm, please refer to the event doc for more details. Include names. Entity names which are included in this rule. Please follow the entity name definitions. Exclude names. Entity names which are excluded from this rule. Please follow the entity name definitions. Include names regex. A regex that includes entity names. If both include-name list and include-name regex are set, both rules will take effect. Exclude names regex. A regex that excludes entity names. If both exclude-name list and exclude-name regex are set, both rules will take effect. Include labels. Metric labels which are included in this rule. Exclude labels. Metric labels which are excluded from this rule. Include labels regex. A regex that includes labels. If both include-label list and include-label regex are set, both rules will take effect. Exclude labels regex. A regex that exclude labels. If both the exclude-label list and exclude-label regex are set, both rules will take effect. Tags. Tags are key/value pairs that are attached to alarms. Tags are used to specify distinguishing attributes of alarms that are meaningful and relevant to users. If you would like to make these tags searchable on the SkyWalking UI, you may set the tag keys in core/default/searchableAlarmTags, or through system environment variable SW_SEARCHABLE_ALARM_TAG_KEYS. The key level is supported by default.  Label settings are required by the meter-system. They are used to store metrics from the label-system platform, such as Prometheus, Micrometer, etc. The four label settings mentioned above must implement LabeledValueHolder.\n Threshold. The target value. For multiple-value metrics, such as percentile, the threshold is an array. It is described as: value1, value2, value3, value4, value5. Each value may serve as the threshold for each value of the metrics. Set the value to - if you do not wish to trigger the alarm by one or more of the values.\nFor example in percentile, value1 is the threshold of P50, and -, -, value3, value4, value5 means that there is no threshold for P50 and P75 in the percentile alarm rule. OP. The operator. It supports \u0026gt;, \u0026gt;=, \u0026lt;, \u0026lt;=, =. We welcome contributions of all OPs. Period. The frequency for checking the alarm rule. This is a time window that corresponds to the backend deployment env time. Count. Within a period window, if the number of times which value goes over the threshold (based on OP) reaches count, then an alarm will be sent. Only as condition. Indicates if the rule can send notifications, or if it simply serves as an condition of the composite rule. Silence period. After the alarm is triggered in Time-N, there will be silence during the TN -\u0026gt; TN + period. By default, it works in the same manner as period. The same alarm (having the same ID in the same metrics name) may only be triggered once within a period.  Composite rules NOTE: Composite rules are only applicable to alarm rules targeting the same entity level, such as service-level alarm rules (service_percent_rule \u0026amp;\u0026amp; service_resp_time_percentile_rule). Do not compose alarm rules of different entity levels, such as an alarm rule of the service metrics with another rule of the endpoint metrics.\nA composite rule is made up of the following elements:\n Rule name. A unique name shown in the alarm message. Must end with _rule. Expression. Specifies how to compose rules, and supports \u0026amp;\u0026amp;, ||, and (). Message. The notification message to be sent out when the rule is triggered. Tags. Tags are key/value pairs that are attached to alarms. Tags are used to specify distinguishing attributes of alarms that are meaningful and relevant to users.  rules: # Rule unique name, must be ended with `_rule`. endpoint_percent_rule: # Metrics value need to be long, double or int metrics-name: endpoint_percent threshold: 75 op: \u0026lt; # The length of time to evaluate the metrics period: 10 # How many times after the metrics match the condition, will trigger alarm count: 3 # How many times of checks, the alarm keeps silence after alarm triggered, default as same as period. silence-period: 10 # Specify if the rule can send notification or just as an condition of composite rule only-as-condition: false tags: level: WARNING service_percent_rule: metrics-name: service_percent # [Optional] Default, match all services in this metrics include-names: - service_a - service_b exclude-names: - service_c # Single value metrics threshold. threshold: 85 op: \u0026lt; period: 10 count: 4 only-as-condition: false service_resp_time_percentile_rule: # Metrics value need to be long, double or int metrics-name: service_percentile op: \u0026#34;\u0026gt;\u0026#34; # Multiple value metrics threshold. Thresholds for P50, P75, P90, P95, P99. threshold: 1000,1000,1000,1000,1000 period: 10 count: 3 silence-period: 5 message: Percentile response time of service {name} alarm in 3 minutes of last 10 minutes, due to more than one condition of p50 \u0026gt; 1000, p75 \u0026gt; 1000, p90 \u0026gt; 1000, p95 \u0026gt; 1000, p99 \u0026gt; 1000 only-as-condition: false meter_service_status_code_rule: metrics-name: meter_status_code exclude-labels: - \u0026#34;200\u0026#34; op: \u0026#34;\u0026gt;\u0026#34; threshold: 10 period: 10 count: 3 silence-period: 5 message: The request number of entity {name} non-200 status is more than expected. only-as-condition: false composite-rules: comp_rule: # Must satisfied percent rule and resp time rule  expression: service_percent_rule \u0026amp;\u0026amp; service_resp_time_percentile_rule message: Service {name} successful rate is less than 80% and P50 of response time is over 1000ms tags: level: CRITICAL Default alarm rules For convenience\u0026rsquo;s sake, we have provided a default alarm-setting.yml in our release. It includes the following rules:\n Service average response time over 1s in the last 3 minutes. Service success rate lower than 80% in the last 2 minutes. Percentile of service response time over 1s in the last 3 minutes Service Instance average response time over 1s in the last 2 minutes, and the instance name matches the regex. Endpoint average response time over 1s in the last 2 minutes. Database access average response time over 1s in the last 2 minutes. Endpoint relation average response time over 1s in the last 2 minutes.  List of all potential metrics name The metrics names are defined in the official OAL scripts and MAL scripts, the Event names can also serve as the metrics names, all possible event names can be also found in the Event doc.\nCurrently, metrics from the Service, Service Instance, Endpoint, Service Relation, Service Instance Relation, Endpoint Relation scopes could be used in Alarm, and the Database access scope is same as Service.\nSubmit an issue or a pull request if you want to support any other scopes in alarm.\nWebhook The Webhook requires the peer to be a web container. The alarm message will be sent through HTTP post by application/json content type. The JSON format is based on List\u0026lt;org.apache.skywalking.oap.server.core.alarm.AlarmMessage\u0026gt; with the following key information:\n scopeId, scope. All scopes are defined in org.apache.skywalking.oap.server.core.source.DefaultScopeDefine. name. Target scope entity name. Please follow the entity name definitions. id0. The ID of the scope entity that matches with the name. When using the relation scope, it is the source entity ID. id1. When using the relation scope, it is the destination entity ID. Otherwise, it is empty. ruleName. The rule name configured in alarm-settings.yml. alarmMessage. The alarm text message. startTime. The alarm time measured in milliseconds, which occurs between the current time and the midnight of January 1, 1970 UTC. tags. The tags configured in alarm-settings.yml.  See the following example:\n[{ \u0026#34;scopeId\u0026#34;: 1, \u0026#34;scope\u0026#34;: \u0026#34;SERVICE\u0026#34;, \u0026#34;name\u0026#34;: \u0026#34;serviceA\u0026#34;, \u0026#34;id0\u0026#34;: \u0026#34;12\u0026#34;, \u0026#34;id1\u0026#34;: \u0026#34;\u0026#34;, \u0026#34;ruleName\u0026#34;: \u0026#34;service_resp_time_rule\u0026#34;, \u0026#34;alarmMessage\u0026#34;: \u0026#34;alarmMessage xxxx\u0026#34;, \u0026#34;startTime\u0026#34;: 1560524171000, \u0026#34;tags\u0026#34;: [{ \u0026#34;key\u0026#34;: \u0026#34;level\u0026#34;, \u0026#34;value\u0026#34;: \u0026#34;WARNING\u0026#34; }] }, { \u0026#34;scopeId\u0026#34;: 1, \u0026#34;scope\u0026#34;: \u0026#34;SERVICE\u0026#34;, \u0026#34;name\u0026#34;: \u0026#34;serviceB\u0026#34;, \u0026#34;id0\u0026#34;: \u0026#34;23\u0026#34;, \u0026#34;id1\u0026#34;: \u0026#34;\u0026#34;, \u0026#34;ruleName\u0026#34;: \u0026#34;service_resp_time_rule\u0026#34;, \u0026#34;alarmMessage\u0026#34;: \u0026#34;alarmMessage yyy\u0026#34;, \u0026#34;startTime\u0026#34;: 1560524171000, \u0026#34;tags\u0026#34;: [{ \u0026#34;key\u0026#34;: \u0026#34;level\u0026#34;, \u0026#34;value\u0026#34;: \u0026#34;CRITICAL\u0026#34; }] }] gRPCHook The alarm message will be sent through remote gRPC method by Protobuf content type. The message contains key information which are defined in oap-server/server-alarm-plugin/src/main/proto/alarm-hook.proto.\nPart of the protocol looks like this:\nmessage AlarmMessage { int64 scopeId = 1; string scope = 2; string name = 3; string id0 = 4; string id1 = 5; string ruleName = 6; string alarmMessage = 7; int64 startTime = 8; AlarmTags tags = 9;}message AlarmTags { // String key, String value pair.  repeated KeyStringValuePair data = 1;}message KeyStringValuePair { string key = 1; string value = 2;}Slack Chat Hook Follow the Getting Started with Incoming Webhooks guide and create new Webhooks.\nThe alarm message will be sent through HTTP post by application/json content type if you have configured Slack Incoming Webhooks as follows:\nslackHooks: textTemplate: |-{ \u0026#34;type\u0026#34;: \u0026#34;section\u0026#34;, \u0026#34;text\u0026#34;: { \u0026#34;type\u0026#34;: \u0026#34;mrkdwn\u0026#34;, \u0026#34;text\u0026#34;: \u0026#34;:alarm_clock: *Apache Skywalking Alarm* \\n **%s**.\u0026#34; } } webhooks: - https://hooks.slack.com/services/x/y/z WeChat Hook Note that only the WeChat Company Edition (WeCom) supports WebHooks. To use the WeChat WebHook, follow the Wechat Webhooks guide. The alarm message will be sent through HTTP post by application/json content type after you have set up Wechat Webhooks as follows:\nwechatHooks: textTemplate: |-{ \u0026#34;msgtype\u0026#34;: \u0026#34;text\u0026#34;, \u0026#34;text\u0026#34;: { \u0026#34;content\u0026#34;: \u0026#34;Apache SkyWalking Alarm: \\n %s.\u0026#34; } } webhooks: - https://qyapi.weixin.qq.com/cgi-bin/webhook/send?key=dummy_key Dingtalk Hook Follow the Dingtalk Webhooks guide and create new Webhooks. For security purposes, you can config an optional secret for an individual webhook URL. The alarm message will be sent through HTTP post by application/json content type if you have configured Dingtalk Webhooks as follows:\ndingtalkHooks: textTemplate: |-{ \u0026#34;msgtype\u0026#34;: \u0026#34;text\u0026#34;, \u0026#34;text\u0026#34;: { \u0026#34;content\u0026#34;: \u0026#34;Apache SkyWalking Alarm: \\n %s.\u0026#34; } } webhooks: - url: https://oapi.dingtalk.com/robot/send?access_token=dummy_token secret: dummysecret Feishu Hook Follow the Feishu Webhooks guide and create new Webhooks. For security purposes, you can config an optional secret for an individual webhook URL. If you would like to direct a text to a user, you can config ats which is the feishu\u0026rsquo;s user_id and separated by \u0026ldquo;,\u0026rdquo; . The alarm message will be sent through HTTP post by application/json content type if you have configured Feishu Webhooks as follows:\nfeishuHooks: textTemplate: |-{ \u0026#34;msg_type\u0026#34;: \u0026#34;text\u0026#34;, \u0026#34;content\u0026#34;: { \u0026#34;text\u0026#34;: \u0026#34;Apache SkyWalking Alarm: \\n %s.\u0026#34; }, \u0026#34;ats\u0026#34;:\u0026#34;feishu_user_id_1,feishu_user_id_2\u0026#34; } webhooks: - url: https://open.feishu.cn/open-apis/bot/v2/hook/dummy_token secret: dummysecret WeLink Hook Follow the WeLink Webhooks guide and create new Webhooks. The alarm message will be sent through HTTP post by application/json content type if you have configured WeLink Webhooks as follows:\nwelinkHooks: textTemplate: \u0026#34;Apache SkyWalking Alarm: \\n %s.\u0026#34; webhooks: # you may find your own client_id and client_secret in your app, below are dummy, need to change. - client_id: \u0026#34;dummy_client_id\u0026#34; client_secret: dummy_secret_key access_token_url: https://open.welink.huaweicloud.com/api/auth/v2/tickets message_url: https://open.welink.huaweicloud.com/api/welinkim/v1/im-service/chat/group-chat # if you send to multi group at a time, separate group_ids with commas, e.g. \u0026#34;123xx\u0026#34;,\u0026#34;456xx\u0026#34; group_ids: \u0026#34;dummy_group_id\u0026#34; # make a name you like for the robot, it will display in group robot_name: robot Update the settings dynamically Since 6.5.0, the alarm settings can be updated dynamically at runtime by Dynamic Configuration, which will override the settings in alarm-settings.yml.\nIn order to determine whether an alarm rule is triggered or not, SkyWalking needs to cache the metrics of a time window for each alarm rule. If any attribute (metrics-name, op, threshold, period, count, etc.) of a rule is changed, the sliding window will be destroyed and re-created, causing the alarm of this specific rule to restart again.\n","excerpt":"Alarm Alarm core is driven by a collection of rules, which are defined in config/alarm-settings.yml. …","ref":"/docs/main/latest/en/setup/backend/backend-alarm/","title":"Alarm"},{"body":"Apache SkyWalking committer SkyWalking Project Management Committee (PMC) is responsible for assessing the contributions of candidates.\nLike many Apache projects, SkyWalking welcome all contributions, including code contributions, blog entries, guides for new users, public speeches, and enhancement of the project in various ways.\nCommitter Nominate new committer In SkyWalking, new committer nomination could only be officially started by existing PMC members. If a new committer feels that he/she is qualified, he/she should contact any existing PMC member and discuss. If this is agreed among some members of the PMC, the process will kick off.\nThe following steps are recommended (to be initiated only by an existing PMC member):\n Send an email titled [DISCUSS] Promote xxx as new committer to private@skywalking.a.o. List the important contributions of the candidate, so you could gather support from other PMC members for your proposal. Keep the discussion open for more than 3 days but no more than 1 week, unless there is any express objection or concern. If the PMC generally agrees to the proposal, send an email titled [VOTE] Promote xxx as new committer to private@skywalking.a.o. Keep the voting process open for more than 3 days, but no more than 1 week. Consider the result as Consensus Approval if there are three +1 votes and +1 votes \u0026gt; -1 votes. Send an email titled [RESULT][VOTE] Promote xxx as new committer to private@skywalking.a.o, and list the voting details, including who the voters are.  Invite new committer The PMC member who starts the promotion is responsible for sending an invitation to the new committer and guiding him/her to set up the ASF env.\nThe PMC member should send an email using the following template to the new committer:\nTo: JoeBloggs@foo.net Cc: private@skywalking.apache.org Subject: Invitation to become SkyWalking committer: Joe Bloggs Hello [invitee name], The SkyWalking Project Management Committee] (PMC) hereby offers you committer privileges to the project. These privileges are offered on the understanding that you'll use them reasonably and with common sense. We like to work on trust rather than unnecessary constraints. Being a committer enables you to more easily make changes without needing to go through the patch submission process. Being a committer does not require you to participate any more than you already do. It does tend to make one even more committed. You will probably find that you spend more time here. Of course, you can decline and instead remain as a contributor, participating as you do now. A. This personal invitation is a chance for you to accept or decline in private. Either way, please let us know in reply to the [private@skywalking.apache.org] address only. B. If you accept, the next step is to register an iCLA: 1. Details of the iCLA and the forms are found through this link: http://www.apache.org/licenses/#clas 2. Instructions for its completion and return to the Secretary of the ASF are found at http://www.apache.org/licenses/#submitting 3. When you transmit the completed iCLA, request to notify the Apache SkyWalking and choose a unique Apache id. Look to see if your preferred id is already taken at http://people.apache.org/committer-index.html This will allow the Secretary to notify the PMC when your iCLA has been recorded. When recording of your iCLA is noticed, you will receive a follow-up message with the next steps for establishing you as a committer. Invitation acceptance process The new committer should reply to private@skywalking.apache.org (choose reply all), and express his/her intention to accept the invitation. Then, this invitation will be treated as accepted by the project\u0026rsquo;s PMC. Of course, the new committer may also choose to decline the invitation.\nOnce the invitation has been accepted, the new committer has to take the following steps:\n Subscribe to dev@skywalking.apache.org. Usually this is already done. Choose a Apache ID that is not on the apache committers list page. Download the ICLA (If the new committer contributes to the project as a day job, CCLA is expected). After filling in the icla.pdf (or ccla.pdf) with the correct information, print, sign it by hand, scan it as an PDF, and send it as an attachment to secretary@apache.org. (If electronic signature is preferred, please follow the steps on this page) The PMC will wait for the Apache secretary to confirm the ICLA (or CCLA) filed. The new committer and PMC will receive the following email:  Dear XXX, This message acknowledges receipt of your ICLA, which has been filed in the Apache Software Foundation records. Your account has been requested for you and you should receive email with next steps within the next few days (can take up to a week). Please refer to https://www.apache.org/foundation/how-it-works.html#developers for more information about roles at Apache. In the unlikely event that the account has not yet been requested, the PMC member should contact the project V.P.. The V.P. could request through the Apache Account Submission Helper Form.\nAfter several days, the new committer will receive an email confirming creation of the account, titled Welcome to the Apache Software Foundation (ASF)!. Congratulations! The new committer now has an official Apache ID.\nThe PMC member should add the new committer to the official committer list through roster.\nSet up the Apache ID and dev env  Go to Apache Account Utility Platform, create your password, set up your personal mailbox (Forwarding email address) and GitHub account(Your GitHub Username). An organizational invite will be sent to you via email shortly thereafter (within 2 hours). If you would like to use the xxx@apache.org email service, please refer to here. Gmail is recommended, because this forwarding mode is not easy to find in most mailbox service settings. Follow the authorized GitHub 2FA wiki to enable two-factor authorization (2FA) on Github. When you set 2FA to \u0026ldquo;off\u0026rdquo;, it will be delisted by the corresponding Apache committer write permission group until you set it up again. (NOTE: Treat your recovery codes with the same level of attention as you would your password!) Use GitBox Account Linking Utility to obtain write permission of the SkyWalking project. Follow this doc to update the website.  If you would like to show up publicly in the Apache GitHub org, you need to go to the Apache GitHub org people page, search for yourself, and choose Organization visibility to Public.\nCommitter rights, duties, and responsibilities The SkyWalking project doesn\u0026rsquo;t require continuing contributions from you after you have become a committer, but we truly hope that you will continue to play a part in our community!\nAs a committer, you could\n Review and merge the pull request to the master branch in the Apache repo. A pull request often contains multiple commits. Those commits must be squashed and merged into a single commit with explanatory comments. It is recommended for new committers to request recheck of the pull request from senior committers. Create and push codes to the new branch in the Apache repo. Follow the release process to prepare a new release. Remember to confirm with the committer team that it is the right time to create the release.  The PMC hopes that the new committer will take part in the release process as well as release voting, even though their vote will be regarded as +1 no binding. Being familiar with the release process is key to being promoted to the role of PMC member.\nProject Management Committee The Project Management Committee (PMC) member does not have any special rights in code contributions. They simply oversee the project and make sure that it follows the Apache requirements. Its functions include:\n Binding voting for releases and license checks; New committer and PMC member recognition; Identification of branding issues and brand protection; and Responding to questions raised by the ASF board, and taking necessary actions.  The V.P. and chair of the PMC is the secretary, who is responsible for initializing the board report.\nIn most cases, a new PMC member is nominated from the committer team. But it is also possible to become a PMC member directly, so long as the PMC agrees to the nomination and is confident that the candidate is ready. For instance, this can be demonstrated by the fact that he/she has been an Apache member, an Apache officer, or a PMC member of another project.\nThe new PMC voting process should also follow the [DISCUSS], [VOTE] and [RESULT][VOTE] procedures using a private mail list, just like the voting process for new committers. Before sending the invitation, the PMC must also send a NOTICE mail to the Apache board.\nTo: board@apache.org Cc: private@skywalking.apache.org Subject: [NOTICE] Jane Doe for SkyWalking PMC SkyWalking proposes to invite Jane Doe (janedoe) to join the PMC. (include if a vote was held) The vote result is available here: https://lists.apache.org/... After 72 hours, if the board doesn\u0026rsquo;t object to the nomination (which it won\u0026rsquo;t most cases), an invitation may then be sent to the candidate.\nOnce the invitation is accepted, a PMC member should add the new member to the official PMC list through roster.\n","excerpt":"Apache SkyWalking committer SkyWalking Project Management Committee (PMC) is responsible for …","ref":"/docs/main/latest/en/guides/asf/committer/","title":"Apache SkyWalking committer"},{"body":"Apdex threshold Apdex is a measure of response time based against a set threshold. It measures the ratio of satisfactory response times to unsatisfactory response times. The response time is measured from an asset request to completed delivery back to the requestor.\nA user defines a response time threshold T. All responses handled in T or less time satisfy the user.\nFor example, if T is 1.2 seconds and a response completes in 0.5 seconds, then the user is satisfied. All responses greater than 1.2 seconds dissatisfy the user. Responses greater than 4.8 seconds frustrate the user.\nThe apdex threshold T can be configured in service-apdex-threshold.yml file or via Dynamic Configuration. The default item will apply to a service that isn\u0026rsquo;t defined in this configuration as the default threshold.\nConfiguration Format The configuration content includes the names and thresholds of the services:\n# default threshold is 500ms default: 500 # example: # the threshold of service \u0026#34;tomcat\u0026#34; is 1s # tomcat: 1000 # the threshold of service \u0026#34;springboot1\u0026#34; is 50ms # springboot1: 50 ","excerpt":"Apdex threshold Apdex is a measure of response time based against a set threshold. It measures the …","ref":"/docs/main/latest/en/setup/backend/apdex-threshold/","title":"Apdex threshold"},{"body":"Backend setup SkyWalking\u0026rsquo;s backend distribution package consists of the following parts:\n  bin/cmd scripts: Located in the /bin folder. Includes startup linux shell and Windows cmd scripts for the backend server and UI startup.\n  Backend config: Located in the /config folder. Includes settings files of the backend, which are:\n application.yml log4j.xml alarm-settings.yml    Libraries of backend: Located in the /oap-libs folder. All dependencies of the backend can be found in it.\n  Webapp env: Located in the webapp folder. UI frontend jar file can be found here, together with its webapp.yml setting file.\n  Requirements and default settings Requirement: JDK8 to JDK12 are tested. Other versions are not tested and may or may not work.\nBefore you start, you should know that the main purpose of quickstart is to help you obtain a basic configuration for previews/demo. Performance and long-term running are not our goals.\nFor production/QA/tests environments, see Backend and UI deployment documents.\nYou can use bin/startup.sh (or cmd) to start up the backend and UI with their default settings, set out as follows:\n Backend storage uses H2 by default (for an easier start) Backend listens on 0.0.0.0/11800 for gRPC APIs and 0.0.0.0/12800 for HTTP REST APIs.  In Java, DotNetCore, Node.js, and Istio agents/probes, you should set the gRPC service address to ip/host:11800, and ip/host should be where your backend is.\n UI listens on 8080 port and request 127.0.0.1/12800 to run a GraphQL query.  Interaction Before deploying Skywalking in your distributed environment, you should learn about how agents/probes, the backend, and the UI communicate with each other:\n All native agents and probes, either language based or mesh probe, use the gRPC service (core/default/gRPC* in application.yml) to report data to the backend. Also, the Jetty service is supported in JSON format. UI uses GraphQL (HTTP) query to access the backend also in Jetty service (core/default/rest* in application.yml).  Startup script The default startup scripts are /bin/oapService.sh(.bat). Read the start up mode document to learn about other ways to start up the backend.\napplication.yml SkyWalking backend startup behaviours are driven by config/application.yml. Understanding the setting file will help you read this document. The core concept behind this setting file is that the SkyWalking collector is based on pure modular design. End users can switch or assemble the collector features according to their own requirements.\nIn application.yml, there are three levels.\n Level 1: Module name. This means that this module is active in running mode. Level 2: Provider option list and provider selector. Available providers are listed here with a selector to indicate which one will actually take effect. If there is only one provider listed, the selector is optional and can be omitted. Level 3. Settings of the provider.  Example:\nstorage: selector: mysql # the mysql storage will actually be activated, while the h2 storage takes no effect h2: driver: ${SW_STORAGE_H2_DRIVER:org.h2.jdbcx.JdbcDataSource} url: ${SW_STORAGE_H2_URL:jdbc:h2:mem:skywalking-oap-db} user: ${SW_STORAGE_H2_USER:sa} metadataQueryMaxSize: ${SW_STORAGE_H2_QUERY_MAX_SIZE:5000} mysql: properties: jdbcUrl: ${SW_JDBC_URL:\u0026#34;jdbc:mysql://localhost:3306/swtest\u0026#34;} dataSource.user: ${SW_DATA_SOURCE_USER:root} dataSource.password: ${SW_DATA_SOURCE_PASSWORD:root@1234} dataSource.cachePrepStmts: ${SW_DATA_SOURCE_CACHE_PREP_STMTS:true} dataSource.prepStmtCacheSize: ${SW_DATA_SOURCE_PREP_STMT_CACHE_SQL_SIZE:250} dataSource.prepStmtCacheSqlLimit: ${SW_DATA_SOURCE_PREP_STMT_CACHE_SQL_LIMIT:2048} dataSource.useServerPrepStmts: ${SW_DATA_SOURCE_USE_SERVER_PREP_STMTS:true} metadataQueryMaxSize: ${SW_STORAGE_MYSQL_QUERY_MAX_SIZE:5000} # other configurations  storage is the module. selector selects one out of all providers listed below. The unselected ones take no effect as if they were deleted. default is the default implementor of the core module. driver, url, \u0026hellip; metadataQueryMaxSize are all setting items of the implementor.  At the same time, there are two types of modules: required and optional. The required modules provide the skeleton of the backend. Even though their modular design supports pluggability, removing those modules does not serve any purpose. For optional modules, some of them have a provider implementation called none, meaning that it only provides a shell with no actual logic, typically such as telemetry. Setting - to the selector means that this whole module will be excluded at runtime. We advise against trying to change the APIs of those modules, unless you understand the SkyWalking project and its codes very well.\nThe required modules are listed here:\n Core. Provides the basic and major skeleton of all data analysis and stream dispatch. Cluster. Manages multiple backend instances in a cluster, which could provide high throughputs process capabilities. Storage. Makes the analysis result persistent. Query. Provides query interfaces to UI.  Cluster and Storage have provided multiple implementors (providers). See Cluster management and Choose storage documents in the link list.\nSeveral receiver modules are also provided. Receiver is the module in charge of accepting incoming data requests to the backend. They usually provide services by some network (RPC) protocols, such as gRPC and HTTPRestful.\nThe receivers have many different module names. You could read the set receivers document in the link list.\nConfiguration Vocabulary All available configurations in application.yml could be found in Configuration Vocabulary.\nAdvanced feature document link list After understanding the setting file structure, you may learn more about the advanced features. You may read the advanced feature documents in the following order.\n Overriding settings in application.yml are supported. IP and port setting. Introduces how IP and port are set and used. Backend init mode startup. How to initialize the environment and exit graciously. Read this before you try to initialize a new cluster. Cluster management. Guides you on how to set the backend server in cluster mode. Deploy in kubernetes. Guides you on how to build and use the SkyWalking image, and deploy in k8s. Choose storage. As we know, in default quick start, the backend is running with H2 DB. But clearly, it doesn\u0026rsquo;t fit the product env. Here you may find out about the other options available to you. We also welcome anyone to contribute a new storage implementor. Set receivers. You may choose receivers according to your requirements. Most receivers are harmless, including our default receivers. You may set and activate all receivers provided. Open fetchers. You may open different fetchers to read metrics from target applications. These ones work like receivers, except that they are in pull mode. A typical example is Prometheus. Token authentication. You may add token authentication mechanisms to prevent OAP from receiving untrusted data. Run trace sampling at the backend. This sample keeps the metrics accurate, although some of the traces in storage are not saved based on rate. Follow slow DB statement threshold config document to learn about how to detect the Slow database statements (including SQL statements) in your system. Official OAL scripts. As you have seen from our OAL introduction, most backend analysis capabilities are based on scripts. Here is a detailed description of the official scripts, which helps you understand which metrics data are in process, and which could be used in alarm. Alarm. Alarm provides a time-series based check mechanism. You may set alarm rules targeting the analysis oal metrics objects. Advanced deployment options. If you want to deploy backend in very large scale and support high payload, you may need this. Metrics exporter. Use metrics data exporter to forward metrics data to 3rd party systems. Time To Live (TTL). Since metrics and trace are time series data, TTL settings affect their expiration time. Dynamic Configuration. Configure the OAP to dynamic from remote service or 3rd party configuration management systems. Uninstrumented Gateways. Configure gateways/proxies that are not supported by SkyWalking agent plugins to reflect the delegation in topology graph. Apdex threshold. Configure the thresholds for different services if Apdex calculation is activated in the OAL. Service Grouping. An automatic grouping mechanism for all services based on name. Group Parameterized Endpoints. Configure the grouping rules for parameterized endpoints to improve the meaning of the metrics. OpenTelemetry Metrics Analysis. Activate built-in configurations to convert the metrics forwarded from OpenTelemetry collector, and learn how to write your own conversion rules. Meter Analysis. Set up the backend analysis rules when using SkyWalking Meter System Toolkit or meter plugins. Spring Sleuth Metrics Analysis. Configure the agent and backend to receiver metrics from micrometer. Log Analyzer  Telemetry for backend The OAP backend cluster itself is a distributed streaming process system. To assist the Ops team, we provide the telemetry for the OAP backend itself. Follow the document to use it.\nAt the same time, we provide Health Check to get a score for the health status.\n 0 means healthy, and more than 0 means unhealthy. less than 0 means that the OAP doesn\u0026rsquo;t start up.\n FAQs Why do we need to set the timezone? And when do we do it? SkyWalking provides downsampling time series metrics features. Query and store at each time dimension (minute, hour, day, month metrics indexes) related to timezone when time formatting.\nFor example, metrics time will be formatted like YYYYMMDDHHmm in minute dimension metrics, which is timezone related.\nBy default, SkyWalking\u0026rsquo;s OAP backend chooses the OS default timezone. If you want to override it, please follow the Java and OS documents.\nHow to query the storage directly from a 3rd party tool? SkyWalking provides different options based on browser UI, CLI and GraphQL to support extensions. But some users may want to query data directly from the storage. For example, in the case of ElasticSearch, Kibana is a great tool for doing this.\nBy default, in order to reduce memory, network and storage space usages, SkyWalking saves based64-encoded ID(s) only in metrics entities. But these tools usually don\u0026rsquo;t support nested query, and are not convenient to work with. For these exceptional reasons, SkyWalking provides a config to add all necessary name column(s) into the final metrics entities with ID as a trade-off.\nTake a look at core/default/activeExtraModelColumns config in the application.yaml, and set it as true to enable this feature.\nNote that this feature is simply for 3rd party integration and doesn\u0026rsquo;t provide any new features to native SkyWalking use cases.\n","excerpt":"Backend setup SkyWalking\u0026rsquo;s backend distribution package consists of the following parts: …","ref":"/docs/main/latest/en/setup/backend/backend-setup/","title":"Backend setup"},{"body":"Backend storage SkyWalking storage is pluggable, we have provided the following storage solutions, you could easily use one of them by specifying it as the selector in the application.yml：\nstorage: selector: ${SW_STORAGE:elasticsearch7} Native supported storage\n H2 OpenSearch ElasticSearch 6, 7 MySQL TiDB InfluxDB PostgreSQL  H2 Active H2 as storage, set storage provider to H2 In-Memory Databases. Default in distribution package. Please read Database URL Overview in H2 official document, you could set the target to H2 in Embedded, Server and Mixed modes.\nSetting fragment example\nstorage: selector: ${SW_STORAGE:h2} h2: driver: org.h2.jdbcx.JdbcDataSource url: jdbc:h2:mem:skywalking-oap-db user: sa OpenSearch OpenSearch storage shares the same configurations as ElasticSearch 7. In order to activate ElasticSearch 7 as storage, set storage provider to elasticsearch7. Please download the apache-skywalking-bin-es7.tar.gz if you want to use OpenSearch as storage.\nElasticSearch NOTICE: Elastic announced through their blog that Elasticsearch will be moving over to a Server Side Public License (SSPL), which is incompatible with Apache License 2.0. This license change is effective from Elasticsearch version 7.11. So please choose the suitable ElasticSearch version according to your usage.\n In order to activate ElasticSearch 6 as storage, set storage provider to elasticsearch In order to activate ElasticSearch 7 as storage, set storage provider to elasticsearch7  Required ElasticSearch 6.3.2 or higher. HTTP RestHighLevelClient is used to connect server.\n For ElasticSearch 6.3.2 ~ 7.0.0 (excluded), please download the apache-skywalking-bin.tar.gz, For ElasticSearch 7.0.0 ~ 8.0.0 (excluded), please download the apache-skywalking-bin-es7.tar.gz.  For now, ElasticSearch 6 and ElasticSearch 7 share the same configurations, as follows:\nstorage: selector: ${SW_STORAGE:elasticsearch} elasticsearch: nameSpace: ${SW_NAMESPACE:\u0026#34;\u0026#34;} clusterNodes: ${SW_STORAGE_ES_CLUSTER_NODES:localhost:9200} protocol: ${SW_STORAGE_ES_HTTP_PROTOCOL:\u0026#34;http\u0026#34;} trustStorePath: ${SW_STORAGE_ES_SSL_JKS_PATH:\u0026#34;\u0026#34;} trustStorePass: ${SW_STORAGE_ES_SSL_JKS_PASS:\u0026#34;\u0026#34;} user: ${SW_ES_USER:\u0026#34;\u0026#34;} password: ${SW_ES_PASSWORD:\u0026#34;\u0026#34;} secretsManagementFile: ${SW_ES_SECRETS_MANAGEMENT_FILE:\u0026#34;\u0026#34;} # Secrets management file in the properties format includes the username, password, which are managed by 3rd party tool. dayStep: ${SW_STORAGE_DAY_STEP:1} # Represent the number of days in the one minute/hour/day index. indexShardsNumber: ${SW_STORAGE_ES_INDEX_SHARDS_NUMBER:1} # Shard number of new indexes indexReplicasNumber: ${SW_STORAGE_ES_INDEX_REPLICAS_NUMBER:1} # Replicas number of new indexes # Super data set has been defined in the codes, such as trace segments.The following 3 config would be improve es performance when storage super size data in es. superDatasetDayStep: ${SW_SUPERDATASET_STORAGE_DAY_STEP:-1} # Represent the number of days in the super size dataset record index, the default value is the same as dayStep when the value is less than 0 superDatasetIndexShardsFactor: ${SW_STORAGE_ES_SUPER_DATASET_INDEX_SHARDS_FACTOR:5} # This factor provides more shards for the super data set, shards number = indexShardsNumber * superDatasetIndexShardsFactor. Also, this factor effects Zipkin and Jaeger traces. superDatasetIndexReplicasNumber: ${SW_STORAGE_ES_SUPER_DATASET_INDEX_REPLICAS_NUMBER:0} # Represent the replicas number in the super size dataset record index, the default value is 0. bulkActions: ${SW_STORAGE_ES_BULK_ACTIONS:1000} # Execute the async bulk record data every ${SW_STORAGE_ES_BULK_ACTIONS} requests flushInterval: ${SW_STORAGE_ES_FLUSH_INTERVAL:10} # flush the bulk every 10 seconds whatever the number of requests concurrentRequests: ${SW_STORAGE_ES_CONCURRENT_REQUESTS:2} # the number of concurrent requests resultWindowMaxSize: ${SW_STORAGE_ES_QUERY_MAX_WINDOW_SIZE:10000} metadataQueryMaxSize: ${SW_STORAGE_ES_QUERY_MAX_SIZE:5000} segmentQueryMaxSize: ${SW_STORAGE_ES_QUERY_SEGMENT_SIZE:200} profileTaskQueryMaxSize: ${SW_STORAGE_ES_QUERY_PROFILE_TASK_SIZE:200} oapAnalyzer: ${SW_STORAGE_ES_OAP_ANALYZER:\u0026#34;{\\\u0026#34;analyzer\\\u0026#34;:{\\\u0026#34;oap_analyzer\\\u0026#34;:{\\\u0026#34;type\\\u0026#34;:\\\u0026#34;stop\\\u0026#34;}}}\u0026#34;} # the oap analyzer. oapLogAnalyzer: ${SW_STORAGE_ES_OAP_LOG_ANALYZER:\u0026#34;{\\\u0026#34;analyzer\\\u0026#34;:{\\\u0026#34;oap_log_analyzer\\\u0026#34;:{\\\u0026#34;type\\\u0026#34;:\\\u0026#34;standard\\\u0026#34;}}}\u0026#34;} # the oap log analyzer. It could be customized by the ES analyzer configuration to support more language log formats, such as Chinese log, Japanese log and etc. advanced: ${SW_STORAGE_ES_ADVANCED:\u0026#34;\u0026#34;} ElasticSearch 6 With Https SSL Encrypting communications. example:\nstorage: selector: ${SW_STORAGE:elasticsearch} elasticsearch: # nameSpace: ${SW_NAMESPACE:\u0026#34;\u0026#34;} user: ${SW_ES_USER:\u0026#34;\u0026#34;} # User needs to be set when Http Basic authentication is enabled password: ${SW_ES_PASSWORD:\u0026#34;\u0026#34;} # Password to be set when Http Basic authentication is enabled clusterNodes: ${SW_STORAGE_ES_CLUSTER_NODES:localhost:443} trustStorePath: ${SW_SW_STORAGE_ES_SSL_JKS_PATH:\u0026#34;../es_keystore.jks\u0026#34;} trustStorePass: ${SW_SW_STORAGE_ES_SSL_JKS_PASS:\u0026#34;\u0026#34;} protocol: ${SW_STORAGE_ES_HTTP_PROTOCOL:\u0026#34;https\u0026#34;} ...  File at trustStorePath is being monitored, once it is changed, the ElasticSearch client will do reconnecting. trustStorePass could be changed on the runtime through Secrets Management File Of ElasticSearch Authentication.  Daily Index Step Daily index step(storage/elasticsearch/dayStep, default 1) represents the index creation period. In this period, several days(dayStep value)' metrics are saved.\nMostly, users don\u0026rsquo;t need to change the value manually. As SkyWalking is designed to observe large scale distributed system. But in some specific cases, users want to set a long TTL value, such as more than 60 days, but their ElasticSearch cluster isn\u0026rsquo;t powerful due to the low traffic in the production environment. This value could be increased to 5(or more), if users could make sure single one index could support these days(5 in this case) metrics and traces.\nSuch as, if dayStep == 11,\n data in [2000-01-01, 2000-01-11] will be merged into the index-20000101. data in [2000-01-12, 2000-01-22] will be merged into the index-20000112.  storage/elasticsearch/superDatasetDayStep override the storage/elasticsearch/dayStep if the value is positive. This would affect the record related entities, such as the trace segment. In some cases, the size of metrics is much less than the record(trace), this would help the shards balance in the ElasticSearch cluster.\nNOTICE, TTL deletion would be affected by these. You should set an extra more dayStep in your TTL. Such as you want to TTL == 30 days and dayStep == 10, you actually need to set TTL = 40;\nSecrets Management File Of ElasticSearch Authentication The value of secretsManagementFile should point to the secrets management file absolute path. The file includes username, password and JKS password of ElasticSearch server in the properties format.\nuser=xxx password=yyy trustStorePass=zzz The major difference between using user, password, trustStorePass configs in the application.yaml file is, the Secrets Management File is being watched by the OAP server. Once it is changed manually or through 3rd party tool, such as Vault, the storage provider will use the new username, password and JKS password to establish the connection and close the old one. If the information exist in the file, the user/password will be overrided.\nAdvanced Configurations For Elasticsearch Index You can add advanced configurations in JSON format to set ElasticSearch index settings by following ElasticSearch doc\nFor example, set translog settings:\nstorage: elasticsearch: # ...... advanced: ${SW_STORAGE_ES_ADVANCED:\u0026#34;{\\\u0026#34;index.translog.durability\\\u0026#34;:\\\u0026#34;request\\\u0026#34;,\\\u0026#34;index.translog.sync_interval\\\u0026#34;:\\\u0026#34;5s\\\u0026#34;}\u0026#34;} Recommended ElasticSearch server-side configurations You could add following config to elasticsearch.yml, set the value based on your env.\n# In tracing scenario, consider to set more than this at least. thread_pool.index.queue_size: 1000 # Only suitable for ElasticSearch 6 thread_pool.write.queue_size: 1000 # Suitable for ElasticSearch 6 and 7 # When you face query error at trace page, remember to check this. index.max_result_window: 1000000 We strongly advice you to read more about these configurations from ElasticSearch official document. This effects the performance of ElasticSearch very much.\nElasticSearch 7 with Zipkin trace extension This implementation shares most of elasticsearch7, just extends to support zipkin span storage. It has all same configs.\nstorage: selector: ${SW_STORAGE:zipkin-elasticsearch7} zipkin-elasticsearch7: nameSpace: ${SW_NAMESPACE:\u0026#34;\u0026#34;} clusterNodes: ${SW_STORAGE_ES_CLUSTER_NODES:localhost:9200} protocol: ${SW_STORAGE_ES_HTTP_PROTOCOL:\u0026#34;http\u0026#34;} user: ${SW_ES_USER:\u0026#34;\u0026#34;} password: ${SW_ES_PASSWORD:\u0026#34;\u0026#34;} indexShardsNumber: ${SW_STORAGE_ES_INDEX_SHARDS_NUMBER:2} indexReplicasNumber: ${SW_STORAGE_ES_INDEX_REPLICAS_NUMBER:0} # Batch process setting, refer to https://www.elastic.co/guide/en/elasticsearch/client/java-api/5.5/java-docs-bulk-processor.html bulkActions: ${SW_STORAGE_ES_BULK_ACTIONS:2000} # Execute the bulk every 2000 requests bulkSize: ${SW_STORAGE_ES_BULK_SIZE:20} # flush the bulk every 20mb flushInterval: ${SW_STORAGE_ES_FLUSH_INTERVAL:10} # flush the bulk every 10 seconds whatever the number of requests concurrentRequests: ${SW_STORAGE_ES_CONCURRENT_REQUESTS:2} # the number of concurrent requests About Namespace When namespace is set, names of all indexes in ElasticSearch will use it as prefix.\nMySQL Active MySQL as storage, set storage provider to mysql.\nNOTICE: MySQL driver is NOT allowed in Apache official distribution and source codes. Please download MySQL driver by yourself. Copy the connection driver jar to oap-libs.\nstorage: selector: ${SW_STORAGE:mysql} mysql: properties: jdbcUrl: ${SW_JDBC_URL:\u0026#34;jdbc:mysql://localhost:3306/swtest\u0026#34;} dataSource.user: ${SW_DATA_SOURCE_USER:root} dataSource.password: ${SW_DATA_SOURCE_PASSWORD:root@1234} dataSource.cachePrepStmts: ${SW_DATA_SOURCE_CACHE_PREP_STMTS:true} dataSource.prepStmtCacheSize: ${SW_DATA_SOURCE_PREP_STMT_CACHE_SQL_SIZE:250} dataSource.prepStmtCacheSqlLimit: ${SW_DATA_SOURCE_PREP_STMT_CACHE_SQL_LIMIT:2048} dataSource.useServerPrepStmts: ${SW_DATA_SOURCE_USE_SERVER_PREP_STMTS:true} metadataQueryMaxSize: ${SW_STORAGE_MYSQL_QUERY_MAX_SIZE:5000} All connection related settings including link url, username and password are in application.yml. Here are some of the settings, please follow HikariCP connection pool document for all the settings.\nTiDB Tested TiDB Server 4.0.8 version and Mysql Client driver 8.0.13 version currently. Active TiDB as storage, set storage provider to tidb.\nstorage: selector: ${SW_STORAGE:tidb} tidb: properties: jdbcUrl: ${SW_JDBC_URL:\u0026#34;jdbc:mysql://localhost:4000/swtest\u0026#34;} dataSource.user: ${SW_DATA_SOURCE_USER:root} dataSource.password: ${SW_DATA_SOURCE_PASSWORD:\u0026#34;\u0026#34;} dataSource.cachePrepStmts: ${SW_DATA_SOURCE_CACHE_PREP_STMTS:true} dataSource.prepStmtCacheSize: ${SW_DATA_SOURCE_PREP_STMT_CACHE_SQL_SIZE:250} dataSource.prepStmtCacheSqlLimit: ${SW_DATA_SOURCE_PREP_STMT_CACHE_SQL_LIMIT:2048} dataSource.useServerPrepStmts: ${SW_DATA_SOURCE_USE_SERVER_PREP_STMTS:true} dataSource.useAffectedRows: ${SW_DATA_SOURCE_USE_AFFECTED_ROWS:true} metadataQueryMaxSize: ${SW_STORAGE_MYSQL_QUERY_MAX_SIZE:5000} maxSizeOfArrayColumn: ${SW_STORAGE_MAX_SIZE_OF_ARRAY_COLUMN:20} numOfSearchableValuesPerTag: ${SW_STORAGE_NUM_OF_SEARCHABLE_VALUES_PER_TAG:2} All connection related settings including link url, username and password are in application.yml. These settings can refer to the configuration of MySQL above.\nInfluxDB InfluxDB storage provides a time-series database as a new storage option.\nstorage: selector: ${SW_STORAGE:influxdb} influxdb: url: ${SW_STORAGE_INFLUXDB_URL:http://localhost:8086} user: ${SW_STORAGE_INFLUXDB_USER:root} password: ${SW_STORAGE_INFLUXDB_PASSWORD:} database: ${SW_STORAGE_INFLUXDB_DATABASE:skywalking} actions: ${SW_STORAGE_INFLUXDB_ACTIONS:1000} # the number of actions to collect duration: ${SW_STORAGE_INFLUXDB_DURATION:1000} # the time to wait at most (milliseconds) fetchTaskLogMaxSize: ${SW_STORAGE_INFLUXDB_FETCH_TASK_LOG_MAX_SIZE:5000} # the max number of fetch task log in a request All connection related settings including link url, username and password are in application.yml. The Metadata storage provider settings can refer to the configuration of H2/MySQL above.\nPostgreSQL PostgreSQL jdbc driver uses version 42.2.18, it supports PostgreSQL 8.2 or newer. Active PostgreSQL as storage, set storage provider to postgresql.\nstorage: selector: ${SW_STORAGE:postgresql} postgresql: properties: jdbcUrl: ${SW_JDBC_URL:\u0026#34;jdbc:postgresql://localhost:5432/skywalking\u0026#34;} dataSource.user: ${SW_DATA_SOURCE_USER:postgres} dataSource.password: ${SW_DATA_SOURCE_PASSWORD:123456} dataSource.cachePrepStmts: ${SW_DATA_SOURCE_CACHE_PREP_STMTS:true} dataSource.prepStmtCacheSize: ${SW_DATA_SOURCE_PREP_STMT_CACHE_SQL_SIZE:250} dataSource.prepStmtCacheSqlLimit: ${SW_DATA_SOURCE_PREP_STMT_CACHE_SQL_LIMIT:2048} dataSource.useServerPrepStmts: ${SW_DATA_SOURCE_USE_SERVER_PREP_STMTS:true} metadataQueryMaxSize: ${SW_STORAGE_MYSQL_QUERY_MAX_SIZE:5000} maxSizeOfArrayColumn: ${SW_STORAGE_MAX_SIZE_OF_ARRAY_COLUMN:20} numOfSearchableValuesPerTag: ${SW_STORAGE_NUM_OF_SEARCHABLE_VALUES_PER_TAG:2} All connection related settings including link url, username and password are in application.yml. Here are some of the settings, please follow HikariCP connection pool document for all the settings.\nMore storage solution extension Follow Storage extension development guide in Project Extensions document in development guide.\n","excerpt":"Backend storage SkyWalking storage is pluggable, we have provided the following storage solutions, …","ref":"/docs/main/latest/en/setup/backend/backend-storage/","title":"Backend storage"},{"body":"Browser Monitoring Apache SkyWalking Client JS is client-side JavaScript exception and tracing library.\n Provide metrics and error collection to SkyWalking backend. Lightweight, no browser plugin, just a simple JavaScript library. Make browser as a start of whole distributed tracing.  Go to the Client JS official doc to learn more.\nNote, make sure the receiver-browser has been opened, default is ON since 8.2.0.\n","excerpt":"Browser Monitoring Apache SkyWalking Client JS is client-side JavaScript exception and tracing …","ref":"/docs/main/latest/en/setup/service-agent/browser-agent/","title":"Browser Monitoring"},{"body":"Browser Protocol Browser protocol describes the data format between skywalking-client-js and the backend.\nOverview Browser protocol is defined and provided in gRPC format, and also implemented in HTTP 1.1\nSend performance data and error logs You can send performance data and error logs using the following services:\n BrowserPerfService#collectPerfData for performance data format. BrowserPerfService#collectErrorLogs for error log format.  For error log format, note that:\n BrowserErrorLog#uniqueId should be unique in all distributed environments.  ","excerpt":"Browser Protocol Browser protocol describes the data format between skywalking-client-js and the …","ref":"/docs/main/latest/en/protocols/browser-protocol/","title":"Browser Protocol"},{"body":"CDS - Configuration Discovery Service CDS - Configuration Discovery Service provides the dynamic configuration for the agent, defined in gRPC.\nConfiguration Format The configuration content includes the service name and their configs. The\nconfigurations: //service name serviceA: // Configurations of service A // Key and Value are determined by the agent side. // Check the agent setup doc for all available configurations. key1: value1 key2: value2 ... serviceB: ... Available key(s) and value(s) in Java Agent. Java agent supports the following dynamic configurations.\n   Config Key Value Description Value Format Example Required Plugin(s)     agent.sample_n_per_3_secs The number of sampled traces per 3 seconds -1 -   agent.ignore_suffix If the operation name of the first span is included in this set, this segment should be ignored. Multiple values should be separated by , .txt,.log -   agent.trace.ignore_path The value is the path that you need to ignore, multiple paths should be separated by , more details /your/path/1/**,/your/path/2/** apm-trace-ignore-plugin   agent.span_limit_per_segment The max number of spans per segment. 300 -     Required plugin(s), the configuration affects only when the required plugins activated.  ","excerpt":"CDS - Configuration Discovery Service CDS - Configuration Discovery Service provides the dynamic …","ref":"/docs/main/latest/en/setup/service-agent/java-agent/configuration-discovery/","title":"CDS - Configuration Discovery Service"},{"body":"Choosing a receiver Receiver is a defined concept in SkyWalking\u0026rsquo;s backend. All modules which are responsible for receiving telemetry or tracing data from other systems being monitored are all called receivers. If you are looking for the pull mode, take a look at the fetcher document.\nWe have the following receivers, and default implementors are provided in our Apache distribution.\n receiver-trace. gRPC and HTTPRestful services that accept SkyWalking format traces. receiver-register. gRPC and HTTPRestful services that provide service, service instance and endpoint register. service-mesh. gRPC services that accept data from inbound mesh probes. receiver-jvm. gRPC services that accept JVM metrics data. envoy-metric. Envoy metrics_service and ALS(access log service) are supported by this receiver. The OAL script supports all GAUGE type metrics. receiver-profile. gRPC services that accept profile task status and snapshot reporter. receiver-otel. See details. A receiver for analyzing metrics data from OpenTelemetry. receiver-meter. See details. A receiver for analyzing metrics in SkyWalking native meter format. receiver-browser. gRPC services that accept browser performance data and error log. receiver-log. A receiver for native log format. See Log Analyzer for advanced features. configuration-discovery. gRPC services that handle configurationDiscovery. receiver-event. gRPC services that handle events data. receiver-zabbix. See details. Experimental receivers.  receiver_zipkin. See details.    The sample settings of these receivers are by default included in application.yml, and also listed here:\nreceiver-register: selector: ${SW_RECEIVER_REGISTER:default} default: receiver-trace: selector: ${SW_RECEIVER_TRACE:default} default: receiver-jvm: selector: ${SW_RECEIVER_JVM:default} default: service-mesh: selector: ${SW_SERVICE_MESH:default} default: envoy-metric: selector: ${SW_ENVOY_METRIC:default} default: acceptMetricsService: ${SW_ENVOY_METRIC_SERVICE:true} alsHTTPAnalysis: ${SW_ENVOY_METRIC_ALS_HTTP_ANALYSIS:\u0026#34;\u0026#34;} receiver_zipkin: selector: ${SW_RECEIVER_ZIPKIN:-} default: host: ${SW_RECEIVER_ZIPKIN_HOST:0.0.0.0} port: ${SW_RECEIVER_ZIPKIN_PORT:9411} contextPath: ${SW_RECEIVER_ZIPKIN_CONTEXT_PATH:/} jettyMinThreads: ${SW_RECEIVER_ZIPKIN_JETTY_MIN_THREADS:1} jettyMaxThreads: ${SW_RECEIVER_ZIPKIN_JETTY_MAX_THREADS:200} jettyIdleTimeOut: ${SW_RECEIVER_ZIPKIN_JETTY_IDLE_TIMEOUT:30000} jettyAcceptorPriorityDelta: ${SW_RECEIVER_ZIPKIN_JETTY_DELTA:0} jettyAcceptQueueSize: ${SW_RECEIVER_ZIPKIN_QUEUE_SIZE:0} receiver-profile: selector: ${SW_RECEIVER_PROFILE:default} default: receiver-browser: selector: ${SW_RECEIVER_BROWSER:default} default: sampleRate: ${SW_RECEIVER_BROWSER_SAMPLE_RATE:10000} log-analyzer: selector: ${SW_LOG_ANALYZER:default} default: lalFiles: ${SW_LOG_LAL_FILES:default} malFiles: ${SW_LOG_MAL_FILES:\u0026#34;\u0026#34;} configuration-discovery: selector: ${SW_CONFIGURATION_DISCOVERY:default} default: receiver-event: selector: ${SW_RECEIVER_EVENT:default} default: gRPC/HTTP server for receiver By default, all gRPC/HTTP services should be served at core/gRPC and core/rest. But the receiver-sharing-server module allows all receivers to be served at different ip:port, if you set them explicitly.\nreceiver-sharing-server: selector: ${SW_RECEIVER_SHARING_SERVER:default} default: host: ${SW_RECEIVER_JETTY_HOST:0.0.0.0} contextPath: ${SW_RECEIVER_JETTY_CONTEXT_PATH:/} authentication: ${SW_AUTHENTICATION:\u0026#34;\u0026#34;} jettyMinThreads: ${SW_RECEIVER_SHARING_JETTY_MIN_THREADS:1} jettyMaxThreads: ${SW_RECEIVER_SHARING_JETTY_MAX_THREADS:200} jettyIdleTimeOut: ${SW_RECEIVER_SHARING_JETTY_IDLE_TIMEOUT:30000} jettyAcceptorPriorityDelta: ${SW_RECEIVER_SHARING_JETTY_DELTA:0} jettyAcceptQueueSize: ${SW_RECEIVER_SHARING_JETTY_QUEUE_SIZE:0} Note: If you add these settings, make sure that they are not the same as the core module. This is because gRPC/HTTP servers of the core are still used for UI and OAP internal communications.\nOpenTelemetry receiver The OpenTelemetry receiver supports ingesting agent metrics by meter-system. The OAP can load the configuration at bootstrap. If the new configuration is not well-formed, the OAP may fail to start up. The files are located at $CLASSPATH/otel-\u0026lt;handler\u0026gt;-rules. E.g. The oc handler loads rules from $CLASSPATH/otel-oc-rules.\nSupported handlers: * oc: OpenCensus gRPC service handler.\nThe rule file should be in YAML format, defined by the scheme described in prometheus-fetcher. Note: receiver-otel only supports the group, defaultMetricLevel, and metricsRules nodes of the scheme due to its push mode.\nTo activate the oc handler and relevant rules of istio:\nreceiver-otel: selector: ${SW_OTEL_RECEIVER:default} default: enabledHandlers: ${SW_OTEL_RECEIVER_ENABLED_HANDLERS:\u0026#34;oc\u0026#34;} enabledOcRules: ${SW_OTEL_RECEIVER_ENABLED_OC_RULES:\u0026#34;istio-controlplane\u0026#34;} The receiver adds labels with key = node_identifier_host_name and key = node_identifier_pid to the collected data samples, and values from Node.identifier.host_name and Node.identifier.pid defined in OpenCensus Agent Proto, for identification of the metric data.\n   Rule Name Description Configuration File Data Source     istio-controlplane Metrics of Istio control panel otel-oc-rules/istio-controlplane.yaml Istio Control Panel -\u0026gt; OpenTelemetry Collector \u0026ndash;OC format\u0026ndash;\u0026gt; SkyWalking OAP Server   oap Metrics of SkyWalking OAP server itself otel-oc-rules/oap.yaml SkyWalking OAP Server(SelfObservability) -\u0026gt; OpenTelemetry Collector \u0026ndash;OC format\u0026ndash;\u0026gt; SkyWalking OAP Server   vm Metrics of VMs otel-oc-rules/vm.yaml Prometheus node-exporter(VMs) -\u0026gt; OpenTelemetry Collector \u0026ndash;OC format\u0026ndash;\u0026gt; SkyWalking OAP Server   k8s-cluster Metrics of K8s cluster otel-oc-rules/k8s-cluster.yaml K8s kube-state-metrics -\u0026gt; OpenTelemetry Collector \u0026ndash;OC format\u0026ndash;\u0026gt; SkyWalking OAP Server   k8s-node Metrics of K8s cluster otel-oc-rules/k8s-node.yaml cAdvisor \u0026amp; K8s kube-state-metrics -\u0026gt; OpenTelemetry Collector \u0026ndash;OC format\u0026ndash;\u0026gt; SkyWalking OAP Server   k8s-service Metrics of K8s cluster otel-oc-rules/k8s-service.yaml cAdvisor \u0026amp; K8s kube-state-metrics -\u0026gt; OpenTelemetry Collector \u0026ndash;OC format\u0026ndash;\u0026gt; SkyWalking OAP Server    Meter receiver The meter receiver supports accepting the metrics into the meter-system. The OAP can load the configuration at bootstrap.\nThe file is written in YAML format, defined by the scheme described in backend-meter.\nTo activate the default implementation:\nreceiver-meter: selector: ${SW_RECEIVER_METER:default} default: Zipkin receiver The Zipkin receiver makes the OAP server work as an alternative Zipkin server implementation. It supports Zipkin v1/v2 formats through HTTP service. Make sure you use this with SW_STORAGE=zipkin-elasticsearch7 option to activate Zipkin storage implementation. Once this receiver and storage are activated, SkyWalking\u0026rsquo;s native traces would be ignored, and SkyWalking wouldn\u0026rsquo;t analyze topology, metrics, and endpoint dependency from Zipkin\u0026rsquo;s trace.\nUse the following config to activate it.\nreceiver_zipkin: selector: ${SW_RECEIVER_ZIPKIN:-} default: host: ${SW_RECEIVER_ZIPKIN_HOST:0.0.0.0} port: ${SW_RECEIVER_ZIPKIN_PORT:9411} contextPath: ${SW_RECEIVER_ZIPKIN_CONTEXT_PATH:/} jettyMinThreads: ${SW_RECEIVER_ZIPKIN_JETTY_MIN_THREADS:1} jettyMaxThreads: ${SW_RECEIVER_ZIPKIN_JETTY_MAX_THREADS:200} jettyIdleTimeOut: ${SW_RECEIVER_ZIPKIN_JETTY_IDLE_TIMEOUT:30000} jettyAcceptorPriorityDelta: ${SW_RECEIVER_ZIPKIN_JETTY_DELTA:0} jettyAcceptQueueSize: ${SW_RECEIVER_ZIPKIN_QUEUE_SIZE:0} NOTE: Zipkin receiver is only provided in apache-skywalking-apm-es7-x.y.z.tar.gz tar. This requires zipkin-elasticsearch7 storage implementation to be activated. Read this doc to learn about Zipkin as a storage option.\n","excerpt":"Choosing a receiver Receiver is a defined concept in SkyWalking\u0026rsquo;s backend. All modules which …","ref":"/docs/main/latest/en/setup/backend/backend-receivers/","title":"Choosing a receiver"},{"body":"Cluster Management In many product environments, the backend needs to support high throughput and provide HA to maintain robustness, so you always need cluster management in product env.\nThere are various ways to manage the cluster in the backend. Choose the one that best suits your needs.\n Zookeeper coordinator. Use Zookeeper to let the backend instances detect and communicate with each other. Kubernetes. When the backend clusters are deployed inside Kubernetes, you could make use of this method by using k8s native APIs to manage clusters. Consul. Use Consul as the backend cluster management implementor and coordinate backend instances. Etcd. Use Etcd to coordinate backend instances. Nacos. Use Nacos to coordinate backend instances. In the application.yml file, there are default configurations for the aforementioned coordinators under the section cluster. You can specify any of them in the selector property to enable it.  Zookeeper coordinator Zookeeper is a very common and widely used cluster coordinator. Set the cluster/selector to zookeeper in the yml to enable it.\nRequired Zookeeper version: 3.4+\ncluster: selector: ${SW_CLUSTER:zookeeper} # other configurations  hostPort is the list of zookeeper servers. Format is IP1:PORT1,IP2:PORT2,...,IPn:PORTn enableACL enable Zookeeper ACL to control access to its znode. schema is Zookeeper ACL schemas. expression is a expression of ACL. The format of the expression is specific to the schema. hostPort, baseSleepTimeMs and maxRetries are settings of Zookeeper curator client.  Note:\n If Zookeeper ACL is enabled and /skywalking exists, you must make sure that SkyWalking has CREATE, READ and WRITE permissions. If /skywalking does not exist, it will be created by SkyWalking and all permissions to the specified user will be granted. Simultaneously, znode grants the READ permission to anyone. If you set schema as digest, the password of the expression is set in clear text.  In some cases, the OAP default gRPC host and port in core are not suitable for internal communication among the OAP nodes. The following settings are provided to set the host and port manually, based on your own LAN env.\n internalComHost: The registered host and other OAP nodes use this to communicate with the current node. internalComPort: the registered port and other OAP nodes use this to communicate with the current node.  zookeeper: nameSpace: ${SW_NAMESPACE:\u0026#34;\u0026#34;} hostPort: ${SW_CLUSTER_ZK_HOST_PORT:localhost:2181} #Retry Policy baseSleepTimeMs: ${SW_CLUSTER_ZK_SLEEP_TIME:1000} # initial amount of time to wait between retries maxRetries: ${SW_CLUSTER_ZK_MAX_RETRIES:3} # max number of times to retry internalComHost: 172.10.4.10 internalComPort: 11800 # Enable ACL enableACL: ${SW_ZK_ENABLE_ACL:false} # disable ACL in default schema: ${SW_ZK_SCHEMA:digest} # only support digest schema expression: ${SW_ZK_EXPRESSION:skywalking:skywalking} Kubernetes The require backend clusters are deployed inside Kubernetes. See the guides in Deploy in kubernetes. Set the selector to kubernetes.\ncluster: selector: ${SW_CLUSTER:kubernetes} # other configurations Consul Recently, the Consul system has become more and more popular, and many companies and developers now use Consul as their service discovery solution. Set the cluster/selector to consul in the yml to enable it.\ncluster: selector: ${SW_CLUSTER:consul} # other configurations Same as the Zookeeper coordinator, in some cases, the OAP default gRPC host and port in core are not suitable for internal communication among the OAP nodes. The following settings are provided to set the host and port manually, based on your own LAN env.\n internalComHost: The registed host and other OAP nodes use this to communicate with the current node. internalComPort: The registered port and other OAP nodes use this to communicate with the current node.  Etcd Set the cluster/selector to etcd in the yml to enable it.\ncluster: selector: ${SW_CLUSTER:etcd} # other configurations Same as the Zookeeper coordinator, in some cases, the OAP default gRPC host and port in core are not suitable for internal communication among the oap nodes. The following settings are provided to set the host and port manually, based on your own LAN env.\n internalComHost: The registered host and other OAP nodes use this to communicate with the current node. internalComPort: The registered port and other OAP nodes use this to communicate with the current node.  Nacos Set the cluster/selector to nacos in the yml to enable it.\ncluster: selector: ${SW_CLUSTER:nacos} # other configurations Nacos supports authentication by username or accessKey. Empty means that there is no need for authentication. Extra config is as follows:\nnacos: username: password: accessKey: secretKey: Same as the Zookeeper coordinator, in some cases, the OAP default gRPC host and port in core are not suitable for internal communication among the OAP nodes. The following settings are provided to set the host and port manually, based on your own LAN env.\n internalComHost: The registered host and other OAP nodes use this to communicate with the current node. internalComPort: The registered port and other OAP nodes use this to communicate with the current node.  ","excerpt":"Cluster Management In many product environments, the backend needs to support high throughput and …","ref":"/docs/main/latest/en/setup/backend/backend-cluster/","title":"Cluster Management"},{"body":"Compatibility with other Java agent bytecode processes Problem   When using the SkyWalking agent, some other agents, such as Arthas, can\u0026rsquo;t work properly. https://github.com/apache/skywalking/pull/4858\n  The retransform classes in the Java agent conflict with the SkyWalking agent, as illustrated in this demo\n  Cause The SkyWalking agent uses ByteBuddy to transform classes when the Java application starts. ByteBuddy generates auxiliary classes with different random names every time.\nWhen another Java agent retransforms the same class, it triggers the SkyWalking agent to enhance the class again. Since the bytecode has been regenerated by ByteBuddy, the fields and imported class names have been modified, and the JVM verifications on class bytecode have failed, the retransform classes would therefore be unsuccessful.\nResolution 1. Enable the class cache feature\nAdd JVM parameters:\n-Dskywalking.agent.is_cache_enhanced_class=true -Dskywalking.agent.class_cache_mode=MEMORY\nOr uncomment the following options in agent.conf:\n# If true, the SkyWalking agent will cache all instrumented classes files to memory or disk files (as determined by the class cache mode), # Allow other Java agents to enhance those classes that are enhanced by the SkyWalking agent. agent.is_cache_enhanced_class = ${SW_AGENT_CACHE_CLASS:false} # The instrumented classes cache mode: MEMORY or FILE # MEMORY: cache class bytes to memory; if there are too many instrumented classes or if their sizes are too large, it may take up more memory # FILE: cache class bytes to user temp folder starts with 'class-cache', and automatically clean up cached class files when the application exits agent.class_cache_mode = ${SW_AGENT_CLASS_CACHE_MODE:MEMORY} If the class cache feature is enabled, save the instrumented class bytecode to memory or a temporary file. When other Java agents retransform the same class, the SkyWalking agent first attempts to load from the cache.\nIf the cached class is found, it will be used directly without regenerating an auxiliary class with a new random name. Then, the process of the subsequent Java agent will not be affected.\n2. Class cache save mode\nWe recommend saving cache classes to memory, if it takes up more memory space. Alternatively, you can use the local file system. Set the class cache mode in one of the folliwng ways:\n-Dskywalking.agent.class_cache_mode=MEMORY : save cache classes to Java memory. -Dskywalking.agent.class_cache_mode=FILE : save cache classes to SkyWalking agent path \u0026lsquo;/class-cache\u0026rsquo;.\nOr modify these options in agent.conf:\nagent.class_cache_mode = ${SW_AGENT_CLASS_CACHE_MODE:MEMORY} agent.class_cache_mode = ${SW_AGENT_CLASS_CACHE_MODE:FILE}\n","excerpt":"Compatibility with other Java agent bytecode processes Problem   When using the SkyWalking agent, …","ref":"/docs/main/latest/en/faq/compatible-with-other-javaagent-bytecode-processing/","title":"Compatibility with other Java agent bytecode processes"},{"body":"Compiling issues on Mac\u0026rsquo;s M1 chip Problem  When compiling according to How-to-build, The following problems may occur, causing the build to fail.  [ERROR] Failed to execute goal org.xolstice.maven.plugins:protobuf-maven-plugin:0.6.1:compile (grpc-build) on project apm-network: Unable to resolve artifact: Missing: [ERROR] ---------- [ERROR] 1) com.google.protobuf:protoc:exe:osx-aarch_64:3.12.0 [ERROR] [ERROR] Try downloading the file manually from the project website. [ERROR] [ERROR] Then, install it using the command: [ERROR] mvn install:install-file -DgroupId=com.google.protobuf -DartifactId=protoc -Dversion=3.12.0 -Dclassifier=osx-aarch_64 -Dpackaging=exe -Dfile=/path/to/file [ERROR] [ERROR] Alternatively, if you host your own repository you can deploy the file there: [ERROR] mvn deploy:deploy-file -DgroupId=com.google.protobuf -DartifactId=protoc -Dversion=3.12.0 -Dclassifier=osx-aarch_64 -Dpackaging=exe -Dfile=/path/to/file -Durl=[url] -DrepositoryId=[id] [ERROR] [ERROR] Path to dependency: [ERROR] 1) org.apache.skywalking:apm-network:jar:8.4.0-SNAPSHOT [ERROR] 2) com.google.protobuf:protoc:exe:osx-aarch_64:3.12.0 [ERROR] [ERROR] ---------- [ERROR] 1 required artifact is missing. Reason The dependent Protocol Buffers v3.14.0 does not come with an osx-aarch_64 version. You may find the osx-aarch_64 version at the Protocol Buffers Releases link here: https://github.com/protocolbuffers/protobuf/releases. Since Mac\u0026rsquo;s M1 is compatible with the osx-x86_64 version, before this version is available for downloading, you need to manually specify the osx-x86_64 version.\nResolution You may add -Dos.detected.classifier=osx-x86_64 after the original compilation parameters, such as: ./mvnw clean package -DskipTests -Dos.detected.classifier=osx-x86_64. After specifying the version, compile and run normally.\n","excerpt":"Compiling issues on Mac\u0026rsquo;s M1 chip Problem  When compiling according to How-to-build, The …","ref":"/docs/main/latest/en/faq/how-to-build-with-mac-m1/","title":"Compiling issues on Mac's M1 chip"},{"body":"Component library settings Component library settings are about your own or third-party libraries used in the monitored application.\nIn agent or SDK, regardless of whether the library name is collected as ID or String (literally, e.g. SpringMVC), the collector formats data in ID for better performance and less storage requirements.\nAlso, the collector conjectures the remote service based on the component library. For example: if the component library is MySQL Driver library, then the remote service should be MySQL Server.\nFor these two reasons, the collector requires two parts of settings in this file:\n Component library ID, names and languages. Remote server mapping based on the local library.  All component names and IDs must be defined in this file.\nComponent Library ID Define all names and IDs from component libraries which are used in the monitored application. This uses a two-way mapping strategy. The agent or SDK could use the value (ID) to represent the component name in uplink data.\n Name: the component name used in agent and UI ID: Unique ID. All IDs are reserved once they are released. Languages: Program languages may use this component. Multi languages should be separated by ,.  ID rules  Java and multi languages shared: (0, 3000) .NET Platform reserved: [3000, 4000) Node.js Platform reserved: [4000, 5000) Go reserved: [5000, 6000) Lua reserved: [6000, 7000) Python reserved: [7000, 8000) PHP reserved: [8000, 9000) C++ reserved: [9000, 10000)  Example:\nTomcat: id: 1 languages: Java HttpClient: id: 2 languages: Java,C#,Node.js Dubbo: id: 3 languages: Java H2: id: 4 languages: Java Remote server mapping The remote server will be conjectured by the local component. The mappings are based on names in the component library.\n Key: client component library name Value: server component name  Component-Server-Mappings: Jedis: Redis StackExchange.Redis: Redis Redisson: Redis Lettuce: Redis Zookeeper: Zookeeper SqlClient: SqlServer Npgsql: PostgreSQL MySqlConnector: Mysql EntityFrameworkCore.InMemory: InMemoryDatabase ","excerpt":"Component library settings Component library settings are about your own or third-party libraries …","ref":"/docs/main/latest/en/guides/component-library-settings/","title":"Component library settings"},{"body":"Configuration Vocabulary Configuration Vocabulary lists all available configurations provided by application.yml.\n   Module Provider Settings Value(s) and Explanation System Environment Variable¹ Default     core default role Option values, Mixed/Receiver/Aggregator. Receiver mode OAP open the service to the agents, analysis and aggregate the results and forward the results for distributed aggregation. Aggregator mode OAP receives data from Mixer and Receiver role OAP nodes, and do 2nd level aggregation. Mixer means being Receiver and Aggregator both. SW_CORE_ROLE Mixed   - - restHost Binding IP of restful service. Services include GraphQL query and HTTP data report SW_CORE_REST_HOST 0.0.0.0   - - restPort Binding port of restful service SW_CORE_REST_PORT 12800   - - restContextPath Web context path of restful service SW_CORE_REST_CONTEXT_PATH /   - - restMinThreads Min threads number of restful service SW_CORE_REST_JETTY_MIN_THREADS 1   - - restMaxThreads Max threads number of restful service SW_CORE_REST_JETTY_MAX_THREADS 200   - - restIdleTimeOut Connector idle timeout in milliseconds of restful service SW_CORE_REST_JETTY_IDLE_TIMEOUT 30000   - - restAcceptorPriorityDelta Thread priority delta to give to acceptor threads of restful service SW_CORE_REST_JETTY_DELTA 0   - - restAcceptQueueSize ServerSocketChannel backlog of restful service SW_CORE_REST_JETTY_QUEUE_SIZE 0   - - gRPCHost Binding IP of gRPC service. Services include gRPC data report and internal communication among OAP nodes SW_CORE_GRPC_HOST 0.0.0.0   - - gRPCPort Binding port of gRPC service SW_CORE_GRPC_PORT 11800   - - gRPCSslEnabled Activate SSL for gRPC service SW_CORE_GRPC_SSL_ENABLED false   - - gRPCSslKeyPath The file path of gRPC SSL key SW_CORE_GRPC_SSL_KEY_PATH -   - - gRPCSslCertChainPath The file path of gRPC SSL cert chain SW_CORE_GRPC_SSL_CERT_CHAIN_PATH -   - - gRPCSslTrustedCAPath The file path of gRPC trusted CA SW_CORE_GRPC_SSL_TRUSTED_CA_PATH -   - - downsampling The activated level of down sampling aggregation  Hour,Day   - - enableDataKeeperExecutor Controller of TTL scheduler. Once disabled, TTL wouldn\u0026rsquo;t work. SW_CORE_ENABLE_DATA_KEEPER_EXECUTOR true   - - dataKeeperExecutePeriod The execution period of TTL scheduler, unit is minute. Execution doesn\u0026rsquo;t mean deleting data. The storage provider could override this, such as ElasticSearch storage. SW_CORE_DATA_KEEPER_EXECUTE_PERIOD 5   - - recordDataTTL The lifecycle of record data. Record data includes traces, top n sampled records, and logs. Unit is day. Minimal value is 2. SW_CORE_RECORD_DATA_TTL 3   - - metricsDataTTL The lifecycle of metrics data, including the metadata. Unit is day. Recommend metricsDataTTL \u0026gt;= recordDataTTL. Minimal value is 2. SW_CORE_METRICS_DATA_TTL 7   - - enableDatabaseSession Cache metrics data for 1 minute to reduce database queries, and if the OAP cluster changes within that minute. SW_CORE_ENABLE_DATABASE_SESSION true   - - topNReportPeriod The execution period of top N sampler, which saves sampled data into the storage. Unit is minute SW_CORE_TOPN_REPORT_PERIOD 10   - - activeExtraModelColumns Append the names of entity, such as service name, into the metrics storage entities. SW_CORE_ACTIVE_EXTRA_MODEL_COLUMNS false   - - serviceNameMaxLength Max length limitation of service name. SW_SERVICE_NAME_MAX_LENGTH 70   - - instanceNameMaxLength Max length limitation of service instance name. The max length of service + instance names should be less than 200. SW_INSTANCE_NAME_MAX_LENGTH 70   - - endpointNameMaxLength Max length limitation of endpoint name. The max length of service + endpoint names should be less than 240. SW_ENDPOINT_NAME_MAX_LENGTH 150   - - searchableTracesTags Define the set of span tag keys, which should be searchable through the GraphQL. Multiple values should be separated through the comma. SW_SEARCHABLE_TAG_KEYS http.method,status_code,db.type,db.instance,mq.queue,mq.topic,mq.broker   - - searchableLogsTags Define the set of log tag keys, which should be searchable through the GraphQL. Multiple values should be separated through the comma. SW_SEARCHABLE_LOGS_TAG_KEYS level   - - searchableAlarmTags Define the set of alarm tag keys, which should be searchable through the GraphQL. Multiple values should be separated through the comma. SW_SEARCHABLE_ALARM_TAG_KEYS level   - - gRPCThreadPoolSize Pool size of gRPC server SW_CORE_GRPC_THREAD_POOL_SIZE CPU core * 4   - - gRPCThreadPoolQueueSize The queue size of gRPC server SW_CORE_GRPC_POOL_QUEUE_SIZE 10000   - - maxConcurrentCallsPerConnection The maximum number of concurrent calls permitted for each incoming connection. Defaults to no limit. SW_CORE_GRPC_MAX_CONCURRENT_CALL -   - - maxMessageSize Sets the maximum message size allowed to be received on the server. Empty means 4 MiB SW_CORE_GRPC_MAX_MESSAGE_SIZE 4M(based on Netty)   - - remoteTimeout Timeout for cluster internal communication, in seconds. - 20   - - maxSizeOfNetworkAddressAlias Max size of network address detected in the be monitored system. - 1_000_000   - - maxPageSizeOfQueryProfileSnapshot The max size in every OAP query for snapshot analysis - 500   - - maxSizeOfAnalyzeProfileSnapshot The max number of snapshots analyzed by OAP - 12000   - - syncThreads The number of threads used to synchronously refresh the metrics data to the storage. SW_CORE_SYNC_THREADS 2   - - maxSyncOperationNum The maximum number of processes supported for each synchronous storage operation. When the number of the flush data is greater than this value, it will be assigned to multiple cores for execution. SW_CORE_MAX_SYNC_OPERATION_NUM 50000   cluster standalone - standalone is not suitable for one node running, no available configuration. - -   - zookeeper nameSpace The namespace, represented by root path, isolates the configurations in the zookeeper. SW_NAMESPACE /, root path   - - hostPort hosts and ports of Zookeeper Cluster SW_CLUSTER_ZK_HOST_PORT localhost:2181   - - baseSleepTimeMs The period of Zookeeper client between two retries. Unit is ms. SW_CLUSTER_ZK_SLEEP_TIME 1000   - - maxRetries The max retry time of re-trying. SW_CLUSTER_ZK_MAX_RETRIES 3   - - enableACL Open ACL by using schema and expression SW_ZK_ENABLE_ACL false   - - schema schema for the authorization SW_ZK_SCHEMA digest   - - expression expression for the authorization SW_ZK_EXPRESSION skywalking:skywalking   - - internalComHost The hostname registered in the Zookeeper for the internal communication of OAP cluster. - -   - - internalComPort The port registered in the Zookeeper for the internal communication of OAP cluster. - -1   - kubernetes namespace Namespace SkyWalking deployed in the k8s SW_CLUSTER_K8S_NAMESPACE default   - - labelSelector Labels used for filtering the OAP deployment in the k8s SW_CLUSTER_K8S_LABEL app=collector,release=skywalking   - - uidEnvName Environment variable name for reading uid. SW_CLUSTER_K8S_UID SKYWALKING_COLLECTOR_UID   - consul serviceName Service name used for SkyWalking cluster. SW_SERVICE_NAME SkyWalking_OAP_Cluster   - - hostPort hosts and ports used of Consul cluster. SW_CLUSTER_CONSUL_HOST_PORT localhost:8500   - - aclToken ALC Token of Consul. Empty string means without ALC token. SW_CLUSTER_CONSUL_ACLTOKEN -   - - internalComHost The hostname registered in the Consul for the internal communication of OAP cluster. - -   - - internalComPort The port registered in the Consul for the internal communication of OAP cluster. - -1   - etcd serviceName Service name used for SkyWalking cluster. SW_SERVICE_NAME SkyWalking_OAP_Cluster   - - hostPort hosts and ports used of etcd cluster. SW_CLUSTER_ETCD_HOST_PORT localhost:2379   - - isSSL Open SSL for the connection between SkyWalking and etcd cluster. - -   - - internalComHost The hostname registered in the etcd for the internal communication of OAP cluster. - -   - - internalComPort The port registered in the etcd for the internal communication of OAP cluster. - -1   - Nacos serviceName Service name used for SkyWalking cluster. SW_SERVICE_NAME SkyWalking_OAP_Cluster   - - hostPort hosts and ports used of Nacos cluster. SW_CLUSTER_NACOS_HOST_PORT localhost:8848   - - namespace Namespace used by SkyWalking node coordination. SW_CLUSTER_NACOS_NAMESPACE public   - - internalComHost The hostname registered in the Nacos for the internal communication of OAP cluster. - -   - - internalComPort The port registered in the Nacos for the internal communication of OAP cluster. - -1   - - username Nacos Auth username SW_CLUSTER_NACOS_USERNAME -   - - password Nacos Auth password SW_CLUSTER_NACOS_PASSWORD -   - - accessKey Nacos Auth accessKey SW_CLUSTER_NACOS_ACCESSKEY -   - - secretKey Nacos Auth secretKey SW_CLUSTER_NACOS_SECRETKEY -   storage elasticsearch - ElasticSearch 6 storage implementation - -   - - nameSpace Prefix of indexes created and used by SkyWalking. SW_NAMESPACE -   - - clusterNodes ElasticSearch cluster nodes for client connection. SW_STORAGE_ES_CLUSTER_NODES localhost   - - protocol HTTP or HTTPs. SW_STORAGE_ES_HTTP_PROTOCOL HTTP   - - user User name of ElasticSearch cluster SW_ES_USER -   - - password Password of ElasticSearch cluster SW_ES_PASSWORD -   - - trustStorePath Trust JKS file path. Only work when user name and password opened SW_STORAGE_ES_SSL_JKS_PATH -   - - trustStorePass Trust JKS file password. Only work when user name and password opened SW_STORAGE_ES_SSL_JKS_PASS -   - - secretsManagementFile Secrets management file in the properties format includes the username, password, which are managed by 3rd party tool. Provide the capability to update them in the runtime. SW_ES_SECRETS_MANAGEMENT_FILE -   - - dayStep Represent the number of days in the one minute/hour/day index. SW_STORAGE_DAY_STEP 1   - - indexShardsNumber Shard number of new indexes SW_STORAGE_ES_INDEX_SHARDS_NUMBER 1   - - indexReplicasNumber Replicas number of new indexes SW_STORAGE_ES_INDEX_REPLICAS_NUMBER 0   - - superDatasetDayStep Represent the number of days in the super size dataset record index, the default value is the same as dayStep when the value is less than 0. SW_SUPERDATASET_STORAGE_DAY_STEP -1   - - superDatasetIndexShardsFactor Super data set has been defined in the codes, such as trace segments. This factor provides more shards for the super data set, shards number = indexShardsNumber * superDatasetIndexShardsFactor. Also, this factor effects Zipkin and Jaeger traces. SW_STORAGE_ES_SUPER_DATASET_INDEX_SHARDS_FACTOR 5   - - superDatasetIndexReplicasNumber Represent the replicas number in the super size dataset record index. SW_STORAGE_ES_SUPER_DATASET_INDEX_REPLICAS_NUMBER 0   - - bulkActions Async bulk size of the record data batch execution. SW_STORAGE_ES_BULK_ACTIONS 1000   - - flushInterval Period of flush, no matter bulkActions reached or not. Unit is second. SW_STORAGE_ES_FLUSH_INTERVAL 10   - - concurrentRequests The number of concurrent requests allowed to be executed. SW_STORAGE_ES_CONCURRENT_REQUESTS 2   - - resultWindowMaxSize The max size of dataset when OAP loading cache, such as network alias. SW_STORAGE_ES_QUERY_MAX_WINDOW_SIZE 10000   - - metadataQueryMaxSize The max size of metadata per query. SW_STORAGE_ES_QUERY_MAX_SIZE 5000   - - segmentQueryMaxSize The max size of trace segments per query. SW_STORAGE_ES_QUERY_SEGMENT_SIZE 200   - - profileTaskQueryMaxSize The max size of profile task per query. SW_STORAGE_ES_QUERY_PROFILE_TASK_SIZE 200   - - advanced All settings of ElasticSearch index creation. The value should be in JSON format SW_STORAGE_ES_ADVANCED -   - elasticsearch7 - ElasticSearch 7 storage implementation - -   - - nameSpace Prefix of indexes created and used by SkyWalking. SW_NAMESPACE -   - - clusterNodes ElasticSearch cluster nodes for client connection. SW_STORAGE_ES_CLUSTER_NODES localhost   - - protocol HTTP or HTTPs. SW_STORAGE_ES_HTTP_PROTOCOL HTTP   - - user User name of ElasticSearch cluster SW_ES_USER -   - - password Password of ElasticSearch cluster SW_ES_PASSWORD -   - - trustStorePath Trust JKS file path. Only work when user name and password opened SW_STORAGE_ES_SSL_JKS_PATH -   - - trustStorePass Trust JKS file password. Only work when user name and password opened SW_STORAGE_ES_SSL_JKS_PASS -   - - secretsManagementFile Secrets management file in the properties format includes the username, password, which are managed by 3rd party tool. Provide the capability to update them in the runtime. SW_ES_SECRETS_MANAGEMENT_FILE -   - - dayStep Represent the number of days in the one minute/hour/day index. SW_STORAGE_DAY_STEP 1   - - indexShardsNumber Shard number of new indexes SW_STORAGE_ES_INDEX_SHARDS_NUMBER 1   - - indexReplicasNumber Replicas number of new indexes SW_STORAGE_ES_INDEX_REPLICAS_NUMBER 0   - - superDatasetDayStep Represent the number of days in the super size dataset record index, the default value is the same as dayStep when the value is less than 0. SW_SUPERDATASET_STORAGE_DAY_STEP -1   - - superDatasetIndexShardsFactor Super data set has been defined in the codes, such as trace segments. This factor provides more shards for the super data set, shards number = indexShardsNumber * superDatasetIndexShardsFactor. Also, this factor effects Zipkin and Jaeger traces. SW_STORAGE_ES_SUPER_DATASET_INDEX_SHARDS_FACTOR 5   - - superDatasetIndexReplicasNumber Represent the replicas number in the super size dataset record index. SW_STORAGE_ES_SUPER_DATASET_INDEX_REPLICAS_NUMBER 0   - - bulkActions Async bulk size of the record data batch execution. SW_STORAGE_ES_BULK_ACTIONS 1000   - - syncBulkActions Sync bulk size of the metrics data batch execution. SW_STORAGE_ES_SYNC_BULK_ACTIONS 50000   - - flushInterval Period of flush, no matter bulkActions reached or not. Unit is second. SW_STORAGE_ES_FLUSH_INTERVAL 10   - - concurrentRequests The number of concurrent requests allowed to be executed. SW_STORAGE_ES_CONCURRENT_REQUESTS 2   - - resultWindowMaxSize The max size of dataset when OAP loading cache, such as network alias. SW_STORAGE_ES_QUERY_MAX_WINDOW_SIZE 10000   - - metadataQueryMaxSize The max size of metadata per query. SW_STORAGE_ES_QUERY_MAX_SIZE 5000   - - segmentQueryMaxSize The max size of trace segments per query. SW_STORAGE_ES_QUERY_SEGMENT_SIZE 200   - - profileTaskQueryMaxSize The max size of profile task per query. SW_STORAGE_ES_QUERY_PROFILE_TASK_SIZE 200   - - advanced All settings of ElasticSearch index creation. The value should be in JSON format SW_STORAGE_ES_ADVANCED -   - h2 - H2 storage is designed for demonstration and running in short term(1-2 hours) only - -   - - driver H2 JDBC driver. SW_STORAGE_H2_DRIVER org.h2.jdbcx.JdbcDataSource   - - url H2 connection URL. Default is H2 memory mode SW_STORAGE_H2_URL jdbc:h2:mem:skywalking-oap-db   - - user User name of H2 database. SW_STORAGE_H2_USER sa   - - password Password of H2 database. - -   - - metadataQueryMaxSize The max size of metadata per query. SW_STORAGE_H2_QUERY_MAX_SIZE 5000   - - maxSizeOfArrayColumn Some entities, such as trace segment, include the logic column with multiple values. In the H2, we use multiple physical columns to host the values, such as, Change column_a with values [1,2,3,4,5] to column_a_0 = 1, column_a_1 = 2, column_a_2 = 3 , column_a_3 = 4, column_a_4 = 5 SW_STORAGE_MAX_SIZE_OF_ARRAY_COLUMN 20   - - numOfSearchableValuesPerTag In a trace segment, it includes multiple spans with multiple tags. Different spans could have same tag keys, such as multiple HTTP exit spans all have their own http.method tag. This configuration set the limitation of max num of values for the same tag key. SW_STORAGE_NUM_OF_SEARCHABLE_VALUES_PER_TAG 2   - mysql - MySQL Storage. The MySQL JDBC Driver is not in the dist, please copy it into oap-lib folder manually - -   - - properties Hikari connection pool configurations - Listed in the application.yaml.   - - metadataQueryMaxSize The max size of metadata per query. SW_STORAGE_MYSQL_QUERY_MAX_SIZE 5000   - - maxSizeOfArrayColumn Some entities, such as trace segment, include the logic column with multiple values. In the MySQL, we use multiple physical columns to host the values, such as, Change column_a with values [1,2,3,4,5] to column_a_0 = 1, column_a_1 = 2, column_a_2 = 3 , column_a_3 = 4, column_a_4 = 5 SW_STORAGE_MAX_SIZE_OF_ARRAY_COLUMN 20   - - numOfSearchableValuesPerTag In a trace segment, it includes multiple spans with multiple tags. Different spans could have same tag keys, such as multiple HTTP exit spans all have their own http.method tag. This configuration set the limitation of max num of values for the same tag key. SW_STORAGE_NUM_OF_SEARCHABLE_VALUES_PER_TAG 2   - postgresql - PostgreSQL storage. - -   - - properties Hikari connection pool configurations - Listed in the application.yaml.   - - metadataQueryMaxSize The max size of metadata per query. SW_STORAGE_MYSQL_QUERY_MAX_SIZE 5000   - - maxSizeOfArrayColumn Some entities, such as trace segment, include the logic column with multiple values. In the PostgreSQL, we use multiple physical columns to host the values, such as, Change column_a with values [1,2,3,4,5] to column_a_0 = 1, column_a_1 = 2, column_a_2 = 3 , column_a_3 = 4, column_a_4 = 5 SW_STORAGE_MAX_SIZE_OF_ARRAY_COLUMN 20   - - numOfSearchableValuesPerTag In a trace segment, it includes multiple spans with multiple tags. Different spans could have same tag keys, such as multiple HTTP exit spans all have their own http.method tag. This configuration set the limitation of max num of values for the same tag key. SW_STORAGE_NUM_OF_SEARCHABLE_VALUES_PER_TAG 2   - influxdb - InfluxDB storage. - -   - - url InfluxDB connection URL. SW_STORAGE_INFLUXDB_URL http://localhost:8086   - - user User name of InfluxDB. SW_STORAGE_INFLUXDB_USER root   - - password Password of InfluxDB. SW_STORAGE_INFLUXDB_PASSWORD -   - - database Database of InfluxDB. SW_STORAGE_INFLUXDB_DATABASE skywalking   - - actions The number of actions to collect. SW_STORAGE_INFLUXDB_ACTIONS 1000   - - duration The time to wait at most (milliseconds). SW_STORAGE_INFLUXDB_DURATION 1000   - - batchEnabled If true, write points with batch api. SW_STORAGE_INFLUXDB_BATCH_ENABLED true   - - fetchTaskLogMaxSize The max number of fetch task log in a request. SW_STORAGE_INFLUXDB_FETCH_TASK_LOG_MAX_SIZE 5000   - - connectionResponseFormat The response format of connection to influxDB, cannot be anything but MSGPACK or JSON. SW_STORAGE_INFLUXDB_CONNECTION_RESPONSE_FORMAT MSGPACK   agent-analyzer default Agent Analyzer. SW_AGENT_ANALYZER default    - - sampleRate Sampling rate for receiving trace. The precision is 1/10000. 10000 means 100% sample in default. SW_TRACE_SAMPLE_RATE 10000   - - slowDBAccessThreshold The slow database access thresholds. Unit ms. SW_SLOW_DB_THRESHOLD default:200,mongodb:100   - - forceSampleErrorSegment When sampling mechanism activated, this config would make the error status segment sampled, ignoring the sampling rate. SW_FORCE_SAMPLE_ERROR_SEGMENT true   - - segmentStatusAnalysisStrategy Determine the final segment status from the status of spans. Available values are FROM_SPAN_STATUS , FROM_ENTRY_SPAN and FROM_FIRST_SPAN. FROM_SPAN_STATUS represents the segment status would be error if any span is in error status. FROM_ENTRY_SPAN means the segment status would be determined by the status of entry spans only. FROM_FIRST_SPAN means the segment status would be determined by the status of the first span only. SW_SEGMENT_STATUS_ANALYSIS_STRATEGY FROM_SPAN_STATUS   - - noUpstreamRealAddressAgents Exit spans with the component in the list would not generate the client-side instance relation metrics. As some tracing plugins can\u0026rsquo;t collect the real peer ip address, such as Nginx-LUA and Envoy. SW_NO_UPSTREAM_REAL_ADDRESS 6000,9000   - - slowTraceSegmentThreshold Setting this threshold about the latency would make the slow trace segments sampled if they cost more time, even the sampling mechanism activated. The default value is -1, which means would not sample slow traces. Unit, millisecond. SW_SLOW_TRACE_SEGMENT_THRESHOLD -1   - - meterAnalyzerActiveFiles Which files could be meter analyzed, files split by \u0026ldquo;,\u0026rdquo; SW_METER_ANALYZER_ACTIVE_FILES    receiver-sharing-server default Sharing server provides new gRPC and restful servers for data collection. Ana make the servers in the core module working for internal communication only. - -    - - restHost Binding IP of restful service. Services include GraphQL query and HTTP data report SW_RECEIVER_SHARING_REST_HOST -   - - restPort Binding port of restful service SW_RECEIVER_SHARING_REST_PORT -   - - restContextPath Web context path of restful service SW_RECEIVER_SHARING_REST_CONTEXT_PATH -   - - restMinThreads Min threads number of restful service SW_RECEIVER_SHARING_JETTY_MIN_THREADS 1   - - restMaxThreads Max threads number of restful service SW_RECEIVER_SHARING_JETTY_MAX_THREADS 200   - - restIdleTimeOut Connector idle timeout in milliseconds of restful service SW_RECEIVER_SHARING_JETTY_IDLE_TIMEOUT 30000   - - restAcceptorPriorityDelta Thread priority delta to give to acceptor threads of restful service SW_RECEIVER_SHARING_JETTY_DELTA 0   - - restAcceptQueueSize ServerSocketChannel backlog of restful service SW_RECEIVER_SHARING_JETTY_QUEUE_SIZE 0   - - gRPCHost Binding IP of gRPC service. Services include gRPC data report and internal communication among OAP nodes SW_RECEIVER_GRPC_HOST 0.0.0.0. Not Activated   - - gRPCPort Binding port of gRPC service SW_RECEIVER_GRPC_PORT Not Activated   - - gRPCThreadPoolSize Pool size of gRPC server SW_RECEIVER_GRPC_THREAD_POOL_SIZE CPU core * 4   - - gRPCThreadPoolQueueSize The queue size of gRPC server SW_RECEIVER_GRPC_POOL_QUEUE_SIZE 10000   - - gRPCSslEnabled Activate SSL for gRPC service SW_RECEIVER_GRPC_SSL_ENABLED false   - - gRPCSslKeyPath The file path of gRPC SSL key SW_RECEIVER_GRPC_SSL_KEY_PATH -   - - gRPCSslCertChainPath The file path of gRPC SSL cert chain SW_RECEIVER_GRPC_SSL_CERT_CHAIN_PATH -   - - maxConcurrentCallsPerConnection The maximum number of concurrent calls permitted for each incoming connection. Defaults to no limit. SW_RECEIVER_GRPC_MAX_CONCURRENT_CALL -   - - authentication The token text for the authentication. Work for gRPC connection only. Once this is set, the client is required to use the same token. SW_AUTHENTICATION -   log-analyzer default Log Analyzer. SW_LOG_ANALYZER default    - - lalFiles The LAL configuration file names (without file extension) to be activated. Read LAL for more details. SW_LOG_LAL_FILES default   - - malFiles The MAL configuration file names (without file extension) to be activated. Read LAL for more details. SW_LOG_MAL_FILES \u0026quot;\u0026quot;   event-analyzer default Event Analyzer. SW_EVENT_ANALYZER default    receiver-register default Read receiver doc for more details - -    receiver-trace default Read receiver doc for more details - -    receiver-jvm default Read receiver doc for more details - -    receiver-clr default Read receiver doc for more details - -    receiver-profile default Read receiver doc for more details - -    receiver-zabbix default Read receiver doc for more details - -    - - port Exported tcp port, Zabbix agent could connect and transport data SW_RECEIVER_ZABBIX_PORT 10051   - - host Bind to host SW_RECEIVER_ZABBIX_HOST 0.0.0.0   - - activeFiles Enable config when receive agent request SW_RECEIVER_ZABBIX_ACTIVE_FILES agent   service-mesh default Read receiver doc for more details - -    envoy-metric default Read receiver doc for more details - -    - - acceptMetricsService Open Envoy Metrics Service analysis SW_ENVOY_METRIC_SERVICE true   - - alsHTTPAnalysis Open Envoy HTTP Access Log Service analysis. Value = k8s-mesh means open the analysis SW_ENVOY_METRIC_ALS_HTTP_ANALYSIS -   - - alsTCPAnalysis Open Envoy TCP Access Log Service analysis. Value = k8s-mesh means open the analysis SW_ENVOY_METRIC_ALS_TCP_ANALYSIS -   - - k8sServiceNameRule k8sServiceNameRule allows you to customize the service name in ALS via Kubernetes metadata, the available variables are pod, service, e.g., you can use ${service.metadata.name}-${pod.metadata.labels.version} to append the version number to the service name. Be careful, when using environment variables to pass this configuration, use single quotes('') to avoid it being evaluated by the shell. -    receiver-otel default Read receiver doc for more details - -    - - enabledHandlers Enabled handlers for otel SW_OTEL_RECEIVER_ENABLED_HANDLERS -   - - enabledOcRules Enabled metric rules for OC handler SW_OTEL_RECEIVER_ENABLED_OC_RULES -   receiver_zipkin default Read receiver doc - -    - - restHost Binding IP of restful service. SW_RECEIVER_ZIPKIN_HOST 0.0.0.0   - - restPort Binding port of restful service SW_RECEIVER_ZIPKIN_PORT 9411   - - restContextPath Web context path of restful service SW_RECEIVER_ZIPKIN_CONTEXT_PATH /   receiver_jaeger default Read receiver doc - -    - - gRPCHost Binding IP of gRPC service. Services include gRPC data report and internal communication among OAP nodes SW_RECEIVER_JAEGER_HOST -   - - gRPCPort Binding port of gRPC service SW_RECEIVER_JAEGER_PORT -   - - gRPCThreadPoolSize Pool size of gRPC server - CPU core * 4   - - gRPCThreadPoolQueueSize The queue size of gRPC server - 10000   - - maxConcurrentCallsPerConnection The maximum number of concurrent calls permitted for each incoming connection. Defaults to no limit. - -   - - maxMessageSize Sets the maximum message size allowed to be received on the server. Empty means 4 MiB - 4M(based on Netty)   prometheus-fetcher default Read fetcher doc for more details - -    - - enabledRules Enable rules. SW_PROMETHEUS_FETCHER_ENABLED_RULES self   - - maxConvertWorker The maximize meter convert worker. SW_PROMETHEUS_FETCHER_NUM_CONVERT_WORKER -1(by default, half the number of CPU core(s))   kafka-fetcher default Read fetcher doc for more details - -    - - bootstrapServers A list of host/port pairs to use for establishing the initial connection to the Kafka cluster. SW_KAFKA_FETCHER_SERVERS localhost:9092   - - namespace namespace aims to isolate multi OAP cluster when using the same Kafka cluster.if you set a namespace for Kafka fetcher, OAP will add a prefix to topic name. you should also set namespace in agent.config, the property named SW_NAMESPACE -   - - groupId A unique string that identifies the consumer group this consumer belongs to. - skywalking-consumer   - - consumePartitions Which PartitionId(s) of the topics assign to the OAP server. If more than one, is separated by commas. SW_KAFKA_FETCHER_CONSUME_PARTITIONS -   - - isSharding it was true when OAP Server in cluster. SW_KAFKA_FETCHER_IS_SHARDING false   - - createTopicIfNotExist If true, create the Kafka topic when it does not exist. - true   - - partitions The number of partitions for the topic being created. SW_KAFKA_FETCHER_PARTITIONS 3   - - enableMeterSystem To enable to fetch and handle Meter System data. SW_KAFKA_FETCHER_ENABLE_METER_SYSTEM false   - - enableNativeProtoLog To enable to fetch and handle native proto log data. SW_KAFKA_FETCHER_ENABLE_NATIVE_PROTO_LOG false   - - enableNativeJsonLog To enable to fetch and handle native json log data. SW_KAFKA_FETCHER_ENABLE_NATIVE_JSON_LOG false   - - replicationFactor The replication factor for each partition in the topic being created. SW_KAFKA_FETCHER_PARTITIONS_FACTOR 2   - - kafkaHandlerThreadPoolSize Pool size of kafka message handler executor. SW_KAFKA_HANDLER_THREAD_POOL_SIZE CPU core * 2   - - kafkaHandlerThreadPoolQueueSize The queue size of kafka message handler executor. SW_KAFKA_HANDLER_THREAD_POOL_QUEUE_SIZE 10000   - - topicNameOfMeters Specifying Kafka topic name for Meter system data. - skywalking-meters   - - topicNameOfMetrics Specifying Kafka topic name for JVM Metrics data. - skywalking-metrics   - - topicNameOfProfiling Specifying Kafka topic name for Profiling data. - skywalking-profilings   - - topicNameOfTracingSegments Specifying Kafka topic name for Tracing data. - skywalking-segments   - - topicNameOfManagements Specifying Kafka topic name for service instance reporting and registering. - skywalking-managements   - - topicNameOfLogs Specifying Kafka topic name for native proto log data. - skywalking-logs   - - topicNameOfJsonLogs Specifying Kafka topic name for native json log data. - skywalking-logs-json   receiver-browser default Read receiver doc for more details - - -   - - sampleRate Sampling rate for receiving trace. The precision is 1/10000. 10000 means 100% sample in default. SW_RECEIVER_BROWSER_SAMPLE_RATE 10000   query graphql - GraphQL query implementation -    - - path Root path of GraphQL query and mutation. SW_QUERY_GRAPHQL_PATH /graphql   alarm default - Read alarm doc for more details. -    telemetry - - Read telemetry doc for more details. -    - none - No op implementation -    - prometheus host Binding host for Prometheus server fetching data SW_TELEMETRY_PROMETHEUS_HOST 0.0.0.0   - - port Binding port for Prometheus server fetching data SW_TELEMETRY_PROMETHEUS_PORT 1234   configuration - - Read dynamic configuration doc for more details. -    - grpc host DCS server binding hostname SW_DCS_SERVER_HOST -   - - port DCS server binding port SW_DCS_SERVER_PORT 80   - - clusterName Cluster name when reading latest configuration from DSC server. SW_DCS_CLUSTER_NAME SkyWalking   - - period The period of OAP reading data from DSC server. Unit is second. SW_DCS_PERIOD 20   - apollo apolloMeta apollo.meta in Apollo SW_CONFIG_APOLLO http://106.12.25.204:8080   - - apolloCluster apollo.cluster in Apollo SW_CONFIG_APOLLO_CLUSTER default   - - apolloEnv env in Apollo SW_CONFIG_APOLLO_ENV -   - - appId app.id in Apollo SW_CONFIG_APOLLO_APP_ID skywalking   - - period The period of data sync. Unit is second. SW_CONFIG_APOLLO_PERIOD 60   - zookeeper nameSpace The namespace, represented by root path, isolates the configurations in the zookeeper. SW_CONFIG_ZK_NAMESPACE /, root path   - - hostPort hosts and ports of Zookeeper Cluster SW_CONFIG_ZK_HOST_PORT localhost:2181   - - baseSleepTimeMs The period of Zookeeper client between two retries. Unit is ms. SW_CONFIG_ZK_BASE_SLEEP_TIME_MS 1000   - - maxRetries The max retry time of re-trying. SW_CONFIG_ZK_MAX_RETRIES 3   - - period The period of data sync. Unit is second. SW_CONFIG_ZK_PERIOD 60   - etcd clusterName Service name used for SkyWalking cluster. SW_CONFIG_ETCD_CLUSTER_NAME default   - - serverAddr hosts and ports used of etcd cluster. SW_CONFIG_ETCD_SERVER_ADDR localhost:2379   - - group Additional prefix of the configuration key SW_CONFIG_ETCD_GROUP skywalking   - - period The period of data sync. Unit is second. SW_CONFIG_ZK_PERIOD 60   - consul hostPort hosts and ports used of Consul cluster. SW_CONFIG_CONSUL_HOST_AND_PORTS localhost:8500   - - aclToken ALC Token of Consul. Empty string means without ALC token. SW_CONFIG_CONSUL_ACL_TOKEN -   - - period The period of data sync. Unit is second. SW_CONFIG_CONSUL_PERIOD 60   - k8s-configmap namespace Deployment namespace of the config map. SW_CLUSTER_K8S_NAMESPACE default   - - labelSelector Labels used for locating configmap. SW_CLUSTER_K8S_LABEL app=collector,release=skywalking   - - period The period of data sync. Unit is second. SW_CONFIG_ZK_PERIOD 60   - nacos serverAddr Nacos Server Host SW_CONFIG_NACOS_SERVER_ADDR 127.0.0.1   - - port Nacos Server Port SW_CONFIG_NACOS_SERVER_PORT 8848   - - group Nacos Configuration namespace SW_CONFIG_NACOS_SERVER_NAMESPACE -   - - period The period of data sync. Unit is second. SW_CONFIG_CONFIG_NACOS_PERIOD 60   - - username Nacos Auth username SW_CONFIG_NACOS_USERNAME -   - - password Nacos Auth password SW_CONFIG_NACOS_PASSWORD -   - - accessKey Nacos Auth accessKey SW_CONFIG_NACOS_ACCESSKEY -   - - secretKey Nacos Auth secretKey SW_CONFIG_NACOS_SECRETKEY -   exporter grpc targetHost The host of target grpc server for receiving export data. SW_EXPORTER_GRPC_HOST 127.0.0.1   - - targetPort The port of target grpc server for receiving export data. SW_EXPORTER_GRPC_PORT 9870   health-checker default checkIntervalSeconds The period of check OAP internal health status. Unit is second. SW_HEALTH_CHECKER_INTERVAL_SECONDS 5   configuration-discovery default disableMessageDigest If true, agent receives the latest configuration every time even without change. In default, OAP uses SHA512 message digest mechanism to detect changes of configuration. SW_DISABLE_MESSAGE_DIGEST false   receiver-event default Read receiver doc for more details - -     Notice ¹ System Environment Variable name could be declared and changed in the application.yml. The names listed here, are just provided in the default application.yml file.\n","excerpt":"Configuration Vocabulary Configuration Vocabulary lists all available configurations provided by …","ref":"/docs/main/latest/en/setup/backend/configuration-vocabulary/","title":"Configuration Vocabulary"},{"body":" Dependency the toolkit, such as using maven or gradle  \u0026lt;dependency\u0026gt; \u0026lt;groupId\u0026gt;org.apache.skywalking\u0026lt;/groupId\u0026gt; \u0026lt;artifactId\u0026gt;apm-toolkit-log4j-1.x\u0026lt;/artifactId\u0026gt; \u0026lt;version\u0026gt;{project.release.version}\u0026lt;/version\u0026gt; \u0026lt;/dependency\u0026gt; Print trace ID in your logs  Config a layout  log4j.appender.CONSOLE.layout=org.apache.skywalking.apm.toolkit.log.log4j.v1.x.TraceIdPatternLayout  set %T in layout.ConversionPattern ( In 2.0-2016, you should use %x, Why change? )  log4j.appender.CONSOLE.layout.ConversionPattern=%d [%T] %-5p %c{1}:%L - %m%n  When you use -javaagent to active the SkyWalking tracer, log4j will output traceId, if it existed. If the tracer is inactive, the output will be TID: N/A.  Print SkyWalking context in your logs   Your only need to replace pattern %T with %T{SW_CTX}.\n  When you use -javaagent to active the SkyWalking tracer, log4j will output SW_CTX: [$serviceName,$instanceName,$traceId,$traceSegmentId,$spanId], if it existed. If the tracer is inactive, the output will be SW_CTX: N/A.\n  gRPC reporter The gRPC report could forward the collected logs to SkyWalking OAP server, or SkyWalking Satellite sidecar. Trace id, segment id, and span id will attach to logs automatically. You don\u0026rsquo;t need to change the layout.\n Add GRPCLogClientAppender in log4j.properties  log4j.rootLogger=INFO,CustomAppender log4j.appender.CustomAppender=org.apache.skywalking.apm.toolkit.log.log4j.v1.x.log.GRPCLogClientAppender log4j.appender.CustomAppender.layout=org.apache.log4j.PatternLayout log4j.appender.CustomAppender.layout.ConversionPattern=[%t] %-5p %c %x - %m%n  Add config of the plugin or use default  plugin.toolkit.log.grpc.reporter.server_host=${SW_GRPC_LOG_SERVER_HOST:127.0.0.1} plugin.toolkit.log.grpc.reporter.server_port=${SW_GRPC_LOG_SERVER_PORT:11800} plugin.toolkit.log.grpc.reporter.max_message_size=${SW_GRPC_LOG_MAX_MESSAGE_SIZE:10485760} plugin.toolkit.log.grpc.reporter.upstream_timeout=${SW_GRPC_LOG_GRPC_UPSTREAM_TIMEOUT:30} ","excerpt":"Dependency the toolkit, such as using maven or gradle  \u0026lt;dependency\u0026gt; …","ref":"/docs/main/latest/en/setup/service-agent/java-agent/application-toolkit-log4j-1.x/","title":"Dependency the toolkit, such as using maven or gradle"},{"body":" Dependency the toolkit, such as using maven or gradle  \u0026lt;dependency\u0026gt; \u0026lt;groupId\u0026gt;org.apache.skywalking\u0026lt;/groupId\u0026gt; \u0026lt;artifactId\u0026gt;apm-toolkit-log4j-2.x\u0026lt;/artifactId\u0026gt; \u0026lt;version\u0026gt;{project.release.version}\u0026lt;/version\u0026gt; \u0026lt;/dependency\u0026gt; Print trace ID in your logs  Config the [%traceId] pattern in your log4j2.xml  \u0026lt;Appenders\u0026gt; \u0026lt;Console name=\u0026#34;Console\u0026#34; target=\u0026#34;SYSTEM_OUT\u0026#34;\u0026gt; \u0026lt;PatternLayout pattern=\u0026#34;%d [%traceId] %-5p %c{1}:%L - %m%n\u0026#34;/\u0026gt; \u0026lt;/Console\u0026gt; \u0026lt;/Appenders\u0026gt;  Support log4j2 AsyncRoot , No additional configuration is required. Refer to the demo of log4j2.xml below. For details: Log4j2 Async Loggers  \u0026lt;Configuration\u0026gt; \u0026lt;Appenders\u0026gt; \u0026lt;Console name=\u0026#34;Console\u0026#34; target=\u0026#34;SYSTEM_OUT\u0026#34;\u0026gt; \u0026lt;PatternLayout pattern=\u0026#34;%d [%traceId] %-5p %c{1}:%L - %m%n\u0026#34;/\u0026gt; \u0026lt;/Console\u0026gt; \u0026lt;/Appenders\u0026gt; \u0026lt;Loggers\u0026gt; \u0026lt;AsyncRoot level=\u0026#34;INFO\u0026#34;\u0026gt; \u0026lt;AppenderRef ref=\u0026#34;Console\u0026#34;/\u0026gt; \u0026lt;/AsyncRoot\u0026gt; \u0026lt;/Loggers\u0026gt; \u0026lt;/Configuration\u0026gt;   Support log4j2 AsyncAppender , No additional configuration is required. Refer to the demo of log4j2.xml below.\nFor details: All Loggers Async\nLog4j-2.9 and higher require disruptor-3.3.4.jar or higher on the classpath. Prior to Log4j-2.9, disruptor-3.0.0.jar or higher was required. This is simplest to configure and gives the best performance. To make all loggers asynchronous, add the disruptor jar to the classpath and set the system property log4j2.contextSelector to org.apache.logging.log4j.core.async.AsyncLoggerContextSelector.\n\u0026lt;Configuration status=\u0026#34;WARN\u0026#34;\u0026gt; \u0026lt;Appenders\u0026gt; \u0026lt;!-- Async Loggers will auto-flush in batches, so switch off immediateFlush. --\u0026gt; \u0026lt;RandomAccessFile name=\u0026#34;RandomAccessFile\u0026#34; fileName=\u0026#34;async.log\u0026#34; immediateFlush=\u0026#34;false\u0026#34; append=\u0026#34;false\u0026#34;\u0026gt; \u0026lt;PatternLayout\u0026gt; \u0026lt;Pattern\u0026gt;%d %p %c{1.} [%t] [%traceId] %m %ex%n\u0026lt;/Pattern\u0026gt; \u0026lt;/PatternLayout\u0026gt; \u0026lt;/RandomAccessFile\u0026gt; \u0026lt;/Appenders\u0026gt; \u0026lt;Loggers\u0026gt; \u0026lt;Root level=\u0026#34;info\u0026#34; includeLocation=\u0026#34;false\u0026#34;\u0026gt; \u0026lt;AppenderRef ref=\u0026#34;RandomAccessFile\u0026#34;/\u0026gt; \u0026lt;/Root\u0026gt; \u0026lt;/Loggers\u0026gt; \u0026lt;/Configuration\u0026gt; For details: Mixed Sync \u0026amp; Async\nLog4j-2.9 and higher require disruptor-3.3.4.jar or higher on the classpath. Prior to Log4j-2.9, disruptor-3.0.0.jar or higher was required. There is no need to set system property Log4jContextSelector to any value.\n\u0026lt;Configuration status=\u0026#34;WARN\u0026#34;\u0026gt; \u0026lt;Appenders\u0026gt; \u0026lt;!-- Async Loggers will auto-flush in batches, so switch off immediateFlush. --\u0026gt; \u0026lt;RandomAccessFile name=\u0026#34;RandomAccessFile\u0026#34; fileName=\u0026#34;asyncWithLocation.log\u0026#34; immediateFlush=\u0026#34;false\u0026#34; append=\u0026#34;false\u0026#34;\u0026gt; \u0026lt;PatternLayout\u0026gt; \u0026lt;Pattern\u0026gt;%d %p %class{1.} [%t] [%traceId] %location %m %ex%n\u0026lt;/Pattern\u0026gt; \u0026lt;/PatternLayout\u0026gt; \u0026lt;/RandomAccessFile\u0026gt; \u0026lt;/Appenders\u0026gt; \u0026lt;Loggers\u0026gt; \u0026lt;!-- pattern layout actually uses location, so we need to include it --\u0026gt; \u0026lt;AsyncLogger name=\u0026#34;com.foo.Bar\u0026#34; level=\u0026#34;trace\u0026#34; includeLocation=\u0026#34;true\u0026#34;\u0026gt; \u0026lt;AppenderRef ref=\u0026#34;RandomAccessFile\u0026#34;/\u0026gt; \u0026lt;/AsyncLogger\u0026gt; \u0026lt;Root level=\u0026#34;info\u0026#34; includeLocation=\u0026#34;true\u0026#34;\u0026gt; \u0026lt;AppenderRef ref=\u0026#34;RandomAccessFile\u0026#34;/\u0026gt; \u0026lt;/Root\u0026gt; \u0026lt;/Loggers\u0026gt; \u0026lt;/Configuration\u0026gt;   Support log4j2 AsyncAppender, For details: Log4j2 AsyncAppender\n  \u0026lt;Configuration\u0026gt; \u0026lt;Appenders\u0026gt; \u0026lt;Console name=\u0026#34;Console\u0026#34; target=\u0026#34;SYSTEM_OUT\u0026#34;\u0026gt; \u0026lt;PatternLayout pattern=\u0026#34;%d [%traceId] %-5p %c{1}:%L - %m%n\u0026#34;/\u0026gt; \u0026lt;/Console\u0026gt; \u0026lt;Async name=\u0026#34;Async\u0026#34;\u0026gt; \u0026lt;AppenderRef ref=\u0026#34;Console\u0026#34;/\u0026gt; \u0026lt;/Async\u0026gt; \u0026lt;/Appenders\u0026gt; \u0026lt;Loggers\u0026gt; \u0026lt;Root level=\u0026#34;INFO\u0026#34;\u0026gt; \u0026lt;AppenderRef ref=\u0026#34;Async\u0026#34;/\u0026gt; \u0026lt;/Root\u0026gt; \u0026lt;/Loggers\u0026gt; \u0026lt;/Configuration\u0026gt;  When you use -javaagent to active the SkyWalking tracer, log4j2 will output traceId, if it existed. If the tracer is inactive, the output will be TID: N/A.  Print SkyWalking context in your logs   Your only need to replace pattern %traceId with %sw_ctx.\n  When you use -javaagent to active the SkyWalking tracer, log4j2 will output SW_CTX: [$serviceName,$instanceName,$traceId,$traceSegmentId,$spanId], if it existed. If the tracer is inactive, the output will be SW_CTX: N/A.\n  gRPC reporter The gRPC report could forward the collected logs to SkyWalking OAP server, or SkyWalking Satellite sidecar. Trace id, segment id, and span id will attach to logs automatically. You don\u0026rsquo;t need to change the layout.\n Add GRPCLogClientAppender in log4j2.xml  \u0026lt;GRPCLogClientAppender name=\u0026#34;grpc-log\u0026#34;\u0026gt; \u0026lt;PatternLayout pattern=\u0026#34;%d{HH:mm:ss.SSS} [%t] %-5level %logger{36} - %msg%n\u0026#34;/\u0026gt; \u0026lt;/GRPCLogClientAppender\u0026gt;  Add config of the plugin or use default  plugin.toolkit.log.grpc.reporter.server_host=${SW_GRPC_LOG_SERVER_HOST:127.0.0.1} plugin.toolkit.log.grpc.reporter.server_port=${SW_GRPC_LOG_SERVER_PORT:11800} plugin.toolkit.log.grpc.reporter.max_message_size=${SW_GRPC_LOG_MAX_MESSAGE_SIZE:10485760} plugin.toolkit.log.grpc.reporter.upstream_timeout=${SW_GRPC_LOG_GRPC_UPSTREAM_TIMEOUT:30} Transmitting un-formatted messages The log4j 2.x gRPC reporter supports transmitting logs as formatted or un-formatted. Transmitting formatted data is the default but can be disabled by adding the following to the agent config:\nplugin.toolkit.log.transmit_formatted=false The above will result in the content field being used for the log pattern with additional log tags of argument.0, argument.1, and so on representing each logged argument as well as an additional exception tag which is only present if a throwable is also logged.\nFor example, the following code:\nlog.info(\u0026#34;{} {} {}\u0026#34;, 1, 2, 3); Will result in:\n{ \u0026#34;content\u0026#34;: \u0026#34;{} {} {}\u0026#34;, \u0026#34;tags\u0026#34;: [ { \u0026#34;key\u0026#34;: \u0026#34;argument.0\u0026#34;, \u0026#34;value\u0026#34;: \u0026#34;1\u0026#34; }, { \u0026#34;key\u0026#34;: \u0026#34;argument.1\u0026#34;, \u0026#34;value\u0026#34;: \u0026#34;2\u0026#34; }, { \u0026#34;key\u0026#34;: \u0026#34;argument.2\u0026#34;, \u0026#34;value\u0026#34;: \u0026#34;3\u0026#34; } ] } ","excerpt":"Dependency the toolkit, such as using maven or gradle  \u0026lt;dependency\u0026gt; …","ref":"/docs/main/latest/en/setup/service-agent/java-agent/application-toolkit-log4j-2.x/","title":"Dependency the toolkit, such as using maven or gradle"},{"body":" Dependency the toolkit, such as using maven or gradle  \u0026lt;dependency\u0026gt; \u0026lt;groupId\u0026gt;org.apache.skywalking\u0026lt;/groupId\u0026gt; \u0026lt;artifactId\u0026gt;apm-toolkit-meter\u0026lt;/artifactId\u0026gt; \u0026lt;version\u0026gt;${skywalking.version}\u0026lt;/version\u0026gt; \u0026lt;/dependency\u0026gt; If you\u0026rsquo;re using Spring sleuth, you could use Spring Sleuth Setup.\n Counter API represents a single monotonically increasing counter, automatic collect data and report to backend.  import org.apache.skywalking.apm.toolkit.meter.MeterFactory; Counter counter = MeterFactory.counter(meterName).tag(\u0026#34;tagKey\u0026#34;, \u0026#34;tagValue\u0026#34;).mode(Counter.Mode.INCREMENT).build(); counter.increment(1d);  MeterFactory.counter Create a new counter builder with the meter name. Counter.Builder.tag(String key, String value) Mark a tag key/value pair. Counter.Builder.mode(Counter.Mode mode) Change the counter mode, RATE mode means reporting rate to the backend. Counter.Builder.build() Build a new Counter which is collected and reported to the backend. Counter.increment(double count) Increment count to the Counter, It could be a positive value.   Gauge API represents a single numerical value.  import org.apache.skywalking.apm.toolkit.meter.MeterFactory; ThreadPoolExecutor threadPool = ...; Gauge gauge = MeterFactory.gauge(meterName, () -\u0026gt; threadPool.getActiveCount()).tag(\u0026#34;tagKey\u0026#34;, \u0026#34;tagValue\u0026#34;).build();  MeterFactory.gauge(String name, Supplier\u0026lt;Double\u0026gt; getter) Create a new gauge builder with the meter name and supplier function, this function need to return a double value. Gauge.Builder.tag(String key, String value) Mark a tag key/value pair. Gauge.Builder.build() Build a new Gauge which is collected and reported to the backend.   Histogram API represents a summary sample observations with customize buckets.  import org.apache.skywalking.apm.toolkit.meter.MeterFactory; Histogram histogram = MeterFactory.histogram(\u0026#34;test\u0026#34;).tag(\u0026#34;tagKey\u0026#34;, \u0026#34;tagValue\u0026#34;).steps(Arrays.asList(1, 5, 10)).minValue(0).build(); histogram.addValue(3);  MeterFactory.histogram(String name) Create a new histogram builder with the meter name. Histogram.Builder.tag(String key, String value) Mark a tag key/value pair. Histogram.Builder.steps(List\u0026lt;Double\u0026gt; steps) Set up the max values of every histogram buckets. Histogram.Builder.minValue(double value) Set up the minimal value of this histogram, default is 0. Histogram.Builder.build() Build a new Histogram which is collected and reported to the backend. Histogram.addValue(double value) Add value into the histogram, automatically analyze what bucket count needs to be increment. rule: count into [step1, step2).  ","excerpt":"Dependency the toolkit, such as using maven or gradle  \u0026lt;dependency\u0026gt; …","ref":"/docs/main/latest/en/setup/service-agent/java-agent/application-toolkit-meter/","title":"Dependency the toolkit, such as using maven or gradle"},{"body":" Dependency the toolkit, such as using maven or gradle  \u0026lt;dependency\u0026gt; \u0026lt;groupId\u0026gt;org.apache.skywalking\u0026lt;/groupId\u0026gt; \u0026lt;artifactId\u0026gt;apm-toolkit-micrometer-registry\u0026lt;/artifactId\u0026gt; \u0026lt;version\u0026gt;${skywalking.version}\u0026lt;/version\u0026gt; \u0026lt;/dependency\u0026gt;  Using org.apache.skywalking.apm.meter.micrometer.SkywalkingMeterRegistry as the registry, it could forward the MicroMeter collected metrics to OAP server.  import org.apache.skywalking.apm.meter.micrometer.SkywalkingMeterRegistry; SkywalkingMeterRegistry registry = new SkywalkingMeterRegistry(); // If you has some counter want to rate by agent side SkywalkingConfig config = new SkywalkingConfig(Arrays.asList(\u0026#34;test_rate_counter\u0026#34;)); new SkywalkingMeterRegistry(config); // Also you could using composite registry to combine multiple meter registry, such as collect to Skywalking and prometheus CompositeMeterRegistry compositeRegistry = new CompositeMeterRegistry(); compositeRegistry.add(new PrometheusMeterRegistry(PrometheusConfig.DEFAULT)); compositeRegistry.add(new SkywalkingMeterRegistry());   Using snake case as the naming convention. Such as test.meter will be send to test_meter.\n  Using Millisecond as the time unit.\n  Adapt micrometer data convention.\n     Micrometer data type Transform to meter name Skywalking data type Description     Counter Counter name Counter Same with counter   Gauges Gauges name Gauges Same with gauges   Timer Timer name + \u0026ldquo;_count\u0026rdquo; Counter Execute finished count    Timer name + \u0026ldquo;_sum\u0026rdquo; Counter Total execute finished duration    Timer name + \u0026ldquo;_max\u0026rdquo; Gauges Max duration of execute finished time    Timer name + \u0026ldquo;_histogram\u0026rdquo; Histogram Histogram of execute finished duration   LongTaskTimer Timer name + \u0026ldquo;_active_count\u0026rdquo; Gauges Executing task count    Timer name + \u0026ldquo;_duration_sum\u0026rdquo; Counter All of executing task sum duration    Timer name + \u0026ldquo;_max\u0026rdquo; Counter Current longest running task execute duration   Function Timer Timer name + \u0026ldquo;_count\u0026rdquo; Gauges Execute finished timer count    Timer name + \u0026ldquo;_sum\u0026rdquo; Gauges Execute finished timer total duration   Function Counter Counter name Counter Custom counter value   Distribution summary Summary name + \u0026ldquo;_count\u0026rdquo; Counter Total record count    Summary name + \u0026ldquo;_sum\u0026rdquo; Counter Total record amount sum    Summary name + \u0026ldquo;_max\u0026rdquo; Gauges Max record amount    Summary name + \u0026ldquo;_histogram\u0026rdquo; Gauges Histogram of the amount     Not Adapt data convention.     Micrometer data type Data type     LongTaskTimer Histogram    ","excerpt":"Dependency the toolkit, such as using maven or gradle  \u0026lt;dependency\u0026gt; …","ref":"/docs/main/latest/en/setup/service-agent/java-agent/application-toolkit-micrometer/","title":"Dependency the toolkit, such as using maven or gradle"},{"body":" Dependency the toolkit, such as using maven or gradle  \u0026lt;dependency\u0026gt; \u0026lt;groupId\u0026gt;org.apache.skywalking\u0026lt;/groupId\u0026gt; \u0026lt;artifactId\u0026gt;apm-toolkit-trace\u0026lt;/artifactId\u0026gt; \u0026lt;version\u0026gt;${skywalking.version}\u0026lt;/version\u0026gt; \u0026lt;/dependency\u0026gt;  Use TraceContext.traceId() API to obtain traceId.  import TraceContext; ... modelAndView.addObject(\u0026#34;traceId\u0026#34;, TraceContext.traceId());  Use TraceContext.segmentId() API to obtain segmentId.  import TraceContext; ... modelAndView.addObject(\u0026#34;segmentId\u0026#34;, TraceContext.segmentId());  Use TraceContext.spanId() API to obtain spanId.  import TraceContext; ... modelAndView.addObject(\u0026#34;spanId\u0026#34;, TraceContext.spanId()); Sample codes only\n  Add @Trace to any method you want to trace. After that, you can see the span in the Stack.\n  Methods annotated with @Tag will try to tag the current active span with the given key (Tag#key()) and (Tag#value()), if there is no active span at all, this annotation takes no effect. @Tag can be repeated, and can be used in companion with @Trace, see examples below. The value of Tag is the same as what are supported in Customize Enhance Trace.\n  Add custom tag in the context of traced method, ActiveSpan.tag(\u0026quot;key\u0026quot;, \u0026quot;val\u0026quot;).\n  ActiveSpan.error() Mark the current span as error status.\n  ActiveSpan.error(String errorMsg) Mark the current span as error status with a message.\n  ActiveSpan.error(Throwable throwable) Mark the current span as error status with a Throwable.\n  ActiveSpan.debug(String debugMsg) Add a debug level log message in the current span.\n  ActiveSpan.info(String infoMsg) Add an info level log message in the current span.\n  ActiveSpan.setOperationName(String operationName) Customize an operation name.\n  ActiveSpan.tag(\u0026#34;my_tag\u0026#34;, \u0026#34;my_value\u0026#34;); ActiveSpan.error(); ActiveSpan.error(\u0026#34;Test-Error-Reason\u0026#34;); ActiveSpan.error(new RuntimeException(\u0026#34;Test-Error-Throwable\u0026#34;)); ActiveSpan.info(\u0026#34;Test-Info-Msg\u0026#34;); ActiveSpan.debug(\u0026#34;Test-debug-Msg\u0026#34;); /** * The codes below will generate a span, * and two types of tags, one type tag: keys are `tag1` and `tag2`, values are the passed-in parameters, respectively, the other type tag: keys are `username` and `age`, values are the return value in User, respectively */ @Trace @Tag(key = \u0026#34;tag1\u0026#34;, value = \u0026#34;arg[0]\u0026#34;) @Tag(key = \u0026#34;tag2\u0026#34;, value = \u0026#34;arg[1]\u0026#34;) @Tag(key = \u0026#34;username\u0026#34;, value = \u0026#34;returnedObj.username\u0026#34;) @Tag(key = \u0026#34;age\u0026#34;, value = \u0026#34;returnedObj.age\u0026#34;) public User methodYouWantToTrace(String param1, String param2) { // ActiveSpan.setOperationName(\u0026#34;Customize your own operation name, if this is an entry span, this would be an endpoint name\u0026#34;);  // ... }  Use TraceContext.putCorrelation() API to put custom data in tracing context.  Optional\u0026lt;String\u0026gt; previous = TraceContext.putCorrelation(\u0026#34;customKey\u0026#34;, \u0026#34;customValue\u0026#34;); CorrelationContext will remove the item when the value is null or empty.\n Use TraceContext.getCorrelation() API to get custom data.  Optional\u0026lt;String\u0026gt; value = TraceContext.getCorrelation(\u0026#34;customKey\u0026#34;); CorrelationContext configuration descriptions could be found in the agent configuration documentation, with correlation. as the prefix.\n","excerpt":"Dependency the toolkit, such as using maven or gradle  \u0026lt;dependency\u0026gt; …","ref":"/docs/main/latest/en/setup/service-agent/java-agent/application-toolkit-trace/","title":"Dependency the toolkit, such as using maven or gradle"},{"body":" Dependency the toolkit, such as using maven or gradle  \u0026lt;dependency\u0026gt; \u0026lt;groupId\u0026gt;org.apache.skywalking\u0026lt;/groupId\u0026gt; \u0026lt;artifactId\u0026gt;apm-toolkit-opentracing\u0026lt;/artifactId\u0026gt; \u0026lt;version\u0026gt;{project.release.version}\u0026lt;/version\u0026gt; \u0026lt;/dependency\u0026gt;  Use our OpenTracing tracer implementation  Tracer tracer = new SkywalkingTracer(); Tracer.SpanBuilder spanBuilder = tracer.buildSpan(\u0026#34;/yourApplication/yourService\u0026#34;); ","excerpt":" Dependency the toolkit, such as using maven or gradle  \u0026lt;dependency\u0026gt; …","ref":"/docs/main/latest/en/setup/service-agent/java-agent/opentracing/","title":"Dependency the toolkit, such as using maven or gradle"},{"body":"Deploy SkyWalking backend and UI in kubernetes Follow instructions in the deploying SkyWalking backend to Kubernetes cluster to deploy oap and ui to a kubernetes cluster.\nPlease read the Readme file.\n","excerpt":"Deploy SkyWalking backend and UI in kubernetes Follow instructions in the deploying SkyWalking …","ref":"/docs/main/latest/en/setup/backend/backend-k8s/","title":"Deploy SkyWalking backend and UI in kubernetes"},{"body":"Design Goals This document outlines the core design goals for the SkyWalking project.\n  Maintaining Observability. Regardless of the deployment method of the target system, SkyWalking provides an integration solution for it to maintain observability. Based on this, SkyWalking provides multiple runtime forms and probes.\n  Topology, Metrics and Trace Together. The first step to understanding a distributed system is the topology map. It visualizes the entire complex system in an easy-to-read layout. Under the topology, the OSS personnel have higher requirements in terms of the metrics for service, instance, endpoint and calls. Traces are in the form of detailed logs to make sense of those metrics. For example, when the endpoint latency becomes long, you want to see the slowest the trace to find out why. So you can see, they are from big picture to details, they are all needed. SkyWalking integrates and provides a lot of features to make this possible and easy understand.\n  Light Weight. There two parts of light weight are needed. (1) In probe, we just depend on network communication framework, prefer gRPC. By that, the probe should be as small as possible, to avoid the library conflicts and the payload of VM, such as permsize requirement in JVM. (2) As an observability platform, it is secondary and third level system in your project environment. So we are using our own light weight framework to build the backend core. Then you don\u0026rsquo;t need to deploy big data tech platform and maintain them. SkyWalking should be simple in tech stack.\n  Pluggable. SkyWalking core team provides many default implementations, but definitely it is not enough, and also don\u0026rsquo;t fit every scenario. So, we provide a lot of features for being pluggable.\n  Portability. SkyWalking can run in multiple environments, including: (1) Use traditional register center like eureka. (2) Use RPC framework including service discovery, like Spring Cloud, Apache Dubbo. (3) Use Service Mesh in modern infrastructure. (4) Use cloud services. (5) Across cloud deployment. SkyWalking should run well in all of these cases.\n  Interoperability. The observability landscape is so vast that it is virtually impossible for SkyWalking to support all systems, even with the support of its community. Currently, it supports interoperability with other OSS systems, especially probes, such as Zipkin, Jaeger, OpenTracing, and OpenCensus. It is very important to end users that SkyWalking has the ability to accept and read these data formats, since the users are not required to switch their libraries.\n  What is next?  See probe Introduction to learn about SkyWalking\u0026rsquo;s probe groups. From backend overview, you can understand what the backend does after it receives probe data. If you want to customize the UI, start with the UI overview document.  ","excerpt":"Design Goals This document outlines the core design goals for the SkyWalking project.\n  Maintaining …","ref":"/docs/main/latest/en/concepts-and-designs/project-goals/","title":"Design Goals"},{"body":"Disable plugins Delete or remove the specific libraries / jars in skywalking-agent/plugins/*.jar\n+-- skywalking-agent +-- activations apm-toolkit-log4j-1.x-activation.jar apm-toolkit-log4j-2.x-activation.jar apm-toolkit-logback-1.x-activation.jar ... +-- config agent.config +-- plugins apm-dubbo-plugin.jar apm-feign-default-http-9.x.jar apm-httpClient-4.x-plugin.jar ..... skywalking-agent.jar ","excerpt":"Disable plugins Delete or remove the specific libraries / jars in skywalking-agent/plugins/*.jar\n+-- …","ref":"/docs/main/latest/en/setup/service-agent/java-agent/how-to-disable-plugin/","title":"Disable plugins"},{"body":"Docker This section introduces how to build your Java application image on top of this image.\nFROMapache/skywalking-java-agent:8.5.0-jdk8# ... build your java applicationYou can start your Java application with CMD or ENTRYPOINT, but you don\u0026rsquo;t need to care about the Java options to enable SkyWalking agent, it should be adopted automatically.\nKubernetes This section introduces how to use this image as sidecar of Kubernetes service.\nIn Kubernetes scenarios, you can also use this agent image as a sidecar.\napiVersion: v1 kind: Pod metadata: name: agent-as-sidecar spec: restartPolicy: Never volumes: - name: skywalking-agent emptyDir: { } containers: - name: agent-container image: apache/skywalking-java-agent:8.4.0-alpine volumeMounts: - name: skywalking-agent mountPath: /agent command: [ \u0026#34;/bin/sh\u0026#34; ] args: [ \u0026#34;-c\u0026#34;, \u0026#34;cp -R /skywalking/agent /agent/\u0026#34; ] - name: app-container image: springio/gs-spring-boot-docker volumeMounts: - name: skywalking-agent mountPath: /skywalking env: - name: JAVA_TOOL_OPTIONS value: \u0026#34;-javaagent:/skywalking/agent/skywalking-agent.jar\u0026#34; ","excerpt":"Docker This section introduces how to build your Java application image on top of this image. …","ref":"/docs/main/latest/en/setup/service-agent/java-agent/containerization/","title":"Docker"},{"body":"Dynamic Configuration SkyWalking Configurations mostly are set through application.yml and OS system environment variables. At the same time, some of them are supporting dynamic settings from upstream management system.\nRight now, SkyWalking supports following dynamic configurations.\n   Config Key Value Description Value Format Example     agent-analyzer.default.slowDBAccessThreshold Thresholds of slow Database statement, override receiver-trace/default/slowDBAccessThreshold of application.yml. default:200,mongodb:50   agent-analyzer.default.uninstrumentedGateways The uninstrumented gateways, override gateways.yml. same as gateways.yml   alarm.default.alarm-settings The alarm settings, will override alarm-settings.yml. same as alarm-settings.yml   core.default.apdexThreshold The apdex threshold settings, will override service-apdex-threshold.yml. same as service-apdex-threshold.yml   core.default.endpoint-name-grouping The endpoint name grouping setting, will override endpoint-name-grouping.yml. same as endpoint-name-grouping.yml   agent-analyzer.default.sampleRate Trace sampling , override receiver-trace/default/sampleRate of application.yml. 10000   agent-analyzer.default.slowTraceSegmentThreshold Setting this threshold about the latency would make the slow trace segments sampled if they cost more time, even the sampling mechanism activated. The default value is -1, which means would not sample slow traces. Unit, millisecond. override receiver-trace/default/slowTraceSegmentThreshold of application.yml. -1   configuration-discovery.default.agentConfigurations The ConfigurationDiscovery settings look at configuration-discovery.md    This feature depends on upstream service, so it is DISABLED by default.\nconfiguration: selector: ${SW_CONFIGURATION:none} none: grpc: host: ${SW_DCS_SERVER_HOST:\u0026#34;\u0026#34;} port: ${SW_DCS_SERVER_PORT:80} clusterName: ${SW_DCS_CLUSTER_NAME:SkyWalking} period: ${SW_DCS_PERIOD:20} # ... other implementations Dynamic Configuration Service, DCS Dynamic Configuration Service is a gRPC service, which requires the upstream system implemented. The SkyWalking OAP fetches the configuration from the implementation(any system), after you open this implementation like this.\nconfiguration: selector: ${SW_CONFIGURATION:grpc} grpc: host: ${SW_DCS_SERVER_HOST:\u0026#34;\u0026#34;} port: ${SW_DCS_SERVER_PORT:80} clusterName: ${SW_DCS_CLUSTER_NAME:SkyWalking} period: ${SW_DCS_PERIOD:20} Dynamic Configuration Zookeeper Implementation Zookeeper is also supported as DCC(Dynamic Configuration Center), to use it, please configure as follows:\nconfiguration: selector: ${SW_CONFIGURATION:zookeeper} zookeeper: period: ${SW_CONFIG_ZK_PERIOD:60} # Unit seconds, sync period. Default fetch every 60 seconds. nameSpace: ${SW_CONFIG_ZK_NAMESPACE:/default} hostPort: ${SW_CONFIG_ZK_HOST_PORT:localhost:2181} # Retry Policy baseSleepTimeMs: ${SW_CONFIG_ZK_BASE_SLEEP_TIME_MS:1000} # initial amount of time to wait between retries maxRetries: ${SW_CONFIG_ZK_MAX_RETRIES:3} # max number of times to retry The nameSpace is the ZooKeeper path. The config key and value are the properties of the namespace folder.\nDynamic Configuration Etcd Implementation Etcd is also supported as DCC(Dynamic Configuration Center), to use it, please configure as follows:\nconfiguration: selector: ${SW_CONFIGURATION:etcd} etcd: period: ${SW_CONFIG_ETCD_PERIOD:60} # Unit seconds, sync period. Default fetch every 60 seconds. group: ${SW_CONFIG_ETCD_GROUP:skywalking} serverAddr: ${SW_CONFIG_ETCD_SERVER_ADDR:localhost:2379} clusterName: ${SW_CONFIG_ETCD_CLUSTER_NAME:default} Dynamic Configuration Consul Implementation Consul is also supported as DCC(Dynamic Configuration Center), to use it, please configure as follows:\nconfiguration: selector: ${SW_CONFIGURATION:consul} consul: # Consul host and ports, separated by comma, e.g. 1.2.3.4:8500,2.3.4.5:8500 hostAndPorts: ${SW_CONFIG_CONSUL_HOST_AND_PORTS:1.2.3.4:8500} # Sync period in seconds. Defaults to 60 seconds. period: ${SW_CONFIG_CONSUL_PERIOD:1} # Consul aclToken aclToken: ${SW_CONFIG_CONSUL_ACL_TOKEN:\u0026#34;\u0026#34;} Dynamic Configuration Apollo Implementation Apollo is also supported as DCC(Dynamic Configuration Center), to use it, just configured as follows:\nconfiguration: selector: ${SW_CONFIGURATION:apollo} apollo: apolloMeta: ${SW_CONFIG_APOLLO:http://106.12.25.204:8080} apolloCluster: ${SW_CONFIG_APOLLO_CLUSTER:default} apolloEnv: ${SW_CONFIG_APOLLO_ENV:\u0026#34;\u0026#34;} appId: ${SW_CONFIG_APOLLO_APP_ID:skywalking} period: ${SW_CONFIG_APOLLO_PERIOD:5} Dynamic Configuration Kuberbetes Configmap Implementation configmap is also supported as DCC(Dynamic Configuration Center), to use it, just configured as follows:\nconfiguration: selector: ${SW_CONFIGURATION:k8s-configmap} # [example] (../../../../oap-server/server-configuration/configuration-k8s-configmap/src/test/resources/skywalking-dynamic-configmap.example.yaml) k8s-configmap: # Sync period in seconds. Defaults to 60 seconds. period: ${SW_CONFIG_CONFIGMAP_PERIOD:60} # Which namespace is confiigmap deployed in. namespace: ${SW_CLUSTER_K8S_NAMESPACE:default} # Labelselector is used to locate specific configmap labelSelector: ${SW_CLUSTER_K8S_LABEL:app=collector,release=skywalking} Dynamic Configuration Nacos Implementation Nacos is also supported as DCC(Dynamic Configuration Center), to use it, please configure as follows:\nconfiguration: selector: ${SW_CONFIGURATION:nacos} nacos: # Nacos Server Host serverAddr: ${SW_CONFIG_NACOS_SERVER_ADDR:127.0.0.1} # Nacos Server Port port: ${SW_CONFIG_NACOS_SERVER_PORT:8848} # Nacos Configuration Group group: ${SW_CONFIG_NACOS_SERVER_GROUP:skywalking} # Nacos Configuration namespace namespace: ${SW_CONFIG_NACOS_SERVER_NAMESPACE:} # Unit seconds, sync period. Default fetch every 60 seconds. period: ${SW_CONFIG_NACOS_PERIOD:60} # the name of current cluster, set the name if you want to upstream system known. clusterName: ${SW_CONFIG_NACOS_CLUSTER_NAME:default} ","excerpt":"Dynamic Configuration SkyWalking Configurations mostly are set through application.yml and OS system …","ref":"/docs/main/latest/en/setup/backend/dynamic-config/","title":"Dynamic Configuration"},{"body":"ElasticSearch Some new users may encounter the following issues:\n The performance of ElasticSearch is not as good as expected. For instance, the latest data cannot be accessed after some time.  Or\n ERROR CODE 429.   Suppressed: org.elasticsearch.client.ResponseException: method [POST], host [http://127.0.0.1:9200], URI [/service_instance_inventory/type/6_tcc-app-gateway-77b98ff6ff-crblx.cards_0_0/_update?refresh=true\u0026amp;timeout=1m], status line [HTTP/1.1 429 Too Many Requests] {\u0026quot;error\u0026quot;:{\u0026quot;root_cause\u0026quot;:[{\u0026quot;type\u0026quot;:\u0026quot;remote_transport_exception\u0026quot;,\u0026quot;reason\u0026quot;:\u0026quot;[elasticsearch-0][10.16.9.130:9300][indices:data/write/update[s]]\u0026quot;}],\u0026quot;type\u0026quot;:\u0026quot;es_rejected_execution_exception\u0026quot;,\u0026quot;reason\u0026quot;:\u0026quot;rejected execution of org.elasticsearch.transport.TransportService$7@19a5cf02 on EsThreadPoolExecutor[name = elasticsearch-0/write, queue capacity = 200, org.elasticsearch.common.util.concurrent.EsThreadPoolExecutor@389297ad[Running, pool size = 2, active threads = 2, queued tasks = 200, completed tasks = 147611]]\u0026quot;},\u0026quot;status\u0026quot;:429} at org.elasticsearch.client.RestClient$SyncResponseListener.get(RestClient.java:705) ~[elasticsearch-rest-client-6.3.2.jar:6.3.2] at org.elasticsearch.client.RestClient.performRequest(RestClient.java:235) ~[elasticsearch-rest-client-6.3.2.jar:6.3.2] at org.elasticsearch.client.RestClient.performRequest(RestClient.java:198) ~[elasticsearch-rest-client-6.3.2.jar:6.3.2] at org.elasticsearch.client.RestHighLevelClient.performRequest(RestHighLevelClient.java:522) ~[elasticsearch You could add the following config to elasticsearch.yml, and set the value based on your environment variable.\n# In the case of tracing, consider setting a value higher than this. thread_pool.index.queue_size: 1000 thread_pool.write.queue_size: 1000 # When you face query error at trace page, remember to check this. index.max_result_window: 1000000 For more information, see ElasticSearch\u0026rsquo;s official documentation.\n","excerpt":"ElasticSearch Some new users may encounter the following issues:\n The performance of ElasticSearch …","ref":"/docs/main/latest/en/faq/es-server-faq/","title":"ElasticSearch"},{"body":"Events SkyWalking already supports the three pillars of observability, namely logs, metrics, and traces. In reality, a production system experiences many other events that may affect the performance of the system, such as upgrading, rebooting, chaos testing, etc. Although some of these events are reflected in the logs, many others are not. Hence, SkyWalking provides a more native way to collect these events. This doc details how SkyWalking collects events and what events look like in SkyWalking.\nHow to Report Events The SkyWalking backend supports three protocols to collect events: gRPC, HTTP, and Kafka. Any agent or CLI that implements one of these protocols can report events to SkyWalking. Currently, the officially supported clients to report events are:\n Java Agent Toolkit: Using the Java agent toolkit to report events within the applications. SkyWalking CLI: Using the CLI to report events from the command line interface. Kubernetes Event Exporter: Deploying an event exporter to refine and report Kubernetes events.  Event Definitions An event contains the following fields. The definitions of event can be found at the protocol repo.\nUUID Unique ID of the event. Since an event may span a long period of time, the UUID is necessary to associate the start time with the end time of the same event.\nSource The source object on which the event occurs. In SkyWalking, the object is typically a service, service instance, etc.\nName Name of the event. For example, Start, Stop, Crash, Reboot, Upgrade, etc.\nType Type of the event. This field is friendly for UI visualization, where events of type Normal are considered normal operations, while Error is considered unexpected operations, such as Crash events. Marking them with different colors allows us to more easily identify them.\nMessage The detail of the event that describes why this event happened. This should be a one-line message that briefly describes why the event is reported. Examples of an Upgrade event may be something like Upgrade from ${from_version} to ${to_version}. It\u0026rsquo;s NOT recommended to include the detailed logs of this event, such as the exception stack trace.\nParameters The parameters in the message field. This is a simple \u0026lt;string,string\u0026gt; map.\nStart Time The start time of the event. This field is mandatory when an event occurs.\nEnd Time The end time of the event. This field may be empty if the event has not ended yet, otherwise there should be a valid timestamp after startTime.\nNOTE: When reporting an event, you typically call the report function twice, the first time for starting of the event and the second time for ending of the event, both with the same UUID. There are also cases where you would already have both the start time and end time. For example, when exporting events from a third-party system, the start time and end time are already known so you may simply call the report function once.\nHow to Configure Alarms for Events Events are derived from metrics, and can be the source to trigger alarms. For example, if a specific event occurs for a certain times in a period, alarms can be triggered and sent.\nEvery event has a default value = 1, when n events with the same name are reported, they are aggregated into value = n as follows.\nEvent{name=Unhealthy, source={service=A,instance=a}, ...} Event{name=Unhealthy, source={service=A,instance=a}, ...} Event{name=Unhealthy, source={service=A,instance=a}, ...} Event{name=Unhealthy, source={service=A,instance=a}, ...} Event{name=Unhealthy, source={service=A,instance=a}, ...} Event{name=Unhealthy, source={service=A,instance=a}, ...} will be aggregated into\nEvent{name=Unhealthy, source={service=A,instance=a}, ...} \u0026lt;value = 6\u0026gt; so you can configure the following alarm rule to trigger alarm when Unhealthy event occurs more than 5 times within 10 minutes.\nrules: unhealthy_event_rule: metrics-name: Unhealthy # Healthiness check is usually a scheduled task, # they may be unhealthy for the first few times, # and can be unhealthy occasionally due to network jitter, # please adjust the threshold as per your actual situation. threshold: 5 op: \u0026#34;\u0026gt;\u0026#34; period: 10 count: 1 message: Service instance has been unhealthy for 10 minutes For more alarm configuration details, please refer to the alarm doc.\nNote that the Unhealthy event above is only for demonstration, they are not detected by default in SkyWalking, however, you can use the methods in How to Report Events to report this kind of events.\nKnown Events    Name Type When Where     Start Normal When your Java Application starts with SkyWalking Agent installed, the Start Event will be created. Reported from SkyWalking agent.   Shutdown Normal When your Java Application stops with SkyWalking Agent installed, the Shutdown Event will be created. Reported from SkyWalking agent.   Alarm Error When the Alarm is triggered, the corresponding Alarm Event will is created. Reported from internal SkyWalking OAP.    The following events are all reported by Kubernetes Event Exporter, in order to see these events, please make sure you have deployed the exporter.\n   Name Type When Where     Killing Normal When the Kubernetes Pod is being killing. Reporter by Kubernetes Event Exporter.   Pulling Normal When a docker image is being pulled for deployment. Reporter by Kubernetes Event Exporter.   Pulled Normal When a docker image is pulled for deployment. Reporter by Kubernetes Event Exporter.   Created Normal When a container inside a Pod is created. Reporter by Kubernetes Event Exporter.   Started Normal When a container inside a Pod is started. Reporter by Kubernetes Event Exporter.   Unhealthy Error When the readiness probe failed. Reporter by Kubernetes Event Exporter.    The complete event lists can be found in the Kubernetes codebase, please note that not all the events are supported by the exporter for now.\n","excerpt":"Events SkyWalking already supports the three pillars of observability, namely logs, metrics, and …","ref":"/docs/main/latest/en/concepts-and-designs/event/","title":"Events"},{"body":"Exporter tool for profile raw data When visualization doesn\u0026rsquo;t work well on the official UI, users may submit issue reports. This tool helps users package the original profile data to assist the community in locating the issues in the users' cases. NOTE: This report includes the class name, method name, line number, etc. Before making your submission, please make sure that the security of your system wouldn\u0026rsquo;t be compromised.\nExport using command line  Set the storage in the tools/profile-exporter/application.yml file based on your use case. Prepare the data  Profile task ID: Profile task ID Trace ID: Trace ID of the profile error Export dir: Directory exported by the data   Enter the Skywalking root path Execute shell command bash tools/profile-exporter/profile_exporter.sh --taskid={profileTaskId} --traceid={traceId} {exportDir}  The file {traceId}.tar.gz will be generated after executing shell.  Exported data content  basic.yml: Contains the complete information of the profiled segments in the trace. snapshot.data: All monitored thread snapshot data in the current segment.  Report profile issues  Provide exported data generated from this tool. Provide the operation name and the mode of analysis (including/excluding child span) for the span. Issue description. (It would be great if you could provide UI screenshots.)  ","excerpt":"Exporter tool for profile raw data When visualization doesn\u0026rsquo;t work well on the official UI, …","ref":"/docs/main/latest/en/guides/backend-profile-export/","title":"Exporter tool for profile raw data"},{"body":"Extend storage SkyWalking has already provided several storage solutions. In this document, you could learn how to easily implement a new storage.\nDefine your storage provider  Define class extension org.apache.skywalking.oap.server.library.module.ModuleProvider. Set this provider targeting to storage module.  @Override public Class\u0026lt;? extends ModuleDefine\u0026gt; module() { return StorageModule.class; } Implement all DAOs Here\u0026rsquo;s a list of all DAO interfaces in storage:\n  IServiceInventoryCacheDAO\n  IServiceInstanceInventoryCacheDAO\n  IEndpointInventoryCacheDAO\n  INetworkAddressInventoryCacheDAO\n  IBatchDAO\n  StorageDAO\n  IRegisterLockDAO\n  ITopologyQueryDAO\n  IMetricsQueryDAO\n  ITraceQueryDAO\n  IMetadataQueryDAO\n  IAggregationQueryDAO\n  IAlarmQueryDAO\n  IHistoryDeleteDAO\n  IMetricsDAO\n  IRecordDAO\n  IRegisterDAO\n  ILogQueryDAO\n  ITopNRecordsQueryDAO\n  IBrowserLogQueryDAO\n  IProfileTaskQueryDAO\n  IProfileTaskLogQueryDAO\n  IProfileThreadSnapshotQueryDAO\n  UITemplateManagementDAO\n  Register all service implementations In public void prepare(), use this#registerServiceImplementation method to register and bind with your implementation of the above interfaces.\nExample org.apache.skywalking.oap.server.storage.plugin.elasticsearch.StorageModuleElasticsearchProvider and org.apache.skywalking.oap.server.storage.plugin.jdbc.mysql.MySQLStorageProvider are good examples.\nRedistribution with new storage implementation To implement the storage, you don\u0026rsquo;t have to clone the main repo. Simply use our Apache releases. Take a look at SkyAPM/SkyWalking-With-Es5x-Storage repo, SkyWalking v6 redistribution with ElasticSearch 5 TCP connection storage implementation.\n","excerpt":"Extend storage SkyWalking has already provided several storage solutions. In this document, you …","ref":"/docs/main/latest/en/guides/storage-extention/","title":"Extend storage"},{"body":"FAQs These are known and frequently asked questions about SkyWalking. We welcome you to contribute here.\nDesign  Why doesn\u0026rsquo;t SkyWalking involve MQ in its architecture?  Compiling  Protoc plugin fails in maven build Required items could not be found when importing project into Eclipse Maven compilation failure with error such as python2 not found Compiling issues on Mac\u0026rsquo;s M1 chip  Runtime  Version 8.x+ upgrade Why do metrics indexes with Hour and Day precisions stop updating after upgrade to 7.x? Version 6.x upgrade Why are there only traces in UI? Tracing doesn\u0026rsquo;t work on the Kafka consumer end Agent or collector version upgrade, 3.x -\u0026gt; 5.0.0-alpha EnhanceRequireObjectCache class cast exception ElasticSearch server performance issues, including ERROR CODE:429 IllegalStateException when installing Java agent on WebSphere 7 \u0026ldquo;FORBIDDEN/12/index read-only / allow delete (api)\u0026rdquo; appears in the log No data shown and backend replies with \u0026ldquo;Variable \u0026lsquo;serviceId\u0026rsquo; has coerced Null value for NonNull type \u0026lsquo;ID!'\u0026quot; Unexpected endpoint register warning after 6.6.0 Use the profile exporter tool if the profile analysis is not right Compatibility with other javaagent bytecode processes Java agent memory leak when enhancing Worker thread at Thread Pool Thrift plugin  UI  What is VNode? And why does SkyWalking have that?  ","excerpt":"FAQs These are known and frequently asked questions about SkyWalking. We welcome you to contribute …","ref":"/docs/main/latest/en/faq/readme/","title":"FAQs"},{"body":"Group Parameterized Endpoints In most cases, the endpoint should be detected automatically through the language agents, service mesh observability solution, or configuration of meter system.\nThere are some special cases, especially when people use REST style URI, the application codes put the parameter in the endpoint name, such as putting order id in the URI, like /prod/ORDER123 and /prod/ORDER123. But logically, people expect they could have an endpoint name like prod/{order-id}. This is the feature of parameterized endpoint grouping designed for.\nCurrent, user could set up grouping rules through the static YAML file, named endpoint-name-grouping.yml, or use Dynamic Configuration to initial and update the endpoint grouping rule.\nConfiguration Format No matter in static local file or dynamic configuration value, they are sharing the same YAML format.\ngrouping: # Endpoint of the service would follow the following rules - service-name: serviceA rules: # Logic name when the regex expression matched. - endpoint-name: /prod/{id} regex: \\/prod\\/.+ ","excerpt":"Group Parameterized Endpoints In most cases, the endpoint should be detected automatically through …","ref":"/docs/main/latest/en/setup/backend/endpoint-grouping-rules/","title":"Group Parameterized Endpoints"},{"body":"Guides There are many ways you can contribute to the SkyWalking community.\n Go through our documents, and point out or fix a problem. Translate the documents into other languages. Download our releases, try to monitor your applications, and provide feedback to us. Read our source codes. For details, reach out to us. If you find any bugs, submit an issue. You can also try to fix it. Find help wanted issues. This is a good place for you to start. Submit an issue or start a discussion at GitHub issue. See all mail list discussions at website list review. If you are already a SkyWalking committer, you can log in and use the mail list in the browser mode. Otherwise, subscribe following the step below. Issue reports and discussions may also take place via dev@skywalking.apache.org. Mail to dev-subscribe@skywalking.apache.org, and follow the instructions in the reply to subscribe to the mail list.  Contact Us All of the following channels are open to the community.\n Submit an issue Mail list: dev@skywalking.apache.org. Mail to dev-subscribe@skywalking.apache.org. Follow the instructions in the reply to subscribe to the mail list. Gitter QQ Group: 392443393  Become an official Apache SkyWalking Committer The PMC assesses the contributions of every contributor, including their code contributions. It also promotes, votes on, and invites new committers and PMC members according to the Apache guides. See Become official Apache SkyWalking Committer for more details.\nFor code developer For developers, the starting point is the Compiling Guide. It guides developers on how to build the project in local and set up the environment.\nIntegration Tests After setting up the environment and writing your codes, to facilitate integration with the SkyWalking project, you\u0026rsquo;ll need to run tests locally to verify that your codes would not break any existing features, as well as write some unit test (UT) codes to verify that the new codes would work well. This will prevent them from being broken by future contributors. If the new codes involve other components or libraries, you should also write integration tests (IT).\nSkyWalking leverages the plugin maven-surefire-plugin to run the UTs and uses maven-failsafe-plugin to run the ITs. maven-surefire-plugin excludes ITs (whose class name starts with IT) and leaves them for maven-failsafe-plugin to run, which is bound to the verify goal and CI-with-IT profile. Therefore, to run the UTs, try ./mvnw clean test, which only runs the UTs but not the ITs.\nIf you would like to run the ITs, please activate the CI-with-IT profile as well as the the profiles of the modules whose ITs you want to run. E.g. if you would like to run the ITs in oap-server, try ./mvnw -Pbackend,CI-with-IT clean verify, and if you would like to run all the ITs, simply run ./mvnw -Pall,CI-with-IT clean verify.\nPlease be advised that if you\u0026rsquo;re writing integration tests, name it with the pattern IT* so they would only run with the CI-with-IT profile.\nEnd to End Tests (E2E) Since version 6.3.0, we have introduced more automatic tests to perform software quality assurance. E2E is an integral part of it.\n End-to-end testing is a methodology used to test whether the flow of an application is performing as designed from start to finish. The purpose of carrying out end-to-end tests is to identify system dependencies and to ensure that the right information is passed between various system components and systems.\n The E2E test involves some/all of the OAP server, storage, coordinator, webapp, and the instrumented services, all of which are orchestrated by docker-compose. Besides, there is a test controller (JUnit test) running outside of the container that sends traffic to the instrumented service, and then verifies the corresponding results after those requests have been made through GraphQL API of the SkyWalking Web App.\nBefore you take the following steps, please set the SkyWalking version sw.version in the pom.xml so that you can build it in your local IDE. Make sure not to check this change into the codebase. However, if you prefer to build it in the command line interface with ./mvnw, you can simply use property -Dsw.version=x.y.z without modifying pom.xml.\nWriting E2E Cases  Set up the environment in IntelliJ IDEA  The E2E test is a separate project under the SkyWalking root directory and the IDEA cannot recognize it by default. Right click on the file test/e2e/pom.xml and click Add as Maven Project. We recommend opening the directory skywalking/test/e2e in a separate IDE window for better experience, since there may be shaded classes issues.\n Orchestrate the components  The goal of the E2E tests is to test the SkyWalking project as a whole, including the OAP server, storage, coordinator, webapp, and even the frontend UI (not for now), on the single node mode as well as the cluster mode. Therefore, the first step is to determine what case we are going to verify, and orchestrate the components.\nTo make the orchestration process easier, we\u0026rsquo;re using a docker-compose that provides a simple file format (docker-compose.yml) for orchestrating the required containers, and offers an opportunity to define the dependencies of the components.\nFollow these steps:\n Decide what (and how many) containers will be needed. For example, for cluster testing, you\u0026rsquo;ll need \u0026gt; 2 OAP nodes, coordinators (e.g. zookeeper), storage (e.g. ElasticSearch), and instrumented services; Define the containers in docker-compose.yml, and carefully specify the dependencies, starting orders, and most importantly, link them together, e.g. set the correct OAP address on the agent end, and set the correct coordinator address in OAP, etc. Write (or hopefully reuse) the test codes to verify that the results are correct.  As for the final step, we have a user-friendly framework to help you get started more quickly. This framework provides the annotation @DockerCompose(\u0026quot;docker-compose.yml\u0026quot;) to load/parse and start up all the containers in the proper order. @ContainerHost/@ContainerPort obtains the real host/port of the container. @ContainerHostAndPort obtains both. @DockerContainer obtains the running container.\n Write test controller  Put it simply, test controllers are tests that can be bound to the maven integration-test/verify phase. They send design requests to the instrumented services, and anticipate corresponding traces/metrics/metadata from the SkyWalking webapp GraphQL API.\nIn the test framework, we provide a TrafficController that periodically sends traffic data to the instrumented services. You can simply enable it by providing a url and traffic data. Refer to this.\n Troubleshooting  We expose all logs from all containers to the stdout in the non-CI (local) mode, but save and upload them to the GitHub server. You can download them (only when the tests have failed) at \u0026ldquo;Artifacts/Download artifacts/logs\u0026rdquo; (see top right) for debugging.\nNOTE: Please verify the newly-added E2E test case locally first. However, if you find that it has passed locally but failed in the PR check status, make sure that all the updated/newly-added files (especially those in the submodules) are committed and included in the PR, or reset the git HEAD to the remote and verify locally again.\nE2E local remote debugging When the E2E test is executed locally, if any test case fails, the E2E local remote debugging function can be used to quickly troubleshoot the bug.\nProject Extensions The SkyWalking project supports various extensions of existing features. If you are interesting in writing extensions, read the following guides.\n Java agent plugin development guide. This guides you in developing SkyWalking agent plugins to support more frameworks. Developers for both open source and private plugins should read this. If you would like to build a new probe or plugin in any language, please read the Component library definition and extension document. Storage extension development guide. Potential contributors can learn how to build a new storage implementor in addition to the official one. Customize analysis using OAL scripts. OAL scripts are located in config/oal/*.oal. You could modify them and reboot the OAP server. Read Observability Analysis Language Introduction to learn more about OAL scripts. Source and scope extension for new metrics. For analysis of a new metric which SkyWalking hasn\u0026rsquo;t yet provided. Add a new receiver, rather than choosing an existing receiver. You would most likely have to add a new source and scope. To learn how to do this, read the document.  UI developer Our UI consists of static pages and the web container.\n RocketBot UI is SkyWalking\u0026rsquo;s primary UI since the 6.1 release. It is built with vue + typescript. Learn more at the rocketbot repository. Web container source codes are in the apm-webapp module. This is a simple zuul proxy which hosts static resources and sends GraphQL query requests to the backend. Legacy UI repository is retained, but not included in SkyWalking releases since 6.0.0-GA.  OAP backend dependency management  This section is only applicable to dependencies of the backend module.\n As one of the Top Level Projects of The Apache Software Foundation (ASF), SkyWalking must follow the ASF 3RD PARTY LICENSE POLICY. So if you\u0026rsquo;re adding new dependencies to the project, you should make sure that the new dependencies would not break the policy, and add their LICENSE and NOTICE to the project.\nWe have a simple script to help you make sure that you haven\u0026rsquo;t missed out any new dependencies:\n Build a distribution package and unzip/untar it to folder dist. Run the script in the root directory. It will print out all new dependencies. Check the LICENSE and NOTICE of those dependencies to make sure that they can be included in an ASF project. Add them to the apm-dist/release-docs/{LICENSE,NOTICE} file. Add the names of these dependencies to the tools/dependencies/known-oap-backend-dependencies.txt file (in alphabetical order). check-LICENSE.sh should pass in the next run.  Profile The performance profile is an enhancement feature in the APM system. We use thread dump to estimate the method execution time, rather than adding multiple local spans. In this way, the cost would be significantly reduced compared to using distributed tracing to locate the slow method. This feature is suitable in the production environment. The following documents are key to understanding the essential parts of this feature.\n Profile data report protocol is provided through gRPC, just like other traces and JVM data. Thread dump merging mechanism introduces the merging mechanism. This mechanism helps end users understand profile reports. Exporter tool of profile raw data guides you on how to package the original profile data for issue reports when the visualization doesn\u0026rsquo;t work well on the official UI.  Release If you\u0026rsquo;re a committer, read the Apache Release Guide to learn about how to create an official Apache version release in accordance with avoid Apache\u0026rsquo;s rules. As long as you keep our LICENSE and NOTICE, the Apache license allows everyone to redistribute.\n","excerpt":"Guides There are many ways you can contribute to the SkyWalking community.\n Go through our …","ref":"/docs/main/latest/en/guides/readme/","title":"Guides"},{"body":"Health Check Health check intends to provide a unique approach to check the health status of the OAP server. It includes the health status of modules, GraphQL, and gRPC services readiness.\nHealth Checker Module. The Health Checker module helps observe the health status of modules. You may activate it as follows:\nhealth-checker: selector: ${SW_HEALTH_CHECKER:default} default: checkIntervalSeconds: ${SW_HEALTH_CHECKER_INTERVAL_SECONDS:5} Note: The telemetry module should be enabled at the same time. This means that the provider should not be - and none.\nAfter that, we can check the OAP server health status by querying GraphQL:\nquery{ checkHealth{ score details } } If the OAP server is healthy, the response should be\n{ \u0026#34;data\u0026#34;: { \u0026#34;checkHealth\u0026#34;: { \u0026#34;score\u0026#34;: 0, \u0026#34;details\u0026#34;: \u0026#34;\u0026#34; } } } If some modules are unhealthy (e.g. storage H2 is down), then the result may look as follows:\n{ \u0026#34;data\u0026#34;: { \u0026#34;checkHealth\u0026#34;: { \u0026#34;score\u0026#34;: 1, \u0026#34;details\u0026#34;: \u0026#34;storage_h2,\u0026#34; } } } Refer to checkHealth query for more details.\nThe readiness of GraphQL and gRPC Use the query above to check the readiness of GraphQL.\nOAP has implemented the gRPC Health Checking Protocol. You may use the grpc-health-probe or any other tools to check the health of OAP gRPC services.\nCLI tool Please follow the CLI doc to get the health status score directly through the checkhealth command.\n","excerpt":"Health Check Health check intends to provide a unique approach to check the health status of the OAP …","ref":"/docs/main/latest/en/setup/backend/backend-health-check/","title":"Health Check"},{"body":"How to build a project This document will help you compile and build a project in your maven and set your IDE.\nBuilding the Project Since we are using Git submodule, we do not recommend using the GitHub tag or release page to download source codes for compiling.\nMaven behind the Proxy If you need to execute build behind the proxy, edit the .mvn/jvm.config and set the follow properties:\n-Dhttp.proxyHost=proxy_ip -Dhttp.proxyPort=proxy_port -Dhttps.proxyHost=proxy_ip -Dhttps.proxyPort=proxy_port -Dhttp.proxyUser=username -Dhttp.proxyPassword=password Building from GitHub   Prepare git, JDK8+, and Maven 3.6+.\n  Clone the project.\nIf you want to build a release from source codes, set a tag name by using git clone -b [tag_name] ... while cloning.\ngit clone --recurse-submodules https://github.com/apache/skywalking.git cd skywalking/ OR git clone https://github.com/apache/skywalking.git cd skywalking/ git submodule init git submodule update   Run ./mvnw clean package -DskipTests\n  All packages are in /dist (.tar.gz for Linux and .zip for Windows).\n  Building from Apache source code release  What is the Apache source code release?  For each official Apache release, there is a complete and independent source code tar, which includes all source codes. You could download it from SkyWalking Apache download page. There is no requirement related to git when compiling this. Just follow these steps.\n Prepare JDK8+ and Maven 3.6+. Run ./mvnw clean package -DskipTests. All packages are in /dist.(.tar.gz for Linux and .zip for Windows).  Advanced compiling SkyWalking is a complex maven project that has many modules. Therefore, the time to compile may be a bit longer than usual. If you just want to recompile part of the project, you have the following options:\n Compile agent and package   ./mvnw package -Pagent,dist\n or\n make build.agent\n If you intend to compile a single plugin, such as one in the dev stage, you could\n cd plugin_module_dir \u0026amp; mvn clean package\n  Compile backend and package   ./mvnw package -Pbackend,dist\n or\n make build.backend\n  Compile UI and package   ./mvnw package -Pui,dist\n or\n make build.ui\n Building docker images You can build docker images of backend and ui with Makefile located in root folder.\nRefer to Build docker image for more details.\nSetting up your IntelliJ IDEA NOTE: If you clone the codes from GitHub, please make sure that you have finished steps 1 to 3 in section Build from GitHub. If you download the source codes from the official website of SkyWalking, please make sure that you have followed the steps in section Build from Apache source code release.\n Import the project as a maven project. Run ./mvnw compile -Dmaven.test.skip=true to compile project and generate source codes. The reason is that we use gRPC and protobuf. Set Generated Source Codes folders.  grpc-java and java folders in apm-protocol/apm-network/target/generated-sources/protobuf grpc-java and java folders in oap-server/server-core/target/generated-sources/protobuf grpc-java and java folders in oap-server/server-receiver-plugin/receiver-proto/target/generated-sources/fbs grpc-java and java folders in oap-server/server-receiver-plugin/receiver-proto/target/generated-sources/protobuf grpc-java and java folders in oap-server/exporter/target/generated-sources/protobuf grpc-java and java folders in oap-server/server-configuration/grpc-configuration-sync/target/generated-sources/protobuf grpc-java and java folders in oap-server/server-alarm-plugin/target/generated-sources/protobuf antlr4 folder in oap-server/oal-grammar/target/generated-sources    ","excerpt":"How to build a project This document will help you compile and build a project in your maven and set …","ref":"/docs/main/latest/en/guides/how-to-build/","title":"How to build a project"},{"body":"How to enable Kafka Reporter The Kafka reporter plugin support report traces, JVM metrics, Instance Properties, and profiled snapshots to Kafka cluster, which is disabled in default. Move the jar of the plugin, kafka-reporter-plugin-x.y.z.jar, from agent/optional-reporter-plugins to agent/plugins for activating.\nNotice, currently, the agent still needs to configure GRPC receiver for delivering the task of profiling. In other words, the following configure cannot be omitted.\n# Backend service addresses. collector.backend_service=${SW_AGENT_COLLECTOR_BACKEND_SERVICES:127.0.0.1:11800} # Kafka producer configuration plugin.kafka.bootstrap_servers=${SW_KAFKA_BOOTSTRAP_SERVERS:localhost:9092} plugin.kafka.producer_config[delivery.timeout.ms]=12000 plugin.kafka.get_topic_timeout=${SW_GET_TOPIC_TIMEOUT:10} Kafka reporter plugin support to customize all configurations of listed in here.\nBefore you activated the Kafka reporter, you have to make sure that Kafka fetcher has been opened in service.\n","excerpt":"How to enable Kafka Reporter The Kafka reporter plugin support report traces, JVM metrics, Instance …","ref":"/docs/main/latest/en/setup/service-agent/java-agent/how-to-enable-kafka-reporter/","title":"How to enable Kafka Reporter"},{"body":"How to tolerate custom exceptions In some codes, the exception is being used as a way of controlling business flow. Skywalking provides 2 ways to tolerate an exception which is traced in a span.\n Set the names of exception classes in the agent config Use our annotation in the codes.  Set the names of exception classes in the agent config The property named \u0026ldquo;statuscheck.ignored_exceptions\u0026rdquo; is used to set up class names in the agent configuration file. if the exception listed here are detected in the agent, the agent core would flag the related span as the error status.\nDemo   A custom exception.\n TestNamedMatchException  package org.apache.skywalking.apm.agent.core.context.status; public class TestNamedMatchException extends RuntimeException { public TestNamedMatchException() { } public TestNamedMatchException(final String message) { super(message); } ... }  TestHierarchyMatchException  package org.apache.skywalking.apm.agent.core.context.status; public class TestHierarchyMatchException extends TestNamedMatchException { public TestHierarchyMatchException() { } public TestHierarchyMatchException(final String message) { super(message); } ... }   When the above exceptions traced in some spans, the status is like the following.\n   The traced exception Final span status     org.apache.skywalking.apm.agent.core.context.status.TestNamedMatchException true   org.apache.skywalking.apm.agent.core.context.status.TestHierarchyMatchException true      After set these class names through \u0026ldquo;statuscheck.ignored_exceptions\u0026rdquo;, the status of spans would be changed.\nstatuscheck.ignored_exceptions=org.apache.skywalking.apm.agent.core.context.status.TestNamedMatchException    The traced exception Final span status     org.apache.skywalking.apm.agent.core.context.status.TestNamedMatchException false   org.apache.skywalking.apm.agent.core.context.status.TestHierarchyMatchException false      Use our annotation in the codes. If an exception has the @IgnoredException annotation, the exception wouldn\u0026rsquo;t be marked as error status when tracing. Because the annotation supports inheritance, also affects the subclasses.\nDependency  Dependency the toolkit, such as using maven or gradle. Since 8.2.0.  \u0026lt;dependency\u0026gt; \u0026lt;groupId\u0026gt;org.apache.skywalking\u0026lt;/groupId\u0026gt; \u0026lt;artifactId\u0026gt;apm-toolkit-log4j-1.x\u0026lt;/artifactId\u0026gt; \u0026lt;version\u0026gt;{project.release.version}\u0026lt;/version\u0026gt; \u0026lt;/dependency\u0026gt; Demo   A custom exception.\npackage org.apache.skywalking.apm.agent.core.context.status; public class TestAnnotatedException extends RuntimeException { public TestAnnotatedException() { } public TestAnnotatedException(final String message) { super(message); } ... }   When the above exception traced in some spans, the status is like the following.\n   The traced exception Final span status     org.apache.skywalking.apm.agent.core.context.status.TestAnnotatedException true      However, when the exception annotated with the annotation, the status would be changed.\npackage org.apache.skywalking.apm.agent.core.context.status; @IgnoredException public class TestAnnotatedException extends RuntimeException { public TestAnnotatedException() { } public TestAnnotatedException(final String message) { super(message); } ... }    The traced exception Final span status     org.apache.skywalking.apm.agent.core.context.status.TestAnnotatedException false      Recursive check Due to the wrapper nature of Java exceptions, sometimes users need recursive checking. Skywalking also supports it. Typically, we don\u0026rsquo;t recommend setting this more than 10, which could cause a performance issue. Negative value and 0 would be ignored, which means all exceptions would make the span tagged in error status.\n statuscheck.max_recursive_depth=${SW_STATUSCHECK_MAX_RECURSIVE_DEPTH:1} ","excerpt":"How to tolerate custom exceptions In some codes, the exception is being used as a way of controlling …","ref":"/docs/main/latest/en/setup/service-agent/java-agent/how-to-tolerate-exceptions/","title":"How to tolerate custom exceptions"},{"body":"HTTP API Protocol HTTP API Protocol defines the API data format, including API request and response data format. They use the HTTP1.1 wrapper of the official SkyWalking Browser Protocol. Read it for more details.\nPerformance Data Report Detailed information about data format can be found in BrowserPerf.proto.\nPOST http://localhost:12800/browser/perfData Send a performance data object in JSON format.\nInput:\n{ \u0026#34;service\u0026#34;: \u0026#34;web\u0026#34;, \u0026#34;serviceVersion\u0026#34;: \u0026#34;v0.0.1\u0026#34;, \u0026#34;pagePath\u0026#34;: \u0026#34;/index.html\u0026#34;, \u0026#34;redirectTime\u0026#34;: 10, \u0026#34;dnsTime\u0026#34;: 10, \u0026#34;ttfbTime\u0026#34;: 10, \u0026#34;tcpTime\u0026#34;: 10, \u0026#34;transTime\u0026#34;: 10, \u0026#34;domAnalysisTime\u0026#34;: 10, \u0026#34;fptTime\u0026#34;: 10, \u0026#34;domReadyTime\u0026#34;: 10, \u0026#34;loadPageTime\u0026#34;: 10, \u0026#34;resTime\u0026#34;: 10, \u0026#34;sslTime\u0026#34;: 10, \u0026#34;ttlTime\u0026#34;: 10, \u0026#34;firstPackTime\u0026#34;: 10, \u0026#34;fmpTime\u0026#34;: 10 } OutPut:\nHttp Status: 204\nError Log Report Detailed information about data format can be found in BrowserPerf.proto.\nPOST http://localhost:12800/browser/errorLogs Send an error log object list in JSON format.\nInput:\n[ { \u0026#34;uniqueId\u0026#34;: \u0026#34;55ec6178-3fb7-43ef-899c-a26944407b01\u0026#34;, \u0026#34;service\u0026#34;: \u0026#34;web\u0026#34;, \u0026#34;serviceVersion\u0026#34;: \u0026#34;v0.0.1\u0026#34;, \u0026#34;pagePath\u0026#34;: \u0026#34;/index.html\u0026#34;, \u0026#34;category\u0026#34;: \u0026#34;ajax\u0026#34;, \u0026#34;message\u0026#34;: \u0026#34;error\u0026#34;, \u0026#34;line\u0026#34;: 1, \u0026#34;col\u0026#34;: 1, \u0026#34;stack\u0026#34;: \u0026#34;error\u0026#34;, \u0026#34;errorUrl\u0026#34;: \u0026#34;/index.html\u0026#34; }, { \u0026#34;uniqueId\u0026#34;: \u0026#34;55ec6178-3fb7-43ef-899c-a26944407b02\u0026#34;, \u0026#34;service\u0026#34;: \u0026#34;web\u0026#34;, \u0026#34;serviceVersion\u0026#34;: \u0026#34;v0.0.1\u0026#34;, \u0026#34;pagePath\u0026#34;: \u0026#34;/index.html\u0026#34;, \u0026#34;category\u0026#34;: \u0026#34;ajax\u0026#34;, \u0026#34;message\u0026#34;: \u0026#34;error\u0026#34;, \u0026#34;line\u0026#34;: 1, \u0026#34;col\u0026#34;: 1, \u0026#34;stack\u0026#34;: \u0026#34;error\u0026#34;, \u0026#34;errorUrl\u0026#34;: \u0026#34;/index.html\u0026#34; } ] OutPut:\nHttp Status: 204\nPOST http://localhost:12800/browser/errorLog Send a single error log object in JSON format.\nInput:\n{ \u0026#34;uniqueId\u0026#34;: \u0026#34;55ec6178-3fb7-43ef-899c-a26944407b01\u0026#34;, \u0026#34;service\u0026#34;: \u0026#34;web\u0026#34;, \u0026#34;serviceVersion\u0026#34;: \u0026#34;v0.0.1\u0026#34;, \u0026#34;pagePath\u0026#34;: \u0026#34;/index.html\u0026#34;, \u0026#34;category\u0026#34;: \u0026#34;ajax\u0026#34;, \u0026#34;message\u0026#34;: \u0026#34;error\u0026#34;, \u0026#34;line\u0026#34;: 1, \u0026#34;col\u0026#34;: 1, \u0026#34;stack\u0026#34;: \u0026#34;error\u0026#34;, \u0026#34;errorUrl\u0026#34;: \u0026#34;/index.html\u0026#34; } OutPut:\nHttp Status: 204\n","excerpt":"HTTP API Protocol HTTP API Protocol defines the API data format, including API request and response …","ref":"/docs/main/latest/en/protocols/browser-http-api-protocol/","title":"HTTP API Protocol"},{"body":"HTTP API Protocol HTTP API Protocol defines the API data format, including API request and response data format. They use the HTTP1.1 wrapper of the official SkyWalking Trace Data Protocol v3. Read it for more details.\nInstance Management Detailed information about data format can be found in Instance Management.\n Report service instance properties   POST http://localhost:12800/v3/management/reportProperties\n Input:\n{ \u0026#34;service\u0026#34;: \u0026#34;User Service Name\u0026#34;, \u0026#34;serviceInstance\u0026#34;: \u0026#34;User Service Instance Name\u0026#34;, \u0026#34;properties\u0026#34;: [{ \u0026#34;language\u0026#34;: \u0026#34;Lua\u0026#34; }] } Output JSON Array:\n{}  Service instance ping   POST http://localhost:12800/v3/management/keepAlive\n Input:\n{ \u0026#34;service\u0026#34;: \u0026#34;User Service Name\u0026#34;, \u0026#34;serviceInstance\u0026#34;: \u0026#34;User Service Instance Name\u0026#34; } OutPut:\n{} Trace Report Detailed information about data format can be found in Instance Management. There are two ways to report segment data: one segment per request or segment array in bulk mode.\nPOST http://localhost:12800/v3/segment Send a single segment object in JSON format.\nInput:\n{ \u0026#34;traceId\u0026#34;: \u0026#34;a12ff60b-5807-463b-a1f8-fb1c8608219e\u0026#34;, \u0026#34;serviceInstance\u0026#34;: \u0026#34;User_Service_Instance_Name\u0026#34;, \u0026#34;spans\u0026#34;: [{ \u0026#34;operationName\u0026#34;: \u0026#34;/ingress\u0026#34;, \u0026#34;startTime\u0026#34;: 1588664577013, \u0026#34;endTime\u0026#34;: 1588664577028, \u0026#34;spanType\u0026#34;: \u0026#34;Exit\u0026#34;, \u0026#34;spanId\u0026#34;: 1, \u0026#34;isError\u0026#34;: false, \u0026#34;parentSpanId\u0026#34;: 0, \u0026#34;componentId\u0026#34;: 6000, \u0026#34;peer\u0026#34;: \u0026#34;upstream service\u0026#34;, \u0026#34;spanLayer\u0026#34;: \u0026#34;Http\u0026#34; }, { \u0026#34;operationName\u0026#34;: \u0026#34;/ingress\u0026#34;, \u0026#34;startTime\u0026#34;: 1588664577013, \u0026#34;tags\u0026#34;: [{ \u0026#34;key\u0026#34;: \u0026#34;http.method\u0026#34;, \u0026#34;value\u0026#34;: \u0026#34;GET\u0026#34; }, { \u0026#34;key\u0026#34;: \u0026#34;http.params\u0026#34;, \u0026#34;value\u0026#34;: \u0026#34;http://localhost/ingress\u0026#34; }], \u0026#34;endTime\u0026#34;: 1588664577028, \u0026#34;spanType\u0026#34;: \u0026#34;Entry\u0026#34;, \u0026#34;spanId\u0026#34;: 0, \u0026#34;parentSpanId\u0026#34;: -1, \u0026#34;isError\u0026#34;: false, \u0026#34;spanLayer\u0026#34;: \u0026#34;Http\u0026#34;, \u0026#34;componentId\u0026#34;: 6000 }], \u0026#34;service\u0026#34;: \u0026#34;User_Service_Name\u0026#34;, \u0026#34;traceSegmentId\u0026#34;: \u0026#34;a12ff60b-5807-463b-a1f8-fb1c8608219e\u0026#34; } OutPut:\nPOST http://localhost:12800/v3/segments Send a segment object list in JSON format.\nInput:\n[{ \u0026#34;traceId\u0026#34;: \u0026#34;a12ff60b-5807-463b-a1f8-fb1c8608219e\u0026#34;, \u0026#34;serviceInstance\u0026#34;: \u0026#34;User_Service_Instance_Name\u0026#34;, \u0026#34;spans\u0026#34;: [{ \u0026#34;operationName\u0026#34;: \u0026#34;/ingress\u0026#34;, \u0026#34;startTime\u0026#34;: 1588664577013, \u0026#34;endTime\u0026#34;: 1588664577028, \u0026#34;spanType\u0026#34;: \u0026#34;Exit\u0026#34;, \u0026#34;spanId\u0026#34;: 1, \u0026#34;isError\u0026#34;: false, \u0026#34;parentSpanId\u0026#34;: 0, \u0026#34;componentId\u0026#34;: 6000, \u0026#34;peer\u0026#34;: \u0026#34;upstream service\u0026#34;, \u0026#34;spanLayer\u0026#34;: \u0026#34;Http\u0026#34; }, { \u0026#34;operationName\u0026#34;: \u0026#34;/ingress\u0026#34;, \u0026#34;startTime\u0026#34;: 1588664577013, \u0026#34;tags\u0026#34;: [{ \u0026#34;key\u0026#34;: \u0026#34;http.method\u0026#34;, \u0026#34;value\u0026#34;: \u0026#34;GET\u0026#34; }, { \u0026#34;key\u0026#34;: \u0026#34;http.params\u0026#34;, \u0026#34;value\u0026#34;: \u0026#34;http://localhost/ingress\u0026#34; }], \u0026#34;endTime\u0026#34;: 1588664577028, \u0026#34;spanType\u0026#34;: \u0026#34;Entry\u0026#34;, \u0026#34;spanId\u0026#34;: 0, \u0026#34;parentSpanId\u0026#34;: -1, \u0026#34;isError\u0026#34;: false, \u0026#34;spanLayer\u0026#34;: \u0026#34;Http\u0026#34;, \u0026#34;componentId\u0026#34;: 6000 }], \u0026#34;service\u0026#34;: \u0026#34;User_Service_Name\u0026#34;, \u0026#34;traceSegmentId\u0026#34;: \u0026#34;a12ff60b-5807-463b-a1f8-fb1c8608219e\u0026#34; }, { \u0026#34;traceId\u0026#34;: \u0026#34;f956699e-5106-4ea3-95e5-da748c55bac1\u0026#34;, \u0026#34;serviceInstance\u0026#34;: \u0026#34;User_Service_Instance_Name\u0026#34;, \u0026#34;spans\u0026#34;: [{ \u0026#34;operationName\u0026#34;: \u0026#34;/ingress\u0026#34;, \u0026#34;startTime\u0026#34;: 1588664577250, \u0026#34;endTime\u0026#34;: 1588664577250, \u0026#34;spanType\u0026#34;: \u0026#34;Exit\u0026#34;, \u0026#34;spanId\u0026#34;: 1, \u0026#34;isError\u0026#34;: false, \u0026#34;parentSpanId\u0026#34;: 0, \u0026#34;componentId\u0026#34;: 6000, \u0026#34;peer\u0026#34;: \u0026#34;upstream service\u0026#34;, \u0026#34;spanLayer\u0026#34;: \u0026#34;Http\u0026#34; }, { \u0026#34;operationName\u0026#34;: \u0026#34;/ingress\u0026#34;, \u0026#34;startTime\u0026#34;: 1588664577250, \u0026#34;tags\u0026#34;: [{ \u0026#34;key\u0026#34;: \u0026#34;http.method\u0026#34;, \u0026#34;value\u0026#34;: \u0026#34;GET\u0026#34; }, { \u0026#34;key\u0026#34;: \u0026#34;http.params\u0026#34;, \u0026#34;value\u0026#34;: \u0026#34;http://localhost/ingress\u0026#34; }], \u0026#34;endTime\u0026#34;: 1588664577250, \u0026#34;spanType\u0026#34;: \u0026#34;Entry\u0026#34;, \u0026#34;spanId\u0026#34;: 0, \u0026#34;parentSpanId\u0026#34;: -1, \u0026#34;isError\u0026#34;: false, \u0026#34;spanLayer\u0026#34;: \u0026#34;Http\u0026#34;, \u0026#34;componentId\u0026#34;: 6000 }], \u0026#34;service\u0026#34;: \u0026#34;User_Service_Name\u0026#34;, \u0026#34;traceSegmentId\u0026#34;: \u0026#34;f956699e-5106-4ea3-95e5-da748c55bac1\u0026#34; }] OutPut:\n","excerpt":"HTTP API Protocol HTTP API Protocol defines the API data format, including API request and response …","ref":"/docs/main/latest/en/protocols/http-api-protocol/","title":"HTTP API Protocol"},{"body":"IllegalStateException when installing Java agent on WebSphere This issue was found in our community discussion and feedback. A user installed the SkyWalking Java agent on WebSphere 7.0.0.11 and ibm jdk 1.8_20160719 and 1.7.0_20150407, and experienced the following error logs:\nWARN 2019-05-09 17:01:35:905 SkywalkingAgent-1-GRPCChannelManager-0 ProtectiveShieldMatcher : Byte-buddy occurs exception when match type. java.lang.IllegalStateException: Cannot resolve type description for java.security.PrivilegedAction at org.apache.skywalking.apm.dependencies.net.bytebuddy.pool.TypePool$Resolution$Illegal.resolve(TypePool.java:144) at org.apache.skywalking.apm.dependencies.net.bytebuddy.pool.TypePool$Default$WithLazyResolution$LazyTypeDescription.delegate(TypePool.java:1392) at org.apache.skywalking.apm.dependencies.net.bytebuddy.description.type.TypeDescription$AbstractBase$OfSimpleType$WithDelegation.getInterfaces(TypeDescription.java:8016) at org.apache.skywalking.apm.dependencies.net.bytebuddy.description.type.TypeDescription$Generic$OfNonGenericType.getInterfaces(TypeDescription.java:3621) at org.apache.skywalking.apm.dependencies.net.bytebuddy.matcher.HasSuperTypeMatcher.hasInterface(HasSuperTypeMatcher.java:53) at org.apache.skywalking.apm.dependencies.net.bytebuddy.matcher.HasSuperTypeMatcher.hasInterface(HasSuperTypeMatcher.java:54) at org.apache.skywalking.apm.dependencies.net.bytebuddy.matcher.HasSuperTypeMatcher.matches(HasSuperTypeMatcher.java:38) at org.apache.skywalking.apm.dependencies.net.bytebuddy.matcher.HasSuperTypeMatcher.matches(HasSuperTypeMatcher.java:15) at org.apache.skywalking.apm.dependencies.net.bytebuddy.matcher.ElementMatcher$Junction$Conjunction.matches(ElementMatcher.java:107) at org.apache.skywalking.apm.dependencies.net.bytebuddy.matcher.ElementMatcher$Junction$Disjunction.matches(ElementMatcher.java:147) at org.apache.skywalking.apm.dependencies.net.bytebuddy.matcher.ElementMatcher$Junction$Disjunction.matches(ElementMatcher.java:147) at org.apache.skywalking.apm.dependencies.net.bytebuddy.matcher.ElementMatcher$Junction$Disjunction.matches(ElementMatcher.java:147) at org.apache.skywalking.apm.dependencies.net.bytebuddy.matcher.ElementMatcher$Junction$Disjunction.matches(ElementMatcher.java:147) at org.apache.skywalking.apm.dependencies.net.bytebuddy.matcher.ElementMatcher$Junction$Disjunction.matches(ElementMatcher.java:147) ... The exception occured because access grant was required in WebSphere. Simply follow these steps:\n Set the agent\u0026rsquo;s owner to the owner of WebSphere. Add \u0026ldquo;grant codeBase \u0026ldquo;file:${agent_dir}/-\u0026rdquo; { permission java.security.AllPermission; };\u0026rdquo; in the file of \u0026ldquo;server.policy\u0026rdquo;.  ","excerpt":"IllegalStateException when installing Java agent on WebSphere This issue was found in our community …","ref":"/docs/main/latest/en/faq/install_agent_on_websphere/","title":"IllegalStateException when installing Java agent on WebSphere"},{"body":"Init mode The SkyWalking backend supports multiple storage implementors. Most of them would automatically initialize the storage, such as Elastic Search or Database, when the backend starts up at first.\nBut there may be some unexpected events that may occur with the storage, such as When multiple Elastic Search indexes are created concurrently, these backend instances would start up at the same time., When there is a change, the APIs of Elastic Search would be blocked without reporting any exception. This often happens on container management platforms, such as k8s.\nThis is where you need the Init mode startup.\nSolution Only one single instance should run in the Init mode before other instances start up. And this instance will exit graciously after all initialization steps are done.\nUse oapServiceInit.sh/oapServiceInit.bat to start up backend. You should see the following logs:\n 2018-11-09 23:04:39,465 - org.apache.skywalking.oap.server.starter.OAPServerStartUp -2214 [main] INFO [] - OAP starts up in init mode successfully, exit now\u0026hellip;\n Kubernetes Initialization in this mode would be included in our Kubernetes scripts and Helm.\n","excerpt":"Init mode The SkyWalking backend supports multiple storage implementors. Most of them would …","ref":"/docs/main/latest/en/setup/backend/backend-init-mode/","title":"Init mode"},{"body":"IP and port setting The backend uses IP and port binding in order to allow the OS to have multiple IPs. The binding/listening IP and port are specified by the core module\ncore: default: restHost: 0.0.0.0 restPort: 12800 restContextPath: / gRPCHost: 0.0.0.0 gRPCPort: 11800 There are two IP/port pairs for gRPC and HTTP REST services.\n Most agents and probes use gRPC service for better performance and code readability. Some agents use REST service, because gRPC may be not supported in that language. The UI uses REST service, but the data is always in GraphQL format.  Note IP binding For users who are not familiar with IP binding, note that once IP binding is complete, the client could only use this IP to access the service. For example, if 172.09.13.28 is bound, even if you are in this machine, you must use 172.09.13.28, rather than 127.0.0.1 or localhost, to access the service.\nModule provider specified IP and port The IP and port in the core module are provided by default. But it is common for some module providers, such as receiver modules, to provide other IP and port settings.\n","excerpt":"IP and port setting The backend uses IP and port binding in order to allow the OS to have multiple …","ref":"/docs/main/latest/en/setup/backend/backend-ip-port/","title":"IP and port setting"},{"body":"JVM Metrics Service Abstract Uplink the JVM metrics, including PermSize, HeapSize, CPU, Memory, etc., every second.\ngRPC service define\n","excerpt":"JVM Metrics Service Abstract Uplink the JVM metrics, including PermSize, HeapSize, CPU, Memory, …","ref":"/docs/main/latest/en/protocols/jvm-protocol/","title":"JVM Metrics Service"},{"body":"Language agents in Service   Java agent. Introduces how to install java agent to your service, without any impact in your code.\n  LUA agent. Introduce how to install the lua agent in Nginx + LUA module or OpenResty.\n  Python Agent. Introduce how to install the Python Agent in a Python service.\n  Node.js agent. Introduce how to install the NodeJS Agent in a NodeJS service.\n  The following agents and SDKs are compatible with the SkyWalking\u0026rsquo;s data formats and network protocols, but are maintained by 3rd-parties. You can go to their project repositories for additional info about guides and releases.\n  SkyAPM .NET Core agent. See .NET Core agent project document for more details.\n  SkyAPM Node.js agent. See Node.js server side agent project document for more details.\n  SkyAPM PHP agent. See PHP agent project document for more details.\n  SkyAPM Go SDK. See go2sky project document for more details.\n  SkyAPM C++ SDK. See cpp2sky project document for more details.\n  ","excerpt":"Language agents in Service   Java agent. Introduces how to install java agent to your service, …","ref":"/docs/main/latest/en/setup/service-agent/server-agents/","title":"Language agents in Service"},{"body":"Locate agent config file by system property Supported version 5.0.0-RC+\nWhat is Locate agent config file by system property ？ In Default. The agent will try to locate agent.config, which should be in the /config dictionary of agent package. If User sets the specified agent config file through system properties, The agent will try to load file from there. By the way, This function has no conflict with Setting Override\nOverride priority The specified agent config \u0026gt; The default agent config\nHow to use The content formats of the specified config must be same as the default config.\nUsing System.Properties(-D) to set the specified config path\n-Dskywalking_config=/path/to/agent.config /path/to/agent.config is the absolute path of the specified config file\n","excerpt":"Locate agent config file by system property Supported version 5.0.0-RC+\nWhat is Locate agent config …","ref":"/docs/main/latest/en/setup/service-agent/java-agent/specified-agent-config/","title":"Locate agent config file by system property"},{"body":"Log Analysis Language Log Analysis Language (LAL) in SkyWalking is essentially a Domain-Specific Language (DSL) to analyze logs. You can use LAL to parse, extract, and save the logs, as well as collaborate the logs with traces (by extracting the trace ID, segment ID and span ID) and metrics (by generating metrics from the logs and sending them to the meter system).\nThe LAL config files are in YAML format, and are located under directory lal. You can set log-analyzer/default/lalFiles in the application.yml file or set environment variable SW_LOG_LAL_FILES to activate specific LAL config files.\nFilter A filter is a group of parser, extractor and sink. Users can use one or more filters to organize their processing logic. Every piece of log will be sent to all filters in an LAL rule. A piece of log sent to the filter is available as property log in the LAL, therefore you can access the log service name via log.service. For all available fields of log, please refer to the protocol definition.\nAll components are executed sequentially in the orders they are declared.\nGlobal Functions Globally available functions may be used them in all components (i.e. parsers, extractors, and sinks) where necessary.\n abort  By default, all components declared are executed no matter what flags (dropped, saved, etc.) have been set. There are cases where you may want the filter chain to stop earlier when specified conditions are met. abort function aborts the remaining filter chain from where it\u0026rsquo;s declared, and all the remaining components won\u0026rsquo;t be executed at all. abort function serves as a fast-fail mechanism in LAL.\nfilter { if (log.service == \u0026#34;TestingService\u0026#34;) { // Don\u0026#39;t waste resources on TestingServices  abort {} // all remaining components won\u0026#39;t be executed at all  } // ... parsers, extractors, sinks } Note that when you put regexp in an if statement, you need to surround the expression with () like regexp(\u0026lt;the expression\u0026gt;), instead of regexp \u0026lt;the expression\u0026gt;.\nParser Parsers are responsible for parsing the raw logs into structured data in SkyWalking for further processing. There are 3 types of parsers at the moment, namely json, yaml, and text.\nWhen a piece of log is parsed, there is a corresponding property available, called parsed, injected by LAL. Property parsed is typically a map, containing all the fields parsed from the raw logs. For example, if the parser is json / yaml, parsed is a map containing all the key-values in the json / yaml; if the parser is text , parsed is a map containing all the captured groups and their values (for regexp and grok).\nAll parsers share the following options:\n   Option Type Description Default Value     abortOnFailure boolean Whether the filter chain should abort if the parser failed to parse / match the logs true    See examples below.\njson filter { json { abortOnFailure true // this is optional because it\u0026#39;s default behaviour  } } yaml filter { yaml { abortOnFailure true // this is optional because it\u0026#39;s default behaviour  } } text For unstructured logs, there are some text parsers for use.\n regexp  regexp parser uses a regular expression (regexp) to parse the logs. It leverages the captured groups of the regexp, all the captured groups can be used later in the extractors or sinks. regexp returns a boolean indicating whether the log matches the pattern or not.\nfilter { text { abortOnFailure true // this is optional because it\u0026#39;s default behaviour  // this is just a demo pattern  regexp \u0026#34;(?\u0026lt;timestamp\u0026gt;\\\\d{8}) (?\u0026lt;thread\u0026gt;\\\\w+) (?\u0026lt;level\u0026gt;\\\\w+) (?\u0026lt;traceId\u0026gt;\\\\w+) (?\u0026lt;msg\u0026gt;.+)\u0026#34; } extractor { tag level: parsed.level // we add a tag called `level` and its value is parsed.level, captured from the regexp above  traceId parsed.traceId // we also extract the trace id from the parsed result, which will be used to associate the log with the trace  } // ... }  grok (TODO)  We\u0026rsquo;re aware of certains performance issues in the grok Java library, and so we\u0026rsquo;re currently conducting investigations and benchmarking. Contributions are welcome.\nExtractor Extractors aim to extract metadata from the logs. The metadata can be a service name, a service instance name, an endpoint name, or even a trace ID, all of which can be associated with the existing traces and metrics.\n service  service extracts the service name from the parsed result, and set it into the LogData, which will be persisted (if not dropped) and is used to associate with traces / metrics.\n instance  instance extracts the service instance name from the parsed result, and set it into the LogData, which will be persisted (if not dropped) and is used to associate with traces / metrics.\n endpoint  endpoint extracts the service instance name from the parsed result, and set it into the LogData, which will be persisted (if not dropped) and is used to associate with traces / metrics.\n traceId  traceId extracts the trace ID from the parsed result, and set it into the LogData, which will be persisted (if not dropped) and is used to associate with traces / metrics.\n segmentId  segmentId extracts the segment ID from the parsed result, and set it into the LogData, which will be persisted (if not dropped) and is used to associate with traces / metrics.\n spanId  spanId extracts the span ID from the parsed result, and set it into the LogData, which will be persisted (if not dropped) and is used to associate with traces / metrics.\n timestamp  timestamp extracts the timestamp from the parsed result, and set it into the LogData, which will be persisted (if not dropped) and is used to associate with traces / metrics.\nThe unit of timestamp is millisecond.\n tag  tag extracts the tags from the parsed result, and set them into the LogData. The form of this extractor should look something like this: tag key1: value, key2: value2. You may use the properties of parsed as both keys and values.\nfilter { // ... parser  extractor { tag level: parsed.level, (parsed.statusCode): parsed.statusMsg tag anotherKey: \u0026#34;anotherConstantValue\u0026#34; } }  metrics  metrics extracts / generates metrics from the logs, and sends the generated metrics to the meter system. You may configure MAL for further analysis of these metrics. The dedicated MAL config files are under directory log-mal-rules, and you can set log-analyzer/default/malFiles to enable configured files.\n# application.yml # ... log-analyzer: selector: ${SW_LOG_ANALYZER:default} default: lalFiles: ${SW_LOG_LAL_FILES:my-lal-config} # files are under \u0026#34;lal\u0026#34; directory malFiles: ${SW_LOG_MAL_FILES:my-lal-mal-config,another-lal-mal-config} # files are under \u0026#34;log-mal-rules\u0026#34; directory Examples are as follows:\nfilter { // ...  extractor { service parsed.serviceName metrics { name \u0026#34;log_count\u0026#34; timestamp parsed.timestamp labels level: parsed.level, service: parsed.service, instance: parsed.instance value 1 } metrics { name \u0026#34;http_response_time\u0026#34; timestamp parsed.timestamp labels status_code: parsed.statusCode, service: parsed.service, instance: parsed.instance value parsed.duration } } // ... } The extractor above generates a metrics named log_count, with tag key level and value 1. After that, you can configure MAL rules to calculate the log count grouping by logging level like this:\n# ... other configurations of MAL metrics: - name: log_count_debug exp: log_count.tagEqual(\u0026#39;level\u0026#39;, \u0026#39;DEBUG\u0026#39;).sum([\u0026#39;service\u0026#39;, \u0026#39;instance\u0026#39;]).increase(\u0026#39;PT1M\u0026#39;) - name: log_count_error exp: log_count.tagEqual(\u0026#39;level\u0026#39;, \u0026#39;ERROR\u0026#39;).sum([\u0026#39;service\u0026#39;, \u0026#39;instance\u0026#39;]).increase(\u0026#39;PT1M\u0026#39;) The other metrics generated is http_response_time, so you can configure MAL rules to generate more useful metrics like percentiles.\n# ... other configurations of MAL metrics: - name: response_time_percentile exp: http_response_time.sum([\u0026#39;le\u0026#39;, \u0026#39;service\u0026#39;, \u0026#39;instance\u0026#39;]).increase(\u0026#39;PT5M\u0026#39;).histogram().histogram_percentile([50,70,90,99]) Sink Sinks are the persistent layer of the LAL. By default, all the logs of each filter are persisted into the storage. However, some mechanisms allow you to selectively save some logs, or even drop all the logs after you\u0026rsquo;ve extracted useful information, such as metrics.\nSampler Sampler allows you to save the logs in a sampling manner. Currently, the sampling strategy rateLimit is supported. We welcome contributions on more sampling strategies. If multiple samplers are specified, the last one determines the final sampling result. See examples in Enforcer.\nrateLimit samples n logs at a maximum rate of 1 second. rateLimit(\u0026quot;SamplerID\u0026quot;) requires an ID for the sampler. Sampler declarations with the same ID share the same sampler instance, thus sharing the same qps and resetting logic.\nExamples:\nfilter { // ... parser  sink { sampler { if (parsed.service == \u0026#34;ImportantApp\u0026#34;) { rateLimit(\u0026#34;ImportantAppSampler\u0026#34;) { qps 30 // samples 30 pieces of logs every second for service \u0026#34;ImportantApp\u0026#34;  } } else { rateLimit(\u0026#34;OtherSampler\u0026#34;) { qps 3 // samples 3 pieces of logs every second for other services than \u0026#34;ImportantApp\u0026#34;  } } } } } Dropper Dropper is a special sink, meaning that all logs are dropped without any exception. This is useful when you want to drop debugging logs.\nfilter { // ... parser  sink { if (parsed.level == \u0026#34;DEBUG\u0026#34;) { dropper {} } else { sampler { // ... configs  } } } } Or if you have multiple filters, some of which are for extracting metrics, only one of them has to be persisted.\nfilter { // filter A: this is for persistence  // ... parser  sink { sampler { // .. sampler configs  } } } filter { // filter B:  // ... extractors to generate many metrics  extractors { metrics { // ... metrics  } } sink { dropper {} // drop all logs because they have been saved in \u0026#34;filter A\u0026#34; above.  } } Enforcer Enforcer is another special sink that forcibly samples the log. A typical use case of enforcer is when you have configured a sampler and want to save some logs forcibly, such as to save error logs even if the sampling mechanism has been configured.\nfilter { // ... parser  sink { sampler { // ... sampler configs  } if (parserd.level == \u0026#34;ERROR\u0026#34; || parsed.userId == \u0026#34;TestingUserId\u0026#34;) { // sample error logs or testing users\u0026#39; logs (userId == \u0026#34;TestingUserId\u0026#34;) even if the sampling strategy is configured  enforcer { } } } } You can use enforcer and dropper to simulate a probabilistic sampler like this.\nfilter { // ... parser  sink { sampler { // simulate a probabilistic sampler with sampler rate 30% (not accurate though)  if (Math.abs(Math.random()) \u0026gt; 0.3) { enforcer {} } else { dropper {} } } } } ","excerpt":"Log Analysis Language Log Analysis Language (LAL) in SkyWalking is essentially a Domain-Specific …","ref":"/docs/main/latest/en/concepts-and-designs/lal/","title":"Log Analysis Language"},{"body":"Log Collecting And Analysis Collecting There are various ways to collect logs from application.\nLog files collector You can use Filebeat 、Fluentd to collect file logs including to use Kafka MQ to transport native-json format logs. When use this, need to open kafka-fetcher and enable configs enableNativeJsonLog.\nCollector config examples:\n filebeat.yml fluentd.conf  Java agent\u0026rsquo;s toolkits Java agent provides toolkit for log4j, log4j2, logback to report logs through gRPC with automatic injected trace context.\nSkyWalking Satellite sidecar is a recommended proxy/side to forward logs including to use Kafka MQ to transport logs. When use this, need to open kafka-fetcher and enable configs enableNativeProtoLog.\nJava agent provides toolkit for log4j, log4j2, logback to report logs through files with automatic injected trace context.\nLog framework config examples:\n log4j1.x fileAppender log4j2.x fileAppender logback fileAppender  Log Analyzer Log analyzer of OAP server supports native log data. OAP could use Log Analysis Language to structurize log content through parse, extract, and save logs. Also the analyzer leverages Meter Analysis Language Engine for further metrics calculation.\nlog-analyzer: selector: ${SW_LOG_ANALYZER:default} default: lalFiles: ${SW_LOG_LAL_FILES:default} malFiles: ${SW_LOG_MAL_FILES:\u0026#34;\u0026#34;} Read Log Analysis Language documentation to learn log structurize and metrics analysis.\n","excerpt":"Log Collecting And Analysis Collecting There are various ways to collect logs from application.\nLog …","ref":"/docs/main/latest/en/setup/backend/log-analyzer/","title":"Log Collecting And Analysis"},{"body":"Log Data Protocol Report log data via protocol.\nNative Proto Protocol Report native-proto format log via gRPC.\ngRPC service define\nNative Kafka Protocol Report native-json format log via kafka.\nJson log record example:\n{ \u0026#34;timestamp\u0026#34;:1618161813371, \u0026#34;service\u0026#34;:\u0026#34;Your_ApplicationName\u0026#34;, \u0026#34;serviceInstance\u0026#34;:\u0026#34;3a5b8da5a5ba40c0b192e91b5c80f1a8@192.168.1.8\u0026#34;, \u0026#34;traceContext\u0026#34;:{ \u0026#34;traceId\u0026#34;:\u0026#34;ddd92f52207c468e9cd03ddd107cd530.69.16181331190470001\u0026#34;, \u0026#34;spanId\u0026#34;:\u0026#34;0\u0026#34;, \u0026#34;traceSegmentId\u0026#34;:\u0026#34;ddd92f52207c468e9cd03ddd107cd530.69.16181331190470000\u0026#34; }, \u0026#34;tags\u0026#34;:{ \u0026#34;data\u0026#34;:[ { \u0026#34;key\u0026#34;:\u0026#34;level\u0026#34;, \u0026#34;value\u0026#34;:\u0026#34;INFO\u0026#34; }, { \u0026#34;key\u0026#34;:\u0026#34;logger\u0026#34;, \u0026#34;value\u0026#34;:\u0026#34;com.example.MyLogger\u0026#34; } ] }, \u0026#34;body\u0026#34;:{ \u0026#34;text\u0026#34;:{ \u0026#34;text\u0026#34;:\u0026#34;log message\u0026#34; } } } HTTP API Report json format logs via HTTP API, the endpoint is http://:12800/v3/logs.\nJson log record example:\n[ { \u0026#34;timestamp\u0026#34;: 1618161813371, \u0026#34;service\u0026#34;: \u0026#34;Your_ApplicationName\u0026#34;, \u0026#34;serviceInstance\u0026#34;: \u0026#34;3a5b8da5a5ba40c0b192e91b5c80f1a8@192.168.1.8\u0026#34;, \u0026#34;traceContext\u0026#34;: { \u0026#34;traceId\u0026#34;: \u0026#34;ddd92f52207c468e9cd03ddd107cd530.69.16181331190470001\u0026#34;, \u0026#34;spanId\u0026#34;: \u0026#34;0\u0026#34;, \u0026#34;traceSegmentId\u0026#34;: \u0026#34;ddd92f52207c468e9cd03ddd107cd530.69.16181331190470000\u0026#34; }, \u0026#34;tags\u0026#34;: { \u0026#34;data\u0026#34;: [ { \u0026#34;key\u0026#34;: \u0026#34;level\u0026#34;, \u0026#34;value\u0026#34;: \u0026#34;INFO\u0026#34; }, { \u0026#34;key\u0026#34;: \u0026#34;logger\u0026#34;, \u0026#34;value\u0026#34;: \u0026#34;com.example.MyLogger\u0026#34; } ] }, \u0026#34;body\u0026#34;: { \u0026#34;text\u0026#34;: { \u0026#34;text\u0026#34;: \u0026#34;log message\u0026#34; } } } ] ","excerpt":"Log Data Protocol Report log data via protocol.\nNative Proto Protocol Report native-proto format log …","ref":"/docs/main/latest/en/protocols/log-data-protocol/","title":"Log Data Protocol"},{"body":"logback plugin  Dependency the toolkit, such as using maven or gradle  \u0026lt;dependency\u0026gt; \u0026lt;groupId\u0026gt;org.apache.skywalking\u0026lt;/groupId\u0026gt; \u0026lt;artifactId\u0026gt;apm-toolkit-logback-1.x\u0026lt;/artifactId\u0026gt; \u0026lt;version\u0026gt;{project.release.version}\u0026lt;/version\u0026gt; \u0026lt;/dependency\u0026gt; Print trace ID in your logs  set %tid in Pattern section of logback.xml  \u0026lt;appender name=\u0026#34;STDOUT\u0026#34; class=\u0026#34;ch.qos.logback.core.ConsoleAppender\u0026#34;\u0026gt; \u0026lt;encoder class=\u0026#34;ch.qos.logback.core.encoder.LayoutWrappingEncoder\u0026#34;\u0026gt; \u0026lt;layout class=\u0026#34;org.apache.skywalking.apm.toolkit.log.logback.v1.x.TraceIdPatternLogbackLayout\u0026#34;\u0026gt; \u0026lt;Pattern\u0026gt;%d{yyyy-MM-dd HH:mm:ss.SSS} [%tid] [%thread] %-5level %logger{36} -%msg%n\u0026lt;/Pattern\u0026gt; \u0026lt;/layout\u0026gt; \u0026lt;/encoder\u0026gt; \u0026lt;/appender\u0026gt;  with the MDC, set %X{tid} in Pattern section of logback.xml  \u0026lt;appender name=\u0026#34;STDOUT\u0026#34; class=\u0026#34;ch.qos.logback.core.ConsoleAppender\u0026#34;\u0026gt; \u0026lt;encoder class=\u0026#34;ch.qos.logback.core.encoder.LayoutWrappingEncoder\u0026#34;\u0026gt; \u0026lt;layout class=\u0026#34;org.apache.skywalking.apm.toolkit.log.logback.v1.x.mdc.TraceIdMDCPatternLogbackLayout\u0026#34;\u0026gt; \u0026lt;Pattern\u0026gt;%d{yyyy-MM-dd HH:mm:ss.SSS} [%X{tid}] [%thread] %-5level %logger{36} -%msg%n\u0026lt;/Pattern\u0026gt; \u0026lt;/layout\u0026gt; \u0026lt;/encoder\u0026gt; \u0026lt;/appender\u0026gt;  Support logback AsyncAppender(MDC also support), No additional configuration is required. Refer to the demo of logback.xml below. For details: Logback AsyncAppender  \u0026lt;configuration scan=\u0026#34;true\u0026#34; scanPeriod=\u0026#34; 5 seconds\u0026#34;\u0026gt; \u0026lt;appender name=\u0026#34;STDOUT\u0026#34; class=\u0026#34;ch.qos.logback.core.ConsoleAppender\u0026#34;\u0026gt; \u0026lt;encoder class=\u0026#34;ch.qos.logback.core.encoder.LayoutWrappingEncoder\u0026#34;\u0026gt; \u0026lt;layout class=\u0026#34;org.apache.skywalking.apm.toolkit.log.logback.v1.x.mdc.TraceIdMDCPatternLogbackLayout\u0026#34;\u0026gt; \u0026lt;Pattern\u0026gt;%d{yyyy-MM-dd HH:mm:ss.SSS} [%X{tid}] [%thread] %-5level %logger{36} -%msg%n\u0026lt;/Pattern\u0026gt; \u0026lt;/layout\u0026gt; \u0026lt;/encoder\u0026gt; \u0026lt;/appender\u0026gt; \u0026lt;appender name=\u0026#34;ASYNC\u0026#34; class=\u0026#34;ch.qos.logback.classic.AsyncAppender\u0026#34;\u0026gt; \u0026lt;discardingThreshold\u0026gt;0\u0026lt;/discardingThreshold\u0026gt; \u0026lt;queueSize\u0026gt;1024\u0026lt;/queueSize\u0026gt; \u0026lt;neverBlock\u0026gt;true\u0026lt;/neverBlock\u0026gt; \u0026lt;appender-ref ref=\u0026#34;STDOUT\u0026#34;/\u0026gt; \u0026lt;/appender\u0026gt; \u0026lt;root level=\u0026#34;INFO\u0026#34;\u0026gt; \u0026lt;appender-ref ref=\u0026#34;ASYNC\u0026#34;/\u0026gt; \u0026lt;/root\u0026gt; \u0026lt;/configuration\u0026gt;  When you use -javaagent to active the SkyWalking tracer, logback will output traceId, if it existed. If the tracer is inactive, the output will be TID: N/A.  Print SkyWalking context in your logs   Your only need to replace pattern %tid or %X{tid]} with %sw_ctx or %X{sw_ctx}.\n  When you use -javaagent to active the SkyWalking tracer, logback will output SW_CTX: [$serviceName,$instanceName,$traceId,$traceSegmentId,$spanId], if it existed. If the tracer is inactive, the output will be SW_CTX: N/A.\n  logstash logback plugin  Dependency the toolkit, such as using maven or gradle  \u0026lt;dependency\u0026gt; \u0026lt;groupId\u0026gt;org.apache.skywalking\u0026lt;/groupId\u0026gt; \u0026lt;artifactId\u0026gt;apm-toolkit-logback-1.x\u0026lt;/artifactId\u0026gt; \u0026lt;version\u0026gt;${skywalking.version}\u0026lt;/version\u0026gt; \u0026lt;/dependency\u0026gt;  set LogstashEncoder of logback.xml  \u0026lt;encoder charset=\u0026#34;UTF-8\u0026#34; class=\u0026#34;net.logstash.logback.encoder.LogstashEncoder\u0026#34;\u0026gt; \u0026lt;!-- add TID(traceId) field --\u0026gt; \u0026lt;provider class=\u0026#34;org.apache.skywalking.apm.toolkit.log.logback.v1.x.logstash.TraceIdJsonProvider\u0026#34;\u0026gt; \u0026lt;/provider\u0026gt; \u0026lt;!-- add SW_CTX(SkyWalking context) field --\u0026gt; \u0026lt;provider class=\u0026#34;org.apache.skywalking.apm.toolkit.log.logback.v1.x.logstash.SkyWalkingContextJsonProvider\u0026#34;\u0026gt; \u0026lt;/provider\u0026gt; \u0026lt;/encoder\u0026gt;  set LoggingEventCompositeJsonEncoder of logstash in logback-spring.xml for custom json format  1.add converter for %tid or %sw_ctx as child of  node\n\u0026lt;!-- add converter for %tid --\u0026gt; \u0026lt;conversionRule conversionWord=\u0026#34;tid\u0026#34; converterClass=\u0026#34;org.apache.skywalking.apm.toolkit.log.logback.v1.x.LogbackPatternConverter\u0026#34;/\u0026gt; \u0026lt;!-- add converter for %sw_ctx --\u0026gt; \u0026lt;conversionRule conversionWord=\u0026#34;sw_ctx\u0026#34; converterClass=\u0026#34;org.apache.skywalking.apm.toolkit.log.logback.v1.x.LogbackSkyWalkingContextPatternConverter\u0026#34;/\u0026gt; 2.add json encoder for custom json format\n\u0026lt;encoder class=\u0026#34;net.logstash.logback.encoder.LoggingEventCompositeJsonEncoder\u0026#34;\u0026gt; \u0026lt;providers\u0026gt; \u0026lt;timestamp\u0026gt; \u0026lt;timeZone\u0026gt;UTC\u0026lt;/timeZone\u0026gt; \u0026lt;/timestamp\u0026gt; \u0026lt;pattern\u0026gt; \u0026lt;pattern\u0026gt; { \u0026#34;level\u0026#34;: \u0026#34;%level\u0026#34;, \u0026#34;tid\u0026#34;: \u0026#34;%tid\u0026#34;, \u0026#34;skyWalkingContext\u0026#34;: \u0026#34;%sw_ctx\u0026#34;, \u0026#34;thread\u0026#34;: \u0026#34;%thread\u0026#34;, \u0026#34;class\u0026#34;: \u0026#34;%logger{1.}:%L\u0026#34;, \u0026#34;message\u0026#34;: \u0026#34;%message\u0026#34;, \u0026#34;stackTrace\u0026#34;: \u0026#34;%exception{10}\u0026#34; } \u0026lt;/pattern\u0026gt; \u0026lt;/pattern\u0026gt; \u0026lt;/providers\u0026gt; \u0026lt;/encoder\u0026gt; gRPC reporter The gRPC reporter could forward the collected logs to SkyWalking OAP server, or SkyWalking Satellite sidecar. Trace id, segment id, and span id will attach to logs automatically. There is no need to modify existing layouts.\n Add GRPCLogClientAppender in logback.xml  \u0026lt;appender name=\u0026#34;grpc-log\u0026#34; class=\u0026#34;org.apache.skywalking.apm.toolkit.log.logback.v1.x.log.GRPCLogClientAppender\u0026#34;\u0026gt; \u0026lt;encoder class=\u0026#34;ch.qos.logback.core.encoder.LayoutWrappingEncoder\u0026#34;\u0026gt; \u0026lt;layout class=\u0026#34;org.apache.skywalking.apm.toolkit.log.logback.v1.x.mdc.TraceIdMDCPatternLogbackLayout\u0026#34;\u0026gt; \u0026lt;Pattern\u0026gt;%d{yyyy-MM-dd HH:mm:ss.SSS} [%X{tid}] [%thread] %-5level %logger{36} -%msg%n\u0026lt;/Pattern\u0026gt; \u0026lt;/layout\u0026gt; \u0026lt;/encoder\u0026gt; \u0026lt;/appender\u0026gt;  Add config of the plugin or use default  plugin.toolkit.log.grpc.reporter.server_host=${SW_GRPC_LOG_SERVER_HOST:127.0.0.1} plugin.toolkit.log.grpc.reporter.server_port=${SW_GRPC_LOG_SERVER_PORT:11800} plugin.toolkit.log.grpc.reporter.max_message_size=${SW_GRPC_LOG_MAX_MESSAGE_SIZE:10485760} plugin.toolkit.log.grpc.reporter.upstream_timeout=${SW_GRPC_LOG_GRPC_UPSTREAM_TIMEOUT:30} Transmitting un-formatted messages The logback 1.x gRPC reporter supports transmitting logs as formatted or un-formatted. Transmitting formatted data is the default but can be disabled by adding the following to the agent config:\nplugin.toolkit.log.transmit_formatted=false The above will result in the content field being used for the log pattern with additional log tags of argument.0, argument.1, and so on representing each logged argument as well as an additional exception tag which is only present if a throwable is also logged.\nFor example, the following code:\nlog.info(\u0026#34;{} {} {}\u0026#34;, 1, 2, 3); Will result in:\n{ \u0026#34;content\u0026#34;: \u0026#34;{} {} {}\u0026#34;, \u0026#34;tags\u0026#34;: [ { \u0026#34;key\u0026#34;: \u0026#34;argument.0\u0026#34;, \u0026#34;value\u0026#34;: \u0026#34;1\u0026#34; }, { \u0026#34;key\u0026#34;: \u0026#34;argument.1\u0026#34;, \u0026#34;value\u0026#34;: \u0026#34;2\u0026#34; }, { \u0026#34;key\u0026#34;: \u0026#34;argument.2\u0026#34;, \u0026#34;value\u0026#34;: \u0026#34;3\u0026#34; } ] } ","excerpt":"logback plugin  Dependency the toolkit, such as using maven or gradle  \u0026lt;dependency\u0026gt; …","ref":"/docs/main/latest/en/setup/service-agent/java-agent/application-toolkit-logback-1.x/","title":"logback plugin"},{"body":"Manual instrument SDK Our incredible community has contributed to the manual instrument SDK.\n Go2Sky. Go SDK follows the SkyWalking format. C++. C++ SDK follows the SkyWalking format.  What are the SkyWalking format and the propagation protocols? See these protocols in protocols document.\nEnvoy tracer Envoy has its internal tracer implementation for SkyWalking. Read SkyWalking Tracer doc and SkyWalking tracing sandbox for more details.\n","excerpt":"Manual instrument SDK Our incredible community has contributed to the manual instrument SDK. …","ref":"/docs/main/latest/en/concepts-and-designs/manual-sdk/","title":"Manual instrument SDK"},{"body":"Meter Analysis Language The meter system provides a functional analysis language called MAL (Meter Analysis Language) that lets users analyze and aggregate meter data in the OAP streaming system. The result of an expression can either be ingested by the agent analyzer, or the OC/Prometheus analyzer.\nLanguage data type In MAL, an expression or sub-expression can evaluate to one of the following two types:\n Sample family: A set of samples (metrics) containing a range of metrics whose names are identical. Scalar: A simple numeric value that supports integer/long and floating/double.  Sample family A set of samples, which acts as the basic unit in MAL. For example:\ninstance_trace_count The sample family above may contain the following samples which are provided by external modules, such as the agent analyzer:\ninstance_trace_count{region=\u0026quot;us-west\u0026quot;,az=\u0026quot;az-1\u0026quot;} 100 instance_trace_count{region=\u0026quot;us-east\u0026quot;,az=\u0026quot;az-3\u0026quot;} 20 instance_trace_count{region=\u0026quot;asia-north\u0026quot;,az=\u0026quot;az-1\u0026quot;} 33 Tag filter MAL supports four type operations to filter samples in a sample family:\n tagEqual: Filter tags exactly equal to the string provided. tagNotEqual: Filter tags not equal to the string provided. tagMatch: Filter tags that regex-match the string provided. tagNotMatch: Filter labels that do not regex-match the string provided.  For example, this filters all instance_trace_count samples for us-west and asia-north region and az-1 az:\ninstance_trace_count.tagMatch(\u0026quot;region\u0026quot;, \u0026quot;us-west|asia-north\u0026quot;).tagEqual(\u0026quot;az\u0026quot;, \u0026quot;az-1\u0026quot;) Value filter MAL supports six type operations to filter samples in a sample family by value:\n valueEqual: Filter values exactly equal to the value provided. valueNotEqual: Filter values equal to the value provided. valueGreater: Filter values greater than the value provided. valueGreaterEqual: Filter values greater than or equal to the value provided. valueLess: Filter values less than the value provided. valueLessEqual: Filter values less than or equal to the value provided.  For example, this filters all instance_trace_count samples for values \u0026gt;= 33:\ninstance_trace_count.valueGreaterEqual(33) Tag manipulator MAL allows tag manipulators to change (i.e. add/delete/update) tags and their values.\nK8s MAL supports using the metadata of K8s to manipulate the tags and their values. This feature requires authorizing the OAP Server to access K8s\u0026rsquo;s API Server.\nretagByK8sMeta retagByK8sMeta(newLabelName, K8sRetagType, existingLabelName, namespaceLabelName). Add a new tag to the sample family based on the value of an existing label. Provide several internal converting types, including\n K8sRetagType.Pod2Service  Add a tag to the sample using service as the key, $serviceName.$namespace as the value, and according to the given value of the tag key, which represents the name of a pod.\nFor example:\ncontainer_cpu_usage_seconds_total{namespace=default, container=my-nginx, cpu=total, pod=my-nginx-5dc4865748-mbczh} 2 Expression:\ncontainer_cpu_usage_seconds_total.retagByK8sMeta('service' , K8sRetagType.Pod2Service , 'pod' , 'namespace') Output:\ncontainer_cpu_usage_seconds_total{namespace=default, container=my-nginx, cpu=total, pod=my-nginx-5dc4865748-mbczh, service='nginx-service.default'} 2 Binary operators The following binary arithmetic operators are available in MAL:\n + (addition) - (subtraction) * (multiplication) / (division)  Binary operators are defined between scalar/scalar, sampleFamily/scalar and sampleFamily/sampleFamily value pairs.\nBetween two scalars: they evaluate to another scalar that is the result of the operator being applied to both scalar operands:\n1 + 2 Between a sample family and a scalar, the operator is applied to the value of every sample in the sample family. For example:\ninstance_trace_count + 2 or\n2 + instance_trace_count results in\ninstance_trace_count{region=\u0026quot;us-west\u0026quot;,az=\u0026quot;az-1\u0026quot;} 102 // 100 + 2 instance_trace_count{region=\u0026quot;us-east\u0026quot;,az=\u0026quot;az-3\u0026quot;} 22 // 20 + 2 instance_trace_count{region=\u0026quot;asia-north\u0026quot;,az=\u0026quot;az-1\u0026quot;} 35 // 33 + 2 Between two sample families, a binary operator is applied to each sample in the sample family on the left and its matching sample in the sample family on the right. A new sample family with empty name will be generated. Only the matched tags will be reserved. Samples with no matching samples in the sample family on the right will not be found in the result.\nAnother sample family instance_trace_analysis_error_count is\ninstance_trace_analysis_error_count{region=\u0026quot;us-west\u0026quot;,az=\u0026quot;az-1\u0026quot;} 20 instance_trace_analysis_error_count{region=\u0026quot;asia-north\u0026quot;,az=\u0026quot;az-1\u0026quot;} 11 Example expression:\ninstance_trace_analysis_error_count / instance_trace_count This returns a resulting sample family containing the error rate of trace analysis. Samples with region us-west and az az-3 have no match and will not show up in the result:\n{region=\u0026quot;us-west\u0026quot;,az=\u0026quot;az-1\u0026quot;} 0.8 // 20 / 100 {region=\u0026quot;asia-north\u0026quot;,az=\u0026quot;az-1\u0026quot;} 0.3333 // 11 / 33 Aggregation Operation Sample family supports the following aggregation operations that can be used to aggregate the samples of a single sample family, resulting in a new sample family having fewer samples (sometimes having just a single sample) with aggregated values:\n sum (calculate sum over dimensions) min (select minimum over dimensions) max (select maximum over dimensions) avg (calculate the average over dimensions)  These operations can be used to aggregate overall label dimensions or preserve distinct dimensions by inputting by parameter.\n\u0026lt;aggr-op\u0026gt;(by: \u0026lt;tag1, tag2, ...\u0026gt;) Example expression:\ninstance_trace_count.sum(by: ['az']) will output the following result:\ninstance_trace_count{az=\u0026quot;az-1\u0026quot;} 133 // 100 + 33 instance_trace_count{az=\u0026quot;az-3\u0026quot;} 20 Function Duraton is a textual representation of a time range. The formats accepted are based on the ISO-8601 duration format {@code PnDTnHnMn.nS} where a day is regarded as exactly 24 hours.\nExamples:\n \u0026ldquo;PT20.345S\u0026rdquo; \u0026ndash; parses as \u0026ldquo;20.345 seconds\u0026rdquo; \u0026ldquo;PT15M\u0026rdquo; \u0026ndash; parses as \u0026ldquo;15 minutes\u0026rdquo; (where a minute is 60 seconds) \u0026ldquo;PT10H\u0026rdquo; \u0026ndash; parses as \u0026ldquo;10 hours\u0026rdquo; (where an hour is 3600 seconds) \u0026ldquo;P2D\u0026rdquo; \u0026ndash; parses as \u0026ldquo;2 days\u0026rdquo; (where a day is 24 hours or 86400 seconds) \u0026ldquo;P2DT3H4M\u0026rdquo; \u0026ndash; parses as \u0026ldquo;2 days, 3 hours and 4 minutes\u0026rdquo; \u0026ldquo;P-6H3M\u0026rdquo; \u0026ndash; parses as \u0026ldquo;-6 hours and +3 minutes\u0026rdquo; \u0026ldquo;-P6H3M\u0026rdquo; \u0026ndash; parses as \u0026ldquo;-6 hours and -3 minutes\u0026rdquo; \u0026ldquo;-P-6H+3M\u0026rdquo; \u0026ndash; parses as \u0026ldquo;+6 hours and -3 minutes\u0026rdquo;  increase increase(Duration): Calculates the increase in the time range.\nrate rate(Duration): Calculates the per-second average rate of increase in the time range.\nirate irate(): Calculates the per-second instant rate of increase in the time range.\ntag tag({allTags -\u0026gt; }): Updates tags of samples. User can add, drop, rename and update tags.\nhistogram histogram(le: '\u0026lt;the tag name of le\u0026gt;'): Transforms less-based histogram buckets to meter system histogram buckets. le parameter represents the tag name of the bucket.\nhistogram_percentile histogram_percentile([\u0026lt;p scalar\u0026gt;]). Represents the meter-system to calculate the p-percentile (0 ≤ p ≤ 100) from the buckets.\ntime time(): Returns the number of seconds since January 1, 1970 UTC.\nDown Sampling Operation MAL should instruct meter-system on how to downsample for metrics. It doesn\u0026rsquo;t only refer to aggregate raw samples to minute level, but also expresses data from minute in higher levels, such as hour and day.\nDown sampling function is called downsampling in MAL, and it accepts the following types:\n AVG SUM LATEST MIN (TODO) MAX (TODO) MEAN (TODO) COUNT (TODO)  The default type is AVG.\nIf users want to get the latest time from last_server_state_sync_time_in_seconds:\nlast_server_state_sync_time_in_seconds.tagEqual('production', 'catalog').downsampling(LATEST) Metric level function There are three levels in metric: service, instance and endpoint. They extract level relevant labels from metric labels, then informs the meter-system the level to which this metric belongs.\n servcie([svc_label1, svc_label2...]) extracts service level labels from the array argument. instance([svc_label1, svc_label2...], [ins_label1, ins_label2...]) extracts service level labels from the first array argument, extracts instance level labels from the second array argument. endpoint([svc_label1, svc_label2...], [ep_label1, ep_label2...]) extracts service level labels from the first array argument, extracts endpoint level labels from the second array argument.  More Examples Please refer to OAP Self-Observability\n","excerpt":"Meter Analysis Language The meter system provides a functional analysis language called MAL (Meter …","ref":"/docs/main/latest/en/concepts-and-designs/mal/","title":"Meter Analysis Language"},{"body":"Meter receiver The meter receiver accepts the metrics of meter protocol into the meter system.\nModule definition receiver-meter: selector: ${SW_RECEIVER_METER:default} default: In Kafka Fetcher, follow these configurations to enable it.\nkafka-fetcher: selector: ${SW_KAFKA_FETCHER:default} default: bootstrapServers: ${SW_KAFKA_FETCHER_SERVERS:localhost:9092} enableMeterSystem: ${SW_KAFKA_FETCHER_ENABLE_METER_SYSTEM:true} Configuration file The meter receiver is configured via a configuration file. The configuration file defines everything related to receiving from agents, as well as which rule files to load.\nThe OAP can load the configuration at bootstrap. If the new configuration is not well-formed, the OAP may fail to start up. The files are located at $CLASSPATH/meter-analyzer-config.\nThe file is written in YAML format, defined by the scheme described below. Brackets indicate that a parameter is optional.\nAn example can be found here. If you\u0026rsquo;re using Spring Sleuth, see Spring Sleuth Setup.\nMeters configuration # expSuffix is appended to all expression in this file. expSuffix: \u0026lt;string\u0026gt; # insert metricPrefix into metric name: \u0026lt;metricPrefix\u0026gt;_\u0026lt;raw_metric_name\u0026gt; metricPrefix: \u0026lt;string\u0026gt; # Metrics rule allow you to recompute queries. metricsRules: # The name of rule, which combinates with a prefix \u0026#39;meter_\u0026#39; as the index/table name in storage. name: \u0026lt;string\u0026gt; # MAL expression. exp: \u0026lt;string\u0026gt; For more information on MAL, please refer to mal.md\nrate, irate, and increase Although we support the rate, irate, increase functions in the backend, we still recommend users to consider using client-side APIs to run these functions. The reasons are as follows:\n The OAP has to set up caches to calculate the values. Once the agent reconnects to another OAP instance, the time windows of rate calculation break. This leads to inaccurate results.  ","excerpt":"Meter receiver The meter receiver accepts the metrics of meter protocol into the meter system. …","ref":"/docs/main/latest/en/setup/backend/backend-meter/","title":"Meter receiver"},{"body":"Meter System Meter system is another streaming calculation mode designed for metrics data. In the OAL, there are clear Scope Definitions, including definitions for native objects. Meter system is focused on the data type itself, and provides a more flexible approach to the end user in defining the scope entity.\nThe meter system is open to different receivers and fetchers in the backend, see the backend setup document for more details.\nEvery metric is declared in the meter system to include the following attributes:\n Metrics Name. A globally unique name to avoid overlapping between the OAL variable names. Function Name. The function used for this metric, namely distributed aggregation, value calculation or down sampling calculation based on the function implementation. Further, the data structure is determined by the function as well, such as function Avg is for Long. Scope Type. Unlike within the OAL, there are plenty of logic scope definitions. In the meter system, only type is required. Type values include service, instance, and endpoint, just as we have described in the Overview section. The values of scope entity name, such as service name, are required when metrics data are generated with the metrics data values.  NOTE: The metrics must be declared in the bootstrap stage, and there must be no change to runtime.\nThe Meter System supports the following binding functions:\n avg. Calculates the avg value for every entity under the same metrics name. histogram. Aggregates the counts in the configurable buckets. Buckets are configurable but must be assigned in the declaration stage. percentile. See percentile in WIKI. Unlike the OAL, we provide 50/75/90/95/99 by default. In the meter system function, the percentile function accepts several ranks, which should be in the (0, 100) range.  ","excerpt":"Meter System Meter system is another streaming calculation mode designed for metrics data. In the …","ref":"/docs/main/latest/en/concepts-and-designs/meter/","title":"Meter System"},{"body":"Metrics Exporter SkyWalking provides basic and most important metrics aggregation, alarm and analysis. In real world, people may want to forward the data to their 3rd party system, for deeper analysis or anything else. Metrics Exporter makes that possible.\nMetrics exporter is an independent module, you need manually active it.\nRight now, we provide the following exporters\n gRPC exporter  gRPC exporter gRPC exporter uses SkyWalking native exporter service definition. Here is proto definition.\nservice MetricExportService { rpc export (stream ExportMetricValue) returns (ExportResponse) { } rpc subscription (SubscriptionReq) returns (SubscriptionsResp) { }}message ExportMetricValue { string metricName = 1; string entityName = 2; string entityId = 3; ValueType type = 4; int64 timeBucket = 5; int64 longValue = 6; double doubleValue = 7; repeated int64 longValues = 8;}message SubscriptionsResp { repeated SubscriptionMetric metrics = 1;}message SubscriptionMetric { string metricName = 1; EventType eventType = 2;}enum ValueType { LONG = 0; DOUBLE = 1; MULTI_LONG = 2;}enum EventType { // The metrics aggregated in this bulk, not include the existing persistent data.  INCREMENT = 0; // Final result of the metrics at this moment.  TOTAL = 1;}message SubscriptionReq {}message ExportResponse {}To active the exporter, you should add this into your application.yml\nexporter: grpc: targetHost: 127.0.0.1 targetPort: 9870  targetHost:targetPort is the expected target service address. You could set any gRPC server to receive the data. Target gRPC service needs to be standby, otherwise, the OAP starts up failure.  For target exporter service subscription implementation Return the expected metrics name list with event type(increment or total), all the names must match the OAL/MAL script definition. Return empty list, if you want to export all metrics in increment event type.\nexport implementation Stream service, all subscribed metrics will be sent to here, based on OAP core schedule. Also, if the OAP deployed as cluster, then this method will be called concurrently. For metrics value, you need follow #type to choose #longValue or #doubleValue.\n","excerpt":"Metrics Exporter SkyWalking provides basic and most important metrics aggregation, alarm and …","ref":"/docs/main/latest/en/setup/backend/metrics-exporter/","title":"Metrics Exporter"},{"body":"Namespace Background SkyWalking is a monitoring tool, which collects metrics from a distributed system. In the real world, a very large distributed system includes hundreds of services, thousands of service instances. In that case, most likely, more than one group, even more than one company are maintaining and monitoring the distributed system. Each one of them takes charge of different parts, don\u0026rsquo;t want or shouldn\u0026rsquo;t share there metrics.\nNamespace is the proposal from this.It is used for tracing and monitoring isolation.\nSet the namespace Set agent.namespace in agent config # The agent namespace # agent.namespace=default-namespace The default value of agent.namespace is empty.\nInfluence The default header key of SkyWalking is sw8, more in this document. After agent.namespace is set, the key changes to namespace-sw8.\nThe across process propagation chain breaks, when the two sides are using different namespace.\n","excerpt":"Namespace Background SkyWalking is a monitoring tool, which collects metrics from a distributed …","ref":"/docs/main/latest/en/setup/service-agent/java-agent/namespace/","title":"Namespace"},{"body":"Observability Analysis Language OAL(Observability Analysis Language) serves to analyze incoming data in streaming mode.\nOAL focuses on metrics in Service, Service Instance and Endpoint. Therefore, the language is easy to learn and use.\nSince 6.3, the OAL engine is embedded in OAP server runtime as oal-rt(OAL Runtime). OAL scripts are now found in the /config folder, and users could simply change and reboot the server to run them. However, the OAL script is a compiled language, and the OAL Runtime generates java codes dynamically.\nYou can open set SW_OAL_ENGINE_DEBUG=Y at system env to see which classes are generated.\nGrammar Scripts should be named *.oal\n// Declare the metrics. METRICS_NAME = from(SCOPE.(* | [FIELD][,FIELD ...])) [.filter(FIELD OP [INT | STRING])] .FUNCTION([PARAM][, PARAM ...]) // Disable hard code disable(METRICS_NAME); Scope Primary SCOPEs are All, Service, ServiceInstance, Endpoint, ServiceRelation, ServiceInstanceRelation, and EndpointRelation. There are also some secondary scopes which belong to a primary scope.\nSee Scope Definitions, where you can find all existing Scopes and Fields.\nFilter Use filter to build conditions for the value of fields by using field name and expression.\nThe expressions support linking by and, or and (...). The OPs support ==, !=, \u0026gt;, \u0026lt;, \u0026gt;=, \u0026lt;=, in [...] ,like %..., like ...% , like %...% , contain and not contain, with type detection based on field type. In the event of incompatibility, compile or code generation errors may be triggered.\nAggregation Function The default functions are provided by the SkyWalking OAP core, and it is possible to implement additional functions.\nFunctions provided\n longAvg. The avg of all input per scope entity. The input field must be a long.   instance_jvm_memory_max = from(ServiceInstanceJVMMemory.max).longAvg();\n In this case, the input represents the request of each ServiceInstanceJVMMemory scope, and avg is based on field max.\n doubleAvg. The avg of all input per scope entity. The input field must be a double.   instance_jvm_cpu = from(ServiceInstanceJVMCPU.usePercent).doubleAvg();\n In this case, the input represents the request of each ServiceInstanceJVMCPU scope, and avg is based on field usePercent.\n percent. The number or ratio is expressed as a fraction of 100, where the input matches with the condition.   endpoint_percent = from(Endpoint.*).percent(status == true);\n In this case, all input represents requests of each endpoint, and the condition is endpoint.status == true.\n rate. The rate expressed is as a fraction of 100, where the input matches with the condition.   browser_app_error_rate = from(BrowserAppTraffic.*).rate(trafficCategory == BrowserAppTrafficCategory.FIRST_ERROR, trafficCategory == BrowserAppTrafficCategory.NORMAL);\n In this case, all input represents requests of each browser app traffic, the numerator condition is trafficCategory == BrowserAppTrafficCategory.FIRST_ERROR and denominator condition is trafficCategory == BrowserAppTrafficCategory.NORMAL. Parameter (1) is the numerator condition. Parameter (2) is the denominator condition.\n count. The sum of calls per scope entity.   service_calls_sum = from(Service.*).count();\n In this case, the number of calls of each service.\n histogram. See Heatmap in WIKI.   all_heatmap = from(All.latency).histogram(100, 20);\n In this case, the thermodynamic heatmap of all incoming requests. Parameter (1) is the precision of latency calculation, such as in the above case, where 113ms and 193ms are considered the same in the 101-200ms group. Parameter (2) is the group amount. In the above case, 21(param value + 1) groups are 0-100ms, 101-200ms, \u0026hellip; 1901-2000ms, 2000+ms\n apdex. See Apdex in WIKI.   service_apdex = from(Service.latency).apdex(name, status);\n In this case, the apdex score of each service. Parameter (1) is the service name, which reflects the Apdex threshold value loaded from service-apdex-threshold.yml in the config folder. Parameter (2) is the status of this request. The status(success/failure) reflects the Apdex calculation.\n p99, p95, p90, p75, p50. See percentile in WIKI.   all_percentile = from(All.latency).percentile(10);\n percentile is the first multiple-value metric, which has been introduced since 7.0.0. As a metric with multiple values, it could be queried through the getMultipleLinearIntValues GraphQL query. In this case, see p99, p95, p90, p75, and p50 of all incoming requests. The parameter is precise to a latency at p99, such as in the above case, and 120ms and 124ms are considered to produce the same response time. Before 7.0.0, p99, p95, p90, p75, p50 func(s) are used to calculate metrics separately. They are still supported in 7.x, but they are no longer recommended and are not included in the current official OAL script.\n all_p99 = from(All.latency).p99(10);\n In this case, the p99 value of all incoming requests. The parameter is precise to a latency at p99, such as in the above case, and 120ms and 124ms are considered to produce the same response time.\nMetrics name The metrics name for storage implementor, alarm and query modules. The type inference is supported by core.\nGroup All metrics data will be grouped by Scope.ID and min-level TimeBucket.\n In the Endpoint scope, the Scope.ID is same as the Endpoint ID (i.e. the unique ID based on service and its endpoint).  Disable Disable is an advanced statement in OAL, which is only used in certain cases. Some of the aggregation and metrics are defined through core hard codes. Examples include segment and top_n_database_statement. This disable statement is designed to render them inactive. By default, none of them are disabled.\nNOTICE, all disable statements should be in oal/disable.oal script file.\nExamples // Calculate p99 of both Endpoint1 and Endpoint2 endpoint_p99 = from(Endpoint.latency).filter(name in (\u0026quot;Endpoint1\u0026quot;, \u0026quot;Endpoint2\u0026quot;)).summary(0.99) // Calculate p99 of Endpoint name started with `serv` serv_Endpoint_p99 = from(Endpoint.latency).filter(name like \u0026quot;serv%\u0026quot;).summary(0.99) // Calculate the avg response time of each Endpoint endpoint_avg = from(Endpoint.latency).avg() // Calculate the p50, p75, p90, p95 and p99 of each Endpoint by 50 ms steps. endpoint_percentile = from(Endpoint.latency).percentile(10) // Calculate the percent of response status is true, for each service. endpoint_success = from(Endpoint.*).filter(status == true).percent() // Calculate the sum of response code in [404, 500, 503], for each service. endpoint_abnormal = from(Endpoint.*).filter(responseCode in [404, 500, 503]).count() // Calculate the sum of request type in [RequestType.RPC, RequestType.gRPC], for each service. endpoint_rpc_calls_sum = from(Endpoint.*).filter(type in [RequestType.RPC, RequestType.gRPC]).count() // Calculate the sum of endpoint name in [\u0026quot;/v1\u0026quot;, \u0026quot;/v2\u0026quot;], for each service. endpoint_url_sum = from(Endpoint.*).filter(name in [\u0026quot;/v1\u0026quot;, \u0026quot;/v2\u0026quot;]).count() // Calculate the sum of calls for each service. endpoint_calls = from(Endpoint.*).count() // Calculate the CPM with the GET method for each service.The value is made up with `tagKey:tagValue`. service_cpm_http_get = from(Service.*).filter(tags contain \u0026quot;http.method:GET\u0026quot;).cpm() // Calculate the CPM with the HTTP method except for the GET method for each service.The value is made up with `tagKey:tagValue`. service_cpm_http_other = from(Service.*).filter(tags not contain \u0026quot;http.method:GET\u0026quot;).cpm() disable(segment); disable(endpoint_relation_server_side); disable(top_n_database_statement); ","excerpt":"Observability Analysis Language OAL(Observability Analysis Language) serves to analyze incoming data …","ref":"/docs/main/latest/en/concepts-and-designs/oal/","title":"Observability Analysis Language"},{"body":"Observability Analysis Platform SkyWalking is an Observability Analysis Platform that provides full observability to services running in both brown and green zones, as well as services using a hybrid model.\nCapabilities SkyWalking covers all 3 areas of observability, including, Tracing, Metrics and Logging.\n Tracing. SkyWalking native data formats, including Zipkin v1 and v2, as well as Jaeger. Metrics. SkyWalking integrates with Service Mesh platforms, such as Istio, Envoy, and Linkerd, to build observability into the data panel or control panel. Also, SkyWalking native agents can run in the metrics mode, which greatly improves performances. Logging. Includes logs collected from disk or through network. Native agents could bind the tracing context with logs automatically, or use SkyWalking to bind the trace and log through the text content.  There are 3 powerful and native language engines designed to analyze observability data from the above areas.\n Observability Analysis Language processes native traces and service mesh data. Meter Analysis Language is responsible for metrics calculation for native meter data, and adopts a stable and widely used metrics system, such as Prometheus and OpenTelemetry. Log Analysis Language focuses on log contents and collaborate with Meter Analysis Language.  ","excerpt":"Observability Analysis Platform SkyWalking is an Observability Analysis Platform that provides full …","ref":"/docs/main/latest/en/concepts-and-designs/backend-overview/","title":"Observability Analysis Platform"},{"body":"Observe Service Mesh through ALS Envoy Access Log Service (ALS) provides full logs about RPC routed, including HTTP and TCP.\nBackground The solution was initialized and firstly implemented by Sheng Wu, Hongtao Gao, Lizan Zhou, and Dhi Aurrahman at 17 May. 2019, and was presented on KubeCon China 2019. Here is the recorded video.\nSkyWalking is the first open source project introducing this ALS based solution to the world. This provides a new way with very low payload to service mesh, but the same observability.\nEnable ALS and SkyWalking Receiver You need the following steps to set up ALS.\n  Enable envoyAccessLogService in ProxyConfig and set the ALS address to where SkyWalking OAP listens. On Istio version 1.6.0+, if Istio is installed with demo profile, you can enable ALS with command:\nistioctl manifest apply \\  --set profile=demo \\  --set meshConfig.enableEnvoyAccessLogService=true \\  --set meshConfig.defaultConfig.envoyAccessLogService.address=\u0026lt;skywalking-oap.skywalking.svc:11800\u0026gt; Note: Replace \u0026lt;skywalking-oap.skywalking.svc:11800\u0026gt; with the real address where SkyWalking OAP is deployed.\n  Activate SkyWalking Envoy Receiver. This is activated by default.\n  Choose an ALS analyzer. There are two available analyzers, k8s-mesh and mx-mesh for both HTTP access logs and TCP access logs. Set the system environment variable SW_ENVOY_METRIC_ALS_HTTP_ANALYSIS and SW_ENVOY_METRIC_ALS_TCP_ANALYSIS such as SW_ENVOY_METRIC_ALS_HTTP_ANALYSIS=mx-mesh, SW_ENVOY_METRIC_ALS_TCP_ANALYSIS=mx-mesh or in the application.yaml to activate the analyzer. For more about the analyzers, see SkyWalking ALS Analyzers\nenvoy-metric: selector: ${SW_ENVOY_METRIC:default} default: acceptMetricsService: ${SW_ENVOY_METRIC_SERVICE:true} alsHTTPAnalysis: ${SW_ENVOY_METRIC_ALS_HTTP_ANALYSIS:\u0026#34;\u0026#34;} # Setting the system env variable would override this.  alsTCPAnalysis: ${SW_ENVOY_METRIC_ALS_TCP_ANALYSIS:\u0026#34;\u0026#34;} To use multiple analyzers as a fallback，please use , to concatenate.\n  Example Here\u0026rsquo;s an example to install Istio and deploy SkyWalking by Helm chart.\nistioctl install \\  --set profile=demo \\  --set meshConfig.enableEnvoyAccessLogService=true \\  --set meshConfig.defaultConfig.envoyAccessLogService.address=skywalking-oap.istio-system:11800 git clone https://github.com/apache/skywalking-kubernetes.git cd skywalking-kubernetes/chart helm repo add elastic https://helm.elastic.co helm dep up skywalking helm install 8.1.0 skywalking -n istio-system \\  --set oap.env.SW_ENVOY_METRIC_ALS_HTTP_ANALYSIS=mx-mesh \\  --set oap.env.SW_ENVOY_METRIC_ALS_TCP_ANALYSIS=mx-mesh \\  --set fullnameOverride=skywalking \\  --set oap.envoy.als.enabled=true You can use kubectl -n istio-system logs -l app=skywalking | grep \u0026quot;K8sALSServiceMeshHTTPAnalysis\u0026quot; to ensure OAP ALS mx-mesh analyzer has been activated.\nSkyWalking ALS Analyzers There are several available analyzers, k8s-mesh, mx-mesh and persistence, you can specify one or more analyzers to analyze the access logs. When multiple analyzers are specified, it acts as a fast-success mechanism: SkyWalking loops over the analyzers and use it to analyze the logs, once there is an analyzer that is able to produce a result, it stops the loop.\nk8s-mesh k8s-mesh uses the metadata from Kubernetes cluster, hence in this analyzer OAP needs access roles to Pod, Service, and Endpoints.\nThe blog illustrates the detail of how it works, and a step-by-step tutorial to apply it into the bookinfo application.\nmx-mesh mx-mesh uses the Envoy metadata exchange mechanism to get the service name, etc., this analyzer requires Istio to enable the metadata exchange plugin (you can enable it by --set values.telemetry.v2.enabled=true, or if you\u0026rsquo;re using Istio 1.7+ and installing it with profile demo/preview, it should be enabled then).\nThe blog illustrates the detail of how it works, and a step-by-step tutorial to apply it into the Online Boutique system.\npersistence persistence analyzer adapts the Envoy access log format to SkyWalking\u0026rsquo;s native log format , and forwards the formatted logs to LAL, where you can configure persistent conditions, such as sampler, only persist error logs, etc. SkyWalking provides a default configuration file envoy-als.yaml that you can adjust as per your needs. Please make sure to activate this rule via adding the rule name envoy-als into config item log-analyzer/default/lalFiles (or environment variable SW_LOG_LAL_FILES, e.g. SW_LOG_LAL_FILES=envoy-als).\nAttention: because persistence analyzer also needs a mechanism to map the logs into responding services, hence, you need to configure at least one of k8s-mesh or mx-mesh as its antecedent so that persistence analyzer knows which service the logs belong to. For example, you should set envoy-metric/default/alsHTTPAnalysis (or environment variable SW_ENVOY_METRIC_ALS_HTTP_ANALYSIS) to something like k8s-mesh,persistence, mx-mesh,persistence or mx-mesh,k8s-mesh,persistence.\n","excerpt":"Observe Service Mesh through ALS Envoy Access Log Service (ALS) provides full logs about RPC routed, …","ref":"/docs/main/latest/en/setup/envoy/als_setting/","title":"Observe Service Mesh through ALS"},{"body":"Official OAL script First, read the OAL introduction.\nFrom 8.0.0, you may find the OAL script at /config/oal/*.oal of the SkyWalking dist. You could change it, such as by adding filter conditions or new metrics. Then, reboot the OAP server and it will come into effect.\nAll metrics named in this script may be used in alarm and UI query.\nNote: If you try to add or remove certain metrics, there is a possibility that the UI would break. You should only do this when you plan to build your own UI based on the customization analysis core.\n","excerpt":"Official OAL script First, read the OAL introduction.\nFrom 8.0.0, you may find the OAL script at …","ref":"/docs/main/latest/en/guides/backend-oal-scripts/","title":"Official OAL script"},{"body":"Open Fetcher Fetcher is a concept in SkyWalking backend. When reading data from target systems, the pull mode is more suitable than the receiver. This mode is typically found in metrics SDKs, such as Prometheus.\nPrometheus Fetcher Suppose you want to enable some metric-custom.yaml files stored at fetcher-prom-rules, append its name to enabledRules of prometheus-fetcher as follows:\nprometheus-fetcher: selector: ${SW_PROMETHEUS_FETCHER:default} default: enabledRules: ${SW_PROMETHEUS_FETCHER_ENABLED_RULES:\u0026#34;self,metric-custom\u0026#34;} Configuration file Prometheus fetcher is configured via a configuration file. The configuration file defines everything related to fetching services and their instances, as well as which rule files to load.\nThe OAP can load the configuration at bootstrap. If the new configuration is not well-formed, the OAP fails to start up. The files are located at $CLASSPATH/fetcher-prom-rules.\nThe file is written in YAML format, defined by the scheme described below. Brackets indicate that a parameter is optional.\nA full example can be found here\nGeneric placeholders are defined as follows:\n \u0026lt;duration\u0026gt;: This is parsed into a textual representation of a duration. The formats accepted are based on the ISO-8601 duration format PnDTnHnMn.nS with days considered to be exactly 24 hours. \u0026lt;labelname\u0026gt;: A string matching the regular expression [a-zA-Z_][a-zA-Z0-9_]*. \u0026lt;labelvalue\u0026gt;: A string of unicode characters. \u0026lt;host\u0026gt;: A valid string consisting of a hostname or IP followed by an optional port number. \u0026lt;path\u0026gt;: A valid URL path. \u0026lt;string\u0026gt;: A regular string.  # How frequently to fetch targets. fetcherInterval: \u0026lt;duration\u0026gt; # Per-fetch timeout when fetching this target. fetcherTimeout: \u0026lt;duration\u0026gt; # The HTTP resource path on which to fetch metrics from targets. metricsPath: \u0026lt;path\u0026gt; #Statically configured targets. staticConfig: # The targets specified by the static config. targets: [ - \u0026lt;target\u0026gt; ] # Labels assigned to all metrics fetched from the targets. labels: [ \u0026lt;labelname\u0026gt;: \u0026lt;labelvalue\u0026gt; ... ] # expSuffix is appended to all expression in this file. expSuffix: \u0026lt;string\u0026gt; # insert metricPrefix into metric name: \u0026lt;metricPrefix\u0026gt;_\u0026lt;raw_metric_name\u0026gt; metricPrefix: \u0026lt;string\u0026gt; # Metrics rule allow you to recompute queries. metricsRules: [ - \u0026lt;metric_rules\u0026gt; ]  # The url of target exporter. the format should be complied with \u0026#34;java.net.URI\u0026#34; url: \u0026lt;string\u0026gt; # The path of root CA file. sslCaFilePath: \u0026lt;string\u0026gt; \u0026lt;metric_rules\u0026gt; # The name of rule, which combinates with a prefix \u0026#39;meter_\u0026#39; as the index/table name in storage. name: \u0026lt;string\u0026gt; # MAL expression. exp: \u0026lt;string\u0026gt; To know more about MAL, please refer to mal.md\nKafka Fetcher The Kafka Fetcher pulls messages from the Kafka Broker to learn about what agent is delivered. Check the agent documentation for details. Typically, tracing segments, service/instance properties, JVM metrics, and meter system data are supported. Kafka Fetcher can work with gRPC/HTTP Receivers at the same time for adopting different transport protocols.\nKafka Fetcher is disabled by default. To enable it, configure as follows.\nNamespace aims to isolate multi OAP cluster when using the same Kafka cluster. If you set a namespace for Kafka fetcher, the OAP will add a prefix to topic name. You should also set namespace in the property named plugin.kafka.namespace in agent.config.\nkafka-fetcher: selector: ${SW_KAFKA_FETCHER:default} default: bootstrapServers: ${SW_KAFKA_FETCHER_SERVERS:localhost:9092} namespace: ${SW_NAMESPACE:\u0026#34;\u0026#34;} skywalking-segments, skywalking-metrics, skywalking-profile, skywalking-managements, skywalking-meters, skywalking-logs and skywalking-logs-json topics are required by kafka-fetcher. If they do not exist, Kafka Fetcher will create them by default. Also, you can create them by yourself before the OAP server starts.\nWhen using the OAP server automatic creation mechanism, you could modify the number of partitions and replications of the topics using the following configurations:\nkafka-fetcher: selector: ${SW_KAFKA_FETCHER:default} default: bootstrapServers: ${SW_KAFKA_FETCHER_SERVERS:localhost:9092} namespace: ${SW_NAMESPACE:\u0026#34;\u0026#34;} partitions: ${SW_KAFKA_FETCHER_PARTITIONS:3} replicationFactor: ${SW_KAFKA_FETCHER_PARTITIONS_FACTOR:2} enableMeterSystem: ${SW_KAFKA_FETCHER_ENABLE_METER_SYSTEM:false} isSharding: ${SW_KAFKA_FETCHER_IS_SHARDING:false} consumePartitions: ${SW_KAFKA_FETCHER_CONSUME_PARTITIONS:\u0026#34;\u0026#34;} In the cluster mode, all topics have the same number of partitions. Set \u0026quot;isSharding\u0026quot; to \u0026quot;true\u0026quot; and assign the partitions to consume for the OAP server. Use commas to separate multiple partitions for the OAP server.\nThe Kafka Fetcher allows you to configure all the Kafka producers listed here in property kafkaConsumerConfig. For example:\nkafka-fetcher: selector: ${SW_KAFKA_FETCHER:default} default: bootstrapServers: ${SW_KAFKA_FETCHER_SERVERS:localhost:9092} namespace: ${SW_NAMESPACE:\u0026#34;\u0026#34;} partitions: ${SW_KAFKA_FETCHER_PARTITIONS:3} replicationFactor: ${SW_KAFKA_FETCHER_PARTITIONS_FACTOR:2} enableMeterSystem: ${SW_KAFKA_FETCHER_ENABLE_METER_SYSTEM:false} isSharding: ${SW_KAFKA_FETCHER_IS_SHARDING:true} consumePartitions: ${SW_KAFKA_FETCHER_CONSUME_PARTITIONS:1,3,5} kafkaConsumerConfig: enable.auto.commit: true ... When using Kafka MirrorMaker 2.0 to replicate topics between Kafka clusters, you can set the source Kafka Cluster alias (mm2SourceAlias) and separator (mm2SourceSeparator) according to your Kafka MirrorMaker config.\nkafka-fetcher: selector: ${SW_KAFKA_FETCHER:default} default: bootstrapServers: ${SW_KAFKA_FETCHER_SERVERS:localhost:9092} namespace: ${SW_NAMESPACE:\u0026#34;\u0026#34;} partitions: ${SW_KAFKA_FETCHER_PARTITIONS:3} replicationFactor: ${SW_KAFKA_FETCHER_PARTITIONS_FACTOR:2} enableMeterSystem: ${SW_KAFKA_FETCHER_ENABLE_METER_SYSTEM:false} isSharding: ${SW_KAFKA_FETCHER_IS_SHARDING:true} consumePartitions: ${SW_KAFKA_FETCHER_CONSUME_PARTITIONS:1,3,5} mm2SourceAlias: ${SW_KAFKA_MM2_SOURCE_ALIAS:\u0026#34;\u0026#34;} mm2SourceSeparator: ${SW_KAFKA_MM2_SOURCE_SEPARATOR:\u0026#34;\u0026#34;} kafkaConsumerConfig: enable.auto.commit: true ... ","excerpt":"Open Fetcher Fetcher is a concept in SkyWalking backend. When reading data from target systems, the …","ref":"/docs/main/latest/en/setup/backend/backend-fetcher/","title":"Open Fetcher"},{"body":"Oracle and Resin plugins These plugins can\u0026rsquo;t be provided in Apache release because of Oracle and Resin Licenses. If you want to know details, please read Apache license legal document\nDue to license incompatibilities/restrictions these plugins are hosted and released in 3rd part repository, go to OpenSkywalking java plugin extension repository to get these.\n","excerpt":"Oracle and Resin plugins These plugins can\u0026rsquo;t be provided in Apache release because of Oracle …","ref":"/docs/main/latest/en/setup/service-agent/java-agent/agent-optional-plugins/oracle-resin-plugins/","title":"Oracle and Resin plugins"},{"body":"Overview SkyWalking is an open source observability platform used to collect, analyze, aggregate and visualize data from services and cloud native infrastructures. SkyWalking provides an easy way to maintain a clear view of your distributed systems, even across Clouds. It is a modern APM, specially designed for cloud native, container based distributed systems.\nWhy use SkyWalking? SkyWalking provides solutions for observing and monitoring distributed systems, in many different scenarios. First of all, like traditional approaches, SkyWalking provides auto instrument agents for services, such as Java, C#, Node.js, Go, PHP and Nginx LUA. (with calls out for Python and C++ SDK contributions). In multi-language, continuously deployed environments, cloud native infrastructures grow more powerful but also more complex. SkyWalking\u0026rsquo;s service mesh receiver allows SkyWalking to receive telemetry data from service mesh frameworks such as Istio/Envoy and Linkerd, allowing users to understand the entire distributed system.\nSkyWalking provides observability capabilities for service(s), service instance(s), endpoint(s). The terms Service, Instance and Endpoint are used everywhere today, so it is worth defining their specific meanings in the context of SkyWalking:\n Service. Represents a set/group of workloads which provide the same behaviours for incoming requests. You can define the service name when you are using instrument agents or SDKs. SkyWalking can also use the name you define in platforms such as Istio. Service Instance. Each individual workload in the Service group is known as an instance. Like pods in Kubernetes, it doesn\u0026rsquo;t need to be a single OS process, however, if you are using instrument agents, an instance is actually a real OS process. Endpoint. A path in a service for incoming requests, such as an HTTP URI path or a gRPC service class + method signature.  SkyWalking allows users to understand the topology relationship between Services and Endpoints, to view the metrics of every Service/Service Instance/Endpoint and to set alarm rules.\nIn addition, you can integrate\n Other distributed tracing using SkyWalking native agents and SDKs with Zipkin, Jaeger and OpenCensus. Other metrics systems, such as Prometheus, Sleuth(Micrometer), OpenTelemetry.  Architecture SkyWalking is logically split into four parts: Probes, Platform backend, Storage and UI.\n Probes collect data and reformat them for SkyWalking requirements (different probes support different sources). Platform backend supports data aggregation, analysis and streaming process covers traces, metrics, and logs. Storage houses SkyWalking data through an open/plugable interface. You can choose an existing implementation, such as ElasticSearch, H2, MySQL, TiDB, InfluxDB, or implement your own. Patches for new storage implementors welcome! UI is a highly customizable web based interface allowing SkyWalking end users to visualize and manage SkyWalking data.  What is next?  Learn SkyWalking\u0026rsquo;s Project Goals FAQ, Why doesn\u0026rsquo;t SkyWalking involve MQ in the architecture in default?  ","excerpt":"Overview SkyWalking is an open source observability platform used to collect, analyze, aggregate and …","ref":"/docs/main/latest/en/concepts-and-designs/overview/","title":"Overview"},{"body":"Plugin automatic test framework The plugin test framework is designed to verify the function and compatibility of plugins. As there are dozens of plugins and hundreds of versions that need to be verified, it is impossible to do it manually. The test framework uses container-based tech stack and requires a set of real services with the agents installed. Then, the test mock OAP backend runs to check the segments data sent from agents.\nEvery plugin maintained in the main repo requires corresponding test cases as well as matching versions in the supported list doc.\nEnvironment Requirements  MacOS/Linux JDK 8+ Docker Docker Compose  Case Base Image Introduction The test framework provides JVM-container and Tomcat-container base images including JDK8 and JDK14. You can choose the best one for your test case. If both are suitable for your case, JVM-container is preferred.\nJVM-container Image Introduction JVM-container uses openjdk:8 as the base image. JVM-container supports JDK14, which inherits openjdk:14. The test case project must be packaged as project-name.zip, including startup.sh and uber jar, by using mvn clean package.\nTake the following test projects as examples:\n sofarpc-scenario is a single project case. webflux-scenario is a case including multiple projects. jdk14-with-gson-scenario is a single project case with JDK14.  Tomcat-container Image Introduction Tomcat-container uses tomcat:8.5.57-jdk8-openjdk or tomcat:8.5.57-jdk14-openjdk as the base image. The test case project must be packaged as project-name.war by using mvn package.\nTake the following test project as an example\n spring-4.3.x-scenario  Test project hierarchical structure The test case is an independent maven project, and it must be packaged as a war tar ball or zip file, depending on the chosen base image. Also, two external accessible endpoints usually two URLs) are required.\nAll test case codes should be in the org.apache.skywalking.apm.testcase.* package. If there are some codes expected to be instrumented, then the classes could be in the test.org.apache.skywalking.apm.testcase.* package.\nJVM-container test project hierarchical structure\n[plugin-scenario] |- [bin] |- startup.sh |- [config] |- expectedData.yaml |- [src] |- [main] |- ... |- [resource] |- log4j2.xml |- pom.xml |- configuration.yaml |- support-version.list [] = directory Tomcat-container test project hierarchical structure\n[plugin-scenario] |- [config] |- expectedData.yaml |- [src] |- [main] |- ... |- [resource] |- log4j2.xml |- [webapp] |- [WEB-INF] |- web.xml |- pom.xml |- configuration.yaml |- support-version.list [] = directory Test case configuration files The following files are required in every test case.\n   File Name Descriptions     configuration.yml Declare the basic case information, including case name, entrance endpoints, mode, and dependencies.   expectedData.yaml Describe the expected segmentItems.   support-version.list List the target versions for this case.   startup.sh JVM-container only. This is not required when using Tomcat-container.    * support-version.list format requires every line for a single version (contains only the last version number of each minor version). You may use # to comment out this version.\nconfiguration.yml    Field description     type Image type, options, jvm, or tomcat. Required.   entryService The entrance endpoint (URL) for test case access. Required. (HTTP Method: GET)   healthCheck The health check endpoint (URL) for test case access. Required. (HTTP Method: HEAD)   startScript Path of the start up script. Required in type: jvm only.   runningMode Running mode with the optional plugin, options, default(default), with_optional, or with_bootstrap.   withPlugins Plugin selector rule, e.g.:apm-spring-annotation-plugin-*.jar. Required for runningMode=with_optional or runningMode=with_bootstrap.   environment Same as docker-compose#environment.   depends_on Same as docker-compose#depends_on.   dependencies Same as docker-compose#services, image, links, hostname, environment and depends_on are supported.    Note:, docker-compose activates only when dependencies is blank.\nrunningMode option description.\n   Option description     default Activate all plugins in plugin folder like the official distribution agent.   with_optional Activate default and plugins in optional-plugin by the give selector.   with_bootstrap Activate default and plugins in bootstrap-plugin by the give selector.    with_optional/with_bootstrap supports multiple selectors, separated by ;.\nFile Format\ntype: entryService: healthCheck: startScript: runningMode: withPlugins: environment: ... depends_on: ... dependencies: service1: image: hostname: expose: ... environment: ... depends_on: ... links: ... entrypoint: ... healthcheck: ...  dependencies support docker compose healthcheck. But the format is a little different. We need to have - as the start of every config item, and describe it as a string line.  For example, in the official document, the health check is:\nhealthcheck: test: [\u0026#34;CMD\u0026#34;, \u0026#34;curl\u0026#34;, \u0026#34;-f\u0026#34;, \u0026#34;http://localhost\u0026#34;] interval: 1m30s timeout: 10s retries: 3 start_period: 40s Here you should write:\nhealthcheck: - \u0026#39;test: [\u0026#34;CMD\u0026#34;, \u0026#34;curl\u0026#34;, \u0026#34;-f\u0026#34;, \u0026#34;http://localhost\u0026#34;]\u0026#39; - \u0026#34;interval: 1m30s\u0026#34; - \u0026#34;timeout: 10s\u0026#34; - \u0026#34;retries: 3\u0026#34; - \u0026#34;start_period: 40s\u0026#34; In some cases, the dependency service (usually a third-party server like the SolrJ server) is required to keep the same version as the client lib version, which is defined as ${test.framework.version} in pom. You may use ${CASE_SERVER_IMAGE_VERSION} as the version number, which will be changed in the test for each version.\n It does not support resource related configurations, such as volumes, ports, and ulimits. The reason for this is that in test scenarios, no mapping is required for any port to the host VM, or to mount any folder.\n Take the following test cases as examples:\n dubbo-2.7.x with JVM-container jetty with JVM-container gateway with runningMode canal with docker-compose  expectedData.yaml Operator for number\n   Operator Description     nq Not equal   eq Equal(default)   ge Greater than or equal   gt Greater than    Operator for String\n   Operator Description     not null Not null   null Null or empty String   eq Equal(default)    Expected Data Format Of The Segment\nsegmentItems: - serviceName: SERVICE_NAME(string) segmentSize: SEGMENT_SIZE(int) segments: - segmentId: SEGMENT_ID(string) spans: ...    Field Description     serviceName Service Name.   segmentSize The number of segments is expected.   segmentId Trace ID.   spans Segment span list. In the next section, you will learn how to describe each span.    Expected Data Format Of The Span\nNote: The order of span list should follow the order of the span finish time.\noperationName: OPERATION_NAME(string) parentSpanId: PARENT_SPAN_ID(int) spanId: SPAN_ID(int) startTime: START_TIME(int) endTime: END_TIME(int) isError: IS_ERROR(string: true, false) spanLayer: SPAN_LAYER(string: DB, RPC_FRAMEWORK, HTTP, MQ, CACHE) spanType: SPAN_TYPE(string: Exit, Entry, Local) componentId: COMPONENT_ID(int) tags: - {key: TAG_KEY(string), value: TAG_VALUE(string)} ... logs: - {key: LOG_KEY(string), value: LOG_VALUE(string)} ... peer: PEER(string) refs: - { traceId: TRACE_ID(string), parentTraceSegmentId: PARENT_TRACE_SEGMENT_ID(string), parentSpanId: PARENT_SPAN_ID(int), parentService: PARENT_SERVICE(string), parentServiceInstance: PARENT_SERVICE_INSTANCE(string), parentEndpoint: PARENT_ENDPOINT_NAME(string), networkAddress: NETWORK_ADDRESS(string), refType: REF_TYPE(string: CrossProcess, CrossThread) } ...    Field Description     operationName Span Operation Name.   parentSpanId Parent span ID. Note: The parent span ID of the first span should be -1.   spanId Span ID. Note: Start from 0.   startTime Span start time. It is impossible to get the accurate time, not 0 should be enough.   endTime Span finish time. It is impossible to get the accurate time, not 0 should be enough.   isError Span status, true or false.   componentId Component id for your plugin.   tags Span tag list. Notice, Keep in the same order as the plugin coded.   logs Span log list. Notice, Keep in the same order as the plugin coded.   SpanLayer Options, DB, RPC_FRAMEWORK, HTTP, MQ, CACHE.   SpanType Span type, options, Exit, Entry or Local.   peer Remote network address, IP + port mostly. For exit span, this should be required.    The verify description for SegmentRef\n   Field Description     traceId    parentTraceSegmentId Parent SegmentId, pointing to the segment id in the parent segment.   parentSpanId Parent SpanID, pointing to the span id in the parent segment.   parentService The service of parent/downstream service name.   parentServiceInstance The instance of parent/downstream service instance name.   parentEndpoint The endpoint of parent/downstream service.   networkAddress The peer value of parent exit span.   refType Ref type, options, CrossProcess or CrossThread.    Expected Data Format Of The Meter Items\nmeterItems: - serviceName: SERVICE_NAME(string) meterSize: METER_SIZE(int) meters: - ...    Field Description     serviceName Service Name.   meterSize The number of meters is expected.   meters meter list. Follow the next section to see how to describe every meter.    Expected Data Format Of The Meter\nmeterId: name: NAME(string) tags: - {name: TAG_NAME(string), value: TAG_VALUE(string)} singleValue: SINGLE_VALUE(double) histogramBuckets: - HISTOGRAM_BUCKET(double) ... The verify description for MeterId\n   Field Description     name meter name.   tags meter tags.   tags.name tag name.   tags.value tag value.   singleValue counter or gauge value. Using condition operate of the number to validate, such as gt, ge. If current meter is histogram, don\u0026rsquo;t need to write this field.   histogramBuckets histogram bucket. The bucket list must be ordered. The tool assert at least one bucket of the histogram having nonzero count. If current meter is counter or gauge, don\u0026rsquo;t need to write this field.    startup.sh This script provide a start point to JVM based service, most of them starts by a java -jar, with some variables. The following system environment variables are available in the shell.\n   Variable Description     agent_opts Agent plugin opts, check the detail in plugin doc or the same opt added in this PR.   SCENARIO_NAME Service name. Default same as the case folder name   SCENARIO_VERSION Version   SCENARIO_ENTRY_SERVICE Entrance URL to access this service   SCENARIO_HEALTH_CHECK_URL Health check URL     ${agent_opts} is required to add into your java -jar command, which including the parameter injected by test framework, and make agent installed. All other parameters should be added after ${agent_opts}.\n The test framework will set the service name as the test case folder name by default, but in some cases, there are more than one test projects are required to run in different service codes, could set it explicitly like the following example.\nExample\nhome=\u0026#34;$(cd \u0026#34;$(dirname $0)\u0026#34;; pwd)\u0026#34; java -jar ${agent_opts} \u0026#34;-Dskywalking.agent.service_name=jettyserver-scenario\u0026#34; ${home}/../libs/jettyserver-scenario.jar \u0026amp; sleep 1 java -jar ${agent_opts} \u0026#34;-Dskywalking.agent.service_name=jettyclient-scenario\u0026#34; ${home}/../libs/jettyclient-scenario.jar \u0026amp;  Only set this or use other skywalking options when it is really necessary.\n Take the following test cases as examples\n undertow webflux  Best Practices How To Use The Archetype To Create A Test Case Project We provided archetypes and a script to make creating a project easier. It creates a completed project of a test case. So that we only need to focus on cases. First, we can use followed command to get usage about the script.\nbash ${SKYWALKING_HOME}/test/plugin/generator.sh\nThen, runs and generates a project, named by scenario_name, in ./scenarios.\nRecommendations for pom \u0026lt;properties\u0026gt; \u0026lt;!-- Provide and use this property in the pom. --\u0026gt; \u0026lt;!-- This version should match the library version, --\u0026gt; \u0026lt;!-- in this case, http components lib version 4.3. --\u0026gt; \u0026lt;test.framework.version\u0026gt;4.3\u0026lt;/test.framework.version\u0026gt; \u0026lt;/properties\u0026gt; \u0026lt;dependencies\u0026gt; \u0026lt;dependency\u0026gt; \u0026lt;groupId\u0026gt;org.apache.httpcomponents\u0026lt;/groupId\u0026gt; \u0026lt;artifactId\u0026gt;httpclient\u0026lt;/artifactId\u0026gt; \u0026lt;version\u0026gt;${test.framework.version}\u0026lt;/version\u0026gt; \u0026lt;/dependency\u0026gt; ... \u0026lt;/dependencies\u0026gt; \u0026lt;build\u0026gt; \u0026lt;!-- Set the package final name as same as the test case folder case. --\u0026gt; \u0026lt;finalName\u0026gt;httpclient-4.3.x-scenario\u0026lt;/finalName\u0026gt; .... \u0026lt;/build\u0026gt; How To Implement Heartbeat Service Heartbeat service is designed for checking the service available status. This service is a simple HTTP service, returning 200 means the target service is ready. Then the traffic generator will access the entry service and verify the expected data. User should consider to use this service to detect such as whether the dependent services are ready, especially when dependent services are database or cluster.\nNotice, because heartbeat service could be traced fully or partially, so, segmentSize in expectedData.yaml should use ge as the operator, and don\u0026rsquo;t include the segments of heartbeat service in the expected segment data.\nThe example Process of Writing Tracing Expected Data Expected data file, expectedData.yaml, include SegmentItems part.\nWe are using the HttpClient plugin to show how to write the expected data.\nThere are two key points of testing\n Whether is HttpClient span created. Whether the ContextCarrier created correctly, and propagates across processes.  +-------------+ +------------------+ +-------------------------+ | Browser | | Case Servlet | | ContextPropagateServlet | | | | | | | +-----|-------+ +---------|--------+ +------------|------------+ | | | | | | | WebHttp +-+ | +------------------------\u0026gt; |-| HttpClient +-+ | |--------------------------------\u0026gt; |-| | |-| |-| | |-| |-| | |-| \u0026lt;--------------------------------| | |-| +-+ | \u0026lt;--------------------------| | | +-+ | | | | | | | | | | | | | + + + segmentItems By following the flow of HttpClient case, there should be two segments created.\n Segment represents the CaseServlet access. Let\u0026rsquo;s name it as SegmentA. Segment represents the ContextPropagateServlet access. Let\u0026rsquo;s name it as SegmentB.  segmentItems: - serviceName: httpclient-case segmentSize: ge 2 # Could have more than one health check segments, because, the dependency is not standby. Because Tomcat plugin is a default plugin of SkyWalking, so, in SegmentA, there are two spans\n Tomcat entry span HttpClient exit span  SegmentA span list should like following\n- segmentId: not null spans: - operationName: /httpclient-case/case/context-propagate parentSpanId: 0 spanId: 1 startTime: nq 0 endTime: nq 0 isError: false spanLayer: Http spanType: Exit componentId: eq 2 tags: - {key: url, value: \u0026#39;http://127.0.0.1:8080/httpclient-case/case/context-propagate\u0026#39;} - {key: http.method, value: GET} logs: [] peer: 127.0.0.1:8080 - operationName: /httpclient-case/case/httpclient parentSpanId: -1 spanId: 0 startTime: nq 0 endTime: nq 0 spanLayer: Http isError: false spanType: Entry componentId: 1 tags: - {key: url, value: \u0026#39;http://localhost:{SERVER_OUTPUT_PORT}/httpclient-case/case/httpclient\u0026#39;} - {key: http.method, value: GET} logs: [] peer: null SegmentB should only have one Tomcat entry span, but includes the Ref pointing to SegmentA.\nSegmentB span list should like following\n- segmentId: not null spans: - operationName: /httpclient-case/case/context-propagate parentSpanId: -1 spanId: 0 tags: - {key: url, value: \u0026#39;http://127.0.0.1:8080/httpclient-case/case/context-propagate\u0026#39;} - {key: http.method, value: GET} logs: [] startTime: nq 0 endTime: nq 0 spanLayer: Http isError: false spanType: Entry componentId: 1 peer: null refs: - {parentEndpoint: /httpclient-case/case/httpclient, networkAddress: \u0026#39;localhost:8080\u0026#39;, refType: CrossProcess, parentSpanId: 1, parentTraceSegmentId: not null, parentServiceInstance: not null, parentService: not null, traceId: not null} The example Process of Writing Meter Expected Data Expected data file, expectedData.yaml, include MeterItems part.\nWe are using the toolkit plugin to demonstrate how to write the expected data. When write the meter plugin, the expected data file keeps the same.\nThere is one key point of testing\n Build a meter and operate it.  Such as Counter:\nMeterFactory.counter(\u0026#34;test_counter\u0026#34;).tag(\u0026#34;ck1\u0026#34;, \u0026#34;cv1\u0026#34;).build().increment(1d); MeterFactory.histogram(\u0026#34;test_histogram\u0026#34;).tag(\u0026#34;hk1\u0026#34;, \u0026#34;hv1\u0026#34;).steps(1d, 5d, 10d).build().addValue(2d); +-------------+ +------------------+ | Plugin | | Agent core | | | | | +-----|-------+ +---------|--------+ | | | | | Build or operate +-+ +------------------------\u0026gt; |-| | |-] | |-| | |-| | |-| | |-| | \u0026lt;--------------------------| | +-+ | | | | | | | | + + meterItems By following the flow of the toolkit case, there should be two meters created.\n Meter test_counter created from MeterFactory#counter. Let\u0026rsquo;s name it as MeterA. Meter test_histogram created from MeterFactory#histogram. Let\u0026rsquo;s name it as MeterB.  meterItems: - serviceName: toolkit-case meterSize: 2 They\u0026rsquo;re showing two kinds of meter, MeterA has a single value, MeterB has a histogram value.\nMeterA should like following, counter and gauge use the same data format.\n- meterId: name: test_counter tags: - {name: ck1, value: cv1} singleValue: gt 0 MeterB should like following.\n- meterId: name: test_histogram tags: - {name: hk1, value: hv1} histogramBuckets: - 0.0 - 1.0 - 5.0 - 10.0 Local Test and Pull Request To The Upstream First of all, the test case project could be compiled successfully, with right project structure and be able to deploy. The developer should test the start script could run in Linux/MacOS, and entryService/health services are able to provide the response.\nYou could run test by using following commands\ncd ${SKYWALKING_HOME} bash ./test/plugin/run.sh -f ${scenario_name} Notice，if codes in ./apm-sniffer have been changed, no matter because your change or git update， please recompile the skywalking-agent. Because the test framework will use the existing skywalking-agent folder, rather than recompiling it every time.\nUse ${SKYWALKING_HOME}/test/plugin/run.sh -h to know more command options.\nIf the local test passed, then you could add it to .github/workflows/plugins-test.\u0026lt;n\u0026gt;.yaml file, which will drive the tests running on the GitHub Actions of official SkyWalking repository. Based on your plugin\u0026rsquo;s name, please add the test case into file .github/workflows/plugins-test.\u0026lt;n\u0026gt;.yaml, by alphabetical orders.\nEvery test case is a GitHub Actions Job. Please use the scenario directory name as the case name, mostly you\u0026rsquo;ll just need to decide which file (plugins-test.\u0026lt;n\u0026gt;.yaml) to add your test case, and simply put one line (as follows) in it, take the existed cases as examples. You can run python3 tools/select-group.py to see which file contains the least cases and add your cases into it, in order to balance the running time of each group.\nIf a test case required to run in JDK 14 environment, please add you test case into file plugins-jdk14-test.\u0026lt;n\u0026gt;.yaml.\njobs: PluginsTest: name: Plugin runs-on: ubuntu-latest timeout-minutes: 90 strategy: fail-fast: true matrix: case: # ... - \u0026lt;your scenario test directory name\u0026gt; # ... ","excerpt":"Plugin automatic test framework The plugin test framework is designed to verify the function and …","ref":"/docs/main/latest/en/guides/plugin-test/","title":"Plugin automatic test framework"},{"body":"Plugin Development Guide This document describes how to understand, develop and contribute a plugin.\nThere are 2 kinds of plugin:\n Tracing plugin. Follow the distributed tracing concept to collect spans with tags and logs. Meter plugin. Collect numeric metrics in Counter, Gauge, and Histogram formats.  We also provide the plugin test tool to verify the data collected and reported by the plugin. If you plan to contribute any plugin to our main repo, the data would be verified by this tool too.\nTracing plugin Concepts Span The span is an important and recognized concept in the distributed tracing system. Learn about the span from the Google Dapper Paper and OpenTracing\nSkyWalking has supported OpenTracing and OpenTracing-Java API since 2017. Our concepts of the span are similar to that of the Google Dapper Paper and OpenTracing. We have also extended the span.\nThere are three types of span:\n1.1 EntrySpan The EntrySpan represents a service provider. It is also an endpoint on the server end. As an APM system, our target is the application servers. Therefore, almost all the services and MQ-consumers are EntrySpan.\n1.2 LocalSpan The LocalSpan represents a normal Java method that does not concern remote services. It is neither a MQ producer/consumer nor a service (e.g. HTTP service) provider/consumer.\n1.3 ExitSpan The ExitSpan represents a client of service or MQ-producer. It is named the LeafSpan in the early versions of SkyWalking. For example, accessing DB through JDBC and reading Redis/Memcached are classified as an ExitSpan.\nContextCarrier In order to implement distributed tracing, cross-process tracing has to be bound, and the context must propagate across the process. This is where the ContextCarrier comes in.\nHere are the steps on how to use the ContextCarrier in an A-\u0026gt;B distributed call.\n Create a new and empty ContextCarrier on the client end. Create an ExitSpan by ContextManager#createExitSpan or use ContextManager#inject to initalize the ContextCarrier. Place all items of ContextCarrier into heads (e.g. HTTP HEAD), attachments (e.g. Dubbo RPC framework) or messages (e.g. Kafka). The ContextCarrier propagates to the server end through the service call. On the server end, obtain all items from the heads, attachments or messages. Create an EntrySpan by ContextManager#createEntrySpan or use ContextManager#extract to bind the client and server ends.  See the following examples, where we use the Apache HTTPComponent client plugin and Tomcat 7 server plugin:\n Using the Apache HTTPComponent client plugin on the client end  span = ContextManager.createExitSpan(\u0026#34;/span/operation/name\u0026#34;, contextCarrier, \u0026#34;ip:port\u0026#34;); CarrierItem next = contextCarrier.items(); while (next.hasNext()) { next = next.next(); httpRequest.setHeader(next.getHeadKey(), next.getHeadValue()); } Using the Tomcat 7 server plugin on the server end  ContextCarrier contextCarrier = new ContextCarrier(); CarrierItem next = contextCarrier.items(); while (next.hasNext()) { next = next.next(); next.setHeadValue(request.getHeader(next.getHeadKey())); } span = ContextManager.createEntrySpan(“/span/operation/name”, contextCarrier); ContextSnapshot Besides cross-process tracing, cross-thread tracing has to be supported as well. For instance, both async process (in-memory MQ) and batch process are common in Java. Cross-process and cross-thread tracing are very similar in that they both require propagating context, except that cross-thread tracing does not require serialization.\nHere are the three steps on cross-thread propagation:\n Use ContextManager#capture to get the ContextSnapshot object. Let the sub-thread access the ContextSnapshot through method arguments or being carried by existing arguments Use ContextManager#continued in sub-thread.  Core APIs ContextManager ContextManager provides all major and primary APIs.\n Create EntrySpan  public static AbstractSpan createEntrySpan(String endpointName, ContextCarrier carrier) Create EntrySpan according to the operation name (e.g. service name, uri) and ContextCarrier.\nCreate LocalSpan  public static AbstractSpan createLocalSpan(String endpointName) Create LocalSpan according to the operation name (e.g. full method signature).\nCreate ExitSpan  public static AbstractSpan createExitSpan(String endpointName, ContextCarrier carrier, String remotePeer) Create ExitSpan according to the operation name (e.g. service name, uri) and the new ContextCarrier and peer address (e.g. ip+port, hostname+port).\nAbstractSpan /** * Set the component id, which defines in {@link ComponentsDefine} * * @param component * @return the span for chaining. */ AbstractSpan setComponent(Component component); AbstractSpan setLayer(SpanLayer layer); /** * Set a key:value tag on the Span. * * @return this Span instance, for chaining */ AbstractSpan tag(String key, String value); /** * Record an exception event of the current walltime timestamp. * * @param t any subclass of {@link Throwable}, which occurs in this span. * @return the Span, for chaining */ AbstractSpan log(Throwable t); AbstractSpan errorOccurred(); /** * Record an event at a specific timestamp. * * @param timestamp The explicit timestamp for the log record. * @param event the events * @return the Span, for chaining */ AbstractSpan log(long timestamp, Map\u0026lt;String, ?\u0026gt; event); /** * Sets the string name for the logical operation this span represents. * * @return this Span instance, for chaining */ AbstractSpan setOperationName(String endpointName); Besides setting the operation name, tags and logs, two attributes must be set, namely the component and layer. This is especially important for the EntrySpan and ExitSpan.\nSpanLayer is the type of span. There are 5 values:\n UNKNOWN (default) DB RPC_FRAMEWORK (designed for the RPC framework, rather than an ordinary HTTP call) HTTP MQ  Component IDs are defined and reserved by the SkyWalking project. For extension of the component name/ID, please follow the component library definitions and extensions document.\nSpecial Span Tags All tags are available in the trace view. Meanwhile, in the OAP backend analysis, some special tags or tag combinations provide other advanced features.\nTag key status_code The value should be an integer. The response code of OAL entities corresponds to this value.\nTag keys db.statement and db.type. The value of db.statement should be a string that represents the database statement, such as SQL, or [No statement]/+span#operationName if the value is empty. When the exit span contains this tag, OAP samples the slow statements based on agent-analyzer/default/maxSlowSQLLength. The threshold of slow statement is defined in accordance with agent-analyzer/default/slowDBAccessThreshold\nExtension logic endpoint: Tag key x-le The logic endpoint is a concept that doesn\u0026rsquo;t represent a real RPC call, but requires the statistic. The value of x-le should be in JSON format. There are two options:\n Define a separated logic endpoint. Provide its own endpoint name, latency and status. Suitable for entry and local span.  { \u0026#34;name\u0026#34;: \u0026#34;GraphQL-service\u0026#34;, \u0026#34;latency\u0026#34;: 100, \u0026#34;status\u0026#34;: true } Declare the current local span representing a logic endpoint.  { \u0026#34;logic-span\u0026#34;: true } Advanced APIs Async Span APIs There is a set of advanced APIs in Span which is specifically designed for async use cases. When tags, logs, and attributes (including end time) of the span need to be set in another thread, you should use these APIs.\n/** * The span finish at current tracing context, but the current span is still alive, until {@link #asyncFinish} * called. * * This method must be called\u0026lt;br/\u0026gt; * 1. In original thread(tracing context). * 2. Current span is active span. * * During alive, tags, logs and attributes of the span could be changed, in any thread. * * The execution times of {@link #prepareForAsync} and {@link #asyncFinish()} must match. * * @return the current span */ AbstractSpan prepareForAsync(); /** * Notify the span, it could be finished. * * The execution times of {@link #prepareForAsync} and {@link #asyncFinish()} must match. * * @return the current span */ AbstractSpan asyncFinish();  Call #prepareForAsync in the original context. Run ContextManager#stopSpan in the original context when your job in the current thread is complete. Propagate the span to any other thread. Once the above steps are all set, call #asyncFinish in any thread. When #prepareForAsync is complete for all spans, the tracing context will be finished and will report to the backend (based on the count of API execution).  Develop a plugin Abstract The basic method to trace is to intercept a Java method, by using byte code manipulation tech and AOP concept. SkyWalking has packaged the byte code manipulation tech and tracing context propagation, so you simply have to define the intercept point (a.k.a. aspect pointcut in Spring).\nIntercept SkyWalking provides two common definitions to intercept constructor, instance method and class method.\nv1 APIs  Extend ClassInstanceMethodsEnhancePluginDefine to define constructor intercept points and instance method intercept points. Extend ClassStaticMethodsEnhancePluginDefine to define class method intercept points.  Of course, you can extend ClassEnhancePluginDefine to set all intercept points, although it is uncommon to do so.\nv2 APIs v2 APIs provide an enhanced interceptor, which could propagate context through MIC(MethodInvocationContext).\n Extend ClassInstanceMethodsEnhancePluginDefineV2 to define constructor intercept points and instance method intercept points. Extend ClassStaticMethodsEnhancePluginDefineV2 to define class method intercept points.  Of course, you can extend ClassEnhancePluginDefineV2 to set all intercept points, although it is uncommon to do so.\nImplement plugin See the following demonstration on how to implement a plugin by extending ClassInstanceMethodsEnhancePluginDefine.\n Define the target class name.  protected abstract ClassMatch enhanceClass(); ClassMatch represents how to match the target classes. There are 4 ways:\n byName: Based on the full class names (package name + . + class name). byClassAnnotationMatch: Depends on whether there are certain annotations in the target classes. byMethodAnnotationMatch: Depends on whether there are certain annotations in the methods of the target classes. byHierarchyMatch: Based on the parent classes or interfaces of the target classes.  Attention:\n Never use ThirdPartyClass.class in the instrumentation definitions, such as takesArguments(ThirdPartyClass.class), or byName(ThirdPartyClass.class.getName()), because of the fact that ThirdPartyClass dose not necessarily exist in the target application and this will break the agent; we have import checks to assist in checking this in CI, but it doesn\u0026rsquo;t cover all scenarios of this limitation, so never try to work around this limitation by something like using full-qualified-class-name (FQCN), i.e. takesArguments(full.qualified.ThirdPartyClass.class) and byName(full.qualified.ThirdPartyClass.class.getName()) will pass the CI check, but are still invalid in the agent codes. Therefore, Use Full Qualified Class Name String Literature Instead. Even if you are perfectly sure that the class to be intercepted exists in the target application (such as JDK classes), still, do not use *.class.getName() to get the class String name. We recommend you to use a literal string. This is to avoid ClassLoader issues. by*AnnotationMatch does not support inherited annotations. We do not recommend using byHierarchyMatch unless necessary. Using it may trigger the interception of many unexcepted methods, which would cause performance issues.  Example：\n@Override protected ClassMatch enhanceClassName() { return byName(\u0026#34;org.apache.catalina.core.StandardEngineValve\u0026#34;);\t}\tDefine an instance method intercept point.  public InstanceMethodsInterceptPoint[] getInstanceMethodsInterceptPoints(); public interface InstanceMethodsInterceptPoint { /** * class instance methods matcher. * * @return methods matcher */ ElementMatcher\u0026lt;MethodDescription\u0026gt; getMethodsMatcher(); /** * @return represents a class name, the class instance must instanceof InstanceMethodsAroundInterceptor. */ String getMethodsInterceptor(); boolean isOverrideArgs(); } You may also use Matcher to set the target methods. Return true in isOverrideArgs, if you want to change the argument ref in interceptor.\nThe following sections will tell you how to implement the interceptor.\nAdd plugin definition into the skywalking-plugin.def file.  tomcat-7.x/8.x=TomcatInstrumentation  Set up witnessClasses and/or witnessMethods if the instrumentation has to be activated in specific versions.\nExample:\n// The plugin is activated only when the foo.Bar class exists. @Override protected String[] witnessClasses() { return new String[] { \u0026#34;foo.Bar\u0026#34; }; } // The plugin is activated only when the foo.Bar#hello method exists. @Override protected List\u0026lt;WitnessMethod\u0026gt; witnessMethods() { List\u0026lt;WitnessMethod\u0026gt; witnessMethodList = new ArrayList\u0026lt;\u0026gt;(); WitnessMethod witnessMethod = new WitnessMethod(\u0026#34;foo.Bar\u0026#34;, ElementMatchers.named(\u0026#34;hello\u0026#34;)); witnessMethodList.add(witnessMethod); return witnessMethodList; } For more examples, see WitnessTest.java\n  Implement an interceptor As an interceptor for an instance method, it has to implement org.apache.skywalking.apm.agent.core.plugin.interceptor.enhance.InstanceMethodsAroundInterceptor\n/** * A interceptor, which intercept method\u0026#39;s invocation. The target methods will be defined in {@link * ClassEnhancePluginDefine}\u0026#39;s subclass, most likely in {@link ClassInstanceMethodsEnhancePluginDefine} */ public interface InstanceMethodsAroundInterceptor { /** * called before target method invocation. * * @param result change this result, if you want to truncate the method. * @throws Throwable */ void beforeMethod(EnhancedInstance objInst, Method method, Object[] allArguments, Class\u0026lt;?\u0026gt;[] argumentsTypes, MethodInterceptResult result) throws Throwable; /** * called after target method invocation. Even method\u0026#39;s invocation triggers an exception. * * @param ret the method\u0026#39;s original return value. * @return the method\u0026#39;s actual return value. * @throws Throwable */ Object afterMethod(EnhancedInstance objInst, Method method, Object[] allArguments, Class\u0026lt;?\u0026gt;[] argumentsTypes, Object ret) throws Throwable; /** * called when occur exception. * * @param t the exception occur. */ void handleMethodException(EnhancedInstance objInst, Method method, Object[] allArguments, Class\u0026lt;?\u0026gt;[] argumentsTypes, Throwable t); } Use the core APIs before and after calling the method, as well as during exception handling.\nV2 APIs The interceptor of V2 API uses MethodInvocationContext context to replace the MethodInterceptResult result in the beforeMethod, and be added as a new parameter in afterMethod and handleMethodException.\nMethodInvocationContext context is only shared in one time execution, and safe to use when face concurrency execution.\n/** * A v2 interceptor, which intercept method\u0026#39;s invocation. The target methods will be defined in {@link * ClassEnhancePluginDefineV2}\u0026#39;s subclass, most likely in {@link ClassInstanceMethodsEnhancePluginDefine} */ public interface InstanceMethodsAroundInterceptorV2 { /** * called before target method invocation. * * @param context the method invocation context including result context. */ void beforeMethod(EnhancedInstance objInst, Method method, Object[] allArguments, Class\u0026lt;?\u0026gt;[] argumentsTypes, MethodInvocationContext context) throws Throwable; /** * called after target method invocation. Even method\u0026#39;s invocation triggers an exception. * * @param ret the method\u0026#39;s original return value. May be null if the method triggers an exception. * @return the method\u0026#39;s actual return value. */ Object afterMethod(EnhancedInstance objInst, Method method, Object[] allArguments, Class\u0026lt;?\u0026gt;[] argumentsTypes, Object ret, MethodInvocationContext context) throws Throwable; /** * called when occur exception. * * @param t the exception occur. */ void handleMethodException(EnhancedInstance objInst, Method method, Object[] allArguments, Class\u0026lt;?\u0026gt;[] argumentsTypes, Throwable t, MethodInvocationContext context); } Bootstrap class instrumentation. SkyWalking has packaged the bootstrap instrumentation in the agent core. You can easily implement it by declaring it in the instrumentation definition.\nOverride the public boolean isBootstrapInstrumentation() and return true. Such as\npublic class URLInstrumentation extends ClassEnhancePluginDefine { private static String CLASS_NAME = \u0026#34;java.net.URL\u0026#34;; @Override protected ClassMatch enhanceClass() { return byName(CLASS_NAME); } @Override public ConstructorInterceptPoint[] getConstructorsInterceptPoints() { return new ConstructorInterceptPoint[] { new ConstructorInterceptPoint() { @Override public ElementMatcher\u0026lt;MethodDescription\u0026gt; getConstructorMatcher() { return any(); } @Override public String getConstructorInterceptor() { return \u0026#34;org.apache.skywalking.apm.plugin.jre.httpurlconnection.Interceptor2\u0026#34;; } } }; } @Override public InstanceMethodsInterceptPoint[] getInstanceMethodsInterceptPoints() { return new InstanceMethodsInterceptPoint[0]; } @Override public StaticMethodsInterceptPoint[] getStaticMethodsInterceptPoints() { return new StaticMethodsInterceptPoint[0]; } @Override public boolean isBootstrapInstrumentation() { return true; } } ClassEnhancePluginDefineV2 is provided in v2 APIs, #isBootstrapInstrumentation works too.\nNOTE: Bootstrap instrumentation should be used only where necessary. During its actual execution, it mostly affects the JRE core(rt.jar). Defining it other than where necessary could lead to unexpected results or side effects.\nProvide custom config for the plugin The config could provide different behaviours based on the configurations. The SkyWalking plugin mechanism provides the configuration injection and initialization system in the agent core.\nEvery plugin could declare one or more classes to represent the config by using @PluginConfig annotation. The agent core could initialize this class' static field through System environments, System properties, and agent.config static file.\nThe #root() method in the @PluginConfig annotation requires declaring the root class for the initialization process. Typically, SkyWalking prefers to use nested inner static classes for the hierarchy of the configuration. We recommend using Plugin/plugin-name/config-key as the nested classes structure of the config class.\nNOTE: because of the Java ClassLoader mechanism, the @PluginConfig annotation should be added on the real class used in the interceptor codes.\nIn the following example, @PluginConfig(root = SpringMVCPluginConfig.class) indicates that initialization should start with using SpringMVCPluginConfig as the root. Then, the config key of the attribute USE_QUALIFIED_NAME_AS_ENDPOINT_NAME should be plugin.springmvc.use_qualified_name_as_endpoint_name.\npublic class SpringMVCPluginConfig { public static class Plugin { // NOTE, if move this annotation on the `Plugin` or `SpringMVCPluginConfig` class, it no longer has any effect.  @PluginConfig(root = SpringMVCPluginConfig.class) public static class SpringMVC { /** * If true, the fully qualified method name will be used as the endpoint name instead of the request URL, * default is false. */ public static boolean USE_QUALIFIED_NAME_AS_ENDPOINT_NAME = false; /** * This config item controls that whether the SpringMVC plugin should collect the parameters of the * request. */ public static boolean COLLECT_HTTP_PARAMS = false; } @PluginConfig(root = SpringMVCPluginConfig.class) public static class Http { /** * When either {@link Plugin.SpringMVC#COLLECT_HTTP_PARAMS} is enabled, how many characters to keep and send * to the OAP backend, use negative values to keep and send the complete parameters, NB. this config item is * added for the sake of performance */ public static int HTTP_PARAMS_LENGTH_THRESHOLD = 1024; } } } Meter Plugin Java agent plugin could use meter APIs to collect metrics for backend analysis.\n Counter API represents a single monotonically increasing counter which automatically collects data and reports to the backend.  import org.apache.skywalking.apm.agent.core.meter.MeterFactory; Counter counter = MeterFactory.counter(meterName).tag(\u0026#34;tagKey\u0026#34;, \u0026#34;tagValue\u0026#34;).mode(Counter.Mode.INCREMENT).build(); counter.increment(1d);  MeterFactory.counter creates a new counter builder with the meter name. Counter.Builder.tag(String key, String value) marks a tag key/value pair. Counter.Builder.mode(Counter.Mode mode) changes the counter mode. RATE mode means the reporting rate to the backend. Counter.Builder.build() builds a new Counter which is collected and reported to the backend. Counter.increment(double count) increment counts to the Counter. It could be a positive value.   Gauge API represents a single numerical value.  import org.apache.skywalking.apm.agent.core.meter.MeterFactory; ThreadPoolExecutor threadPool = ...; Gauge gauge = MeterFactory.gauge(meterName, () -\u0026gt; threadPool.getActiveCount()).tag(\u0026#34;tagKey\u0026#34;, \u0026#34;tagValue\u0026#34;).build();  MeterFactory.gauge(String name, Supplier\u0026lt;Double\u0026gt; getter) creates a new gauge builder with the meter name and supplier function. This function must return a double value. Gauge.Builder.tag(String key, String value) marks a tag key/value pair. Gauge.Builder.build() builds a new Gauge which is collected and reported to the backend.   Histogram API represents a summary sample observations with customized buckets.  import org.apache.skywalking.apm.agent.core.meter.MeterFactory; Histogram histogram = MeterFactory.histogram(\u0026#34;test\u0026#34;).tag(\u0026#34;tagKey\u0026#34;, \u0026#34;tagValue\u0026#34;).steps(Arrays.asList(1, 5, 10)).minValue(0).build(); histogram.addValue(3);  MeterFactory.histogram(String name) creates a new histogram builder with the meter name. Histogram.Builder.tag(String key, String value) marks a tag key/value pair. Histogram.Builder.steps(List\u0026lt;Double\u0026gt; steps) sets up the max values of every histogram buckets. Histogram.Builder.minValue(double value) sets up the minimal value of this histogram. Default is 0. Histogram.Builder.build() builds a new Histogram which is collected and reported to the backend. Histogram.addValue(double value) adds value into the histogram, and automatically analyzes what bucket count needs to be incremented. Rule: count into [step1, step2).  Plugin Test Tool The Apache SkyWalking Agent Test Tool Suite is an incredibly useful test tool suite that is available in a wide variety of agent languages. It includes the mock collector and validator. The mock collector is a SkyWalking receiver, like the OAP server.\nYou could learn how to use this tool to test the plugin in this doc. This is a must if you want to contribute plugins to the SkyWalking official repo.\nContribute plugins to the Apache SkyWalking repository We welcome everyone to contribute their plugins.\nPlease follow these steps:\n Submit an issue for your plugin, including any supported versions. Create sub modules under apm-sniffer/apm-sdk-plugin or apm-sniffer/optional-plugins, and the name should include supported library name and versions. Follow this guide to develop. Make sure comments and test cases are provided. Develop and test. Provide the automatic test cases. Learn how to write the plugin test case from this doc Send a pull request and ask for review. The plugin committers will approve your plugins, plugin CI-with-IT, e2e, and the plugin tests will be passed. The plugin is accepted by SkyWalking.  ","excerpt":"Plugin Development Guide This document describes how to understand, develop and contribute a plugin. …","ref":"/docs/main/latest/en/guides/java-plugin-development-guide/","title":"Plugin Development Guide"},{"body":"Probe Introduction In SkyWalking, probe means an agent or SDK library integrated into a target system that takes charge of collecting telemetry data, including tracing and metrics. Depending on the target system tech stack, there are very different ways how the probe performs such tasks. But ultimately, they all work towards the same goal — to collect and reformat data, and then to send them to the backend.\nOn a high level, there are three typical categories in all SkyWalking probes.\n  Language based native agent. These agents run in target service user spaces, such as a part of user codes. For example, the SkyWalking Java agent uses the -javaagent command line argument to manipulate codes in runtime, where manipulate means to change and inject user\u0026rsquo;s codes. Another kind of agents uses certain hook or intercept mechanism provided by target libraries. As you can see, these agents are based on languages and libraries.\n  Service Mesh probes. Service Mesh probes collect data from sidecar, control panel in service mesh or proxy. In the old days, proxy is only used as an ingress of the whole cluster, but with the Service Mesh and sidecar, we can now perform observability functions.\n  3rd-party instrument library. SkyWalking accepts many widely used instrument libraries data formats. It analyzes the data, transfers it to SkyWalking\u0026rsquo;s formats of trace, metrics or both. This feature starts with accepting Zipkin span data. See Receiver for other tracers for more information.\n  You don\u0026rsquo;t need to use Language based native agent and Service Mesh probe at the same time, since they both serve to collect metrics data. Otherwise, your system will suffer twice the payload, and the analytic numbers will be doubled.\nThere are several recommended ways on how to use these probes:\n Use Language based native agent only. Use 3rd-party instrument library only, like the Zipkin instrument ecosystem. Use Service Mesh probe only. Use Service Mesh probe with Language based native agent or 3rd-party instrument library in tracing status. (Advanced usage)  What is the meaning of in tracing status?\nBy default, Language based native agent and 3rd-party instrument library both send distributed traces to the backend, where analyses and aggregation on those traces are performed. In tracing status means that the backend considers these traces as something like logs. In other words, the backend saves them, and builds the links between traces and metrics, like which endpoint and service does the trace belong?.\nWhat is next?  Learn more about the probes supported by SkyWalking in Service auto instrument agent, Manual instrument SDK, Service Mesh probe and Zipkin receiver. After understanding how the probe works, see the backend overview for more on analysis and persistence.  ","excerpt":"Probe Introduction In SkyWalking, probe means an agent or SDK library integrated into a target …","ref":"/docs/main/latest/en/concepts-and-designs/probe-introduction/","title":"Probe Introduction"},{"body":"Problem When you start your application with the skywalking agent, you may find this exception in your agent log which means that EnhanceRequireObjectCache cannot be casted to EnhanceRequireObjectCache. For example:\nERROR 2018-05-07 21:31:24 InstMethodsInter : class[class org.springframework.web.method.HandlerMethod] after method[getBean] intercept failure java.lang.ClassCastException: org.apache.skywalking.apm.plugin.spring.mvc.commons.EnhanceRequireObjectCache cannot be cast to org.apache.skywalking.apm.plugin.spring.mvc.commons.EnhanceRequireObjectCache at org.apache.skywalking.apm.plugin.spring.mvc.commons.interceptor.GetBeanInterceptor.afterMethod(GetBeanInterceptor.java:45) at org.apache.skywalking.apm.agent.core.plugin.interceptor.enhance.InstMethodsInter.intercept(InstMethodsInter.java:105) at org.springframework.web.method.HandlerMethod.getBean(HandlerMethod.java) at org.springframework.web.servlet.handler.AbstractHandlerMethodExceptionResolver.shouldApplyTo(AbstractHandlerMethodExceptionResolver.java:47) at org.springframework.web.servlet.handler.AbstractHandlerExceptionResolver.resolveException(AbstractHandlerExceptionResolver.java:131) at org.springframework.web.servlet.handler.HandlerExceptionResolverComposite.resolveException(HandlerExceptionResolverComposite.java:76) ... Reason This exception may be caused by hot deployment tools (spring-boot-devtool) or otherwise, which changes the classloader in runtime.\nResolution  This error does not occur under the production environment, since developer tools are automatically disabled: See spring-boot-devtools. If you would like to debug in your development environment as usual, you should temporarily remove such hot deployment package in your lib path.  ","excerpt":"Problem When you start your application with the skywalking agent, you may find this exception in …","ref":"/docs/main/latest/en/faq/enhancerequireobjectcache-cast-exception/","title":"Problem"},{"body":"Problem  When importing the SkyWalking project to Eclipse, the following errors may occur:   Software being installed: Checkstyle configuration plugin for M2Eclipse 1.0.0.201705301746 (com.basistech.m2e.code.quality.checkstyle.feature.feature.group 1.0.0.201705301746) Missing requirement: Checkstyle configuration plugin for M2Eclipse 1.0.0.201705301746 (com.basistech.m2e.code.quality.checkstyle.feature.feature.group 1.0.0.201705301746) requires \u0026lsquo;net.sf.eclipsecs.core 5.2.0\u0026rsquo; but it could not be found\n Reason The Eclipse Checkstyle Plug-in has not been installed.\nResolution Download the plug-in at the link here: https://sourceforge.net/projects/eclipse-cs/?source=typ_redirect Eclipse Checkstyle Plug-in version 8.7.0.201801131309 is required. Plug-in notification: The Eclipse Checkstyle plug-in integrates the Checkstyle Java code auditor into the Eclipse IDE. The plug-in provides real-time feedback to the user on rule violations, including checking against coding style and error-prone code constructs.\n","excerpt":"Problem  When importing the SkyWalking project to Eclipse, the following errors may occur: …","ref":"/docs/main/latest/en/faq/import-project-eclipse-requireitems-exception/","title":"Problem"},{"body":"Problem Tracing doesn\u0026rsquo;t work on the Kafka consumer end.\nReason The kafka client is responsible for pulling messages from the brokers, after which the data will be processed by user-defined codes. However, only the poll action can be traced by the plug-in and the subsequent data processing work inevitably goes beyond the scope of the trace context. Thus, in order to complete tracing on the client end, manual instrumentation is required, i.e. the poll action and the processing action should be wrapped manually.\nResolve For a native Kafka client, please use the Application Toolkit libraries to do the manual instrumentation, with the help of the @KafkaPollAndInvoke annotation in apm-toolkit-kafka or with OpenTracing API. If you\u0026rsquo;re using spring-kafka 1.3.x, 2.2.x or above, you can easily trace the consumer end without further configuration.\n","excerpt":"Problem Tracing doesn\u0026rsquo;t work on the Kafka consumer end.\nReason The kafka client is responsible …","ref":"/docs/main/latest/en/faq/kafka-plugin/","title":"Problem"},{"body":"Problem When using a thread pool, TraceSegment data in a thread cannot be reported and there are memory data that cannot be recycled (memory leaks).\nExample ThreadPoolTaskExecutor executor = new ThreadPoolTaskExecutor(); executor.setThreadFactory(r -\u0026gt; new Thread(RunnableWrapper.of(r))); Reason  Worker threads are enhanced when using the thread pool. Based on the design of the SkyWalking Java Agent, when tracing a cross thread, you must enhance the task thread.  Resolution   When using Thread Schedule Framework: See SkyWalking Thread Schedule Framework at SkyWalking Java agent supported list, such as Spring FrameWork @Async, which can implement tracing without any modification.\n  When using Custom Thread Pool: Enhance the task thread with the following code.\n  ExecutorService executorService = Executors.newFixedThreadPool(1); executorService.execute(RunnableWrapper.of(new Runnable() { @Override public void run() { //your code  } })); See across thread solution APIs for more use cases.\n","excerpt":"Problem When using a thread pool, TraceSegment data in a thread cannot be reported and there are …","ref":"/docs/main/latest/en/faq/memory-leak-enhance-worker-thread/","title":"Problem"},{"body":"Problem  In maven build, the following error may occur with the protoc-plugin:  [ERROR] Failed to execute goal org.xolstice.maven.plugins:protobuf-maven-plugin:0.5.0:compile-custom (default) on project apm-network: Unable to copy the file to \\skywalking\\apm-network\\target\\protoc-plugins: \\skywalking\\apm-network\\target\\protoc-plugins\\protoc-3.3.0-linux-x86_64.exe (The process cannot access the file because it is being used by another process) -\u0026gt; [Help 1] Reason  The Protobuf compiler is dependent on the glibc. However, glibc has not been installed, or there is an old version already installed in the system.  Resolution  Install or upgrade to the latest version of the glibc library. Under the container environment, the latest glibc version of the alpine system is recommended. Please refer to http://www.gnu.org/software/libc/documentation.html.  ","excerpt":"Problem  In maven build, the following error may occur with the protoc-plugin:  [ERROR] Failed to …","ref":"/docs/main/latest/en/faq/protoc-plugin-fails-when-build/","title":"Problem"},{"body":"Problem The message with Field ID, 8888, must be reserved.\nReason Because Thrift cannot carry metadata to transport Trace Header in the original API, we transport them by wrapping TProtocolFactory.\nThrift allows us to append any additional fields in the message even if the receiver doesn\u0026rsquo;t deal with them. Those data will be skipped and left unread. Based on this, the 8888th field of the message is used to store Trace Header (or metadata) and to transport them. That means the message with Field ID, 8888, must be reserved.\nResolution Avoid using the Field(ID is 8888) in your application.\n","excerpt":"Problem The message with Field ID, 8888, must be reserved.\nReason Because Thrift cannot carry …","ref":"/docs/main/latest/en/faq/thrift-plugin/","title":"Problem"},{"body":"Problem  There is no abnormal log in Agent log and Collector log. The traces can be seen, but no other information is available in UI.  Reason The operating system where the monitored system is located is not set as the current time zone, causing statistics collection time points to deviate.\nResolution Make sure the time is synchronized between collector servers and monitored application servers.\n","excerpt":"Problem  There is no abnormal log in Agent log and Collector log. The traces can be seen, but no …","ref":"/docs/main/latest/en/faq/why-have-traces-no-others/","title":"Problem"},{"body":"Problem： Maven compilation failure with error such as Error: not found: python2 When you compile the project via Maven, it fails at module apm-webapp and the following error occurs.\nPay attention to keywords such as node-sass and Error: not found: python2.\n[INFO] \u0026gt; node-sass@4.11.0 postinstall C:\\XXX\\skywalking\\skywalking-ui\\node_modules\\node-sass [INFO] \u0026gt; node scripts/build.js [ERROR] gyp verb check python checking for Python executable \u0026quot;python2\u0026quot; in the PATH [ERROR] gyp verb `which` failed Error: not found: python2 [ERROR] gyp verb `which` failed at getNotFoundError (C:\\XXX\\skywalking\\skywalking-ui\\node_modules\\which\\which.js:13:12) [ERROR] gyp verb `which` failed at F (C:\\XXX\\skywalking\\skywalking-ui\\node_modules\\which\\which.js:68:19) [ERROR] gyp verb `which` failed at E (C:\\XXX\\skywalking\\skywalking-ui\\node_modules\\which\\which.js:80:29) [ERROR] gyp verb `which` failed at C:\\XXX\\skywalking\\skywalking-ui\\node_modules\\which\\which.js:89:16 [ERROR] gyp verb `which` failed at C:\\XXX\\skywalking\\skywalking-ui\\node_modules\\isexe\\index.js:42:5 [ERROR] gyp verb `which` failed at C:\\XXX\\skywalking\\skywalking-ui\\node_modules\\isexe\\windows.js:36:5 [ERROR] gyp verb `which` failed at FSReqWrap.oncomplete (fs.js:152:21) [ERROR] gyp verb `which` failed code: 'ENOENT' } [ERROR] gyp verb check python checking for Python executable \u0026quot;python\u0026quot; in the PATH [ERROR] gyp verb `which` succeeded python C:\\Users\\XXX\\AppData\\Local\\Programs\\Python\\Python37\\python.EXE [ERROR] gyp ERR! configure error [ERROR] gyp ERR! stack Error: Command failed: C:\\Users\\XXX\\AppData\\Local\\Programs\\Python\\Python37\\python.EXE -c import sys; print \u0026quot;%s.%s.%s\u0026quot; % sys.version_info[:3]; [ERROR] gyp ERR! stack File \u0026quot;\u0026lt;string\u0026gt;\u0026quot;, line 1 [ERROR] gyp ERR! stack import sys; print \u0026quot;%s.%s.%s\u0026quot; % sys.version_info[:3]; [ERROR] gyp ERR! stack ^ [ERROR] gyp ERR! stack SyntaxError: invalid syntax [ERROR] gyp ERR! stack [ERROR] gyp ERR! stack at ChildProcess.exithandler (child_process.js:275:12) [ERROR] gyp ERR! stack at emitTwo (events.js:126:13) [ERROR] gyp ERR! stack at ChildProcess.emit (events.js:214:7) [ERROR] gyp ERR! stack at maybeClose (internal/child_process.js:925:16) [ERROR] gyp ERR! stack at Process.ChildProcess._handle.onexit (internal/child_process.js:209:5) [ERROR] gyp ERR! System Windows_NT 10.0.17134 ...... [INFO] server-starter-es7 ................................. SUCCESS [ 11.657 s] [INFO] apm-webapp ......................................... FAILURE [ 25.857 s] [INFO] apache-skywalking-apm .............................. SKIPPED [INFO] apache-skywalking-apm-es7 .......................... SKIPPED Reason The error has nothing to do with SkyWalking.\nAccording to the issue here (https://github.com/sass/node-sass/issues/1176), if you live in countries where requesting resources from GitHub and npmjs.org runs slow, some precompiled binaries for dependency node-sass would fail to be downloaded during npm install, and npm would try to compile them itself. That\u0026rsquo;s why python2 is needed.\nResolution 1. Use mirror. For instance, if you\u0026rsquo;re in China, please edit skywalking\\apm-webapp\\pom.xml as follows. Find\n\u0026lt;configuration\u0026gt; \u0026lt;arguments\u0026gt;install --registry=https://registry.npmjs.org/\u0026lt;/arguments\u0026gt; \u0026lt;/configuration\u0026gt; Replace it with\n\u0026lt;configuration\u0026gt; \u0026lt;arguments\u0026gt;install --registry=https://registry.npm.taobao.org/ --sass_binary_site=https://npm.taobao.org/mirrors/node-sass/\u0026lt;/arguments\u0026gt; \u0026lt;/configuration\u0026gt; 2. Get a sufficiently powerful VPN. ","excerpt":"Problem： Maven compilation failure with error such as Error: not found: python2 When you compile the …","ref":"/docs/main/latest/en/faq/maven-compile-npm-failure/","title":"Problem： Maven compilation failure with error such as `Error： not found： python2`"},{"body":"Protocols There are two different types of protocols.\n  Probe Protocol. It includes descriptions and definitions on how agents send collected metrics data and traces, as well as the format of each entity.\n  Query Protocol. The backend enables the query function in SkyWalking\u0026rsquo;s own UI and other UIs. These queries are based on GraphQL.\n  Probe Protocols They also related to the probe group. For more information, see Concepts and Designs. These groups are language-based native agent protocol, service mesh protocol and 3rd-party instrument protocol.\nLanguage-based native agent protocol There are two types of protocols that help language agents work in distributed environments.\n Cross Process Propagation Headers Protocol and Cross Process Correlation Headers Protocol come in in-wire data format. Agent/SDK usually uses HTTP/MQ/HTTP2 headers to carry the data with the RPC request. The remote agent will receive this in the request handler, and bind the context with this specific request. Trace Data Protocol is in out-of-wire data format. Agent/SDK uses this to send traces and metrics to SkyWalking or other compatible backends.  Cross Process Propagation Headers Protocol v3 has been the new protocol for in-wire context propagation since the version 8.0.0 release.\nCross Process Correlation Headers Protocol v1 is a new in-wire context propagation protocol which is additional and optional. Please read SkyWalking language agents documentation to see whether it is supported. This protocol defines the data format of transporting custom data with Cross Process Propagation Headers Protocol. It has been supported by the SkyWalking javaagent since 8.0.0,\nSkyWalking Trace Data Protocol v3 defines the communication method and format between the agent and backend.\nSkyWalking Log Data Protocol defines the communication method and format between the agent and backend.\nBrowser probe protocol The browser probe, such as skywalking-client-js, could use this protocol to send data to the backend. This service is provided by gRPC.\nSkyWalking Browser Protocol defines the communication method and format between skywalking-client-js and backend.\nService Mesh probe protocol The probe in sidecar or proxy could use this protocol to send data to the backend. This service provided by gRPC requires the following key information:\n Service Name or ID on both sides. Service Instance Name or ID on both sides. Endpoint. URI in HTTP, service method full signature in gRPC. Latency. In milliseconds. Response code in HTTP Status. Success or fail. Protocol. HTTP, gRPC DetectPoint. In Service Mesh sidecar, client or server. In normal L7 proxy, value is proxy.  Events Report Protocol The protocol is used to report events to the backend. The doc introduces the definition of an event, and the protocol repository defines gRPC services and message formats of events.\n3rd-party instrument protocol 3rd-party instrument protocols are not defined by SkyWalking. They are just protocols/formats with which SkyWalking is compatible, and SkyWalking could receive them from their existing libraries. SkyWalking starts with supporting Zipkin v1, v2 data formats.\nThe backend has a modular design, so it is very easy to extend a new receiver to support a new protocol/format.\nQuery Protocol The query protocol follows GraphQL grammar, and provides data query capabilities, which depends on your analysis metrics. Read query protocol doc for more details.\n","excerpt":"Protocols There are two different types of protocols.\n  Probe Protocol. It includes descriptions and …","ref":"/docs/main/latest/en/protocols/readme/","title":"Protocols"},{"body":"Query Protocol Query Protocol defines a set of APIs in GraphQL grammar to provide data query and interactive capabilities with SkyWalking native visualization tool or 3rd party system, including Web UI, CLI or private system.\nQuery protocol official repository, https://github.com/apache/skywalking-query-protocol.\nMetadata Metadata contains concise information on all services and their instances, endpoints, etc. under monitoring. You may query the metadata in different ways.\nextend type Query { getGlobalBrief(duration: Duration!): ClusterBrief # Normal service related metainfo  getAllServices(duration: Duration!): [Service!]! searchServices(duration: Duration!, keyword: String!): [Service!]! searchService(serviceCode: String!): Service # Fetch all services of Browser type getAllBrowserServices(duration: Duration!): [Service!]! # Service intance query getServiceInstances(duration: Duration!, serviceId: ID!): [ServiceInstance!]! # Endpoint query # Consider there are huge numbers of endpoint, # must use endpoint owner\u0026#39;s service id, keyword and limit filter to do query. searchEndpoint(keyword: String!, serviceId: ID!, limit: Int!): [Endpoint!]! getEndpointInfo(endpointId: ID!): EndpointInfo # Database related meta info. getAllDatabases(duration: Duration!): [Database!]! getTimeInfo: TimeInfo } Topology The topology and dependency graphs among services, instances and endpoints. Includes direct relationships or global maps.\nextend type Query { # Query the global topology getGlobalTopology(duration: Duration!): Topology # Query the topology, based on the given service getServiceTopology(serviceId: ID!, duration: Duration!): Topology # Query the topology, based on the given services. # `#getServiceTopology` could be replaced by this. getServicesTopology(serviceIds: [ID!]!, duration: Duration!): Topology # Query the instance topology, based on the given clientServiceId and serverServiceId getServiceInstanceTopology(clientServiceId: ID!, serverServiceId: ID!, duration: Duration!): ServiceInstanceTopology # Query the topology, based on the given endpoint getEndpointTopology(endpointId: ID!, duration: Duration!): Topology # v2 of getEndpointTopology getEndpointDependencies(endpointId: ID!, duration: Duration!): EndpointTopology } Metrics Metrics query targets all objects defined in OAL script and MAL. You may obtain the metrics data in linear or thermodynamic matrix formats based on the aggregation functions in script.\nV2 APIs Provide Metrics V2 query APIs since 8.0.0, including metadata, single/multiple values, heatmap, and sampled records metrics.\nextend type Query { # Metrics definition metadata query. Response the metrics type which determines the suitable query methods. typeOfMetrics(name: String!): MetricsType! # Get the list of all available metrics in the current OAP server. # Param, regex, could be used to filter the metrics by name. listMetrics(regex: String): [MetricDefinition!]! # Read metrics single value in the duration of required metrics readMetricsValue(condition: MetricsCondition!, duration: Duration!): Long! # Read time-series values in the duration of required metrics readMetricsValues(condition: MetricsCondition!, duration: Duration!): MetricsValues! # Read entity list of required metrics and parent entity type. sortMetrics(condition: TopNCondition!, duration: Duration!): [SelectedRecord!]! # Read value in the given time duration, usually as a linear. # labels: the labels you need to query. readLabeledMetricsValues(condition: MetricsCondition!, labels: [String!]!, duration: Duration!): [MetricsValues!]! # Heatmap is bucket based value statistic result. readHeatMap(condition: MetricsCondition!, duration: Duration!): HeatMap # Read the sampled records # TopNCondition#scope is not required. readSampledRecords(condition: TopNCondition!, duration: Duration!): [SelectedRecord!]! } V1 APIs 3 types of metrics can be queried. V1 APIs were introduced since 6.x. Now they are a shell to V2 APIs.\n Single value. Most default metrics are in single value. getValues and getLinearIntValues are suitable for this purpose. Multiple value. A metric defined in OAL includes multiple value calculations. Use getMultipleLinearIntValues to obtain all values. percentile is a typical multiple value function in OAL. Heatmap value. Read Heatmap in WIKI for details. thermodynamic is the only OAL function. Use getThermodynamic to get the values.  extend type Query { getValues(metric: BatchMetricConditions!, duration: Duration!): IntValues getLinearIntValues(metric: MetricCondition!, duration: Duration!): IntValues # Query the type of metrics including multiple values, and format them as multiple linears. # The seq of these multiple lines base on the calculation func in OAL # Such as, should us this to query the result of func percentile(50,75,90,95,99) in OAL, # then five lines will be responsed, p50 is the first element of return value. getMultipleLinearIntValues(metric: MetricCondition!, numOfLinear: Int!, duration: Duration!): [IntValues!]! getThermodynamic(metric: MetricCondition!, duration: Duration!): Thermodynamic } Metrics are defined in the config/oal/*.oal files.\nAggregation Aggregation query means that the metrics data need a secondary aggregation at query stage, which causes the query interfaces to have some different arguments. A typical example of aggregation query is the TopN list of services. Metrics stream aggregation simply calculates the metrics values of each service, but the expected list requires ordering metrics data by their values.\nAggregation query is for single value metrics only.\n# The aggregation query is different with the metric query. # All aggregation queries require backend or/and storage do aggregation in query time. extend type Query { # TopN is an aggregation query. getServiceTopN(name: String!, topN: Int!, duration: Duration!, order: Order!): [TopNEntity!]! getAllServiceInstanceTopN(name: String!, topN: Int!, duration: Duration!, order: Order!): [TopNEntity!]! getServiceInstanceTopN(serviceId: ID!, name: String!, topN: Int!, duration: Duration!, order: Order!): [TopNEntity!]! getAllEndpointTopN(name: String!, topN: Int!, duration: Duration!, order: Order!): [TopNEntity!]! getEndpointTopN(serviceId: ID!, name: String!, topN: Int!, duration: Duration!, order: Order!): [TopNEntity!]! } Others The following queries are for specific features, including trace, alarm, and profile.\n Trace. Query distributed traces by this. Alarm. Through alarm query, you can find alarm trends and their details.  The actual query GraphQL scripts can be found in the query-protocol folder here.\nCondition Duration Duration is a widely used parameter type as the APM data is time-related. See the following for more details. Step relates to precision.\n# The Duration defines the start and end time for each query operation. # Fields: `start` and `end` # represents the time span. And each of them matches the step. # ref https://www.ietf.org/rfc/rfc3339.txt # The time formats are # `SECOND` step: yyyy-MM-dd HHmmss # `MINUTE` step: yyyy-MM-dd HHmm # `HOUR` step: yyyy-MM-dd HH # `DAY` step: yyyy-MM-dd # `MONTH` step: yyyy-MM # Field: `step` # represents the accurate time point. # e.g. # if step==HOUR , start=2017-11-08 09, end=2017-11-08 19 # then # metrics from the following time points expected # 2017-11-08 9:00 -\u0026gt; 2017-11-08 19:00 # there are 11 time points (hours) in the time span. input Duration { start: String! end: String! step: Step! } enum Step { MONTH DAY HOUR MINUTE SECOND } ","excerpt":"Query Protocol Query Protocol defines a set of APIs in GraphQL grammar to provide data query and …","ref":"/docs/main/latest/en/protocols/query-protocol/","title":"Query Protocol"},{"body":"Register mechanism is no longer required for local / exit span Since version 6.6.0, SkyWalking has removed the local and exit span registers. If an old java agent (before 6.6.0) is still running, which registers to the 6.6.0+ backend, you will face the following warning message.\nclass=RegisterServiceHandler, message = Unexpected endpoint register, endpoint isn't detected from server side. This will not harm the backend or cause any issues, but serves as a reminder that your agent or other clients should follow the new protocol requirements.\nYou could simply use log4j2.xml to filter this warning message out.\n","excerpt":"Register mechanism is no longer required for local / exit span Since version 6.6.0, SkyWalking has …","ref":"/docs/main/latest/en/faq/unexpected-endpoint-register/","title":"Register mechanism is no longer required for local / exit span"},{"body":"Scopes and Fields Using the Aggregation Function, the requests will be grouped by time and Group Key(s) in each scope.\nSCOPE All    Name Remarks Group Key Type     name The service name of each request.  string   serviceInstanceName The name of the service instance ID.  string   endpoint The endpoint path of each request.  string   latency The time taken by each request.  int(in ms)   status The success or failure of the request.  bool(true for success)   responseCode The response code of the HTTP response, and if this request is the HTTP call. E.g. 200, 404, 302  int   type The type of each request, such as Database, HTTP, RPC, or gRPC.  enum   tags The labels of each request. Each value is made up by TagKey:TagValue in the segment.  List\u0026lt;String\u0026gt;    SCOPE Service This calculates the metrics data from each request of the service.\n   Name Remarks Group Key Type     name The name of the service.  string   nodeType The kind of node to which the Service or Network address belongs, such as Normal, Database, MQ, or Cache.  enum   serviceInstanceName The name of the service instance ID.  string   endpointName The name of the endpoint, such as a full path of HTTP URI.  string   latency The time taken by each request.  int   status Indicates the success or failure of the request.  bool(true for success)   responseCode The response code of the HTTP response, if this request is an HTTP call.  int   type The type of each request. Such as: Database, HTTP, RPC, gRPC.  enum   tags The labels of each request. Each value is made up by TagKey:TagValue in the segment.  List\u0026lt;String\u0026gt;   sideCar.internalErrorCode The sidecar/gateway proxy internal error code. The value is based on the implementation.  string   tcpInfo.receivedBytes The received bytes of the TCP traffic, if this request is a TCP call.  long   tcpInfo.sentBytes The sent bytes of the TCP traffic, if this request is a TCP call.  long    SCOPE ServiceInstance This calculates the metrics data from each request of the service instance.\n   Name Remarks Group Key Type     name The name of the service instance, such as ip:port@Service Name. Note: Currently, the native agent uses uuid@ipv4 as the instance name, which does not assist in setting up a filter in aggregation.  string   serviceName The name of the service.  string   nodeType The kind of node to which the Service or Network address belongs, such as Normal, Database, MQ, or Cache.  enum   endpointName The name of the endpoint, such as a full path of the HTTP URI.  string   latency The time taken by each request.  int   status Indicates the success or failure of the request.  bool(true for success)   responseCode The response code of HTTP response, if this request is an HTTP call.  int   type The type of each request, such as Database, HTTP, RPC, or gRPC.  enum   tags The labels of each request. Each value is made up by TagKey:TagValue in the segment.  List\u0026lt;String\u0026gt;   sideCar.internalErrorCode The sidecar/gateway proxy internal error code. The value is based on the implementation.  string   tcpInfo.receivedBytes The received bytes of the TCP traffic, if this request is a TCP call.  long   tcpInfo.sentBytes The sent bytes of the TCP traffic, if this request is a TCP call.  long    Secondary scopes of ServiceInstance This calculates the metrics data if the service instance is a JVM and collects through javaagent.\n SCOPE ServiceInstanceJVMCPU     Name Remarks Group Key Type     name The name of the service instance, such as ip:port@Service Name. Note: Currently, the native agent uses uuid@ipv4 as the instance name, which does not assist in setting up a filter in aggregation.  string   serviceName The name of the service.  string   usePercent The percentage of CPU time spent.  double    SCOPE ServiceInstanceJVMMemory     Name Remarks Group Key Type     name The name of the service instance, such as ip:port@Service Name. Note: Currently, the native agent uses uuid@ipv4 as the instance name, which does not assist in setting up a filter in aggregation.  string   serviceName The name of the service.  string   heapStatus Indicates whether the metric has a heap property or not.  bool   init See the JVM documentation.  long   max See the JVM documentation.  long   used See the JVM documentation.  long   committed See the JVM documentation.  long    SCOPE ServiceInstanceJVMMemoryPool     Name Remarks Group Key Type     name The name of the service instance, such as ip:port@Service Name. Note: Currently, the native agent uses uuid@ipv4 as the instance name, which does not assist in setting up a filter in aggregation.  string   serviceName The name of the service.  string   poolType The type may be CODE_CACHE_USAGE, NEWGEN_USAGE, OLDGEN_USAGE, SURVIVOR_USAGE, PERMGEN_USAGE, or METASPACE_USAGE based on different versions of JVM.  enum   init See the JVM documentation.  long   max See the JVM documentation.  long   used See the JVM documentation.  long   committed See the JVM documentation.  long    SCOPE ServiceInstanceJVMGC     Name Remarks Group Key Type     name The name of the service instance, such as ip:port@Service Name. Note: Currently, the native agent uses uuid@ipv4 as the instance name, which does not assist in setting up a filter in aggregation.  string   serviceName The name of the service.  string   phrase Includes both NEW and OLD.  Enum   time The time spent in GC.  long   count The count in GC operations.  long    SCOPE ServiceInstanceJVMThread     Name Remarks Group Key Type     name The name of the service instance, such as ip:port@Service Name. Note: Currently, the native agent uses uuid@ipv4 as the instance name, which does not assist in setting up a filter in aggregation.  string   serviceName The name of the service.  string   liveCount The current number of live threads.  int   daemonCount The current number of daemon threads.  int   peakCount The current number of peak threads.  int    SCOPE Endpoint This calculates the metrics data from each request of the endpoint in the service.\n   Name Remarks Group Key Type     name The name of the endpoint, such as a full path of the HTTP URI.  string   serviceName The name of the service.  string   serviceNodeType The type of node to which the Service or Network address belongs, such as Normal, Database, MQ, or Cache.  enum   serviceInstanceName The name of the service instance ID.  string   latency The time taken by each request.  int   status Indicates the success or failure of the request.  bool(true for success)   responseCode The response code of HTTP response, if this request is an HTTP call.  int   type The type of each request, such as Database, HTTP, RPC, or gRPC.  enum   tags The labels of each request. Each value is made up by TagKey:TagValue in the segment.  List\u0026lt;String\u0026gt;   sideCar.internalErrorCode The sidecar/gateway proxy internal error code. The value is based on the implementation.  string   tcpInfo.receivedBytes The received bytes of the TCP traffic, if this request is a TCP call.  long   tcpInfo.sentBytes The sent bytes of the TCP traffic, if this request is a TCP call.  long    SCOPE ServiceRelation This calculates the metrics data from each request between services.\n   Name Remarks Group Key Type     sourceServiceName The name of the source service.  string   sourceServiceNodeType The type of node to which the Service or Network address belongs, such as Normal, Database, MQ, or Cache.  enum   sourceServiceInstanceName The name of the source service instance.  string   destServiceName The name of the destination service.  string   destServiceNodeType The type of node of to which the Service or Network address belongs.  enum   destServiceInstanceName The name of the destination service instance.  string   endpoint The endpoint used in this call.  string   componentId The ID of component used in this call. yes string   latency The time taken by each request.  int   status Indicates the success or failure of the request.  bool(true for success)   responseCode The response code of HTTP response, if this request is an HTTP call.  int   type The type of each request, such as Database, HTTP, RPC, or gRPC.  enum   detectPoint Where the relation is detected. The value may be client, server, or proxy. yes enum   tlsMode The TLS mode between source and destination services, such as service_relation_mtls_cpm = from(ServiceRelation.*).filter(tlsMode == \u0026quot;mTLS\u0026quot;).cpm()  string   sideCar.internalErrorCode The sidecar/gateway proxy internal error code. The value is based on the implementation.  string   tcpInfo.receivedBytes The received bytes of the TCP traffic, if this request is a TCP call.  long   tcpInfo.sentBytes The sent bytes of the TCP traffic, if this request is a TCP call.  long    SCOPE ServiceInstanceRelation This calculates the metrics data from each request between service instances.\n   Name Remarks Group Key Type     sourceServiceName The name of the source service.  string   sourceServiceNodeType The type of node to which the Service or Network address belongs, such as Normal, Database, MQ, or Cache.  enum   sourceServiceInstanceName The name of the source service instance.  string   destServiceName The name of the destination service.     destServiceNodeType The type of node to which the Service or Network address belongs, such as Normal, Database, MQ, or Cache.  string   destServiceInstanceName The name of the destination service instance.  string   endpoint The endpoint used in this call.  string   componentId The ID of the component used in this call. yes string   latency The time taken by each request.  int   status Indicates the success or failure of the request.  bool(true for success)   responseCode The response code of the HTTP response, if this request is an HTTP call.  int   type The type of each request, such as Database, HTTP, RPC, or gRPC.  enum   detectPoint Where the relation is detected. The value may be client, server, or proxy. yes enum   tlsMode The TLS mode between source and destination service instances, such as service_instance_relation_mtls_cpm = from(ServiceInstanceRelation.*).filter(tlsMode == \u0026quot;mTLS\u0026quot;).cpm()  string   sideCar.internalErrorCode The sidecar/gateway proxy internal error code. The value is based on the implementation.  string   tcpInfo.receivedBytes The received bytes of the TCP traffic, if this request is a TCP call.  long   tcpInfo.sentBytes The sent bytes of the TCP traffic, if this request is a TCP call.  long    SCOPE EndpointRelation This calculates the metrics data of the dependency between endpoints. This relation is hard to detect, and it depends on the tracing library to propagate the previous endpoint. Therefore, the EndpointRelation scope aggregation comes into effect only in services under tracing by SkyWalking native agents, including auto instrument agents (like Java and .NET), OpenCensus SkyWalking exporter implementation, or other tracing context propagation in SkyWalking specification.\n   Name Remarks Group Key Type     endpoint The parent endpoint in the dependency.  string   serviceName The name of the service.  string   serviceNodeType The type of node to which the Service or Network address belongs, such as Normal, Database, MQ, or Cache.  enum   childEndpoint The endpoint used by the parent endpoint in row(1).  string   childServiceName The endpoint used by the parent service in row(1).  string   childServiceNodeType The type of node to which the Service or Network address belongs, such as Normal, Database, MQ, or Cache.  string   childServiceInstanceName The endpoint used by the parent service instance in row(1).  string   rpcLatency The latency of the RPC between the parent endpoint and childEndpoint, excluding the latency caused by the parent endpoint itself.     componentId The ID of the component used in this call. yes string   status Indicates the success or failure of the request.  bool(true for success)   responseCode The response code of the HTTP response, if this request is an HTTP call.  int   type The type of each request, such as Database, HTTP, RPC, or gRPC.  enum   detectPoint Indicates where the relation is detected. The value may be client, server, or proxy. yes enum    SCOPE BrowserAppTraffic This calculates the metrics data from each request of the browser application (browser only).\n   Name Remarks Group Key Type     name The browser application name of each request.  string   count The number of request, which is fixed at 1.  int   trafficCategory The traffic category. The value may be NORMAL, FIRST_ERROR, or ERROR.  enum   errorCategory The error category. The value may be AJAX, RESOURCE, VUE, PROMISE, or UNKNOWN.  enum    SCOPE BrowserAppSingleVersionTraffic This calculates the metrics data from each request of a single version in the browser application (browser only).\n   Name Remarks Group Key Type     name The single version name of each request.  string   serviceName The name of the browser application.  string   count The number of request, which is fixed at 1.  int   trafficCategory The traffic category. The value may be NORMAL, FIRST_ERROR, or ERROR.  enum   errorCategory The error category. The value may be AJAX, RESOURCE, VUE, PROMISE, or UNKNOWN.  enum    SCOPE BrowserAppPageTraffic This calculates the metrics data from each request of the page in the browser application (browser only).\n   Name Remarks Group Key Type     name The page name of each request.  string   serviceName The name of the browser application.  string   count The number of request, which is fixed at 1.  int   trafficCategory The traffic category. The value may be NORMAL, FIRST_ERROR, or ERROR.  enum   errorCategory The error category. The value may be AJAX, RESOURCE, VUE, PROMISE, or UNKNOWN.  enum    SCOPE BrowserAppPagePerf This calculates the metrics data form each request of the page in the browser application (browser only).\n   Name Remarks Group Key Type     name The page name of each request.  string   serviceName The name of the browser application.  string   redirectTime The time taken to redirect.  int(in ms)   dnsTime The DNS query time.  int(in ms)   ttfbTime Time to first byte.  int(in ms)   tcpTime TCP connection time.  int(in ms)   transTime Content transfer time.  int(in ms)   domAnalysisTime Dom parsing time.  int(in ms)   fptTime First paint time or blank screen time.  int(in ms)   domReadyTime Dom ready time.  int(in ms)   loadPageTime Page full load time.  int(in ms)   resTime Synchronous load resources in the page.  int(in ms)   sslTime Only valid for HTTPS.  int(in ms)   ttlTime Time to interact.  int(in ms)   firstPackTime First pack time.  int(in ms)   fmpTime First Meaningful Paint.  int(in ms)    ","excerpt":"Scopes and Fields Using the Aggregation Function, the requests will be grouped by time and Group …","ref":"/docs/main/latest/en/concepts-and-designs/scope-definitions/","title":"Scopes and Fields"},{"body":"Send Envoy metrics to SkyWalking with / without Istio Envoy defines a gRPC service to emit the metrics, whatever implements this protocol can be used to receive the metrics. SkyWalking has a built-in receiver that implements this protocol so that you can configure Envoy to emit its metrics to SkyWalking.\nAs an APM system, SkyWalking does not only receive and store the metrics emitted by Envoy, it also analyzes the topology of services and service instances.\nAttention: There are two versions of Envoy metrics service protocol up to date, v2 and v3, SkyWalking (8.3.0+) supports both of them.\nConfigure Envoy to send metrics to SkyWalking without Istio Envoy can be used with / without Istio\u0026rsquo;s control. This section introduces how to configure the standalone Envoy to send the metrics to SkyWalking.\nIn order to let Envoy send metrics to SkyWalking, we need to feed Envoy with a configuration which contains stats_sinks that includes envoy.metrics_service. This envoy.metrics_service should be configured as a config.grpc_service entry.\nThe interesting parts of the config is shown in the config below:\nstats_sinks: - name: envoy.metrics_service config: grpc_service: # Note: we can use google_grpc implementation as well. envoy_grpc: cluster_name: service_skywalking static_resources: ... clusters: - name: service_skywalking connect_timeout: 5s type: LOGICAL_DNS http2_protocol_options: {} dns_lookup_family: V4_ONLY lb_policy: ROUND_ROBIN load_assignment: cluster_name: service_skywalking endpoints: - lb_endpoints: - endpoint: address: socket_address: address: skywalking # This is the port where SkyWalking serving the Envoy Metrics Service gRPC stream. port_value: 11800 A more complete static configuration, can be observed here.\nNote that Envoy can also be configured dynamically through xDS Protocol.\nAs mentioned above, SkyWalking also builds the topology of services from the metrics, this is because Envoy also carries the service metadata along with the metrics, to feed the Envoy such metadata, another configuration part is as follows:\nnode: # ... other configs metadata: LABELS: app: test-app NAME: service-instance-name Configure Envoy to send metrics to SkyWalking with Istio Typically, Envoy can be also used under Istio\u0026rsquo;s control, where the configurations are much more simple because Istio composes the configurations for you and sends them to Envoy via xDS Protocol. Istio also automatically injects the metadata such as service name and instance name into the bootstrap configurations.\nUnder this circumstance, emitting the metrics to SyWalking is as simple as adding the option --set meshConfig.defaultConfig.envoyMetricsService.address=\u0026lt;skywalking.address.port.11800\u0026gt; to Istio install command, for example:\nistioctl install -y \\  --set profile=demo `# replace the profile as per your need` \\ --set meshConfig.defaultConfig.envoyMetricsService.address=\u0026lt;skywalking.address.port.11800\u0026gt; # replace \u0026lt;skywalking.address.port.11800\u0026gt; with your actual SkyWalking OAP address If you already have Istio installed, you can use the following command to apply the config without re-installing Istio:\nistioctl manifest install -y \\  --set profile=demo `# replace the profile as per your need` \\ --set meshConfig.defaultConfig.envoyMetricsService.address=\u0026lt;skywalking.address.port.11800\u0026gt; # replace \u0026lt;skywalking.address.port.11800\u0026gt; with your actual SkyWalking OAP address Metrics data Some Envoy statistics are listed in this list. A sample data that contains identifier can be found here, while the metrics only can be observed here.\n","excerpt":"Send Envoy metrics to SkyWalking with / without Istio Envoy defines a gRPC service to emit the …","ref":"/docs/main/latest/en/setup/envoy/metrics_service_setting/","title":"Send Envoy metrics to SkyWalking with / without Istio"},{"body":"Sending Envoy Metrics to SkyWalking OAP Server Example This is an example of sending Envoy Stats to SkyWalking OAP server through Metric Service.\nRunning the example The example requires docker and docker-compose to be installed in your local. It fetches images from Docker Hub.\nNote that in ths setup, we override the log4j2.xml config to set the org.apache.skywalking.oap.server.receiver.envoy logger level to DEBUG. This enables us to see the messages sent by Envoy to SkyWalking OAP server.\n$ make up $ docker-compose logs -f skywalking $ # Please wait for a moment until SkyWalking is ready and Envoy starts sending the stats. You will see similar messages like the following: skywalking_1 | 2019-08-31 23:57:40,672 - org.apache.skywalking.oap.server.receiver.envoy.MetricServiceGRPCHandler -26870 [grpc-default-executor-0] DEBUG [] - Received msg identifier { skywalking_1 | node { skywalking_1 | id: \u0026quot;ingress\u0026quot; skywalking_1 | cluster: \u0026quot;envoy-proxy\u0026quot; skywalking_1 | metadata { skywalking_1 | fields { skywalking_1 | key: \u0026quot;skywalking\u0026quot; skywalking_1 | value { skywalking_1 | string_value: \u0026quot;iscool\u0026quot; skywalking_1 | } skywalking_1 | } skywalking_1 | fields { skywalking_1 | key: \u0026quot;envoy\u0026quot; skywalking_1 | value { skywalking_1 | string_value: \u0026quot;isawesome\u0026quot; skywalking_1 | } skywalking_1 | } skywalking_1 | } skywalking_1 | locality { skywalking_1 | region: \u0026quot;ap-southeast-1\u0026quot; skywalking_1 | zone: \u0026quot;zone1\u0026quot; skywalking_1 | sub_zone: \u0026quot;subzone1\u0026quot; skywalking_1 | } skywalking_1 | build_version: \u0026quot;e349fb6139e4b7a59a9a359be0ea45dd61e589c5/1.11.1/Clean/RELEASE/BoringSSL\u0026quot; skywalking_1 | } skywalking_1 | } skywalking_1 | envoy_metrics { skywalking_1 | name: \u0026quot;cluster.service_skywalking.update_success\u0026quot; skywalking_1 | type: COUNTER skywalking_1 | metric { skywalking_1 | counter { skywalking_1 | value: 2.0 skywalking_1 | } skywalking_1 | timestamp_ms: 1567295859556 skywalking_1 | } skywalking_1 | } ... $ # To tear down: $ make down ","excerpt":"Sending Envoy Metrics to SkyWalking OAP Server Example This is an example of sending Envoy Stats to …","ref":"/docs/main/latest/en/setup/envoy/examples/metrics/readme/","title":"Sending Envoy Metrics to SkyWalking OAP Server Example"},{"body":"Service Auto Grouping SkyWalking supports various default and customized dashboard templates. Each template provides the reasonable layout for the services in the particular field. Such as, services with a language agent installed could have different metrics with service detected by the service mesh observability solution, and different with SkyWalking\u0026rsquo;s self-observability metrics dashboard.\nTherefore, since 8.3.0, SkyWalking OAP would generate the group based on this simple naming format.\n${service name} = [${group name}::]${logic name} Once the service name includes double colons(::), the literal string before the colons would be considered as the group name. In the latest GraphQL query, the group name has been provided as an option parameter.\n getAllServices(duration: Duration!, group: String): [Service!]!\n RocketBot UI dashboards(Standard type) support the group name for default and custom configurations.\n","excerpt":"Service Auto Grouping SkyWalking supports various default and customized dashboard templates. Each …","ref":"/docs/main/latest/en/setup/backend/service-auto-grouping/","title":"Service Auto Grouping"},{"body":"Service Auto Instrument Agent The service auto instrument agent is a subset of language-based native agents. This kind of agents is based on some language-specific features, especially those of a VM-based language.\nWhat does Auto Instrument mean? Many users learned about these agents when they first heard that \u0026ldquo;Not a single line of code has to be changed\u0026rdquo;. SkyWalking used to mention this in its readme page as well. However, this does not reflect the full picture. For end users, it is true that they no longer have to modify their codes in most cases. But it is important to understand that the codes are in fact still modified by the agent, which is usually known as \u0026ldquo;runtime code manipulation\u0026rdquo;. The underlying logic is that the auto instrument agent uses the VM interface for code modification to dynamically add in the instrument code, such as modifying the class in Java through javaagent premain.\nIn fact, although the SkyWalking team has mentioned that most auto instrument agents are VM-based, you may build such tools during compiling time rather than runtime.\nWhat are the limitations? Auto instrument is very helpful, as you may perform auto instrument during compiling time, without having to depend on VM features. But there are also certain limitations that come with it:\n  Higher possibility of in-process propagation in many cases. Many high-level languages, such as Java and .NET, are used for building business systems. Most business logic codes run in the same thread for each request, which causes propagation to be based on thread ID, in order for the stack module to make sure that the context is safe.\n  Only works in certain frameworks or libraries. Since the agents are responsible for modifying the codes during runtime, the codes are already known to the agent plugin developers. There is usually a list of frameworks or libraries supported by this kind of probes. For example, see the SkyWalking Java agent supported list.\n  Cross-thread operations are not always supported. Like what is mentioned above regarding in-process propagation, most codes (especially business codes) run in a single thread per request. But in some other cases, they operate across different threads, such as assigning tasks to other threads, task pools or batch processes. Some languages may even provide coroutine or similar components like Goroutine, which allows developers to run async process with low payload. In such cases, auto instrument will face problems.\n  So, there\u0026rsquo;s nothing mysterious about auto instrument. In short, agent developers write an activation script to make instrument codes work for you. That\u0026rsquo;s it!\nWhat is next? If you want to learn about manual instrument libs in SkyWalking, see the Manual instrument SDK section.\n","excerpt":"Service Auto Instrument Agent The service auto instrument agent is a subset of language-based native …","ref":"/docs/main/latest/en/concepts-and-designs/service-agent/","title":"Service Auto Instrument Agent"},{"body":"Service Mesh Probe Service Mesh probes use the extendable mechanism provided in the Service Mesh implementor, like Istio.\nWhat is Service Mesh? The following explanation comes from Istio\u0026rsquo;s documentation.\n The term \u0026ldquo;service mesh\u0026rdquo; is often used to describe the networks of microservices that make up such applications and the interactions between them. As a service mesh grows in size and complexity, it can become harder to understand and manage. Its requirements can include discovery, load balancing, failure recovery, metrics, and monitoring, and often more complex operational requirements such as A/B testing, canary releases, rate limiting, access control, and end-to-end authentication.\n Where does the probe collect data from? Istio is a typical Service Mesh design and implementor. It defines Control Panel and Data Panel, which are widely used. Here is the Istio Architecture:\nThe Service Mesh probe can choose to collect data from Data Panel. In Istio, it means collecting telemetry data from Envoy sidecar (Data Panel). The probe collects two telemetry entities from the client end and the server end per request.\nHow does Service Mesh make backend work? In this kind of probes, you can see that there is no trace related to them. So how does the SkyWalking platform manage to work?\nThe Service Mesh probe collects telemetry data from each request, so they know about information such as the source, destination, endpoint, latency and status. From these information, the backend can tell the whole topology map by combining these calls into lines, as well as the metrics of each node through their incoming requests. The backend requests for the same metrics data by parsing the trace data. In short: The Service Mesh metrics work exactly the same way as the metrics that are generated by trace parsers.\n","excerpt":"Service Mesh Probe Service Mesh probes use the extendable mechanism provided in the Service Mesh …","ref":"/docs/main/latest/en/concepts-and-designs/service-mesh-probe/","title":"Service Mesh Probe"},{"body":"Setting Override SkyWalking backend supports setting overrides by system properties and system environment variables. You may override the settings in application.yml\nSystem properties key rule ModuleName.ProviderName.SettingKey.\n  Example\nOverride restHost in this setting segment\n  core: default: restHost: ${SW_CORE_REST_HOST:0.0.0.0} restPort: ${SW_CORE_REST_PORT:12800} restContextPath: ${SW_CORE_REST_CONTEXT_PATH:/} gRPCHost: ${SW_CORE_GRPC_HOST:0.0.0.0} gRPCPort: ${SW_CORE_GRPC_PORT:11800} Use command arg\n-Dcore.default.restHost=172.0.4.12 System environment variables   Example\nOverride restHost in this setting segment through environment variables\n  core: default: restHost: ${REST_HOST:0.0.0.0} restPort: ${SW_CORE_REST_PORT:12800} restContextPath: ${SW_CORE_REST_CONTEXT_PATH:/} gRPCHost: ${SW_CORE_GRPC_HOST:0.0.0.0} gRPCPort: ${SW_CORE_GRPC_PORT:11800} If the REST_HOST  environment variable exists in your operating system and its value is 172.0.4.12, then the value of restHost here will be overwritten to 172.0.4.12; otherwise, it will be set to 0.0.0.0.\nPlaceholder nesting is also supported, like ${REST_HOST:${ANOTHER_REST_HOST:127.0.0.1}}. In this case, if the REST_HOST  environment variable does not exist, but the REST_ANOTHER_REST_HOSTHOST environment variable exists and its value is 172.0.4.12, then the value of restHost here will be overwritten to 172.0.4.12; otherwise, it will be set to 127.0.0.1.\n","excerpt":"Setting Override SkyWalking backend supports setting overrides by system properties and system …","ref":"/docs/main/latest/en/setup/backend/backend-setting-override/","title":"Setting Override"},{"body":"Setting Override In default, SkyWalking provide agent.config for agent.\nSetting override means end user can override the settings in these config file, through using system properties or agent options.\nSystem properties Use skywalking. + key in config file as system properties key, to override the value.\n  Why need this prefix?\nThe agent system properties and env share with target application, this prefix can avoid variable conflict.\n  Example\nOverride agent.application_code by this.\n  -Dskywalking.agent.application_code=31200 Agent options Add the properties after the agent path in JVM arguments.\n-javaagent:/path/to/skywalking-agent.jar=[option1]=[value1],[option2]=[value2]   Example\nOverride agent.application_code and logging.level by this.\n  -javaagent:/path/to/skywalking-agent.jar=agent.application_code=31200,logging.level=debug   Special characters\nIf a separator(, or =) in the option or value, it should be wrapped in quotes.\n  -javaagent:/path/to/skywalking-agent.jar=agent.ignore_suffix='.jpg,.jpeg' System environment variables   Example\nOverride agent.application_code and logging.level by this.\n  # The service name in UI agent.service_name=${SW_AGENT_NAME:Your_ApplicationName} # Logging level logging.level=${SW_LOGGING_LEVEL:INFO} If the SW_AGENT_NAME  environment variable exists in your operating system and its value is skywalking-agent-demo, then the value of agent.service_name here will be overwritten to skywalking-agent-demo, otherwise, it will be set to Your_ApplicationName.\nBy the way, Placeholder nesting is also supported, like ${SW_AGENT_NAME:${ANOTHER_AGENT_NAME:Your_ApplicationName}}. In this case, if the SW_AGENT_NAME  environment variable not exists, but the ANOTHER_AGENT_NAME environment variable exists and its value is skywalking-agent-demo, then the value of agent.service_name here will be overwritten to skywalking-agent-demo,otherwise, it will be set to Your_ApplicationName.\nOverride priority Agent Options \u0026gt; System.Properties(-D) \u0026gt; System environment variables \u0026gt; Config file\n","excerpt":"Setting Override In default, SkyWalking provide agent.config for agent.\nSetting override means end …","ref":"/docs/main/latest/en/setup/service-agent/java-agent/setting-override/","title":"Setting Override"},{"body":"Setup java agent  Agent is available for JDK 8 - 14. Find agent folder in SkyWalking release package Set agent.service_name in config/agent.config. Could be any String in English. Set collector.backend_service in config/agent.config. Default point to 127.0.0.1:11800, only works for local backend. Add -javaagent:/path/to/skywalking-package/agent/skywalking-agent.jar to JVM argument. And make sure to add it before the -jar argument.  The agent release dist is included in Apache official release. New agent package looks like this.\n+-- agent +-- activations apm-toolkit-log4j-1.x-activation.jar apm-toolkit-log4j-2.x-activation.jar apm-toolkit-logback-1.x-activation.jar ... +-- config agent.config +-- plugins apm-dubbo-plugin.jar apm-feign-default-http-9.x.jar apm-httpClient-4.x-plugin.jar ..... +-- optional-plugins apm-gson-2.x-plugin.jar ..... +-- bootstrap-plugins jdk-http-plugin.jar ..... +-- logs skywalking-agent.jar  Start your application.  Supported middleware, framework and library SkyWalking agent has supported various middlewares, frameworks and libraries. Read supported list to get them and supported version. If the plugin is in Optional² catalog, go to optional plugins section to learn how to active it.\n All plugins are in /plugins folder. The plugin jar is active when it is in there. Remove the plugin jar, it disabled. The default logging output folder is /logs.  Install javaagent FAQs  Linux Tomcat 7, Tomcat 8, Tomcat 9\nChange the first line of tomcat/bin/catalina.sh.  CATALINA_OPTS=\u0026#34;$CATALINA_OPTS-javaagent:/path/to/skywalking-agent/skywalking-agent.jar\u0026#34;; export CATALINA_OPTS  Windows Tomcat 7, Tomcat 8, Tomcat 9\nChange the first line of tomcat/bin/catalina.bat.  set \u0026#34;CATALINA_OPTS=-javaagent:/path/to/skywalking-agent/skywalking-agent.jar\u0026#34;  JAR file\nAdd -javaagent argument to command line in which you start your app. eg:  java -javaagent:/path/to/skywalking-agent/skywalking-agent.jar -jar yourApp.jar  Jetty\nModify jetty.sh, add -javaagent argument to command line in which you start your app. eg:  export JAVA_OPTIONS=\u0026#34;${JAVA_OPTIONS}-javaagent:/path/to/skywalking-agent/skywalking-agent.jar\u0026#34; Table of Agent Configuration Properties This is the properties list supported in agent/config/agent.config.\n   property key Description Default     agent.namespace Namespace isolates headers in cross process propagation. The HEADER name will be HeaderName:Namespace. Not set   agent.service_name The service name to represent a logic group providing the same capabilities/logic. Suggestion: set a unique name for every logic service group, service instance nodes share the same code, Max length is 50(UTF-8 char). Optional, once service_name follows \u0026lt;group name\u0026gt;::\u0026lt;logic name\u0026gt; format, OAP server assigns the group name to the service metadata. Your_ApplicationName   agent.sample_n_per_3_secs Negative or zero means off, by default.SAMPLE_N_PER_3_SECS means sampling N TraceSegment in 3 seconds tops. Not set   agent.authentication Authentication active is based on backend setting, see application.yml for more details.For most scenarios, this needs backend extensions, only basic match auth provided in default implementation. Not set   agent.trace_segment_ref_limit_per_span The max number of TraceSegmentRef in a single span to keep memory cost estimatable. 500   agent.span_limit_per_segment The max number of spans in a single segment. Through this config item, SkyWalking keep your application memory cost estimated. 300   agent.ignore_suffix If the operation name of the first span is included in this set, this segment should be ignored. Not set   agent.is_open_debugging_class If true, skywalking agent will save all instrumented classes files in /debugging folder. SkyWalking team may ask for these files in order to resolve compatible problem. Not set   agent.is_cache_enhanced_class If true, SkyWalking agent will cache all instrumented classes files to memory or disk files (decided by class cache mode), allow another java agent to enhance those classes that enhanced by SkyWalking agent. To use some Java diagnostic tools (such as BTrace, Arthas) to diagnose applications or add a custom java agent to enhance classes, you need to enable this feature. Read this FAQ for more details false   agent.class_cache_mode The instrumented classes cache mode: MEMORY or FILE. MEMORY: cache class bytes to memory, if instrumented classes is too many or too large, it may take up more memory. FILE: cache class bytes in /class-cache folder, automatically clean up cached class files when the application exits. MEMORY   agent.instance_name Instance name is the identity of an instance, should be unique in the service. If empty, SkyWalking agent will generate an 32-bit uuid. Default, use UUID@hostname as the instance name. Max length is 50(UTF-8 char) \u0026quot;\u0026quot;   agent.instance_properties[key]=value Add service instance custom properties. Not set   agent.cause_exception_depth How depth the agent goes, when log all cause exceptions. 5   agent.force_reconnection_period  Force reconnection period of grpc, based on grpc_channel_check_interval. 1   agent.operation_name_threshold  The operationName max length, setting this value \u0026gt; 190 is not recommended. 150   agent.keep_tracing Keep tracing even the backend is not available if this value is true. false   agent.force_tls Force open TLS for gRPC channel if this value is true. false   osinfo.ipv4_list_size Limit the length of the ipv4 list size. 10   collector.grpc_channel_check_interval grpc channel status check interval. 30   collector.heartbeat_period agent heartbeat report period. Unit, second. 30   collector.properties_report_period_factor The agent sends the instance properties to the backend every collector.heartbeat_period * collector.properties_report_period_factor seconds 10   collector.backend_service Collector SkyWalking trace receiver service addresses. 127.0.0.1:11800   collector.grpc_upstream_timeout How long grpc client will timeout in sending data to upstream. Unit is second. 30 seconds   collector.get_profile_task_interval Sniffer get profile task list interval. 20   collector.get_agent_dynamic_config_interval Sniffer get agent dynamic config interval 20   collector.dns_period_resolve_active If true, skywalking agent will enable periodically resolving DNS to update receiver service addresses. false   logging.level Log level: TRACE, DEBUG, INFO, WARN, ERROR, OFF. Default is info. INFO   logging.file_name Log file name. skywalking-api.log   logging.output Log output. Default is FILE. Use CONSOLE means output to stdout. FILE   logging.dir Log files directory. Default is blank string, means, use \u0026ldquo;{theSkywalkingAgentJarDir}/logs \u0026quot; to output logs. {theSkywalkingAgentJarDir} is the directory where the skywalking agent jar file is located \u0026quot;\u0026quot;   logging.resolver Logger resolver: PATTERN or JSON. The default is PATTERN, which uses logging.pattern to print traditional text logs. JSON resolver prints logs in JSON format. PATTERN   logging.pattern  Logging format. There are all conversion specifiers: * %level means log level. * %timestamp means now of time with format yyyy-MM-dd HH:mm:ss:SSS.\n* %thread means name of current thread.\n* %msg means some message which user logged. * %class means SimpleName of TargetClass. * %throwable means a throwable which user called. * %agent_name means agent.service_name. Only apply to the PatternLogger. %level %timestamp %thread %class : %msg %throwable   logging.max_file_size The max size of log file. If the size is bigger than this, archive the current file, and write into a new file. 300 * 1024 * 1024   logging.max_history_files The max history log files. When rollover happened, if log files exceed this number,then the oldest file will be delete. Negative or zero means off, by default. -1   statuscheck.ignored_exceptions Listed exceptions would not be treated as an error. Because in some codes, the exception is being used as a way of controlling business flow. \u0026quot;\u0026quot;   statuscheck.max_recursive_depth The max recursive depth when checking the exception traced by the agent. Typically, we don\u0026rsquo;t recommend setting this more than 10, which could cause a performance issue. Negative value and 0 would be ignored, which means all exceptions would make the span tagged in error status. 1   correlation.element_max_number Max element count in the correlation context. 3   correlation.value_max_length Max value length of each element. 128   correlation.auto_tag_keys Tag the span by the key/value in the correlation context, when the keys listed here exist. \u0026quot;\u0026quot;   jvm.buffer_size The buffer size of collected JVM info. 60 * 10   buffer.channel_size The buffer channel size. 5   buffer.buffer_size The buffer size. 300   profile.active If true, skywalking agent will enable profile when user create a new profile task. Otherwise disable profile. true   profile.max_parallel Parallel monitor segment count 5   profile.duration Max monitor segment time(minutes), if current segment monitor time out of limit, then stop it. 10   profile.dump_max_stack_depth Max dump thread stack depth 500   profile.snapshot_transport_buffer_size Snapshot transport to backend buffer size 50   meter.active If true, the agent collects and reports metrics to the backend. true   meter.report_interval Report meters interval. The unit is second 20   meter.max_meter_size Max size of the meter pool 500   plugin.mount Mount the specific folders of the plugins. Plugins in mounted folders would work. plugins,activations   plugin.peer_max_length  Peer maximum description limit. 200   plugin.exclude_plugins  Exclude some plugins define in plugins dir.Plugin names is defined in Agent plugin list \u0026quot;\u0026quot;   plugin.mongodb.trace_param If true, trace all the parameters in MongoDB access, default is false. Only trace the operation, not include parameters. false   plugin.mongodb.filter_length_limit If set to positive number, the WriteRequest.params would be truncated to this length, otherwise it would be completely saved, which may cause performance problem. 256   plugin.elasticsearch.trace_dsl If true, trace all the DSL(Domain Specific Language) in ElasticSearch access, default is false. false   plugin.springmvc.use_qualified_name_as_endpoint_name If true, the fully qualified method name will be used as the endpoint name instead of the request URL, default is false. false   plugin.toolit.use_qualified_name_as_operation_name If true, the fully qualified method name will be used as the operation name instead of the given operation name, default is false. false   plugin.jdbc.trace_sql_parameters If set to true, the parameters of the sql (typically java.sql.PreparedStatement) would be collected. false   plugin.jdbc.sql_parameters_max_length If set to positive number, the db.sql.parameters would be truncated to this length, otherwise it would be completely saved, which may cause performance problem. 512   plugin.jdbc.sql_body_max_length If set to positive number, the db.statement would be truncated to this length, otherwise it would be completely saved, which may cause performance problem. 2048   plugin.solrj.trace_statement If true, trace all the query parameters(include deleteByIds and deleteByQuery) in Solr query request, default is false. false   plugin.solrj.trace_ops_params If true, trace all the operation parameters in Solr request, default is false. false   plugin.light4j.trace_handler_chain If true, trace all middleware/business handlers that are part of the Light4J handler chain for a request. false   plugin.opgroup.* Support operation name customize group rules in different plugins. Read Group rule supported plugins Not set   plugin.springtransaction.simplify_transaction_definition_name If true, the transaction definition name will be simplified. false   plugin.jdkthreading.threading_class_prefixes Threading classes (java.lang.Runnable and java.util.concurrent.Callable) and their subclasses, including anonymous inner classes whose name match any one of the THREADING_CLASS_PREFIXES (splitted by ,) will be instrumented, make sure to only specify as narrow prefixes as what you\u0026rsquo;re expecting to instrument, (java. and javax. will be ignored due to safety issues) Not set   plugin.tomcat.collect_http_params This config item controls that whether the Tomcat plugin should collect the parameters of the request. Also, activate implicitly in the profiled trace. false   plugin.springmvc.collect_http_params This config item controls that whether the SpringMVC plugin should collect the parameters of the request, when your Spring application is based on Tomcat, consider only setting either plugin.tomcat.collect_http_params or plugin.springmvc.collect_http_params. Also, activate implicitly in the profiled trace. false   plugin.httpclient.collect_http_params This config item controls that whether the HttpClient plugin should collect the parameters of the request false   plugin.http.http_params_length_threshold When COLLECT_HTTP_PARAMS is enabled, how many characters to keep and send to the OAP backend, use negative values to keep and send the complete parameters, NB. this config item is added for the sake of performance. 1024   plugin.http.http_headers_length_threshold When include_http_headers declares header names, this threshold controls the length limitation of all header values. use negative values to keep and send the complete headers. Note. this config item is added for the sake of performance. 2048   plugin.http.include_http_headers Set the header names, which should be collected by the plugin. Header name must follow javax.servlet.http definition. Multiple names should be split by comma. ``(No header would be collected) |   plugin.feign.collect_request_body This config item controls that whether the Feign plugin should collect the http body of the request. false   plugin.feign.filter_length_limit When COLLECT_REQUEST_BODY is enabled, how many characters to keep and send to the OAP backend, use negative values to keep and send the complete body. 1024   plugin.feign.supported_content_types_prefix When COLLECT_REQUEST_BODY is enabled and content-type start with SUPPORTED_CONTENT_TYPES_PREFIX, collect the body of the request , multiple paths should be separated by , application/json,text/   plugin.influxdb.trace_influxql If true, trace all the influxql(query and write) in InfluxDB access, default is true. true   plugin.dubbo.collect_consumer_arguments Apache Dubbo consumer collect arguments in RPC call, use Object#toString to collect arguments. false   plugin.dubbo.consumer_arguments_length_threshold When plugin.dubbo.collect_consumer_arguments is true, Arguments of length from the front will to the OAP backend 256   plugin.dubbo.collect_provider_arguments Apache Dubbo provider collect arguments in RPC call, use Object#toString to collect arguments. false   plugin.dubbo.consumer_provider_length_threshold When plugin.dubbo.provider_consumer_arguments is true, Arguments of length from the front will to the OAP backend 256   plugin.kafka.bootstrap_servers A list of host/port pairs to use for establishing the initial connection to the Kafka cluster. localhost:9092   plugin.kafka.get_topic_timeout Timeout period of reading topics from the Kafka server, the unit is second. 10   plugin.kafka.consumer_config Kafka producer configuration.    plugin.kafka.producer_config Kafka producer configuration. Read producer configure to get more details. Check Kafka report doc for more details and examples.    plugin.kafka.topic_meter Specify which Kafka topic name for Meter System data to report to. skywalking_meters   plugin.kafka.topic_metrics Specify which Kafka topic name for JVM metrics data to report to. skywalking_metrics   plugin.kafka.topic_segment Specify which Kafka topic name for traces data to report to. skywalking_segments   plugin.kafka.topic_profilings Specify which Kafka topic name for Thread Profiling snapshot to report to. skywalking_profilings   plugin.kafka.topic_management Specify which Kafka topic name for the register or heartbeat data of Service Instance to report to. skywalking_managements   plugin.kafka.namespace isolate multi OAP server when using same Kafka cluster (final topic name will append namespace before Kafka topics with - ). `` |   plugin.springannotation.classname_match_regex Match spring beans with regular expression for the class name. Multiple expressions could be separated by a comma. This only works when Spring annotation plugin has been activated. All the spring beans tagged with @Bean,@Service,@Dao, or @Repository.   plugin.toolkit.log.transmit_formatted Whether or not to transmit logged data as formatted or un-formatted. true   plugin.toolkit.log.grpc.reporter.server_host Specify which grpc server\u0026rsquo;s host for log data to report to. 127.0.0.1   plugin.toolkit.log.grpc.reporter.server_port Specify which grpc server\u0026rsquo;s port for log data to report to. 11800   plugin.toolkit.log.grpc.reporter.max_message_size Specify the maximum size of log data for grpc client to report to. 10485760   plugin.toolkit.log.grpc.reporter.upstream_timeout How long grpc client will timeout in sending data to upstream. Unit is second. 30 seconds   plugin.lettuce.trace_redis_parameters If set to true, the parameters of Redis commands would be collected by Lettuce agent. false   plugin.lettuce.redis_parameter_max_length If set to positive number and plugin.lettuce.trace_redis_parameters is set to true, Redis command parameters would be collected and truncated to this length. 128    Dynamic Configurations All configurations above are static, if you need to change some agent settings at runtime, please read CDS - Configuration Discovery Service document for more details.\nOptional Plugins Java agent plugins are all pluggable. Optional plugins could be provided in optional-plugins folder under agent or 3rd party repositories. For using these plugins, you need to put the target plugin jar file into /plugins.\nNow, we have the following known optional plugins.\n Plugin of tracing Spring annotation beans Plugin of tracing Oracle and Resin Filter traces through specified endpoint name patterns Plugin of Gson serialization lib in optional plugin folder. Plugin of Zookeeper 3.4.x in optional plugin folder. The reason of being optional plugin is, many business irrelevant traces are generated, which cause extra payload to agents and backends. At the same time, those traces may be just heartbeat(s). Customize enhance Trace methods based on description files, rather than write plugin or change source codes. Plugin of Spring Cloud Gateway 2.1.x in optional plugin folder. Please only active this plugin when you install agent in Spring Gateway. spring-cloud-gateway-2.x-plugin and spring-webflux-5.x-plugin are both required. Plugin of Spring Transaction in optional plugin folder. The reason of being optional plugin is, many local span are generated, which also spend more CPU, memory and network. Plugin of Kotlin coroutine provides the tracing across coroutines automatically. As it will add local spans to all across routines scenarios, Please assess the performance impact. Plugin of quartz-scheduler-2.x in the optional plugin folder. The reason for being an optional plugin is, many task scheduling systems are based on quartz-scheduler, this will cause duplicate tracing and link different sub-tasks as they share the same quartz level trigger, such as ElasticJob. Plugin of spring-webflux-5.x in the optional plugin folder. Please only activate this plugin when you use webflux alone as a web container. If you are using SpringMVC 5 or Spring Gateway, you don\u0026rsquo;t need this plugin. Plugin of mybatis-3.x in optional plugin folder. The reason of being optional plugin is, many local span are generated, which also spend more CPU, memory and network.  Bootstrap class plugins All bootstrap plugins are optional, due to unexpected risk. Bootstrap plugins are provided in bootstrap-plugins folder. For using these plugins, you need to put the target plugin jar file into /plugins.\nNow, we have the following known bootstrap plugins.\n Plugin of JDK HttpURLConnection. Agent is compatible with JDK 1.6+ Plugin of JDK Callable and Runnable. Agent is compatible with JDK 1.6+  The Logic Endpoint In default, all the RPC server-side names as entry spans, such as RESTFul API path and gRPC service name, would be endpoints with metrics. At the same time, SkyWalking introduces the logic endpoint concept, which allows plugins and users to add new endpoints without adding new spans. The following logic endpoints are added automatically by plugins.\n GraphQL Query and Mutation are logic endpoints by using the names of them. Spring\u0026rsquo;s ScheduledMethodRunnable jobs are logic endpoints. The name format is SpringScheduled/${className}/${methodName}. Apache ShardingSphere ElasticJob\u0026rsquo;s jobs are logic endpoints. The name format is ElasticJob/${jobName}. XXLJob\u0026rsquo;s jobs are logic endpoints. The name formats include xxl-job/MethodJob/${className}.${methodName}, xxl-job/ScriptJob/${GlueType}/id/${jobId}, and xxl-job/SimpleJob/${className}. Quartz(optional plugin)\u0026rsquo;s jobs are logic endpoints. the name format is quartz-scheduler/${className}.  User could use the SkyWalking\u0026rsquo;s application toolkits to add the tag into the local span to label the span as a logic endpoint in the analysis stage. The tag is, key=x-le and value = {\u0026quot;logic-span\u0026quot;:true}.\nAdvanced Features  Set the settings through system properties for config file override. Read setting override. Use gRPC TLS to link backend. See open TLS Monitor a big cluster by different SkyWalking services. Use Namespace to isolate the context propagation. Set client token if backend open token authentication. Application Toolkit, are a collection of libraries, provided by SkyWalking APM. Using them, you have a bridge between your application and SkyWalking APM agent.  If you want your codes to interact with SkyWalking agent, including getting trace id, setting tags, propagating custom data etc.. Try SkyWalking manual APIs. If you require customized metrics, try SkyWalking Meter System Toolkit. If you want to continue traces across thread manually, use across thread solution APIs. If you want to forward MicroMeter/Spring Sleuth metrics to Meter System, use SkyWalking MicroMeter Register. If you want to use OpenTracing Java APIs, try SkyWalking OpenTracing compatible tracer. More details you could find at http://opentracing.io If you want to tolerate some exceptions, read tolerate custom exception doc. If you want to print trace context(e.g. traceId) in your logs, or collect logs, choose the log frameworks, log4j, log4j2, logback.   If you want to specify the path of your agent.config file. Read set config file through system properties  Advanced Reporters The advanced report provides an alternative way to submit the agent collected data to the backend. All of them are in the optional-reporter-plugins folder, move the one you needed into the reporter-plugins folder for the activation. Notice, don\u0026rsquo;t try to activate multiple reporters, that could cause unexpected fatal errors.\n Use Kafka to transport the traces, JVM metrics, instance properties, and profiled snapshots to the backend. Read the How to enable Kafka Reporter for more details.  Plugin Development Guide SkyWalking java agent supports plugin to extend the supported list. Please follow our Plugin Development Guide.\n","excerpt":"Setup java agent  Agent is available for JDK 8 - 14. Find agent folder in SkyWalking release package …","ref":"/docs/main/latest/en/setup/service-agent/java-agent/readme/","title":"Setup java agent"},{"body":"Skywalking Agent List  activemq-5.x armeria-063-084 armeria-085 armeria-086 armeria-098 async-http-client-2.x avro-1.x brpc-java canal-1.x cassandra-java-driver-3.x dbcp-2.x dubbo ehcache-2.x elastic-job-2.x elastic-job-3.x elasticsearch-5.x elasticsearch-6.x elasticsearch-7.x feign-default-http-9.x feign-pathvar-9.x finagle graphql grpc-1.x gson-2.8.x h2-1.x hbase-1.x/2.x httpasyncclient-4.x httpclient-3.x httpclient-4.x hystrix-1.x influxdb-2.x jdk-http-plugin jdk-threading-plugin jedis-2.x jetty-client-9.0 jetty-client-9.x jetty-server-9.x kafka-0.11.x/1.x/2.x kotlin-coroutine lettuce-5.x light4j mariadb-2.x memcache-2.x mongodb-2.x mongodb-3.x mongodb-4.x motan-0.x mybatis-3.x mysql-5.x mysql-6.x mysql-8.x netty-socketio nutz-http-1.x nutz-mvc-annotation-1.x okhttp-3.x okhttp-4.x play-2.x postgresql-8.x pulsar quasar quartz-scheduler-2.x rabbitmq-5.x redisson-3.x resteasy-server-3.x rocketMQ-3.x rocketMQ-4.x servicecomb-0.x servicecomb-1.x sharding-jdbc-1.5.x sharding-sphere-3.x sharding-sphere-4.0.0 sharding-sphere-4.1.0 sharding-sphere-4.x sharding-sphere-4.x-rc3 sofarpc solrj-7.x spring-annotation spring-async-annotation-5.x spring-cloud-feign-1.x spring-cloud-feign-2.x spring-cloud-gateway-2.0.x spring-cloud-gateway-2.1.x spring-concurrent-util-4.x spring-core-patch spring-kafka-1.x spring-kafka-2.x spring-mvc-annotation spring-mvc-annotation-3.x spring-mvc-annotation-4.x spring-mvc-annotation-5.x spring-resttemplate-4.x spring-scheduled-annotation spring-tx spring-webflux-5.x spring-webflux-5.x-webclient spymemcached-2.x struts2-2.x thrift tomcat-7.x/8.x toolkit-counter toolkit-gauge toolkit-histogram toolkit-kafka toolkit-log4j toolkit-log4j2 toolkit-logback toolkit-opentracing toolkit-tag toolkit-trace toolkit-exception undertow-2.x-plugin vertx-core-3.x xxl-job-2.x zookeeper-3.4.x mssql-jtds-1.x mssql-jdbc apache-cxf-3.x jsonrpc4j spring-cloud-gateway-3.x  ","excerpt":"Skywalking Agent List  activemq-5.x armeria-063-084 armeria-085 armeria-086 armeria-098 …","ref":"/docs/main/latest/en/setup/service-agent/java-agent/plugin-list/","title":"Skywalking Agent List"},{"body":"SkyWalking Cross Process Correlation Headers Protocol  Version 1.0  The Cross Process Correlation Headers Protocol is used to transport custom data by leveraging the capability of Cross Process Propagation Headers Protocol.\nThis is an optional and additional protocol for language tracer implementation. All tracer implementation could consider implementing this. Cross Process Correlation Header key is sw8-correlation. The value is the encoded(key):encoded(value) list with elements splitted by , such as base64(string key):base64(string value),base64(string key2):base64(string value2).\nRecommendations for language APIs The following implementation method is recommended for different language APIs.\n TraceContext#putCorrelation and TraceContext#getCorrelation are recommended to write and read the correlation context, with key/value string. The key should be added if it is absent. The latter writes should override the previous value. The total number of all keys should be less than 3, and the length of each value should be less than 128 bytes. The context should be propagated as well when tracing context is propagated across threads and processes.  ","excerpt":"SkyWalking Cross Process Correlation Headers Protocol  Version 1.0  The Cross Process Correlation …","ref":"/docs/main/latest/en/protocols/skywalking-cross-process-correlation-headers-protocol-v1/","title":"SkyWalking Cross Process Correlation Headers Protocol"},{"body":"SkyWalking Cross Process Propagation Headers Protocol  Version 3.0  SkyWalking is more akin to an APM system, rather than a common distributed tracing system. SkyWalking\u0026rsquo;s headers are much more complex than those found in a common distributed tracing system. The reason behind their complexity is for better analysis performance of the OAP. You can find many similar mechanisms in other commercial APM systems (some of which are even more complex than ours!).\nAbstract The SkyWalking Cross Process Propagation Headers Protocol v3, also known as the sw8 protocol, is designed for context propagation.\nStandard Header Item The standard header is the minimal requirement for context propagation.\n Header Name: sw8. Header Value: 8 fields split by -. The length of header value must be less than 2k (default).  Example of the value format: XXXXX-XXXXX-XXXX-XXXX\nValues Values must include the following segments, and all string type values are in BASE64 encoding.\n Required:   Sample. 0 or 1. 0 means that the context exists, but it could (and most likely will) be ignored. 1 means this trace needs to be sampled and sent to the backend. Trace ID. String(BASE64 encoded). A literal string that is globally unique. Parent trace segment ID. String(BASE64 encoded). A literal string that is globally unique. Parent span ID. Must be an integer. It begins with 0. This span ID points to the parent span in parent trace segment. Parent service. String(BASE64 encoded). Its length should be no more than 50 UTF-8 characters. Parent service instance. String(BASE64 encoded). Its length should be no more than 50 UTF-8 characters. Parent endpoint. String(BASE64 encoded). The operation name of the first entry span in the parent segment. Its length should be less than 150 UTF-8 characters. Target address of this request used on the client end. String(BASE64 encoded). The network address (not necessarily IP + port) used on the client end to access this target service.   Sample values: 1-TRACEID-SEGMENTID-3-PARENT_SERVICE-PARENT_INSTANCE-PARENT_ENDPOINT-IPPORT  Extension Header Item The extension header item is designed for advanced features. It provides interaction capabilities between the agents deployed in upstream and downstream services.\n Header Name: sw8-x Header Value: Split by -. The fields are extendable.  Values The current value includes fields.\n Tracing Mode. Empty, 0, or 1. Empty or 0 is the default. 1 indicates that all spans generated in this context will skip analysis, spanObject#skipAnalysis=true. This context is propagated to upstream by default, unless it is changed in the tracing process. The timestamp of sending on the client end. This is used in async RPC, such as MQ. Once it is set, the consumer end would calculate the latency between sending and receiving, and tag the latency in the span by using key transmission.latency automatically.  ","excerpt":"SkyWalking Cross Process Propagation Headers Protocol  Version 3.0  SkyWalking is more akin to an …","ref":"/docs/main/latest/en/protocols/skywalking-cross-process-propagation-headers-protocol-v3/","title":"SkyWalking Cross Process Propagation Headers Protocol"},{"body":"Apache SkyWalking release guide If you\u0026rsquo;re a committer, you can learn how to release SkyWalking in The Apache Way and start the voting process by reading this document.\nSet up your development environment Follow the steps in the Apache maven deployment environment document to set gpg tool and encrypt passwords.\nUse the following block as a template and place it in ~/.m2/settings.xml.\n\u0026lt;settings\u0026gt; ... \u0026lt;servers\u0026gt; \u0026lt;!-- To publish a snapshot of some part of Maven --\u0026gt; \u0026lt;server\u0026gt; \u0026lt;id\u0026gt;apache.snapshots.https\u0026lt;/id\u0026gt; \u0026lt;username\u0026gt; \u0026lt;!-- YOUR APACHE LDAP USERNAME --\u0026gt; \u0026lt;/username\u0026gt; \u0026lt;password\u0026gt; \u0026lt;!-- YOUR APACHE LDAP PASSWORD (encrypted) --\u0026gt; \u0026lt;/password\u0026gt; \u0026lt;/server\u0026gt; \u0026lt;!-- To stage a release of some part of Maven --\u0026gt; \u0026lt;server\u0026gt; \u0026lt;id\u0026gt;apache.releases.https\u0026lt;/id\u0026gt; \u0026lt;username\u0026gt; \u0026lt;!-- YOUR APACHE LDAP USERNAME --\u0026gt; \u0026lt;/username\u0026gt; \u0026lt;password\u0026gt; \u0026lt;!-- YOUR APACHE LDAP PASSWORD (encrypted) --\u0026gt; \u0026lt;/password\u0026gt; \u0026lt;/server\u0026gt; ... \u0026lt;/servers\u0026gt; \u0026lt;/settings\u0026gt; Add your GPG public key  Add your GPG public key into the SkyWalking GPG KEYS file. If you are a committer, use your Apache ID and password to log in this svn, and update the file. Don\u0026rsquo;t override the existing file. Upload your GPG public key to the public GPG site, such as MIT\u0026rsquo;s site. This site should be in the Apache maven staging repository checklist.  Test your settings This step is only for testing purpose. If your env is correctly set, you don\u0026rsquo;t need to check every time.\n./mvnw clean install -Pall (this will build artifacts, sources and sign) Prepare for the release ./mvnw release:clean ./mvnw release:prepare -DautoVersionSubmodules=true -Pall  Set version number as x.y.z, and tag as vx.y.z (The version tag must start with v. You will find out why this is necessary in the next step.)  You could do a GPG signature before preparing for the release. If you need to input the password to sign, and the maven doesn\u0026rsquo;t provide you with the opportunity to do so, this may lead to failure of the release. To resolve this, you may run gpg --sign xxx in any file. This will allow it to remember the password for long enough to prepare for the release.\nStage the release ./mvnw release:perform -DskipTests -Pall  The release will be automatically inserted into a temporary staging repository.  Build and sign the source code package export RELEASE_VERSION=x.y.z (example: RELEASE_VERSION=5.0.0-alpha) cd tools/releasing bash create_source_release.sh This script takes care of the following things:\n Use v + RELEASE_VERSION as tag to clone the codes. Complete git submodule init/update. Exclude all unnecessary files in the target source tar, such as .git, .github, and .gitmodules. See the script for more details. Execute gpg and shasum 512.  apache-skywalking-apm-x.y.z-src.tgz and files ending with .asc and .sha512 may be found in the tools/releasing folder.\nLocate and download the distribution package in Apache Nexus Staging repositories  Use your Apache ID to log in to https://repository.apache.org/. Go to https://repository.apache.org/#stagingRepositories. Search skywalking and find your staging repository. Close the repository and wait for all checks to pass. In this step, your GPG KEYS will be checked. See the set PGP document, if you haven\u0026rsquo;t done it before. Go to {REPO_URL}/org/apache/skywalking/apache-skywalking-apm/x.y.z. Download .tar.gz and .zip and files ending with .asc and .sha1.  Upload to Apache svn  Use your Apache ID to log in to https://dist.apache.org/repos/dist/dev/skywalking/. Create a folder and name it by the release version and round, such as: x.y.z Upload the source code package to the folder with files ending with .asc and .sha512.  Package name: apache-skywalking-x.y.z-src.tar.gz See Section \u0026ldquo;Build and sign the source code package\u0026rdquo; for more details   Upload the distribution package to the folder with files ending with .asc and .sha512.  Package name: apache-skywalking-bin-x.y.z.tar.gz and apache-skywalking-bin-x.y.z.zip See Section \u0026ldquo;Locate and download the distribution package in Apache Nexus Staging repositories\u0026rdquo; for more details. Create a .sha512 package: shasum -a 512 file \u0026gt; file.sha512    Make the internal announcements Send an announcement mail in dev mail list.\nMail title: [ANNOUNCE] SkyWalking x.y.z test build available Mail content: The test build of x.y.z is available. We welcome any comments you may have, and will take all feedback into account if a quality vote is called for this build. Release notes: * https://github.com/apache/skywalking/blob/master/changes/changes-x.y.z.md Release Candidate: * https://dist.apache.org/repos/dist/dev/skywalking/xxxx * sha512 checksums - sha512xxxxyyyzzz apache-skywalking-apm-x.x.x-src.tgz - sha512xxxxyyyzzz apache-skywalking-apm-bin-x.x.x.tar.gz - sha512xxxxyyyzzz apache-skywalking-apm-bin-x.x.x.zip Maven 2 staging repository: * https://repository.apache.org/content/repositories/xxxx/org/apache/skywalking/ Release Tag : * (Git Tag) x.y.z Release CommitID : * https://github.com/apache/skywalking/tree/(Git Commit ID) * Git submodule * skywalking-ui: https://github.com/apache/skywalking-rocketbot-ui/tree/(Git Commit ID) * apm-protocol/apm-network/src/main/proto: https://github.com/apache/skywalking-data-collect-protocol/tree/(Git Commit ID) * oap-server/server-query-plugin/query-graphql-plugin/src/main/resources/query-protocol https://github.com/apache/skywalking-query-protocol/tree/(Git Commit ID) Keys to verify the Release Candidate : * https://dist.apache.org/repos/dist/release/skywalking/KEYS Guide to build the release from source : * https://github.com/apache/skywalking/blob/x.y.z/docs/en/guides/How-to-build.md A vote regarding the quality of this test build will be initiated within the next couple of days. Wait for at least 48 hours for test responses Any PMC member, committer or contributor can test the release features and provide feedback. Based on that, the PMC will decide whether to start the voting process.\nCall a vote in dev Call a vote in dev@skywalking.apache.org\nMail title: [VOTE] Release Apache SkyWalking version x.y.z Mail content: Hi All, This is a call for vote to release Apache SkyWalking version x.y.z. Release notes: * https://github.com/apache/skywalking/blob/master/changes/changes-x.y.z.md Release Candidate: * https://dist.apache.org/repos/dist/dev/skywalking/xxxx * sha512 checksums - sha512xxxxyyyzzz apache-skywalking-apm-x.x.x-src.tgz - sha512xxxxyyyzzz apache-skywalking-apm-bin-x.x.x.tar.gz - sha512xxxxyyyzzz apache-skywalking-apm-bin-x.x.x.zip Maven 2 staging repository: * https://repository.apache.org/content/repositories/xxxx/org/apache/skywalking/ Release Tag : * (Git Tag) x.y.z Release CommitID : * https://github.com/apache/skywalking/tree/(Git Commit ID) * Git submodule * skywalking-ui: https://github.com/apache/skywalking-rocketbot-ui/tree/(Git Commit ID) * apm-protocol/apm-network/src/main/proto: https://github.com/apache/skywalking-data-collect-protocol/tree/(Git Commit ID) * oap-server/server-query-plugin/query-graphql-plugin/src/main/resources/query-protocol https://github.com/apache/skywalking-query-protocol/tree/(Git Commit ID) Keys to verify the Release Candidate : * https://dist.apache.org/repos/dist/release/skywalking/KEYS Guide to build the release from source : * https://github.com/apache/skywalking/blob/x.y.z/docs/en/guides/How-to-build.md Voting will start now (xxxx date) and will remain open for at least 72 hours, Request all PMC members to give their vote. [ ] +1 Release this package. [ ] +0 No opinion. [ ] -1 Do not release this package because.... Vote Check All PMC members and committers should check these before casting +1 votes.\n Features test. All artifacts in staging repository are published with .asc, .md5, and *sha1 files. Source code and distribution package (apache-skywalking-x.y.z-src.tar.gz, apache-skywalking-bin-x.y.z.tar.gz, apache-skywalking-bin-x.y.z.zip) are found in https://dist.apache.org/repos/dist/dev/skywalking/x.y.z with .asc and .sha512. LICENSE and NOTICE are in the source code and distribution package. Check shasum -c apache-skywalking-apm-x.y.z-src.tgz.sha512. Check gpg --verify apache-skywalking-apm-x.y.z-src.tgz.asc apache-skywalking-apm-x.y.z-src.tgz Build a distribution package from the source code package (apache-skywalking-x.y.z-src.tar.gz) by following this doc. Check the Apache License Header. Run docker run --rm -v $(pwd):/github/workspace apache/skywalking-eyes header check. (No binaries in source codes)  The voting process is as follows:\n All PMC member votes are +1 binding, and all other votes are +1 but non-binding. If you obtain at least 3 (+1 binding) votes with more +1 than -1 votes within 72 hours, the release will be approved.  Publish the release  Move source codes tar and distribution packages to https://dist.apache.org/repos/dist/release/skywalking/.  \u0026gt; export SVN_EDITOR=vim \u0026gt; svn mv https://dist.apache.org/repos/dist/dev/skywalking/x.y.z https://dist.apache.org/repos/dist/release/skywalking .... enter your apache password .... Release in the nexus staging repo. Public download source and distribution tar/zip are located in http://www.apache.org/dyn/closer.cgi/skywalking/x.y.z/xxx. The Apache mirror path is the only release information that we publish. Public asc and sha512 are located in https://www.apache.org/dist/skywalking/x.y.z/xxx. Public KEYS point to https://www.apache.org/dist/skywalking/KEYS. Update the website download page. http://skywalking.apache.org/downloads/ . Add a new download source, distribution, sha512, asc, and document links. The links can be found following rules (3) to (6) above. Add a release event on the website homepage and event page. Announce the public release with changelog or key features. Send ANNOUNCE email to dev@skywalking.apache.org, announce@apache.org. The sender should use the Apache email account.  Mail title: [ANNOUNCE] Apache SkyWalking x.y.z released Mail content: Hi all, Apache SkyWalking Team is glad to announce the first release of Apache SkyWalking x.y.z. SkyWalking: APM (application performance monitor) tool for distributed systems, especially designed for microservices, cloud native and container-based (Docker, Kubernetes, Mesos) architectures. This release contains a number of new features, bug fixes and improvements compared to version a.b.c(last release). The notable changes since x.y.z include: (Highlight key changes) 1. ... 2. ... 3. ... Please refer to the change log for the complete list of changes: https://github.com/apache/skywalking/blob/master/changes/changes-x.y.z.md Apache SkyWalking website: http://skywalking.apache.org/ Downloads: http://skywalking.apache.org/downloads/ Twitter: https://twitter.com/ASFSkyWalking SkyWalking Resources: - GitHub: https://github.com/apache/skywalking - Issue: https://github.com/apache/skywalking/issues - Mailing list: dev@skywalkiing.apache.org - Apache SkyWalking Team Clean up the old releases Once the latest release has been published, you should clean up the old releases from the mirror system.\n Update the download links (source, dist, asc, and sha512) on the website to the archive repo (https://archive.apache.org/dist/skywalking). Remove previous releases from https://dist.apache.org/repos/dist/release/skywalking/.  ","excerpt":"Apache SkyWalking release guide If you\u0026rsquo;re a committer, you can learn how to release SkyWalking …","ref":"/docs/main/latest/en/guides/how-to-release/","title":"SkyWalking release guide"},{"body":"Skywalking with Kotlin coroutine This Plugin provides an auto instrument support plugin for Kotlin coroutine based on context snapshot.\nDescription SkyWalking provide tracing context propagation inside thread. In order to support Kotlin Coroutine, we provide this additional plugin.\nImplementation principle As we know, Kotlin coroutine switches the execution thread by CoroutineDispatcher.\n Create a snapshot of the current context before dispatch the continuation. Then create a coroutine span after thread switched, mark the span continued with the snapshot. Every new span which created in the new thread will be a child of this coroutine span. So we can link those span together in a tracing. After the original runnable executed, we need to stop the coroutine span for cleaning thread state.  Some screenshots Run without the plugin We run a Kotlin coroutine based gRPC server without this coroutine plugin.\nYou can find, the one call (client -\u0026gt; server1 -\u0026gt; server2) has been split two tracing paths.\n Server1 without exit span and server2 tracing path.  Server2 tracing path.   Run with the plugin Without changing codes manually, just install the plugin. We can find the spans be connected together. We can get all info of one client call.\n","excerpt":"Skywalking with Kotlin coroutine This Plugin provides an auto instrument support plugin for Kotlin …","ref":"/docs/main/latest/en/setup/service-agent/java-agent/agent-optional-plugins/kotlin-coroutine-plugin/","title":"Skywalking with Kotlin coroutine"},{"body":"Slow Database Statement Slow Database statements are significant important to find out the bottleneck of the system, which relied on Database.\nSlow DB statements are based on sampling, right now, the core samples top 50 slowest in every 10 minutes. But duration of those statements must be slower than threshold.\nThe setting format is following, unit is millisecond.\n database-type:thresholdValue,database-type2:thresholdValue2\n Default setting is default:200,mongodb:100. Reserved DB type is default, which be as default threshold for all database types, except set explicitly.\nNotice, the threshold should not be too small, like 1ms. Functionally, it works, but would cost OAP performance issue, if your system statement access time are mostly more than 1ms.\n","excerpt":"Slow Database Statement Slow Database statements are significant important to find out the …","ref":"/docs/main/latest/en/setup/backend/slow-db-statement/","title":"Slow Database Statement"},{"body":"Source and scope extension for new metrics From the OAL scope introduction, you should already have understood what a scope is. If you would like to create more extensions, you need to have a deeper understanding of what a source is.\nSource and scope are interrelated concepts. Scope declares the ID (int) and name, while source declares the attributes. Follow these steps to create a new source and sccope.\n In the OAP core module, it provides SourceReceiver internal services.  public interface SourceReceiver extends Service { void receive(Source source); } All data of the analysis must be a org.apache.skywalking.oap.server.core.source.Source sub class that is tagged by @SourceType annotation, and included in the org.apache.skywalking package. Then, it can be supported by the OAL script and OAP core.  Take the existing source service as an example.\n@ScopeDeclaration(id = SERVICE_INSTANCE, name = \u0026#34;ServiceInstance\u0026#34;, catalog = SERVICE_INSTANCE_CATALOG_NAME) @ScopeDefaultColumn.VirtualColumnDefinition(fieldName = \u0026#34;entityId\u0026#34;, columnName = \u0026#34;entity_id\u0026#34;, isID = true, type = String.class) public class ServiceInstance extends Source { @Override public int scope() { return DefaultScopeDefine.SERVICE_INSTANCE; } @Override public String getEntityId() { return String.valueOf(id); } @Getter @Setter private int id; @Getter @Setter @ScopeDefaultColumn.DefinedByField(columnName = \u0026#34;service_id\u0026#34;) private int serviceId; @Getter @Setter private String name; @Getter @Setter private String serviceName; @Getter @Setter private String endpointName; @Getter @Setter private int latency; @Getter @Setter private boolean status; @Getter @Setter private int responseCode; @Getter @Setter private RequestType type; }  The scope() method in source returns an ID, which is not a random value. This ID must be declared through the @ScopeDeclaration annotation too. The ID in @ScopeDeclaration and ID in scope() method must be the same for this source.\n  The String getEntityId() method in source requests the return value representing the unique entity to which the scope relates. For example, in this service scope, the ID is the service ID, which represents a particular service, like the Order service. This value is used in the OAL group mechanism.\n  @ScopeDefaultColumn.VirtualColumnDefinition and @ScopeDefaultColumn.DefinedByField are required. All declared fields (virtual/byField) will be pushed into a persistent entity, and maps to lists such as the ElasticSearch index and Database table column. For example, the entity ID and service ID for endpoint and service instance level scope are usually included. Take a reference from all existing scopes. All these fields are detected by OAL Runtime, and are required during query.\n  Add scope name as keyword to OAL grammar definition file, OALLexer.g4, which is at the antlr4 folder of the generate-tool-grammar module.\n  Add scope name as keyword to the parser definition file, OALParser.g4, which is located in the same folder as OALLexer.g4.\n   After finishing these steps, you could build a receiver, which do\n Obtain the original data of the metrics. Build the source, and send to SourceReceiver. Complete your OAL scripts. Repackage the project.  ","excerpt":"Source and scope extension for new metrics From the OAL scope introduction, you should already have …","ref":"/docs/main/latest/en/guides/source-extension/","title":"Source and scope extension for new metrics"},{"body":"Spring annotation plugin This plugin allows to trace all methods of beans in Spring context, which are annotated with @Bean, @Service, @Component and @Repository.\n Why does this plugin optional?  Tracing all methods in Spring context all creates a lot of spans, which also spend more CPU, memory and network. Of course you want to have spans as many as possible, but please make sure your system payload can support these.\n","excerpt":"Spring annotation plugin This plugin allows to trace all methods of beans in Spring context, which …","ref":"/docs/main/latest/en/setup/service-agent/java-agent/agent-optional-plugins/spring-annotation-plugin/","title":"Spring annotation plugin"},{"body":"Spring sleuth setup Spring Sleuth provides Spring Boot auto-configuration for distributed tracing. Skywalking integrates it\u0026rsquo;s micrometer part, and it can send metrics to the Skywalking Meter System.\nSet up agent  Add the Micrometer and Skywalking meter registry dependency into project pom.xml file. Also you could found more detail at Toolkit micrometer.  \u0026lt;dependency\u0026gt; \u0026lt;groupId\u0026gt;org.springframework.boot\u0026lt;/groupId\u0026gt; \u0026lt;artifactId\u0026gt;spring-boot-starter-actuator\u0026lt;/artifactId\u0026gt; \u0026lt;/dependency\u0026gt; \u0026lt;dependency\u0026gt; \u0026lt;groupId\u0026gt;org.apache.skywalking\u0026lt;/groupId\u0026gt; \u0026lt;artifactId\u0026gt;apm-toolkit-micrometer-registry\u0026lt;/artifactId\u0026gt; \u0026lt;version\u0026gt;${skywalking.version}\u0026lt;/version\u0026gt; \u0026lt;/dependency\u0026gt; Create the Skywalking meter resgitry into spring bean management.  @Bean SkywalkingMeterRegistry skywalkingMeterRegistry() { // Add rate configs If you need, otherwise using none args construct  SkywalkingConfig config = new SkywalkingConfig(Arrays.asList(\u0026#34;\u0026#34;)); return new SkywalkingMeterRegistry(config); } Set up backend receiver  Make sure enable meter receiver in the applicaiton.yml.  receiver-meter: selector: ${SW_RECEIVER_METER:default} default: Configure the meter config file, It already has the spring sleuth meter config. If you also has some customized meter at the agent side, please read meter document to configure meter.  Add UI dashboard   Open the dashboard view, click edit button to edit the templates.\n  Create a new template. Template type: Standard -\u0026gt; Template Configuration: Spring -\u0026gt; Input the Template Name.\n  Click view button, Finally get the spring sleuth dashboard.\n  Supported meter Supported 3 types information: Application, System, JVM.\n Application: HTTP request count and duration, JDBC max/idle/active connection count, Tomcat session active/reject count. System: CPU system/process usage, OS System load, OS Process file count. JVM: GC pause count and duration, Memory max/used/committed size, Thread peak/live/daemon count, Classes loaded/unloaded count.  ","excerpt":"Spring sleuth setup Spring Sleuth provides Spring Boot auto-configuration for distributed tracing. …","ref":"/docs/main/latest/en/setup/backend/spring-sleuth-setup/","title":"Spring sleuth setup"},{"body":"Start up mode In different deployment tool, such as k8s, you may need different startup mode. We provide another two optional startup modes.\nDefault mode Default mode. Do initialization works if necessary, start listen and provide service.\nRun /bin/oapService.sh(.bat) to start in this mode. Also when use startup.sh(.bat) to start.\nInit mode In this mode, oap server starts up to do initialization works, then exit. You could use this mode to init your storage, such as ElasticSearch indexes, MySQL and TiDB tables, and init data.\nRun /bin/oapServiceInit.sh(.bat) to start in this mode.\nNo-init mode In this mode, oap server starts up without initialization works, but it waits for ElasticSearch indexes, MySQL and TiDB tables existed, start listen and provide service. Meaning, this oap server expect another oap server to do the initialization.\nRun /bin/oapServiceNoInit.sh(.bat) to start in this mode.\n","excerpt":"Start up mode In different deployment tool, such as k8s, you may need different startup mode. We …","ref":"/docs/main/latest/en/setup/backend/backend-start-up-mode/","title":"Start up mode"},{"body":"Support custom enhance Here is an optional plugin apm-customize-enhance-plugin\nIntroduce SkyWalking has provided Java agent plugin development guide to help developers to build new plugin.\nThis plugin is not designed for replacement but for user convenience. The behaviour is very similar with @Trace toolkit, but without code change requirement, and more powerful, such as provide tag and log.\nHow to configure Implementing enhancements to custom classes requires two steps.\n Active the plugin, move the optional-plugins/apm-customize-enhance-plugin.jar to plugin/apm-customize-enhance-plugin.jar. Set plugin.customize.enhance_file in agent.config, which targets to rule file, such as /absolute/path/to/customize_enhance.xml. Set enhancement rules in customize_enhance.xml. \u0026lt;?xml version=\u0026#34;1.0\u0026#34; encoding=\u0026#34;UTF-8\u0026#34;?\u0026gt; \u0026lt;enhanced\u0026gt; \u0026lt;class class_name=\u0026#34;test.apache.skywalking.testcase.customize.service.TestService1\u0026#34;\u0026gt; \u0026lt;method method=\u0026#34;staticMethod()\u0026#34; operation_name=\u0026#34;/is_static_method\u0026#34; static=\u0026#34;true\u0026#34;/\u0026gt; \u0026lt;method method=\u0026#34;staticMethod(java.lang.String,int.class,java.util.Map,java.util.List,[Ljava.lang.Object;)\u0026#34; operation_name=\u0026#34;/is_static_method_args\u0026#34; static=\u0026#34;true\u0026#34;\u0026gt; \u0026lt;operation_name_suffix\u0026gt;arg[0]\u0026lt;/operation_name_suffix\u0026gt; \u0026lt;operation_name_suffix\u0026gt;arg[1]\u0026lt;/operation_name_suffix\u0026gt; \u0026lt;operation_name_suffix\u0026gt;arg[3].[0]\u0026lt;/operation_name_suffix\u0026gt; \u0026lt;tag key=\u0026#34;tag_1\u0026#34;\u0026gt;arg[2].[\u0026#39;k1\u0026#39;]\u0026lt;/tag\u0026gt; \u0026lt;tag key=\u0026#34;tag_2\u0026#34;\u0026gt;arg[4].[1]\u0026lt;/tag\u0026gt; \u0026lt;log key=\u0026#34;log_1\u0026#34;\u0026gt;arg[4].[2]\u0026lt;/log\u0026gt; \u0026lt;/method\u0026gt; \u0026lt;method method=\u0026#34;method()\u0026#34; static=\u0026#34;false\u0026#34;/\u0026gt; \u0026lt;method method=\u0026#34;method(java.lang.String,int.class)\u0026#34; operation_name=\u0026#34;/method_2\u0026#34; static=\u0026#34;false\u0026#34;\u0026gt; \u0026lt;operation_name_suffix\u0026gt;arg[0]\u0026lt;/operation_name_suffix\u0026gt; \u0026lt;tag key=\u0026#34;tag_1\u0026#34;\u0026gt;arg[0]\u0026lt;/tag\u0026gt; \u0026lt;log key=\u0026#34;log_1\u0026#34;\u0026gt;arg[1]\u0026lt;/log\u0026gt; \u0026lt;/method\u0026gt; \u0026lt;method method=\u0026#34;method(test.apache.skywalking.testcase.customize.model.Model0,java.lang.String,int.class)\u0026#34; operation_name=\u0026#34;/method_3\u0026#34; static=\u0026#34;false\u0026#34;\u0026gt; \u0026lt;operation_name_suffix\u0026gt;arg[0].id\u0026lt;/operation_name_suffix\u0026gt; \u0026lt;operation_name_suffix\u0026gt;arg[0].model1.name\u0026lt;/operation_name_suffix\u0026gt; \u0026lt;operation_name_suffix\u0026gt;arg[0].model1.getId()\u0026lt;/operation_name_suffix\u0026gt; \u0026lt;tag key=\u0026#34;tag_os\u0026#34;\u0026gt;arg[0].os.[1]\u0026lt;/tag\u0026gt; \u0026lt;log key=\u0026#34;log_map\u0026#34;\u0026gt;arg[0].getM().[\u0026#39;k1\u0026#39;]\u0026lt;/log\u0026gt; \u0026lt;/method\u0026gt; \u0026lt;/class\u0026gt; \u0026lt;class class_name=\u0026#34;test.apache.skywalking.testcase.customize.service.TestService2\u0026#34;\u0026gt; \u0026lt;method method=\u0026#34;staticMethod(java.lang.String,int.class)\u0026#34; operation_name=\u0026#34;/is_2_static_method\u0026#34; static=\u0026#34;true\u0026#34;\u0026gt; \u0026lt;tag key=\u0026#34;tag_2_1\u0026#34;\u0026gt;arg[0]\u0026lt;/tag\u0026gt; \u0026lt;log key=\u0026#34;log_1_1\u0026#34;\u0026gt;arg[1]\u0026lt;/log\u0026gt; \u0026lt;/method\u0026gt; \u0026lt;method method=\u0026#34;method([Ljava.lang.Object;)\u0026#34; operation_name=\u0026#34;/method_4\u0026#34; static=\u0026#34;false\u0026#34;\u0026gt; \u0026lt;tag key=\u0026#34;tag_4_1\u0026#34;\u0026gt;arg[0].[0]\u0026lt;/tag\u0026gt; \u0026lt;/method\u0026gt; \u0026lt;method method=\u0026#34;method(java.util.List,int.class)\u0026#34; operation_name=\u0026#34;/method_5\u0026#34; static=\u0026#34;false\u0026#34;\u0026gt; \u0026lt;tag key=\u0026#34;tag_5_1\u0026#34;\u0026gt;arg[0].[0]\u0026lt;/tag\u0026gt; \u0026lt;log key=\u0026#34;log_5_1\u0026#34;\u0026gt;arg[1]\u0026lt;/log\u0026gt; \u0026lt;/method\u0026gt; \u0026lt;/class\u0026gt; \u0026lt;/enhanced\u0026gt; ``\n   Explanation of the configuration in the file    configuration explanation     class_name The enhanced class   method The interceptor method of the class   operation_name If fill it out, will use it instead of the default operation_name.   operation_name_suffix What it means adding dynamic data after the operation_name.   static Is this method static.   tag Will add a tag in local span. The value of key needs to be represented on the XML node.   log Will add a log in local span. The value of key needs to be represented on the XML node.   arg[x] What it means is to get the input arguments. such as arg[0] is means get first arguments.   .[x] When the parsing object is Array or List, you can use it to get the object at the specified index.   .[\u0026lsquo;key\u0026rsquo;] When the parsing object is Map, you can get the map \u0026lsquo;key\u0026rsquo; through it.      ","excerpt":"Support custom enhance Here is an optional plugin apm-customize-enhance-plugin\nIntroduce SkyWalking …","ref":"/docs/main/latest/en/setup/service-agent/java-agent/customize-enhance-trace/","title":"Support custom enhance"},{"body":"Support custom trace ignore Here is an optional plugin apm-trace-ignore-plugin\nNotice: Sampling still works when the trace ignores plug-in activation.\nIntroduce  The purpose of this plugin is to filter endpoint which are expected to be ignored by the tracing system. You can setup multiple URL path patterns, The endpoints match these patterns wouldn\u0026rsquo;t be traced. The current matching rules follow Ant Path match style , like /path/*, /path/**, /path/?. Copy apm-trace-ignore-plugin-x.jar to agent/plugins, restarting the agent can effect the plugin.  How to configure There are two ways to configure ignore patterns. Settings through system env has higher priority.\n Set through the system environment variable,you need to add skywalking.trace.ignore_path to the system variables, the value is the path that you need to ignore, multiple paths should be separated by , Copy/agent/optional-plugins/apm-trace-ignore-plugin/apm-trace-ignore-plugin.config to /agent/config/ dir, and add rules to filter traces  trace.ignore_path=/your/path/1/**,/your/path/2/** ","excerpt":"Support custom trace ignore Here is an optional plugin apm-trace-ignore-plugin\nNotice: Sampling …","ref":"/docs/main/latest/en/setup/service-agent/java-agent/agent-optional-plugins/trace-ignore-plugin/","title":"Support custom trace ignore"},{"body":"Support gRPC SSL transportation for OAP server For OAP communication we are currently using gRPC, a multi-platform RPC framework that uses protocol buffers for message serialization. The nice part about gRPC is that it promotes the use of SSL/TLS to authenticate and encrypt exchanges. Now OAP supports to enable SSL transportation for gRPC receivers.\nYou can follow below steps to enable this feature\nCreating SSL/TLS Certificates It seems like step one is to generate certificates and key files for encrypting communication. I thought this would be fairly straightforward using openssl from the command line.\nUse this script if you are not familiar with how to generate key files.\nWe need below files:\n server.pem a private RSA key to sign and authenticate the public key. It\u0026rsquo;s either a PKCS#8(PEM) or PKCS#1(DER). server.crt self-signed X.509 public keys for distribution. ca.crt a certificate authority public key for a client to validate the server\u0026rsquo;s certificate.  Config OAP server You can enable gRPC SSL by add following lines to application.yml/core/default.\ngRPCSslEnabled: true gRPCSslKeyPath: /path/to/server.pem gRPCSslCertChainPath: /path/to/server.crt gRPCSslTrustedCAPath: /path/to/ca.crt gRPCSslKeyPath and gRPCSslCertChainPath are loaded by OAP server to encrypt the communication. gRPCSslTrustedCAPath helps gRPC client to verify server certificates in cluster mode.\nWhen new files are in place, they can be load dynamically instead of restarting OAP instance.\nIf you enable sharding-server to ingest data from external, add following lines to application.yml/receiver-sharing-server/default:\ngRPCSslEnabled: true gRPCSslKeyPath: /path/to/server.pem gRPCSslCertChainPath: /path/to/server.crt Because sharding-server only receives data from external, so it doesn\u0026rsquo;t need CA at all.\nIf you port to java agent, refer to TLS.md to config java agent to enable TLS.\n","excerpt":"Support gRPC SSL transportation for OAP server For OAP communication we are currently using gRPC, a …","ref":"/docs/main/latest/en/setup/backend/grpc-ssl/","title":"Support gRPC SSL transportation for OAP server"},{"body":"Support Transport Layer Security (TLS) Transport Layer Security (TLS) is a very common security way when transport data through Internet. In some use cases, end users report the background:\n Target(under monitoring) applications are in a region, which also named VPC, at the same time, the SkyWalking backend is in another region (VPC).\nBecause of that, security requirement is very obvious.\n Authentication Mode Only support no mutual auth.\n Use this script if you are not familiar with how to generate key files. Find ca.crt, and use it at client side Find server.crt ,server.pem and ca.crt. Use them at server side. Please refer to gRPC SSL for more details.  Open and config TLS Agent config   Place ca.crt into /ca folder in agent package. Notice, /ca is not created in distribution, please create it by yourself.\n  Agent open TLS automatically after the /ca/ca.crt file detected.\n  TLS with no CA mode could be activated by this setting.\n  agent.force_tls=${SW_AGENT_FORCE_TLS:false} ","excerpt":"Support Transport Layer Security (TLS) Transport Layer Security (TLS) is a very common security way …","ref":"/docs/main/latest/en/setup/service-agent/java-agent/tls/","title":"Support Transport Layer Security (TLS)"},{"body":"Telemetry for backend By default, the telemetry is disabled by setting selector to none, like this\ntelemetry: selector: ${SW_TELEMETRY:none} none: prometheus: host: ${SW_TELEMETRY_PROMETHEUS_HOST:0.0.0.0} port: ${SW_TELEMETRY_PROMETHEUS_PORT:1234} sslEnabled: ${SW_TELEMETRY_PROMETHEUS_SSL_ENABLED:false} sslKeyPath: ${SW_TELEMETRY_PROMETHEUS_SSL_KEY_PATH:\u0026#34;\u0026#34;} sslCertChainPath: ${SW_TELEMETRY_PROMETHEUS_SSL_CERT_CHAIN_PATH:\u0026#34;\u0026#34;} but you can set one of prometheus to enable them, for more information, refer to the details below.\nSelf Observability SkyWalking supports to collect telemetry data into OAP backend directly. Users could check them out through UI or GraphQL API then.\nAdding following configuration to enable self-observability related modules.\n Setting up prometheus telemetry.  telemetry: selector: ${SW_TELEMETRY:prometheus} prometheus: host: 127.0.0.1 port: 1543 Setting up prometheus fetcher  prometheus-fetcher: selector: ${SW_PROMETHEUS_FETCHER:default} default: enabledRules: ${SW_PROMETHEUS_FETCHER_ENABLED_RULES:\u0026#34;self\u0026#34;} Make sure config/fetcher-prom-rules/self.yaml exists.  Once you deploy an oap-server cluster, the target host should be replaced with a dedicated IP or hostname. For instances, there are three oap server in your cluster, their host is service1, service2 and service3 respectively. You should update each self.yaml to twist target host.\nservice1:\nfetcherInterval: PT15S fetcherTimeout: PT10S metricsPath: /metrics staticConfig: # targets will be labeled as \u0026#34;instance\u0026#34; targets: - service1:1234 labels: service: oap-server ... service2:\nfetcherInterval: PT15S fetcherTimeout: PT10S metricsPath: /metrics staticConfig: # targets will be labeled as \u0026#34;instance\u0026#34; targets: - service2:1234 labels: service: oap-server ... service3:\nfetcherInterval: PT15S fetcherTimeout: PT10S metricsPath: /metrics staticConfig: # targets will be labeled as \u0026#34;instance\u0026#34; targets: - service3:1234 labels: service: oap-server ...  WARNING, since Apr 21, 2021, Grafana project has been relicensed to AGPL-v3, no as Apache 2.0 anymore. Check the LICENSE details. The following Prometheus + Grafana solution is optional, not a recommendation.\nPrometheus Prometheus is supported as telemetry implementor. By using this, prometheus collects metrics from SkyWalking backend.\nSet prometheus to provider. The endpoint open at http://0.0.0.0:1234/ and http://0.0.0.0:1234/metrics.\ntelemetry: selector: ${SW_TELEMETRY:prometheus} prometheus: Set host and port if needed.\ntelemetry: selector: ${SW_TELEMETRY:prometheus} prometheus: host: 127.0.0.1 port: 1543 Set SSL relevant settings to expose a secure endpoint. Notice private key file and cert chain file could be uploaded once changes are applied to them.\ntelemetry: selector: ${SW_TELEMETRY:prometheus} prometheus: host: 127.0.0.1 port: 1543 sslEnabled: true sslKeyPath: /etc/ssl/key.pem sslCertChainPath: /etc/ssl/cert-chain.pem Grafana Visualization Provide the grafana dashboard settings. Check SkyWalking OAP Cluster Monitor Dashboard config and SkyWalking OAP Instance Monitor Dashboard config.\n","excerpt":"Telemetry for backend By default, the telemetry is disabled by setting selector to none, like this …","ref":"/docs/main/latest/en/setup/backend/backend-telemetry/","title":"Telemetry for backend"},{"body":"Thread dump merging mechanism The performance profile is an enhancement feature in the APM system. We are using the thread dump to estimate the method execution time, rather than adding multiple local spans. In this way, the resource cost would be much less than using distributed tracing to locate slow method. This feature is suitable in the production environment. This document introduces how thread dumps are merged into the final report as a stack tree(s).\nThread analyst Read data and transform Read the data from the database and convert it to a data structure in gRPC.\nst=\u0026gt;start: Start e=\u0026gt;end: End op1=\u0026gt;operation: Load data using paging op2=\u0026gt;operation: Transform data using parallel st(right)-\u0026gt;op1(right)-\u0026gt;op2 op2(right)-\u0026gt;e Copy the code and paste it into this link to generate flow chart.\n Use the stream to read data by page (50 records per page). Convert the data into gRPC data structures in the form of parallel streams. Merge into a list of data.  Data analysis Use the group-by and collector modes in the Java parallel stream to group according to the first stack element in the database records, and use the collector to perform data aggregation. Generate a multi-root tree.\nst=\u0026gt;start: Start e=\u0026gt;end: End op1=\u0026gt;operation: Group by first stack element sup=\u0026gt;operation: Generate empty stack tree acc=\u0026gt;operation: Accumulator data to stack tree com=\u0026gt;operation: Combine stack trees fin=\u0026gt;operation: Calculate durations and build result st(right)-\u0026gt;op1-\u0026gt;sup(right)-\u0026gt;acc acc(right)-\u0026gt;com(right)-\u0026gt;fin-\u0026gt;e Copy the code and paste it into this link to generate a flow chart.\n Group by first stack element: Use the first level element in each stack to group, ensuring that the stacks have the same root node. Generate empty stack tree: Generate multiple top-level empty trees to prepare for the following steps. The reason for generating multiple top-level trees is that original data can be added in parallel without generating locks. Accumulator data to stack tree: Add every thread dump into the generated trees.  Iterate through each element in the thread dump to find if there is any child element with the same code signature and same stack depth in the parent element. If not, add this element. Keep the dump sequences and timestamps in each nodes from the source.   Combine stack trees: Combine all trees structures into one by using the same rules as the Accumulator.  Use LDR to traverse the tree node. Use the Stack data structure to avoid recursive calls. Each stack element represents the node that needs to be merged. The task of merging two nodes is to merge the list of children nodes. If they have the same code signature and same parents, save the dump sequences and timestamps in this node. Otherwise, the node needs to be added into the target node as a new child.   Calculate durations and build result: Calculate relevant statistics and generate response.  Use the same traversal node logic as in the Combine stack trees step. Convert to a GraphQL data structure, and put all nodes into a list for subsequent duration calculations. Calculate each node\u0026rsquo;s duration in parallel. For each node, sort the sequences. If there are two continuous sequences, the duration should add the duration of these two seq\u0026rsquo;s timestamp. Calculate each node execution in parallel. For each node, the duration of the current node should deduct the time consumed by all children.    Profile data debuggiing Please follow the exporter tool to package profile data. Unzip the profile data and use analyzer main function to run it.\n","excerpt":"Thread dump merging mechanism The performance profile is an enhancement feature in the APM system. …","ref":"/docs/main/latest/en/guides/backend-profile/","title":"Thread dump merging mechanism"},{"body":"Token Authentication Supported version 7.0.0+\nWhy need token authentication after we have TLS? TLS is about transport security, which makes sure the network can be trusted. The token authentication is about monitoring application data can be trusted.\nToken In current version, Token is considered as a simple string.\nSet Token  Set token in agent.config file  # Authentication active is based on backend setting, see application.yml for more details. agent.authentication = ${SW_AGENT_AUTHENTICATION:xxxx} Set token in application.yml file  ······ receiver-sharing-server: default: authentication: ${SW_AUTHENTICATION:\u0026#34;\u0026#34;} ······ Authentication fails The Skywalking OAP verifies every request from agent, only allows requests whose token matches the one configured in application.yml.\nIf the token is not right, you will see the following log in agent\norg.apache.skywalking.apm.dependencies.io.grpc.StatusRuntimeException: PERMISSION_DENIED FAQ Can I use token authentication instead of TLS? No, you shouldn\u0026rsquo;t. In tech way, you can of course, but token and TLS are used for untrusted network env. In that circumstance, TLS has higher priority than this. Token can be trusted only under TLS protection.Token can be stolen easily if you send it through a non-TLS network.\nDo you support other authentication mechanisms? Such as ak/sk? For now, no. But we appreciate someone contributes this feature.\n","excerpt":"Token Authentication Supported version 7.0.0+\nWhy need token authentication after we have TLS? TLS …","ref":"/docs/main/latest/en/setup/backend/backend-token-auth/","title":"Token Authentication"},{"body":"Token Authentication Token In current version, Token is considered as a simple string.\nSet Token Set token in agent.config file\n# Authentication active is based on backend setting, see application.yml for more details. agent.authentication = xxxx Meanwhile, open the backend token authentication.\nAuthentication fails The Collector verifies every request from agent, allowed only the token match.\nIf the token is not right, you will see the following log in agent\norg.apache.skywalking.apm.dependencies.io.grpc.StatusRuntimeException: PERMISSION_DENIED FAQ Can I use token authentication instead of TLS? No, you shouldn\u0026rsquo;t. In tech way, you can of course, but token and TLS are used for untrusted network env. In that circumstance, TLS has higher priority than this. Token can be trusted only under TLS protection.Token can be stolen easily if you send it through a non-TLS network.\nDo you support other authentication mechanisms? Such as ak/sk? For now, no. But we appreciate someone contributes this feature.\n","excerpt":"Token Authentication Token In current version, Token is considered as a simple string.\nSet Token Set …","ref":"/docs/main/latest/en/setup/service-agent/java-agent/token-auth/","title":"Token Authentication"},{"body":"trace cross thread  Dependency the toolkit, such as using maven or gradle  \u0026lt;dependency\u0026gt; \u0026lt;groupId\u0026gt;org.apache.skywalking\u0026lt;/groupId\u0026gt; \u0026lt;artifactId\u0026gt;apm-toolkit-trace\u0026lt;/artifactId\u0026gt; \u0026lt;version\u0026gt;${skywalking.version}\u0026lt;/version\u0026gt; \u0026lt;/dependency\u0026gt;  usage 1.  @TraceCrossThread public static class MyCallable\u0026lt;String\u0026gt; implements Callable\u0026lt;String\u0026gt; { @Override public String call() throws Exception { return null; } } ... ExecutorService executorService = Executors.newFixedThreadPool(1); executorService.submit(new MyCallable());  usage 2.  ExecutorService executorService = Executors.newFixedThreadPool(1); executorService.submit(CallableWrapper.of(new Callable\u0026lt;String\u0026gt;() { @Override public String call() throws Exception { return null; } })); or\nExecutorService executorService = Executors.newFixedThreadPool(1); executorService.execute(RunnableWrapper.of(new Runnable() { @Override public void run() { //your code  } }));  usage 3.  @TraceCrossThread public class MySupplier\u0026lt;String\u0026gt; implements Supplier\u0026lt;String\u0026gt; { @Override public String get() { return null; } } ... CompletableFuture.supplyAsync(new MySupplier\u0026lt;String\u0026gt;()); or\nCompletableFuture.supplyAsync(SupplierWrapper.of(()-\u0026gt;{ return \u0026#34;SupplierWrapper\u0026#34;; })).thenAccept(System.out::println); Sample codes only\n","excerpt":"trace cross thread  Dependency the toolkit, such as using maven or gradle  \u0026lt;dependency\u0026gt; …","ref":"/docs/main/latest/en/setup/service-agent/java-agent/application-toolkit-trace-cross-thread/","title":"trace cross thread"},{"body":"Trace Data Protocol v3 Trace Data Protocol describes the data format between SkyWalking agent/sniffer and backend.\nOverview Trace data protocol is defined and provided in gRPC format, also implemented in HTTP 1.1\nReport service instance status   Service Instance Properties Service instance contains more information than just a name. Once the agent wants to report this, use ManagementService#reportInstanceProperties service to provide a string-key/string-value pair list as the parameter. language of target instance is expected at least.\n  Service Ping Service instance should keep alive with the backend. The agent should set a scheduler using ManagementService#keepAlive service every minute.\n  Send trace and metrics After you have the service ID and service instance ID ready, you could send traces and metrics. Now we have\n TraceSegmentReportService#collect for the SkyWalking native trace format JVMMetricReportService#collect for the SkyWalking native jvm format  For trace format, note that:\n The segment is a unique concept in SkyWalking. It should include all spans for each request in a single OS process, which is usually a single language-based thread. There are three types of spans.    EntrySpan EntrySpan represents a service provider, which is also the endpoint on the server end. As an APM system, SkyWalking targets the application servers. Therefore, almost all the services and MQ-consumers are EntrySpans.\n  LocalSpan LocalSpan represents a typical Java method which is not related to remote services. It is neither a MQ producer/consumer nor a provider/consumer of a service (e.g. HTTP service).\n  ExitSpan ExitSpan represents a client of service or MQ-producer. It is known as the LeafSpan in the early stages of SkyWalking. For example, accessing DB by JDBC, and reading Redis/Memcached are classified as ExitSpans.\n   Cross-thread/process span parent information is called \u0026ldquo;reference\u0026rdquo;. Reference carries the trace ID, segment ID, span ID, service name, service instance name, endpoint name, and target address used on the client end (note: this is not required in cross-thread operations) of this request in the parent. See Cross Process Propagation Headers Protocol v3 for more details.\n  Span#skipAnalysis may be TRUE, if this span doesn\u0026rsquo;t require backend analysis.\n  ","excerpt":"Trace Data Protocol v3 Trace Data Protocol describes the data format between SkyWalking …","ref":"/docs/main/latest/en/protocols/trace-data-protocol-v3/","title":"Trace Data Protocol v3"},{"body":"Trace Sampling at server side When we run a distributed tracing system, the trace bring us detailed info, but cost a lot at storage. Open server side trace sampling mechanism, the metrics of service, service instance, endpoint and topology are all accurate as before, but only don\u0026rsquo;t save all the traces into storage.\nOf course, even you open sampling, the traces will be kept as consistent as possible. Consistent means, once the trace segments have been collected and reported by agents, the backend would do their best to don\u0026rsquo;t break the trace. See Recommendation to understand why we called it as consistent as possible and do their best to don't break the trace.\nSet the sample rate In agent-analyzer module, you will find sampleRate setting.\nagent-analyzer: default: ... sampleRate: ${SW_TRACE_SAMPLE_RATE:10000} # The sample rate precision is 1/10000. 10000 means 100% sample in default. forceSampleErrorSegment: ${SW_FORCE_SAMPLE_ERROR_SEGMENT:true} # When sampling mechanism activated, this config would make the error status segment sampled, ignoring the sampling rate. slowTraceSegmentThreshold: ${SW_SLOW_TRACE_SEGMENT_THRESHOLD:-1} # Setting this threshold about the latency would make the slow trace segments sampled if they cost more time, even the sampling mechanism activated. The default value is `-1`, which means would not sample slow traces. Unit, millisecond. sampleRate is for you to set sample rate to this backend. The sample rate precision is 1/10000. 10000 means 100% sample in default.\nforceSampleErrorSegment is for you to save all error segments when sampling mechanism actived. When sampling mechanism activated, this config would make the error status segment sampled, ignoring the sampling rate.\nslowTraceSegmentThreshold is for you to save all slow trace segments when sampling mechanism actived. Setting this threshold about the latency would make the slow trace segments sampled if they cost more time, even the sampling mechanism activated. The default value is -1, which means would not sample slow traces. Unit, millisecond.\nRecommendation You could set different backend instances with different sampleRate values, but we recommend you to set the same.\nWhen you set the rate different, let\u0026rsquo;s say\n Backend-InstanceA.sampleRate = 35 Backend-InstanceB.sampleRate = 55  And we assume the agents reported all trace segments to backend, Then the 35% traces in the global will be collected and saved in storage consistent/complete, with all spans. 20% trace segments, which reported to Backend-InstanceB, will saved in storage, maybe miss some trace segments, because they are reported to Backend-InstanceA and ignored.\nNote When you open sampling, the actual sample rate could be over sampleRate. Because currently, all error/slow segments will be saved, meanwhile, the upstream and downstream may not be sampled. This feature is going to make sure you could have the error/slow stacks and segments, but don\u0026rsquo;t guarantee you would have the whole trace.\nAlso, the side effect would be, if most of the accesses are fail/slow, the sampling rate would be closing to 100%, which could crash the backend or storage clusters.\n","excerpt":"Trace Sampling at server side When we run a distributed tracing system, the trace bring us detailed …","ref":"/docs/main/latest/en/setup/backend/trace-sampling/","title":"Trace Sampling at server side"},{"body":"Tracing and Tracing based Metrics Analyze Plugins The following plugins provide the distributed tracing capability, and the OAP backend would analyze the topology and metrics based on the tracing data.\n HTTP Server  Tomcat 7 Tomcat 8 Tomcat 9 Spring Boot Web 4.x Spring MVC 3.x, 4.x 5.x with servlet 3.x Nutz Web Framework 1.x Struts2 MVC 2.3.x -\u0026gt; 2.5.x Resin 3 (Optional¹) Resin 4 (Optional¹) Jetty Server 9 Spring WebFlux 5.x (Optional¹) Undertow 1.3.0.Final -\u0026gt; 2.0.27.Final RESTEasy 3.1.0.Final -\u0026gt; 3.7.0.Final Play Framework 2.6.x -\u0026gt; 2.8.x Light4J Microservices Framework 1.6.x -\u0026gt; 2.x Netty SocketIO 1.x   HTTP Client  Feign 9.x Netflix Spring Cloud Feign 1.1.x -\u0026gt; 2.x Okhttp 3.x -\u0026gt; 4.x Apache httpcomponent HttpClient 2.0 -\u0026gt; 3.1, 4.2, 4.3 Spring RestTemplete 4.x Jetty Client 9 Apache httpcomponent AsyncClient 4.x AsyncHttpClient 2.x JRE HttpURLConnection (Optional²)   HTTP Gateway  Spring Cloud Gateway 2.0.2.RELEASE -\u0026gt; 3.x (Optional²)   JDBC  Mysql Driver 5.x, 6.x, 8.x Oracle Driver (Optional¹) H2 Driver 1.3.x -\u0026gt; 1.4.x Sharding-JDBC 1.5.x ShardingSphere 3.0.0, 4.0.0-RC1, 4.0.0, 4.0.1, 4.1.0, 4.1.1 PostgreSQL Driver 8.x, 9.x, 42.x Mariadb Driver 2.x, 1.8 InfluxDB 2.5 -\u0026gt; 2.17 Mssql-Jtds 1.x Mssql-jdbc 6.x -\u0026gt; 8.x   RPC Frameworks  Dubbo 2.5.4 -\u0026gt; 2.6.0 Dubbox 2.8.4 Apache Dubbo 2.7.0 Motan 0.2.x -\u0026gt; 1.1.0 gRPC 1.x Apache ServiceComb Java Chassis 0.1 -\u0026gt; 0.5,1.x SOFARPC 5.4.0 Armeria 0.63.0 -\u0026gt; 0.98.0 Apache Avro 1.7.0 - 1.8.x Finagle 6.44.0 -\u0026gt; 20.1.0 (6.25.0 -\u0026gt; 6.44.0 not tested) Brpc-Java 2.3.7 -\u0026gt; 2.5.3 Thrift 0.10.0 -\u0026gt; 0.12.0 Apache CXF 3.x   MQ  RocketMQ 4.x Kafka 0.11.0.0 -\u0026gt; 2.8.0 Spring-Kafka Spring Kafka Consumer 1.3.x -\u0026gt; 2.3.x (2.0.x and 2.1.x not tested and not recommended by the official document) ActiveMQ 5.10.0 -\u0026gt; 5.15.4 RabbitMQ 5.x Pulsar 2.2.x -\u0026gt; 2.4.x Aliyun ONS 1.x (Optional¹)   NoSQL  Redis  Jedis 2.x Redisson Easy Java Redis client 3.5.2+ Lettuce 5.x   MongoDB Java Driver 2.13-2.14, 3.4.0-3.12.7, 4.0.0-4.1.0 Memcached Client  Spymemcached 2.x Xmemcached 2.x   Elasticsearch  transport-client 5.2.x-5.6.x transport-client 6.7.1-6.8.4 transport-client 7.0.0-7.5.2 rest-high-level-client 6.7.1-6.8.4 rest-high-level-client 7.0.0-7.5.2   Solr  SolrJ 7.x   Cassandra 3.x  cassandra-java-driver 3.7.0-3.7.2   HBase  hbase-client HTable 1.0.0-2.4.2     Service Discovery  Netflix Eureka   Distributed Coordination  Zookeeper 3.4.x (Optional² \u0026amp; Except 3.4.4)   Spring Ecosystem  Spring Bean annotations(@Bean, @Service, @Component, @Repository) 3.x and 4.x (Optional²) Spring Core Async SuccessCallback/FailureCallback/ListenableFutureCallback 4.x Spring Transaction 4.x and 5.x (Optional²)   Hystrix: Latency and Fault Tolerance for Distributed Systems 1.4.20 -\u0026gt; 1.5.18 Scheduler  Elastic Job 2.x Apache ShardingSphere-Elasticjob 3.0.0-alpha Spring @Scheduled 3.1+ Quartz Scheduler 2.x (Optional²) XXL Job 2.x   OpenTracing community supported Canal: Alibaba mysql database binlog incremental subscription \u0026amp; consumer components 1.0.25 -\u0026gt; 1.1.2 JSON  GSON 2.8.x (Optional²)   Vert.x Ecosystem  Vert.x Eventbus 3.2+ Vert.x Web 3.x   Thread Schedule Framework  Spring @Async 4.x and 5.x Quasar 0.7.x JRE Callable and Runnable (Optional²)   Cache  Ehcache 2.x   Kotlin  Coroutine 1.0.1 -\u0026gt; 1.3.x (Optional²)   GraphQL  Graphql 8.0 -\u0026gt; 15.x   Pool  Apache Commons DBCP 2.x   Logging Framework  log4j 2.x log4j2 1.2.x logback 1.2.x   ORM  MyBatis 3.4.x -\u0026gt; 3.5.x    Meter Plugins The meter plugin provides the advanced metrics collections, which are not a part of tracing.\n ¹Due to license incompatibilities/restrictions these plugins are hosted and released in 3rd part repository, go to SkyAPM java plugin extension repository to get these.\n²These plugins affect the performance or must be used under some conditions, from experiences. So only released in /optional-plugins or /bootstrap-plugins, copy to /plugins in order to make them work.\n","excerpt":"Tracing and Tracing based Metrics Analyze Plugins The following plugins provide the distributed …","ref":"/docs/main/latest/en/setup/service-agent/java-agent/supported-list/","title":"Tracing and Tracing based Metrics Analyze Plugins"},{"body":"TTL In SkyWalking, there are two types of observability data, besides metadata.\n Record, including trace and alarm. Maybe log in the future. Metric, including such as percentile, heat map, success rate, cpm(rpm) etc.  You have following settings for different types.\n# Set a timeout on metrics data. After the timeout has expired, the metrics data will automatically be deleted. recordDataTTL: ${SW_CORE_RECORD_DATA_TTL:3} # Unit is day metricsDataTTL: ${SW_CORE_METRICS_DATA_TTL:7} # Unit is day  recordDataTTL affects Record data, including tracing and alarm. metricsDataTTL affects all metrics, including service, instance, endpoint metrics and topology map metrics.  ","excerpt":"TTL In SkyWalking, there are two types of observability data, besides metadata.\n Record, including …","ref":"/docs/main/latest/en/setup/backend/ttl/","title":"TTL"},{"body":"UI SkyWalking UI distribution is already included in our Apache official release.\nStartup Startup script is also in /bin/webappService.sh(.bat). UI runs as an OS Java process, powered-by Zuul.\nSettings Setting file of UI is webapp/webapp.yml in distribution package. It is constituted by three parts.\n Listening port. Backend connect info.  server: port: 8080 collector: path: /graphql ribbon: ReadTimeout: 10000 # Point to all backend\u0026#39;s restHost:restPort, split by ,  listOfServers: 10.2.34.1:12800,10.2.34.2:12800 ","excerpt":"UI SkyWalking UI distribution is already included in our Apache official release.\nStartup Startup …","ref":"/docs/main/latest/en/setup/backend/ui-setup/","title":"UI"},{"body":"UI Introduction SkyWalking official UI provides the default and powerful visualization capabilities for SkyWalking observing distributed cluster.\nThe latest introduction video could be found on the Youtube\n\nSkyWalking dashboard includes the following part.\n Feature Tab Selector Zone. The key features are list there. The more details will be introduced below. Reload Zone. Control the reload mechanism, including reload periodically or manually. Time Selector Zone. Control the timezone and time range. And a Chinese/English switch button here, default, the UI uses the browser language setting. We also welcome to contribute more languages.  Dashboard Dashboard provide metrics of service, service instance and endpoint. There are a few metrics terms you need to understand\n Throughput CPM , represents calls per minute. Apdex score, Read Apdex in WIKI Response Time Percentile, including p99, p95, p90, p75, p50. Read percentile in WIKI SLA, represents the successful rate. For HTTP, it means the rate of 200 response code.  Service, Instance and Dashboard selector could reload manually rather than reload the whole page. NOTICE, the Reload Zone wouldn\u0026rsquo;t reload these selectors.\nTwo default dashboards are provided to visualize the metrics of service and database.\nUser could click the lock button left aside the Service/Instance/Endpoint Reload button to custom your own dashboard.\nCustom Dashboard Users could customize the dashboard. The default dashboards are provided through the default templates located in /ui-initialized-templates folders.\nThe template file follows this format.\ntemplates: - name: template name # The unique name # The type includes DASHBOARD, TOPOLOGY_INSTANCE, TOPOLOGY_ENDPOINT. # DASHBOARD type templates could have multiple definitions, by using different names. # TOPOLOGY_INSTANCE, TOPOLOGY_ENDPOINT type templates should be defined once,  # as they are used in the topology page only. type: \u0026#34;DASHBOARD\u0026#34; # Custom the dashboard or create a new one on the UI, set the metrics as you like in the edit mode. # Then, you could export this configuration through the page and add it here. configuration: |-[ { \u0026#34;name\u0026#34;:\u0026#34;Spring Sleuth\u0026#34;, \u0026#34;type\u0026#34;:\u0026#34;service\u0026#34;, \u0026#34;children\u0026#34;:[ { \u0026#34;name\u0026#34;:\u0026#34;Sleuth\u0026#34;, \u0026#34;children\u0026#34;: [{ \u0026#34;width\u0026#34;: \u0026#34;3\u0026#34;, \u0026#34;title\u0026#34;: \u0026#34;HTTP Request\u0026#34;, \u0026#34;height\u0026#34;: \u0026#34;200\u0026#34;, \u0026#34;entityType\u0026#34;: \u0026#34;ServiceInstance\u0026#34;, \u0026#34;independentSelector\u0026#34;: false, \u0026#34;metricType\u0026#34;: \u0026#34;REGULAR_VALUE\u0026#34;, \u0026#34;metricName\u0026#34;: \u0026#34;meter_http_server_requests_count\u0026#34;, \u0026#34;queryMetricType\u0026#34;: \u0026#34;readMetricsValues\u0026#34;, \u0026#34;chartType\u0026#34;: \u0026#34;ChartLine\u0026#34;, \u0026#34;unit\u0026#34;: \u0026#34;Count\u0026#34; } ... ] } ] } ] # Activated means this templates added into the UI page automatically. # False means providing a basic template, user needs to add it manually on the page. activated: false # True means wouldn\u0026#39;t show up on the dashboard. Only keeps the definition in the storage. disabled: false NOTE, UI initialized templates would only be initialized if there is no template in the storage has the same name. Check the entity named as ui_template in your storage.\nTopology Topology map shows the relationship among the services and instances with metrics.\n Topology shows the default global topology including all services. Service Selector provides 2 level selectors, service group list and service name list. The group name is separated from the service name if it follows \u0026lt;group name\u0026gt;::\u0026lt;logic name\u0026gt; format. Topology map is available for single group, single service, or global(include all services). Custom Group provides the any sub topology capability of service group. Service Deep Dive opens when you click any service. The honeycomb could do metrics, trace and alarm query of the selected service. Service Relationship Metrics gives the metrics of service RPC interactions and instances of these two services.  Trace Query Trace query is a typical feature as SkyWalking provided distributed agents.\n Trace Segment List is not the trace list. Every trace has several segments belonging to different services. If\nquery by all services or by trace id, different segments with same trace id could be list there. Span is clickable, the detail of each span will pop up on the left side. Trace Views provides 3 typical and different usage views to visualize the trace.  Profile Profile is an interaction feature. It provides the method level performance diagnosis.\nTo start the profile analysis, user need to create the profile task\n Select the specific service. Set the endpoint name. This endpoint name typically is the operation name of the first span. Find this on the trace segment list view. Monitor time could start right now or from any given future time. Monitor duration defines the observation time window to find the suitable request to do performance analysis. Even the profile add a very limited performance impact to the target system, but it is still an additional load. This duration make the impact controllable. Min duration threshold provides a filter mechanism, if a request of the given endpoint response quickly, it wouldn\u0026rsquo;t be profiled. This could make sure, the profiled data is the expected one. Max sampling count gives the max dataset of agent will collect. It helps to reduce the memory and network load. One implicit condition, in any moment, SkyWalking only accept one profile task for each service. Agent could have different settings to control or limit this feature, read document setup for more details. Not all SkyWalking ecosystem agent supports this feature, java agent from 7.0.0 supports this in default.  Once the profile done, the profiled trace segments would show up. And you could request for analysis for any span. Typically, we analysis spans having long self duration, if the span and its children both have long duration, you could choose include children or exclude childrend to set the analysis boundaries.\nAfter choose the right span, and click the analysis button, you will see the stack based analysis result. The slowest methods have been highlighted.\nAdvanced features  Since 7.1.0, the profiled trace collects the HTTP request parameters for Tomcat and SpringMVC Controller automatically.  Log Since 8.3.0, SkyWalking provides log query for the browser monitoring. Use Apache SkyWalking Client JS agent would collect metrics and error logs.\nAlarm Alarm page lists all triggered alarm. Read the backend setup documentation to know how to set up the alarm rule or integrate with 3rd party system.\n","excerpt":"UI Introduction SkyWalking official UI provides the default and powerful visualization capabilities …","ref":"/docs/main/latest/en/ui/readme/","title":"UI Introduction"},{"body":"Uninstrumented Gateways/Proxies The uninstrumented gateways are not instrumented by SkyWalking agent plugin when they are started, but they can be configured in gateways.yml file or via Dynamic Configuration. The reason why they can\u0026rsquo;t register to backend automatically is that there\u0026rsquo;re no suitable agent plugins, for example, there is no agent plugins for Nginx, haproxy, etc. So in order to visualize the real topology, we provide a way to configure the gateways/proxies manually.\nConfiguration Format The configuration content includes the gateways' names and their instances:\ngateways: - name: proxy0 # the name is not used for now instances: - host: 127.0.0.1 # the host/ip of this gateway instance port: 9099 # the port of this gateway instance, defaults to 80 Note that the host of the instance must be the one that is actually used in client side, for example, if the instance proxyA has 2 IPs, say 192.168.1.110 and 192.168.1.111, both of which delegates the target service, and the client connects to 192.168.1.110, then configuring 192.168.1.111 as the host won\u0026rsquo;t work properly.\n","excerpt":"Uninstrumented Gateways/Proxies The uninstrumented gateways are not instrumented by SkyWalking agent …","ref":"/docs/main/latest/en/setup/backend/uninstrumented-gateways/","title":"Uninstrumented Gateways/Proxies"},{"body":"Using E2E local remote debugging The E2E remote debugging port of service containers is 5005. If the developer wants to use remote debugging, he needs to add remote debugging parameters to the start service command, and then expose the port 5005.\nFor example, this is the configuration of a container in skywalking/test/e2e/e2e-test/docker/base-compose.yml. JAVA_OPTS is a preset variable for passing additional parameters in the AOP service startup command, so we only need to add the JAVA remote debugging parameters agentlib:jdwp=transport=dt_socket,server=y,suspend=n,address=5005 to the configuration and expose the port 5005.\noap: image: skywalking/oap:latest expose: ... - 5005 ... environment: ... JAVA_OPTS: \u0026gt;-... -agentlib:jdwp=transport=dt_socket,server=y,suspend=n,address=5005 ... At last, if the E2E test fails and is retrying, the developer may get the ports mapping in the file skywalking/test/e2e/e2e-test/remote_real_port and select the host port of the corresponding service for remote debugging. For example,\n#remote_real_port #The remote debugging port on the host is 32783 oap-localhost:32783 #The remote debugging port on the host is 32782 provider-localhost:32782 ","excerpt":"Using E2E local remote debugging The E2E remote debugging port of service containers is 5005. If the …","ref":"/docs/main/latest/en/guides/e2e-local-remote-debug/","title":"Using E2E local remote debugging"},{"body":"V6 upgrade SkyWalking v6 is widely used in many production environments. Follow the steps in the guide below to learn how to upgrade to a new release.\nNOTE: The ways to upgrade are not limited to the steps below.\nUse Canary Release Like all applications, you may upgrade SkyWalking using the canary release method through the following steps.\n Deploy a new cluster by using the latest version of SkyWalking OAP cluster with the new database cluster. Once the target service (i.e. the service being monitored) has upgraded the agent.jar (or simply by rebooting), have collector.backend_service pointing to the new OAP backend, and use/add a new namespace(agent.namespace in Table of Agent Configuration Properties). The namespace will prevent conflicts from arising between different versions. When all target services have been rebooted, the old OAP clusters could be discarded.  The Canary Release method works for any version upgrades.\nOnline Hot Reboot Upgrade The reason we require Canary Release is that the SkyWalking agent has cache mechanisms, and switching to a new cluster causes the cache to become unavailable for new OAP clusters. In version 6.5.0+ (especially for agent versions), we have Agent hot reboot trigger mechanism. This streamlines the upgrade process as we deploy a new cluster by using the latest version of SkyWalking OAP cluster with the new database cluster, and shift the traffic to the new cluster once and for all. Based on the mechanism, all agents will enter the cool_down mode, and come back online. For more details, see the backend setup documentation.\nNOTE: A known bug in 6.4.0 is that its agent may have re-connection issues; therefore, even though this bot reboot mechanism has been included in 6.4.0, it may not work under some network scenarios, especially in Kubernetes.\nAgent Compatibility All versions of SkyWalking 6.x (and even 7.x) are compatible with each other, so users could simply upgrade the OAP servers. As the agent has also been enhanced in the latest versions, according to the SkyWalking team\u0026rsquo;s recommendation, upgrade the agent as soon as practicable.\n","excerpt":"V6 upgrade SkyWalking v6 is widely used in many production environments. Follow the steps in the …","ref":"/docs/main/latest/en/faq/v6-version-upgrade/","title":"V6 upgrade"},{"body":"V8 upgrade Starting from SkyWalking v8, the v3 protocol has been used. This makes it incompatible with previous releases. Users who intend to upgrade in v8 series releases could follow the steps below.\nRegisters in v6 and v7 have been removed in v8 for better scaling out performance. Please upgrade following the instructions below.\n Use a different storage or a new namespace. You may also consider erasing the whole storage indexes or tables related to SkyWalking. Deploy the whole SkyWalking cluster, and expose it in a new network address. If you are using language agents, upgrade the new agents too; meanwhile, make sure the agents are supported in a different language. Then, set up the backend address to the new SkyWalking OAP cluster.  ","excerpt":"V8 upgrade Starting from SkyWalking v8, the v3 protocol has been used. This makes it incompatible …","ref":"/docs/main/latest/en/faq/v8-version-upgrade/","title":"V8 upgrade"},{"body":"Version 3.x -\u0026gt; 5.0.0-alpha Upgrade FAQs Collector Problem There is no information showing in the UI.\nCause In the upgrade from version 3.2.6 to 5.0.0, the existing Elasticsearch indexes are kept, but aren\u0026rsquo;t compatible with 5.0.0-alpha. When service name is registered, ElasticSearch will create this column by default type string, which will lead to an error.\nSolution Clean the data folder in ElasticSearch and restart ElasticSearch, collector and your application under monitoring.\n","excerpt":"Version 3.x -\u0026gt; 5.0.0-alpha Upgrade FAQs Collector Problem There is no information showing in the …","ref":"/docs/main/latest/en/faq/v3-version-upgrade/","title":"Version 3.x -\u003e 5.0.0-alpha Upgrade FAQs"},{"body":"Visualization The SkyWalking native UI provides a default solution for visualization. It provides observability related graphs on overview, service, service instance, endpoint, trace, and alarm, such as topology maps, dependency graphs, heatmaps, etc.\nWe know that many of our users have integrated SkyWalking into their own products. If you would like to do that too, please refer to the SkyWalking query protocol.\n","excerpt":"Visualization The SkyWalking native UI provides a default solution for visualization. It provides …","ref":"/docs/main/latest/en/concepts-and-designs/ui-overview/","title":"Visualization"},{"body":"VMs monitoring SkyWalking leverages Prometheus node-exporter for collecting metrics data from the VMs, and leverages OpenTelemetry Collector to transfer the metrics to OpenTelemetry receiver and into the Meter System.\nWe define the VM entity as a Service in OAP, and use vm:: as a prefix to identify it.\nData flow  The Prometheus node-exporter collects metrics data from the VMs. The OpenTelemetry Collector fetches metrics from the node-exporter via Prometheus Receiver and pushes metrics to SkyWalking OAP Server via the OpenCensus gRPC Exporter. The SkyWalking OAP Server parses the expression with MAL to filter/calculate/aggregate and store the results.  Setup  Setup Prometheus node-exporter. Setup OpenTelemetry Collector . This is an example for OpenTelemetry Collector configuration otel-collector-config.yaml. Config SkyWalking OpenTelemetry receiver.  Supported Metrics    Monitoring Panel Unit Metric Name Description Data Source     CPU Usage % cpu_total_percentage The total percentage usage of the CPU core. If there are 2 cores, the maximum usage is 200%. Prometheus node-exporter   Memory RAM Usage MB meter_vm_memory_used The total RAM usage Prometheus node-exporter   Memory Swap Usage % meter_vm_memory_swap_percentage The percentage usage of swap memory Prometheus node-exporter   CPU Average Used % meter_vm_cpu_average_used The percentage usage of the CPU core in each mode Prometheus node-exporter   CPU Load  meter_vm_cpu_load1\nmeter_vm_cpu_load5\nmeter_vm_cpu_load15 The CPU 1m / 5m / 15m average load Prometheus node-exporter   Memory RAM MB meter_vm_memory_total\nmeter_vm_memory_available\nmeter_vm_memory_used The RAM statistics, including Total / Available / Used Prometheus node-exporter   Memory Swap MB meter_vm_memory_swap_free\nmeter_vm_memory_swap_total The swap memory statistics, including Free / Total Prometheus node-exporter   File System Mountpoint Usage % meter_vm_filesystem_percentage The percentage usage of the file system at each mount point Prometheus node-exporter   Disk R/W KB/s meter_vm_disk_read,meter_vm_disk_written The disk read and written Prometheus node-exporter   Network Bandwidth Usage KB/s meter_vm_network_receive\nmeter_vm_network_transmit The network receive and transmit Prometheus node-exporter   Network Status  meter_vm_tcp_curr_estab\nmeter_vm_tcp_tw\nmeter_vm_tcp_alloc\nmeter_vm_sockets_used\nmeter_vm_udp_inuse The number of TCPs established / TCP time wait / TCPs allocated / sockets in use / UDPs in use Prometheus node-exporter   Filefd Allocated  meter_vm_filefd_allocated The number of file descriptors allocated Prometheus node-exporter    Customizing You can customize your own metrics/expression/dashboard panel.\nThe metrics definition and expression rules are found in /config/otel-oc-rules/vm.yaml.\nThe dashboard panel confirmations are found in /config/ui-initialized-templates/vm.yml.\nBlog For more details, see blog article SkyWalking 8.4 provides infrastructure monitoring.\nK8s monitoring SkyWalking leverages K8s kube-state-metrics and cAdvisor for collecting metrics data from K8s, and leverages OpenTelemetry Collector to transfer the metrics to OpenTelemetry receiver and into the Meter System. This feature requires authorizing the OAP Server to access K8s\u0026rsquo;s API Server.\nWe define the k8s-cluster as a Service in the OAP, and use k8s-cluster:: as a prefix to identify it.\nWe define the k8s-node as an Instance in the OAP, and set its name as the K8s node name.\nWe define the k8s-service as an Endpoint in the OAP, and set its name as $serviceName.$namespace.\nData flow  K8s kube-state-metrics and cAdvisor collect metrics data from K8s. OpenTelemetry Collector fetches metrics from kube-state-metrics and cAdvisor via Prometheus Receiver and pushes metrics to SkyWalking OAP Server via the OpenCensus GRPC Exporter. The SkyWalking OAP Server access to K8s\u0026rsquo;s API Server gets meta info and parses the expression with MAL to filter/calculate/aggregate and store the results.  Setup  Setup kube-state-metric. cAdvisor is integrated into kubelet by default. Set up OpenTelemetry Collector . For details on Prometheus Receiver in OpenTelemetry Collector for K8s, refer to here. For a quick start, we have provided a full example for OpenTelemetry Collector configuration otel-collector-config.yaml. Config SkyWalking OpenTelemetry receiver.  Supported Metrics From the different points of view to monitor K8s, there are 3 kinds of metrics: Cluster / Node / Service\nCluster These metrics are related to the selected cluster (Current Service in the dashboard).\n   Monitoring Panel Unit Metric Name Description Data Source     Node Total  k8s_cluster_node_total The number of nodes K8s kube-state-metrics   Namespace Total  k8s_cluster_namespace_total The number of namespaces K8s kube-state-metrics   Deployment Total  k8s_cluster_deployment_total The number of deployments K8s kube-state-metrics   Service Total  k8s_cluster_service_total The number of services K8s kube-state-metrics   Pod Total  k8s_cluster_pod_total The number of pods K8s kube-state-metrics   Container Total  k8s_cluster_container_total The number of containers K8s kube-state-metrics   CPU Resources m k8s_cluster_cpu_cores\nk8s_cluster_cpu_cores_requests\nk8s_cluster_cpu_cores_limits\nk8s_cluster_cpu_cores_allocatable The capacity and the Requests / Limits / Allocatable of the CPU K8s kube-state-metrics   Memory Resources GB k8s_cluster_memory_total\nk8s_cluster_memory_requests\nk8s_cluster_memory_limits\nk8s_cluster_memory_allocatable The capacity and the Requests / Limits / Allocatable of the memory K8s kube-state-metrics   Storage Resources GB k8s_cluster_storage_total\nk8s_cluster_storage_allocatable The capacity and allocatable of the storage K8s kube-state-metrics   Node Status  k8s_cluster_node_status The current status of the nodes K8s kube-state-metrics   Deployment Status  k8s_cluster_deployment_status The current status of the deployment K8s kube-state-metrics   Deployment Spec Replicas  k8s_cluster_deployment_spec_replicas The number of desired pods for a deployment K8s kube-state-metrics   Service Status  k8s_cluster_service_pod_status The services current status, depending on the related pods' status K8s kube-state-metrics   Pod Status Not Running  k8s_cluster_pod_status_not_running The pods which are not running in the current phase K8s kube-state-metrics   Pod Status Waiting  k8s_cluster_pod_status_waiting The pods and containers which are currently in the waiting status, with reasons shown K8s kube-state-metrics   Pod Status Terminated  k8s_cluster_container_status_terminated The pods and containers which are currently in the terminated status, with reasons shown K8s kube-state-metrics    Node These metrics are related to the selected node (Current Instance in the dashboard).\n   Monitoring Panel Unit Metric Name Description Data Source     Pod Total  k8s_node_pod_total The number of pods in this node K8s kube-state-metrics   Node Status  k8s_node_node_status The current status of this node K8s kube-state-metrics   CPU Resources m k8s_node_cpu_cores\nk8s_node_cpu_cores_allocatable\nk8s_node_cpu_cores_requests\nk8s_node_cpu_cores_limits The capacity and the requests / Limits / Allocatable of the CPU K8s kube-state-metrics   Memory Resources GB k8s_node_memory_total\nk8s_node_memory_allocatable\nk8s_node_memory_requests\nk8s_node_memory_limits The capacity and the requests / Limits / Allocatable of the memory K8s kube-state-metrics   Storage Resources GB k8s_node_storage_total\nk8s_node_storage_allocatable The capacity and allocatable of the storage K8s kube-state-metrics   CPU Usage m k8s_node_cpu_usage The total usage of the CPU core, if there are 2 cores the maximum usage is 2000m cAdvisor   Memory Usage GB k8s_node_memory_usage The totaly memory usage cAdvisor   Network I/O KB/s k8s_node_network_receive\nk8s_node_network_transmit The network receive and transmit cAdvisor    Service In these metrics, the pods are related to the selected service (Current Endpoint in the dashboard).\n   Monitoring Panel Unit Metric Name Description Data Source     Service Pod Total  k8s_service_pod_total The number of pods K8s kube-state-metrics   Service Pod Status  k8s_service_pod_status The current status of pods K8s kube-state-metrics   Service CPU Resources m k8s_service_cpu_cores_requests\nk8s_service_cpu_cores_limits The CPU resources requests / Limits of this service K8s kube-state-metrics   Service Memory Resources MB k8s_service_memory_requests\nk8s_service_memory_limits The memory resources requests / Limits of this service K8s kube-state-metrics   Pod CPU Usage m k8s_service_pod_cpu_usage The CPU resources total usage of pods cAdvisor   Pod Memory Usage MB k8s_service_pod_memory_usage The memory resources total usage of pods cAdvisor   Pod Waiting  k8s_service_pod_status_waiting The pods and containers which are currently in the waiting status, with reasons shown K8s kube-state-metrics   Pod Terminated  k8s_service_pod_status_terminated The pods and containers which are currently in the terminated status, with reasons shown K8s kube-state-metrics   Pod Restarts  k8s_service_pod_status_restarts_total The number of per container restarts related to the pods K8s kube-state-metrics   Pod Network Receive KB/s k8s_service_pod_network_receive The network receive of the pods cAdvisor   Pod Network Transmit KB/s k8s_service_pod_network_transmit The network transmit of the pods cAdvisor   Pod Storage Usage MB k8s_service_pod_fs_usage The storage resources total usage of pods related to this service cAdvisor    Customizing You can customize your own metrics/expression/dashboard panel.\nThe metrics definition and expression rules are found in /config/otel-oc-rules/k8s-cluster.yaml，/config/otel-oc-rules/k8s-node.yaml, /config/otel-oc-rules/k8s-service.yaml.\nThe dashboard panel configurations are found in /config/ui-initialized-templates/k8s.yml.\n","excerpt":"VMs monitoring SkyWalking leverages Prometheus node-exporter for collecting metrics data from the …","ref":"/docs/main/latest/en/setup/backend/backend-infrastructure-monitoring/","title":"VMs monitoring"},{"body":"Welcome This is the official documentation of SkyWalking 8. Welcome to the SkyWalking community!\nHere you can learn all you need to know about SkyWalking’s architecture, understand how to deploy and use SkyWalking, and contribute to the project based on SkyWalking\u0026rsquo;s contributing guidelines.\nNOTE: SkyWalking 8 uses brand new tracing APIs which are incompatible with all previous releases.\n  Concepts and Designs. You\u0026rsquo;ll find the core logic behind SkyWalking. You may start from here if you want to understand what is going on under our cool features and visualization.\n  Setup. A guide to installing SkyWalking for different use cases. It is an observability platform that supports multiple observability modes.\n  UI Introduction. An introduction to the UI components and their features.\n  Contributing Guides. If you are a PMC member, a committer, or a new contributor, learn how to start contributing with these guides!\n  Protocols. The protocols show how agents/probes and the backend communicate with one another. Anyone interested in uplink telemetry data should definitely read this.\n  FAQs. A manifest of known issues with setup and secondary developments processes. Should you encounter any problems, check here first.\n  You might also find these links interesting:\n  The latest and old releases are all available at Apache SkyWalking release page. The change logs can be found here.\n  SkyWalking WIKI hosts the context of some changes and events.\n  You can find the conference schedules, video recordings, and articles about SkyWalking in the community resource catalog.\n  We\u0026rsquo;re always looking for help to improve our documentation and codes, so please don’t hesitate to file an issue if you see any problems. Or better yet, directly contribute by submitting a pull request to help us get better!\n","excerpt":"Welcome This is the official documentation of SkyWalking 8. Welcome to the SkyWalking community! …","ref":"/docs/main/latest/readme/","title":"Welcome"},{"body":"What is VNode? On the trace page, you may sometimes find nodes with their spans named VNode, and that there are no attributes for such spans.\nVNode is created by the UI itself, rather than being reported by the agent or tracing SDK. It indicates that some spans are missed in the trace data in this query.\nHow does the UI detect the missing span(s)? The UI checks the parent spans and reference segments of all spans in real time. If no parent id(segment id + span id) could be found, then it creates a VNode automatically.\nHow did this happen? The VNode appears when the trace data is incomplete.\n The agent fail-safe mechanism has been activated. The SkyWalking agent could abandon the trace data if there are any network issues between the agent and the OAP (e.g. failure to connect, slow network speeds, etc.), or if the OAP cluster is not capable of processing all traces. Some plug-ins may have bugs, and some segments in the trace do not stop correctly and are held in the memory.  In such case, the trace would not exist in the query, thus the VNode shows up.\n","excerpt":"What is VNode? On the trace page, you may sometimes find nodes with their spans named VNode, and …","ref":"/docs/main/latest/en/faq/vnode/","title":"What is VNode?"},{"body":"Why can\u0026rsquo;t I see any data in the UI? There are three main reasons no data can be shown by the UI:\n No traces have been sent to the collector. Traces have been sent, but the timezone of your containers is incorrect. Traces are in the collector, but you\u0026rsquo;re not watching the correct timeframe in the UI.  No traces Be sure to check the logs of your agents to see if they are connected to the collector and traces are being sent.\nIncorrect timezone in containers Be sure to check the time in your containers.\nThe UI isn\u0026rsquo;t showing any data Be sure to configure the timeframe shown by the UI.\n","excerpt":"Why can\u0026rsquo;t I see any data in the UI? There are three main reasons no data can be shown by the …","ref":"/docs/main/latest/en/faq/time-and-timezone/","title":"Why can't I see any data in the UI?"},{"body":"Why do metrics indexes with Hour and Day precisions stop updating after upgrade to 7.x? This issue is to be expected with an upgrade from 6.x to 7.x. See the Downsampling Data Packing feature of the ElasticSearch storage.\nYou may simply delete all expired *-day_xxxxx and *-hour_xxxxx(xxxxx is a timestamp) indexes. Currently, SkyWalking uses the metrics name-xxxxx and metrics name-month_xxxxx indexes only.\n","excerpt":"Why do metrics indexes with Hour and Day precisions stop updating after upgrade to 7.x? This issue …","ref":"/docs/main/latest/en/faq/hour-day-metrics-stopping/","title":"Why do metrics indexes with Hour and Day precisions stop updating after upgrade to 7.x?"},{"body":"Why doesn\u0026rsquo;t SkyWalking involve MQ in its architecture? This is often asked by those who are first introduced to SkyWalking. Many believe that MQ should have better performance and should be able to support higher throughput, like the following:\nHere\u0026rsquo;s what we think.\nIs MQ appropriate for communicating with the OAP backend? This question arises when users consider the circumstances where the OAP cluster may not be powerful enough or becomes offline. But the following issues must first be addressed:\n Why do you think that the OAP is not powerful enough? Were it not powerful, the speed of data analysis wouldn\u0026rsquo;t have caught up with the producers (or agents). Then what is the point of adding new deployment requirements? Some may argue that the payload is sometimes higher than usual during peak times. But we must consider how much higher the payload really is. If it is higher by less than 40%, how many resources would you use for the new MQ cluster? How about moving them to new OAP and ES nodes? Say it is higher by 40% or more, such as by 70% to 200%. Then, it is likely that your MQ would use up more resources than it saves. Your MQ would support 2 to 3 times the payload using 10%-20% of the cost during usual times. Furthermore, in this case, if the payload/throughput are so high, how long would it take for the OAP cluster to catch up? The challenge here is that well before it catches up, the next peak times would have come.  With the analysis above in mind, why would you still want the traces to be 100%, given the resources they would cost? The preferred way to do this would be adding a better dynamic trace sampling mechanism at the backend. When throughput exceeds the threshold, gradually modify the active sampling rate from 100% to 10%, which means you could get the OAP and ES 3 times more powerful than usual, while ignoring the traces at peak times.\nIs MQ transport recommended despite its side effects? Even though MQ transport is not recommended from the production perspective, SkyWalking still provides optional plugins named kafka-reporter and kafka-fetcher for this feature since 8.1.0.\nHow about MQ metrics data exporter? The answer is that the MQ metrics data exporter is already readily available. The exporter module with gRPC default mechanism is there, and you can easily provide a new implementor of this module.\n","excerpt":"Why doesn\u0026rsquo;t SkyWalking involve MQ in its architecture? This is often asked by those who are …","ref":"/docs/main/latest/en/faq/why_mq_not_involved/","title":"Why doesn't SkyWalking involve MQ in its architecture?"},{"body":"Work with Istio Instructions for transport Istio\u0026rsquo;s metrics to the SkyWalking OAP server.\nPrerequisites Istio should be installed in the Kubernetes cluster. Follow Istio getting start to finish it.\nDeploy SkyWalking backend Follow the deploying backend in Kubernetes to install the OAP server in the kubernetes cluster. Refer to OpenTelemetry receiver to ingest metrics. otel-receiver defaults to be inactive. Set env var SW_OTEL_RECEIVER to default to enable it.\nDeploy OpenTelemetry collector OpenTelemetry collector is the location Istio telemetry sends metrics, then processing and sending them to SkyWalking backend.\nFollowing the Getting Started to deploy this collector. There are several components available in the collector, and they could be combined for different scenarios. For the sake of brevity, we use the Prometheus receiver to retrieve metrics from Istio control and data plane, then send them to SkyWalking by OpenCensus exporter.\nPrometheus receiver Refer to Prometheus Receiver to set up this receiver. you could find more configuration details in Prometheus Integration of Istio to figure out how to direct Prometheus receiver to query Istio metrics.\nSkyWalking supports receiving multi-cluster metrics in a single OAP cluster. A cluster label should be appended to every metric fetched by this receiver even there\u0026rsquo;s only a single cluster needed to be collected. You could leverage relabel to add it like below:\nrelabel_configs: - source_labels: [] target_label: cluster replacement: \u0026lt;cluster name\u0026gt; or opt to Resource Processor:\nprocessors: resource: attributes: - key: cluster value: \u0026quot;\u0026lt;cluster name\u0026gt;\u0026quot; action: upsert Notice, if you try the sample of istio Prometheus Kubernetes configuration, the issues described here might block you. Try to use the solution indicated in this issue if it\u0026rsquo;s not fixed.\nOpenCensus exporter Follow OpenCensus exporter configuration to set up a connection between OpenTelemetry collector and OAP cluster. endpoint is the address of OAP gRPC service.\nObserve Istio Open Istio Dashboard in SkyWaling UI by clicking Dashboard -\u0026gt; Istio, then you\u0026rsquo;re able to view charts and diagrams generated by Istio metrics. You also could view them by swctl and set up alarm rules based on them.\nNOTICE, if you want metrics of Istio managed services, including topology among them, we recommend you to consider our ALS solution\n","excerpt":"Work with Istio Instructions for transport Istio\u0026rsquo;s metrics to the SkyWalking OAP server. …","ref":"/docs/main/latest/en/setup/istio/readme/","title":"Work with Istio"},{"body":"Zabbix Receiver Zabbix receiver is accepting the metrics of Zabbix Agent Active Checks protocol format into the Meter System. Zabbix Agent is base on GPL-2.0 License.\nModule define receiver-zabbix: selector: ${SW_RECEIVER_ZABBIX:default} default: # Export tcp port, Zabbix agent could connected and transport data port: 10051 # Bind to host host: 0.0.0.0 # Enable config when receive agent request activeFiles: agent Configuration file Zabbix receiver is configured via a configuration file. The configuration file defines everything related to receiving from agents, as well as which rule files to load.\nOAP can load the configuration at bootstrap. If the new configuration is not well-formed, OAP fails to start up. The files are located at $CLASSPATH/zabbix-rules.\nThe file is written in YAML format, defined by the scheme described below. Square brackets indicate that a parameter is optional.\nAn example for zabbix agent configuration could be found here. You could find the Zabbix agent detail items from Zabbix Agent documentation.\nConfiguration file # insert metricPrefix into metric name: \u0026lt;metricPrefix\u0026gt;_\u0026lt;raw_metric_name\u0026gt; metricPrefix: \u0026lt;string\u0026gt; # expSuffix is appended to all expression in this file. expSuffix: \u0026lt;string\u0026gt; # Datasource from Zabbix Item keys. requiredZabbixItemKeys: - \u0026lt;zabbix item keys\u0026gt; # Support agent entities information. entities: # Allow hostname patterns to build metrics. hostPatterns: - \u0026lt;regex string\u0026gt; # Customized metrics label before parse to meter system. labels: [- \u0026lt;labels\u0026gt; ] # Metrics rule allow you to recompute queries. metrics: [ - \u0026lt;metrics_rules\u0026gt; ]  # Define the label name. The label value must query from `value` or `fromItem` attribute. name: \u0026lt;string\u0026gt; # Appoint value to label. [value: \u0026lt;string\u0026gt;] # Query label value from Zabbix Agent Item key. [fromItem: \u0026lt;string\u0026gt;] \u0026lt;metric_rules\u0026gt; # The name of rule, which combinates with a prefix \u0026#39;meter_\u0026#39; as the index/table name in storage. name: \u0026lt;string\u0026gt; # MAL expression. exp: \u0026lt;string\u0026gt; More about MAL, please refer to mal.md.\n","excerpt":"Zabbix Receiver Zabbix receiver is accepting the metrics of Zabbix Agent Active Checks protocol …","ref":"/docs/main/latest/en/setup/backend/backend-zabbix/","title":"Zabbix Receiver"},{"body":"Advanced deployment OAP servers communicate with each other in a cluster environment. In the cluster mode, you could run in different roles.\n Mixed(default) Receiver Aggregator  Sometimes users may wish to deploy cluster nodes with a clearly defined role. They could then use this function.\nMixed By default, the OAP is responsible for:\n Receiving agent traces or metrics. L1 aggregation Internal communication (sending/receiving) L2 aggregation Persistence Alarm  Receiver The OAP is responsible for:\n Receiving agent traces or metrics. L1 aggregation Internal communication (sending)  Aggregator The OAP is responsible for:\n Internal communication(receive) L2 aggregation Persistence Alarm   These roles are designed for complex deployment requirements on security and network policy.\nKubernetes If you are using our native Kubernetes coordinator, the labelSelector setting is used for Aggregator role selection rules. Choose the right OAP deployment based on your needs.\n","excerpt":"Advanced deployment OAP servers communicate with each other in a cluster environment. In the cluster …","ref":"/docs/main/v8.6.0/en/setup/backend/advanced-deployment/","title":"Advanced deployment"},{"body":"Alarm Alarm core is driven by a collection of rules, which are defined in config/alarm-settings.yml. There are three parts in alarm rule definition.\n Alarm rules. They define how metrics alarm should be triggered and what conditions should be considered. Webhooks. The list of web service endpoints, which should be called after the alarm is triggered. gRPCHook. The host and port of the remote gRPC method, which should be called after the alarm is triggered.  Entity name Defines the relation between scope and entity name.\n Service: Service name Instance: {Instance name} of {Service name} Endpoint: {Endpoint name} in {Service name} Database: Database service name Service Relation: {Source service name} to {Dest service name} Instance Relation: {Source instance name} of {Source service name} to {Dest instance name} of {Dest service name} Endpoint Relation: {Source endpoint name} in {Source Service name} to {Dest endpoint name} in {Dest service name}  Rules There are two types of rules: individual rules and composite rules. A composite rule is a combination of individual rules.\nIndividual rules An alarm rule is made up of the following elements:\n Rule name. A unique name shown in the alarm message. It must end with _rule. Metrics name. This is also the metrics name in the OAL script. Only long, double, int types are supported. See the list of all potential metrics name. Events can be also configured as the source of alarm, please refer to the event doc for more details. Include names. Entity names which are included in this rule. Please follow the entity name definitions. Exclude names. Entity names which are excluded from this rule. Please follow the entity name definitions. Include names regex. A regex that includes entity names. If both include-name list and include-name regex are set, both rules will take effect. Exclude names regex. A regex that excludes entity names. If both exclude-name list and exclude-name regex are set, both rules will take effect. Include labels. Metric labels which are included in this rule. Exclude labels. Metric labels which are excluded from this rule. Include labels regex. A regex that includes labels. If both include-label list and include-label regex are set, both rules will take effect. Exclude labels regex. A regex that exclude labels. If both the exclude-label list and exclude-label regex are set, both rules will take effect. Tags. Tags are key/value pairs that are attached to alarms. Tags are used to specify distinguishing attributes of alarms that are meaningful and relevant to users. If you would like to make these tags searchable on the SkyWalking UI, you may set the tag keys in core/default/searchableAlarmTags, or through system environment variable SW_SEARCHABLE_ALARM_TAG_KEYS. The key level is supported by default.  Label settings are required by the meter-system. They are used to store metrics from the label-system platform, such as Prometheus, Micrometer, etc. The four label settings mentioned above must implement LabeledValueHolder.\n Threshold. The target value. For multiple-value metrics, such as percentile, the threshold is an array. It is described as: value1, value2, value3, value4, value5. Each value may serve as the threshold for each value of the metrics. Set the value to - if you do not wish to trigger the alarm by one or more of the values.\nFor example in percentile, value1 is the threshold of P50, and -, -, value3, value4, value5 means that there is no threshold for P50 and P75 in the percentile alarm rule. OP. The operator. It supports \u0026gt;, \u0026gt;=, \u0026lt;, \u0026lt;=, =. We welcome contributions of all OPs. Period. The frequency for checking the alarm rule. This is a time window that corresponds to the backend deployment env time. Count. Within a period window, if the number of times which value goes over the threshold (based on OP) reaches count, then an alarm will be sent. Only as condition. Indicates if the rule can send notifications, or if it simply serves as an condition of the composite rule. Silence period. After the alarm is triggered in Time-N, there will be silence during the TN -\u0026gt; TN + period. By default, it works in the same manner as period. The same alarm (having the same ID in the same metrics name) may only be triggered once within a period.  Composite rules NOTE: Composite rules are only applicable to alarm rules targeting the same entity level, such as service-level alarm rules (service_percent_rule \u0026amp;\u0026amp; service_resp_time_percentile_rule). Do not compose alarm rules of different entity levels, such as an alarm rule of the service metrics with another rule of the endpoint metrics.\nA composite rule is made up of the following elements:\n Rule name. A unique name shown in the alarm message. Must end with _rule. Expression. Specifies how to compose rules, and supports \u0026amp;\u0026amp;, ||, and (). Message. The notification message to be sent out when the rule is triggered. Tags. Tags are key/value pairs that are attached to alarms. Tags are used to specify distinguishing attributes of alarms that are meaningful and relevant to users.  rules: # Rule unique name, must be ended with `_rule`. endpoint_percent_rule: # Metrics value need to be long, double or int metrics-name: endpoint_percent threshold: 75 op: \u0026lt; # The length of time to evaluate the metrics period: 10 # How many times after the metrics match the condition, will trigger alarm count: 3 # How many times of checks, the alarm keeps silence after alarm triggered, default as same as period. silence-period: 10 # Specify if the rule can send notification or just as an condition of composite rule only-as-condition: false tags: level: WARNING service_percent_rule: metrics-name: service_percent # [Optional] Default, match all services in this metrics include-names: - service_a - service_b exclude-names: - service_c # Single value metrics threshold. threshold: 85 op: \u0026lt; period: 10 count: 4 only-as-condition: false service_resp_time_percentile_rule: # Metrics value need to be long, double or int metrics-name: service_percentile op: \u0026#34;\u0026gt;\u0026#34; # Multiple value metrics threshold. Thresholds for P50, P75, P90, P95, P99. threshold: 1000,1000,1000,1000,1000 period: 10 count: 3 silence-period: 5 message: Percentile response time of service {name} alarm in 3 minutes of last 10 minutes, due to more than one condition of p50 \u0026gt; 1000, p75 \u0026gt; 1000, p90 \u0026gt; 1000, p95 \u0026gt; 1000, p99 \u0026gt; 1000 only-as-condition: false meter_service_status_code_rule: metrics-name: meter_status_code exclude-labels: - \u0026#34;200\u0026#34; op: \u0026#34;\u0026gt;\u0026#34; threshold: 10 period: 10 count: 3 silence-period: 5 message: The request number of entity {name} non-200 status is more than expected. only-as-condition: false composite-rules: comp_rule: # Must satisfied percent rule and resp time rule  expression: service_percent_rule \u0026amp;\u0026amp; service_resp_time_percentile_rule message: Service {name} successful rate is less than 80% and P50 of response time is over 1000ms tags: level: CRITICAL Default alarm rules For convenience\u0026rsquo;s sake, we have provided a default alarm-setting.yml in our release. It includes the following rules:\n Service average response time over 1s in the last 3 minutes. Service success rate lower than 80% in the last 2 minutes. Percentile of service response time over 1s in the last 3 minutes Service Instance average response time over 1s in the last 2 minutes, and the instance name matches the regex. Endpoint average response time over 1s in the last 2 minutes. Database access average response time over 1s in the last 2 minutes. Endpoint relation average response time over 1s in the last 2 minutes.  List of all potential metrics name The metrics names are defined in the official OAL scripts and MAL scripts, the Event names can also serve as the metrics names, all possible event names can be also found in the Event doc.\nCurrently, metrics from the Service, Service Instance, Endpoint, Service Relation, Service Instance Relation, Endpoint Relation scopes could be used in Alarm, and the Database access scope is same as Service.\nSubmit an issue or a pull request if you want to support any other scopes in alarm.\nWebhook The Webhook requires the peer to be a web container. The alarm message will be sent through HTTP post by application/json content type. The JSON format is based on List\u0026lt;org.apache.skywalking.oap.server.core.alarm.AlarmMessage\u0026gt; with the following key information:\n scopeId, scope. All scopes are defined in org.apache.skywalking.oap.server.core.source.DefaultScopeDefine. name. Target scope entity name. Please follow the entity name definitions. id0. The ID of the scope entity that matches with the name. When using the relation scope, it is the source entity ID. id1. When using the relation scope, it is the destination entity ID. Otherwise, it is empty. ruleName. The rule name configured in alarm-settings.yml. alarmMessage. The alarm text message. startTime. The alarm time measured in milliseconds, which occurs between the current time and the midnight of January 1, 1970 UTC. tags. The tags configured in alarm-settings.yml.  See the following example:\n[{ \u0026#34;scopeId\u0026#34;: 1, \u0026#34;scope\u0026#34;: \u0026#34;SERVICE\u0026#34;, \u0026#34;name\u0026#34;: \u0026#34;serviceA\u0026#34;, \u0026#34;id0\u0026#34;: \u0026#34;12\u0026#34;, \u0026#34;id1\u0026#34;: \u0026#34;\u0026#34;, \u0026#34;ruleName\u0026#34;: \u0026#34;service_resp_time_rule\u0026#34;, \u0026#34;alarmMessage\u0026#34;: \u0026#34;alarmMessage xxxx\u0026#34;, \u0026#34;startTime\u0026#34;: 1560524171000, \u0026#34;tags\u0026#34;: [{ \u0026#34;key\u0026#34;: \u0026#34;level\u0026#34;, \u0026#34;value\u0026#34;: \u0026#34;WARNING\u0026#34; }] }, { \u0026#34;scopeId\u0026#34;: 1, \u0026#34;scope\u0026#34;: \u0026#34;SERVICE\u0026#34;, \u0026#34;name\u0026#34;: \u0026#34;serviceB\u0026#34;, \u0026#34;id0\u0026#34;: \u0026#34;23\u0026#34;, \u0026#34;id1\u0026#34;: \u0026#34;\u0026#34;, \u0026#34;ruleName\u0026#34;: \u0026#34;service_resp_time_rule\u0026#34;, \u0026#34;alarmMessage\u0026#34;: \u0026#34;alarmMessage yyy\u0026#34;, \u0026#34;startTime\u0026#34;: 1560524171000, \u0026#34;tags\u0026#34;: [{ \u0026#34;key\u0026#34;: \u0026#34;level\u0026#34;, \u0026#34;value\u0026#34;: \u0026#34;CRITICAL\u0026#34; }] }] gRPCHook The alarm message will be sent through remote gRPC method by Protobuf content type. The message contains key information which are defined in oap-server/server-alarm-plugin/src/main/proto/alarm-hook.proto.\nPart of the protocol looks like this:\nmessage AlarmMessage { int64 scopeId = 1; string scope = 2; string name = 3; string id0 = 4; string id1 = 5; string ruleName = 6; string alarmMessage = 7; int64 startTime = 8; AlarmTags tags = 9;}message AlarmTags { // String key, String value pair.  repeated KeyStringValuePair data = 1;}message KeyStringValuePair { string key = 1; string value = 2;}Slack Chat Hook Follow the Getting Started with Incoming Webhooks guide and create new Webhooks.\nThe alarm message will be sent through HTTP post by application/json content type if you have configured Slack Incoming Webhooks as follows:\nslackHooks: textTemplate: |-{ \u0026#34;type\u0026#34;: \u0026#34;section\u0026#34;, \u0026#34;text\u0026#34;: { \u0026#34;type\u0026#34;: \u0026#34;mrkdwn\u0026#34;, \u0026#34;text\u0026#34;: \u0026#34;:alarm_clock: *Apache Skywalking Alarm* \\n **%s**.\u0026#34; } } webhooks: - https://hooks.slack.com/services/x/y/z WeChat Hook Note that only the WeChat Company Edition (WeCom) supports WebHooks. To use the WeChat WebHook, follow the Wechat Webhooks guide. The alarm message will be sent through HTTP post by application/json content type after you have set up Wechat Webhooks as follows:\nwechatHooks: textTemplate: |-{ \u0026#34;msgtype\u0026#34;: \u0026#34;text\u0026#34;, \u0026#34;text\u0026#34;: { \u0026#34;content\u0026#34;: \u0026#34;Apache SkyWalking Alarm: \\n %s.\u0026#34; } } webhooks: - https://qyapi.weixin.qq.com/cgi-bin/webhook/send?key=dummy_key Dingtalk Hook Follow the Dingtalk Webhooks guide and create new Webhooks. For security purposes, you can config an optional secret for an individual webhook URL. The alarm message will be sent through HTTP post by application/json content type if you have configured Dingtalk Webhooks as follows:\ndingtalkHooks: textTemplate: |-{ \u0026#34;msgtype\u0026#34;: \u0026#34;text\u0026#34;, \u0026#34;text\u0026#34;: { \u0026#34;content\u0026#34;: \u0026#34;Apache SkyWalking Alarm: \\n %s.\u0026#34; } } webhooks: - url: https://oapi.dingtalk.com/robot/send?access_token=dummy_token secret: dummysecret Feishu Hook Follow the Feishu Webhooks guide and create new Webhooks. For security purposes, you can config an optional secret for an individual webhook URL. If you would like to direct a text to a user, you can config ats which is the feishu\u0026rsquo;s user_id and separated by \u0026ldquo;,\u0026rdquo; . The alarm message will be sent through HTTP post by application/json content type if you have configured Feishu Webhooks as follows:\nfeishuHooks: textTemplate: |-{ \u0026#34;msg_type\u0026#34;: \u0026#34;text\u0026#34;, \u0026#34;content\u0026#34;: { \u0026#34;text\u0026#34;: \u0026#34;Apache SkyWalking Alarm: \\n %s.\u0026#34; }, \u0026#34;ats\u0026#34;:\u0026#34;feishu_user_id_1,feishu_user_id_2\u0026#34; } webhooks: - url: https://open.feishu.cn/open-apis/bot/v2/hook/dummy_token secret: dummysecret WeLink Hook Follow the WeLink Webhooks guide and create new Webhooks. The alarm message will be sent through HTTP post by application/json content type if you have configured WeLink Webhooks as follows:\nwelinkHooks: textTemplate: \u0026#34;Apache SkyWalking Alarm: \\n %s.\u0026#34; webhooks: # you may find your own client_id and client_secret in your app, below are dummy, need to change. - client_id: \u0026#34;dummy_client_id\u0026#34; client_secret: dummy_secret_key access_token_url: https://open.welink.huaweicloud.com/api/auth/v2/tickets message_url: https://open.welink.huaweicloud.com/api/welinkim/v1/im-service/chat/group-chat # if you send to multi group at a time, separate group_ids with commas, e.g. \u0026#34;123xx\u0026#34;,\u0026#34;456xx\u0026#34; group_ids: \u0026#34;dummy_group_id\u0026#34; # make a name you like for the robot, it will display in group robot_name: robot Update the settings dynamically Since 6.5.0, the alarm settings can be updated dynamically at runtime by Dynamic Configuration, which will override the settings in alarm-settings.yml.\nIn order to determine whether an alarm rule is triggered or not, SkyWalking needs to cache the metrics of a time window for each alarm rule. If any attribute (metrics-name, op, threshold, period, count, etc.) of a rule is changed, the sliding window will be destroyed and re-created, causing the alarm of this specific rule to restart again.\n","excerpt":"Alarm Alarm core is driven by a collection of rules, which are defined in config/alarm-settings.yml. …","ref":"/docs/main/v8.6.0/en/setup/backend/backend-alarm/","title":"Alarm"},{"body":"Apache SkyWalking committer SkyWalking Project Management Committee (PMC) is responsible for assessing the contributions of candidates.\nLike many Apache projects, SkyWalking welcome all contributions, including code contributions, blog entries, guides for new users, public speeches, and enhancement of the project in various ways.\nCommitter Nominate new committer In SkyWalking, new committer nomination could only be officially started by existing PMC members. If a new committer feels that he/she is qualified, he/she should contact any existing PMC member and discuss. If this is agreed among some members of the PMC, the process will kick off.\nThe following steps are recommended (to be initiated only by an existing PMC member):\n Send an email titled [DISCUSS] Promote xxx as new committer to private@skywalking.a.o. List the important contributions of the candidate, so you could gather support from other PMC members for your proposal. Keep the discussion open for more than 3 days but no more than 1 week, unless there is any express objection or concern. If the PMC generally agrees to the proposal, send an email titled [VOTE] Promote xxx as new committer to private@skywalking.a.o. Keep the voting process open for more than 3 days, but no more than 1 week. Consider the result as Consensus Approval if there are three +1 votes and +1 votes \u0026gt; -1 votes. Send an email titled [RESULT][VOTE] Promote xxx as new committer to private@skywalking.a.o, and list the voting details, including who the voters are.  Invite new committer The PMC member who starts the promotion is responsible for sending an invitation to the new committer and guiding him/her to set up the ASF env.\nThe PMC member should send an email using the following template to the new committer:\nTo: JoeBloggs@foo.net Cc: private@skywalking.apache.org Subject: Invitation to become SkyWalking committer: Joe Bloggs Hello [invitee name], The SkyWalking Project Management Committee] (PMC) hereby offers you committer privileges to the project. These privileges are offered on the understanding that you'll use them reasonably and with common sense. We like to work on trust rather than unnecessary constraints. Being a committer enables you to more easily make changes without needing to go through the patch submission process. Being a committer does not require you to participate any more than you already do. It does tend to make one even more committed. You will probably find that you spend more time here. Of course, you can decline and instead remain as a contributor, participating as you do now. A. This personal invitation is a chance for you to accept or decline in private. Either way, please let us know in reply to the [private@skywalking.apache.org] address only. B. If you accept, the next step is to register an iCLA: 1. Details of the iCLA and the forms are found through this link: http://www.apache.org/licenses/#clas 2. Instructions for its completion and return to the Secretary of the ASF are found at http://www.apache.org/licenses/#submitting 3. When you transmit the completed iCLA, request to notify the Apache SkyWalking and choose a unique Apache id. Look to see if your preferred id is already taken at http://people.apache.org/committer-index.html This will allow the Secretary to notify the PMC when your iCLA has been recorded. When recording of your iCLA is noticed, you will receive a follow-up message with the next steps for establishing you as a committer. Invitation acceptance process The new committer should reply to private@skywalking.apache.org (choose reply all), and express his/her intention to accept the invitation. Then, this invitation will be treated as accepted by the project\u0026rsquo;s PMC. Of course, the new committer may also choose to decline the invitation.\nOnce the invitation has been accepted, the new committer has to take the following steps:\n Subscribe to dev@skywalking.apache.org. Usually this is already done. Choose a Apache ID that is not on the apache committers list page. Download the ICLA (If the new committer contributes to the project as a day job, CCLA is expected). After filling in the icla.pdf (or ccla.pdf) with the correct information, print, sign it by hand, scan it as an PDF, and send it as an attachment to secretary@apache.org. (If electronic signature is preferred, please follow the steps on this page) The PMC will wait for the Apache secretary to confirm the ICLA (or CCLA) filed. The new committer and PMC will receive the following email:  Dear XXX, This message acknowledges receipt of your ICLA, which has been filed in the Apache Software Foundation records. Your account has been requested for you and you should receive email with next steps within the next few days (can take up to a week). Please refer to https://www.apache.org/foundation/how-it-works.html#developers for more information about roles at Apache. In the unlikely event that the account has not yet been requested, the PMC member should contact the project V.P.. The V.P. could request through the Apache Account Submission Helper Form.\nAfter several days, the new committer will receive an email confirming creation of the account, titled Welcome to the Apache Software Foundation (ASF)!. Congratulations! The new committer now has an official Apache ID.\nThe PMC member should add the new committer to the official committer list through roster.\nSet up the Apache ID and dev env  Go to Apache Account Utility Platform, create your password, set up your personal mailbox (Forwarding email address) and GitHub account(Your GitHub Username). An organizational invite will be sent to you via email shortly thereafter (within 2 hours). If you would like to use the xxx@apache.org email service, please refer to here. Gmail is recommended, because this forwarding mode is not easy to find in most mailbox service settings. Follow the authorized GitHub 2FA wiki to enable two-factor authorization (2FA) on Github. When you set 2FA to \u0026ldquo;off\u0026rdquo;, it will be delisted by the corresponding Apache committer write permission group until you set it up again. (NOTE: Treat your recovery codes with the same level of attention as you would your password!) Use GitBox Account Linking Utility to obtain write permission of the SkyWalking project. Follow this doc to update the website.  If you would like to show up publicly in the Apache GitHub org, you need to go to the Apache GitHub org people page, search for yourself, and choose Organization visibility to Public.\nCommitter rights, duties, and responsibilities The SkyWalking project doesn\u0026rsquo;t require continuing contributions from you after you have become a committer, but we truly hope that you will continue to play a part in our community!\nAs a committer, you could\n Review and merge the pull request to the master branch in the Apache repo. A pull request often contains multiple commits. Those commits must be squashed and merged into a single commit with explanatory comments. It is recommended for new committers to request recheck of the pull request from senior committers. Create and push codes to the new branch in the Apache repo. Follow the release process to prepare a new release. Remember to confirm with the committer team that it is the right time to create the release.  The PMC hopes that the new committer will take part in the release process as well as release voting, even though their vote will be regarded as +1 no binding. Being familiar with the release process is key to being promoted to the role of PMC member.\nProject Management Committee The Project Management Committee (PMC) member does not have any special rights in code contributions. They simply oversee the project and make sure that it follows the Apache requirements. Its functions include:\n Binding voting for releases and license checks; New committer and PMC member recognition; Identification of branding issues and brand protection; and Responding to questions raised by the ASF board, and taking necessary actions.  The V.P. and chair of the PMC is the secretary, who is responsible for initializing the board report.\nIn most cases, a new PMC member is nominated from the committer team. But it is also possible to become a PMC member directly, so long as the PMC agrees to the nomination and is confident that the candidate is ready. For instance, this can be demonstrated by the fact that he/she has been an Apache member, an Apache officer, or a PMC member of another project.\nThe new PMC voting process should also follow the [DISCUSS], [VOTE] and [RESULT][VOTE] procedures using a private mail list, just like the voting process for new committers. Before sending the invitation, the PMC must also send a NOTICE mail to the Apache board.\nTo: board@apache.org Cc: private@skywalking.apache.org Subject: [NOTICE] Jane Doe for SkyWalking PMC SkyWalking proposes to invite Jane Doe (janedoe) to join the PMC. (include if a vote was held) The vote result is available here: https://lists.apache.org/... After 72 hours, if the board doesn\u0026rsquo;t object to the nomination (which it won\u0026rsquo;t most cases), an invitation may then be sent to the candidate.\nOnce the invitation is accepted, a PMC member should add the new member to the official PMC list through roster.\n","excerpt":"Apache SkyWalking committer SkyWalking Project Management Committee (PMC) is responsible for …","ref":"/docs/main/v8.6.0/en/guides/asf/committer/","title":"Apache SkyWalking committer"},{"body":"Apdex threshold Apdex is a measure of response time based against a set threshold. It measures the ratio of satisfactory response times to unsatisfactory response times. The response time is measured from an asset request to completed delivery back to the requestor.\nA user defines a response time threshold T. All responses handled in T or less time satisfy the user.\nFor example, if T is 1.2 seconds and a response completes in 0.5 seconds, then the user is satisfied. All responses greater than 1.2 seconds dissatisfy the user. Responses greater than 4.8 seconds frustrate the user.\nThe apdex threshold T can be configured in service-apdex-threshold.yml file or via Dynamic Configuration. The default item will apply to a service that isn\u0026rsquo;t defined in this configuration as the default threshold.\nConfiguration Format The configuration content includes the names and thresholds of the services:\n# default threshold is 500ms default: 500 # example: # the threshold of service \u0026#34;tomcat\u0026#34; is 1s # tomcat: 1000 # the threshold of service \u0026#34;springboot1\u0026#34; is 50ms # springboot1: 50 ","excerpt":"Apdex threshold Apdex is a measure of response time based against a set threshold. It measures the …","ref":"/docs/main/v8.6.0/en/setup/backend/apdex-threshold/","title":"Apdex threshold"},{"body":"Backend setup SkyWalking backend distribution package includes the following parts:\n  bin/cmd scripts, in /bin folder. Includes startup linux shell and Windows cmd scripts for Backend server and UI startup.\n  Backend config, in /config folder. Includes settings files of the backend, which are:\n application.yml log4j.xml alarm-settings.yml    Libraries of backend, in /oap-libs folder. All the dependencies of the backend are in it.\n  Webapp env, in webapp folder. UI frontend jar file is here, with its webapp.yml setting file.\n  Requirements and default settings Requirement: JDK8 to JDK12 are tested, other versions are not tested and may or may not work.\nBefore you start, you should know that the quickstart aims to get you a basic configuration mostly for previews/demo, performance and long-term running are not our goals.\nFor production/QA/tests environments, you should head to Backend and UI deployment documents.\nYou can use bin/startup.sh (or cmd) to startup the backend and UI with their default settings, which are:\n Backend storage uses H2 by default (for an easier start) Backend listens 0.0.0.0/11800 for gRPC APIs and 0.0.0.0/12800 for http rest APIs.  In Java, DotNetCore, Node.js, Istio agents/probe, you should set the gRPC service address to ip/host:11800, with ip/host where your backend is.\n UI listens on 8080 port and request 127.0.0.1/12800 to do GraphQL query.  Interaction Before deploying Skywalking in your distributed environment, you should know how agents/probes, backend, UI communicates with each other:\n All native agents and probes, either language based or mesh probe, are using gRPC service (core/default/gRPC* in application.yml) to report data to the backend. Also, jetty service supported in JSON format. UI uses GraphQL (HTTP) query to access the backend also in Jetty service (core/default/rest* in application.yml).  Startup script The default startup scripts are /bin/oapService.sh(.bat). Read start up mode document to know other options of starting backend.\napplication.yml SkyWalking backend startup behaviours are driven by config/application.yml. Understood the setting file will help you to read this document. The core concept behind this setting file is, SkyWalking collector is based on pure modularization design. End user can switch or assemble the collector features by their own requirements.\nSo, in application.yml, there are three levels.\n Level 1, module name. Meaning this module is active in running mode. Level 2, provider option list and provider selector. Available providers are listed here with a selector to indicate which one will actually take effect, if there is only one provider listed, the selector is optional and can be omitted. Level 3. settings of the provider.  Example:\nstorage: selector: mysql # the mysql storage will actually be activated, while the h2 storage takes no effect h2: driver: ${SW_STORAGE_H2_DRIVER:org.h2.jdbcx.JdbcDataSource} url: ${SW_STORAGE_H2_URL:jdbc:h2:mem:skywalking-oap-db} user: ${SW_STORAGE_H2_USER:sa} metadataQueryMaxSize: ${SW_STORAGE_H2_QUERY_MAX_SIZE:5000} mysql: properties: jdbcUrl: ${SW_JDBC_URL:\u0026#34;jdbc:mysql://localhost:3306/swtest\u0026#34;} dataSource.user: ${SW_DATA_SOURCE_USER:root} dataSource.password: ${SW_DATA_SOURCE_PASSWORD:root@1234} dataSource.cachePrepStmts: ${SW_DATA_SOURCE_CACHE_PREP_STMTS:true} dataSource.prepStmtCacheSize: ${SW_DATA_SOURCE_PREP_STMT_CACHE_SQL_SIZE:250} dataSource.prepStmtCacheSqlLimit: ${SW_DATA_SOURCE_PREP_STMT_CACHE_SQL_LIMIT:2048} dataSource.useServerPrepStmts: ${SW_DATA_SOURCE_USE_SERVER_PREP_STMTS:true} metadataQueryMaxSize: ${SW_STORAGE_MYSQL_QUERY_MAX_SIZE:5000} # other configurations  storage is the module. selector selects one out of the all providers listed below, the unselected ones take no effect as if they were deleted. default is the default implementor of core module. driver, url, \u0026hellip; metadataQueryMaxSize are all setting items of the implementor.  At the same time, modules includes required and optional, the required modules provide the skeleton of backend, even modularization supported pluggable, removing those modules are meaningless, for optional modules, some of them have a provider implementation called none, meaning it only provides a shell with no actual logic, typically such as telemetry. Setting - to the selector means this whole module will be excluded at runtime. We highly recommend you don\u0026rsquo;t try to change APIs of those modules, unless you understand SkyWalking project and its codes very well.\nList the required modules here\n Core. Do basic and major skeleton of all data analysis and stream dispatch. Cluster. Manage multiple backend instances in a cluster, which could provide high throughputs process capabilities. Storage. Make the analysis result persistence. Query. Provide query interfaces to UI.  For Cluster and Storage have provided multiple implementors(providers), see Cluster management and Choose storage documents in the link list.\nAlso, several receiver modules are provided. Receiver is the module in charge of accepting incoming data requests to backend. Most(all) provide service by some network(RPC) protocol, such as gRPC, HTTPRestful.\nThe receivers have many different module names, you could read Set receivers document in the link list.\nConfiguration Vocabulary All available configurations in application.yml could be found in Configuration Vocabulary.\nAdvanced feature document link list After understand the setting file structure, you could choose your interesting feature document. We recommend you to read the feature documents in our following order.\n Overriding settings in application.yml is supported IP and port setting. Introduce how IP and port set and be used. Backend init mode startup. How to init the environment and exit graciously. Read this before you try to initial a new cluster. Cluster management. Guide you to set backend server in cluster mode. Deploy in kubernetes. Guide you to build and use SkyWalking image, and deploy in k8s. Choose storage. As we know, in default quick start, backend is running with H2 DB. But clearly, it doesn\u0026rsquo;t fit the product env. In here, you could find what other choices do you have. Choose the ones you like, we are also welcome anyone to contribute new storage implementor. Set receivers. You could choose receivers by your requirements, most receivers are harmless, at least our default receivers are. You would set and active all receivers provided. Open fetchers. You could open different fetchers to read metrics from the target applications. These ones work like receivers, but in pulling mode, typically like Prometheus. Token authentication. You could add token authentication mechanisms to avoid OAP receiving untrusted data. Do trace sampling at backend. This sample keep the metrics accurate, only don\u0026rsquo;t save some of traces in storage based on rate. Follow slow DB statement threshold config document to understand that, how to detect the Slow database statements(including SQL statements) in your system. Official OAL scripts. As you have known from our OAL introduction, most of backend analysis capabilities based on the scripts. Here is the description of official scripts, which helps you to understand which metrics data are in process, also could be used in alarm. Alarm. Alarm provides a time-series based check mechanism. You could set alarm rules targeting the analysis oal metrics objects. Advanced deployment options. If you want to deploy backend in very large scale and support high payload, you may need this. Metrics exporter. Use metrics data exporter to forward metrics data to 3rd party system. Time To Live (TTL). Metrics and trace are time series data, TTL settings affect the expired time of them. Dynamic Configuration. Make configuration of OAP changed dynamic, from remote service or 3rd party configuration management system. Uninstrumented Gateways. Configure gateways/proxies that are not supported by SkyWalking agent plugins, to reflect the delegation in topology graph. Apdex threshold. Configure the thresholds for different services if Apdex calculation is activated in the OAL. Service Grouping. An automatic grouping mechanism for all services based on name. Group Parameterized Endpoints. Configure the grouping rules for parameterized endpoints, to improve the meaning of the metrics. OpenTelemetry Metrics Analysis. Activate built-in configurations to convert the metrics forwarded from OpenTelemetry collector. And learn how to write your own conversion rules. Meter Analysis. Set up the backend analysis rules, when use SkyWalking Meter System Toolkit or meter plugins. Spring Sleuth Metrics Analysis. Configure the agent and backend to receiver metrics from micrometer. Log Analyzer  Telemetry for backend OAP backend cluster itself underlying is a distributed streaming process system. For helping the Ops team, we provide the telemetry for OAP backend itself. Follow document to use it.\nAt the same time, we provide Health Check to get a score for the health status.\n 0 means healthy, more than 0 means unhealthy and less than 0 means oap doesn\u0026rsquo;t startup.\n FAQs When and why do we need to set Timezone? SkyWalking provides downsampling time series metrics features. Query and storage at each time dimension(minute, hour, day, month metrics indexes) related to timezone when doing time format.\nFor example, metrics time will be formatted like YYYYMMDDHHmm in minute dimension metrics, which format process is timezone related.\nIn default, SkyWalking OAP backend choose the OS default timezone. If you want to override it, please follow Java and OS documents to do so.\nHow to query the storage directly from 3rd party tool? SkyWalking provides browser UI, CLI and GraphQL ways to support extensions. But some users may have the idea to query data directly from the storage. Such as in ElasticSearch case, Kibana is a great tool to do this.\nIn default, due to reduce memory, network and storage space usages, SkyWalking saves based64-encoded id(s) only in the metrics entities. But these tools usually don\u0026rsquo;t support nested query, or don\u0026rsquo;t work conveniently. In this special case, SkyWalking provide a config to add all necessary name column(s) into the final metrics entities with ID as a trade-off.\nTake a look at core/default/activeExtraModelColumns config in the application.yaml, and set it as true to open this feature.\nThis feature wouldn\u0026rsquo;t provide any new feature to the native SkyWalking scenarios, just for the 3rd party integration.\n","excerpt":"Backend setup SkyWalking backend distribution package includes the following parts:\n  bin/cmd …","ref":"/docs/main/v8.6.0/en/setup/backend/backend-setup/","title":"Backend setup"},{"body":"Backend storage SkyWalking storage is pluggable, we have provided the following storage solutions, you could easily use one of them by specifying it as the selector in the application.yml：\nstorage: selector: ${SW_STORAGE:elasticsearch7} Native supported storage\n H2 OpenSearch ElasticSearch 6, 7 MySQL TiDB InfluxDB PostgreSQL  H2 Active H2 as storage, set storage provider to H2 In-Memory Databases. Default in distribution package. Please read Database URL Overview in H2 official document, you could set the target to H2 in Embedded, Server and Mixed modes.\nSetting fragment example\nstorage: selector: ${SW_STORAGE:h2} h2: driver: org.h2.jdbcx.JdbcDataSource url: jdbc:h2:mem:skywalking-oap-db user: sa OpenSearch OpenSearch storage shares the same configurations as ElasticSearch 7. In order to activate ElasticSearch 7 as storage, set storage provider to elasticsearch7. Please download the apache-skywalking-bin-es7.tar.gz if you want to use OpenSearch as storage.\nElasticSearch NOTICE: Elastic announced through their blog that Elasticsearch will be moving over to a Server Side Public License (SSPL), which is incompatible with Apache License 2.0. This license change is effective from Elasticsearch version 7.11. So please choose the suitable ElasticSearch version according to your usage.\n In order to activate ElasticSearch 6 as storage, set storage provider to elasticsearch In order to activate ElasticSearch 7 as storage, set storage provider to elasticsearch7  Required ElasticSearch 6.3.2 or higher. HTTP RestHighLevelClient is used to connect server.\n For ElasticSearch 6.3.2 ~ 7.0.0 (excluded), please download the apache-skywalking-bin.tar.gz, For ElasticSearch 7.0.0 ~ 8.0.0 (excluded), please download the apache-skywalking-bin-es7.tar.gz.  For now, ElasticSearch 6 and ElasticSearch 7 share the same configurations, as follows:\nstorage: selector: ${SW_STORAGE:elasticsearch} elasticsearch: nameSpace: ${SW_NAMESPACE:\u0026#34;\u0026#34;} clusterNodes: ${SW_STORAGE_ES_CLUSTER_NODES:localhost:9200} protocol: ${SW_STORAGE_ES_HTTP_PROTOCOL:\u0026#34;http\u0026#34;} trustStorePath: ${SW_STORAGE_ES_SSL_JKS_PATH:\u0026#34;\u0026#34;} trustStorePass: ${SW_STORAGE_ES_SSL_JKS_PASS:\u0026#34;\u0026#34;} user: ${SW_ES_USER:\u0026#34;\u0026#34;} password: ${SW_ES_PASSWORD:\u0026#34;\u0026#34;} secretsManagementFile: ${SW_ES_SECRETS_MANAGEMENT_FILE:\u0026#34;\u0026#34;} # Secrets management file in the properties format includes the username, password, which are managed by 3rd party tool. dayStep: ${SW_STORAGE_DAY_STEP:1} # Represent the number of days in the one minute/hour/day index. indexShardsNumber: ${SW_STORAGE_ES_INDEX_SHARDS_NUMBER:1} # Shard number of new indexes indexReplicasNumber: ${SW_STORAGE_ES_INDEX_REPLICAS_NUMBER:1} # Replicas number of new indexes # Super data set has been defined in the codes, such as trace segments.The following 3 config would be improve es performance when storage super size data in es. superDatasetDayStep: ${SW_SUPERDATASET_STORAGE_DAY_STEP:-1} # Represent the number of days in the super size dataset record index, the default value is the same as dayStep when the value is less than 0 superDatasetIndexShardsFactor: ${SW_STORAGE_ES_SUPER_DATASET_INDEX_SHARDS_FACTOR:5} # This factor provides more shards for the super data set, shards number = indexShardsNumber * superDatasetIndexShardsFactor. Also, this factor effects Zipkin and Jaeger traces. superDatasetIndexReplicasNumber: ${SW_STORAGE_ES_SUPER_DATASET_INDEX_REPLICAS_NUMBER:0} # Represent the replicas number in the super size dataset record index, the default value is 0. bulkActions: ${SW_STORAGE_ES_BULK_ACTIONS:1000} # Execute the async bulk record data every ${SW_STORAGE_ES_BULK_ACTIONS} requests flushInterval: ${SW_STORAGE_ES_FLUSH_INTERVAL:10} # flush the bulk every 10 seconds whatever the number of requests concurrentRequests: ${SW_STORAGE_ES_CONCURRENT_REQUESTS:2} # the number of concurrent requests resultWindowMaxSize: ${SW_STORAGE_ES_QUERY_MAX_WINDOW_SIZE:10000} metadataQueryMaxSize: ${SW_STORAGE_ES_QUERY_MAX_SIZE:5000} segmentQueryMaxSize: ${SW_STORAGE_ES_QUERY_SEGMENT_SIZE:200} profileTaskQueryMaxSize: ${SW_STORAGE_ES_QUERY_PROFILE_TASK_SIZE:200} oapAnalyzer: ${SW_STORAGE_ES_OAP_ANALYZER:\u0026#34;{\\\u0026#34;analyzer\\\u0026#34;:{\\\u0026#34;oap_analyzer\\\u0026#34;:{\\\u0026#34;type\\\u0026#34;:\\\u0026#34;stop\\\u0026#34;}}}\u0026#34;} # the oap analyzer. oapLogAnalyzer: ${SW_STORAGE_ES_OAP_LOG_ANALYZER:\u0026#34;{\\\u0026#34;analyzer\\\u0026#34;:{\\\u0026#34;oap_log_analyzer\\\u0026#34;:{\\\u0026#34;type\\\u0026#34;:\\\u0026#34;standard\\\u0026#34;}}}\u0026#34;} # the oap log analyzer. It could be customized by the ES analyzer configuration to support more language log formats, such as Chinese log, Japanese log and etc. advanced: ${SW_STORAGE_ES_ADVANCED:\u0026#34;\u0026#34;} ElasticSearch 6 With Https SSL Encrypting communications. example:\nstorage: selector: ${SW_STORAGE:elasticsearch} elasticsearch: # nameSpace: ${SW_NAMESPACE:\u0026#34;\u0026#34;} user: ${SW_ES_USER:\u0026#34;\u0026#34;} # User needs to be set when Http Basic authentication is enabled password: ${SW_ES_PASSWORD:\u0026#34;\u0026#34;} # Password to be set when Http Basic authentication is enabled clusterNodes: ${SW_STORAGE_ES_CLUSTER_NODES:localhost:443} trustStorePath: ${SW_SW_STORAGE_ES_SSL_JKS_PATH:\u0026#34;../es_keystore.jks\u0026#34;} trustStorePass: ${SW_SW_STORAGE_ES_SSL_JKS_PASS:\u0026#34;\u0026#34;} protocol: ${SW_STORAGE_ES_HTTP_PROTOCOL:\u0026#34;https\u0026#34;} ...  File at trustStorePath is being monitored, once it is changed, the ElasticSearch client will do reconnecting. trustStorePass could be changed on the runtime through Secrets Management File Of ElasticSearch Authentication.  Daily Index Step Daily index step(storage/elasticsearch/dayStep, default 1) represents the index creation period. In this period, several days(dayStep value)' metrics are saved.\nMostly, users don\u0026rsquo;t need to change the value manually. As SkyWalking is designed to observe large scale distributed system. But in some specific cases, users want to set a long TTL value, such as more than 60 days, but their ElasticSearch cluster isn\u0026rsquo;t powerful due to the low traffic in the production environment. This value could be increased to 5(or more), if users could make sure single one index could support these days(5 in this case) metrics and traces.\nSuch as, if dayStep == 11,\n data in [2000-01-01, 2000-01-11] will be merged into the index-20000101. data in [2000-01-12, 2000-01-22] will be merged into the index-20000112.  storage/elasticsearch/superDatasetDayStep override the storage/elasticsearch/dayStep if the value is positive. This would affect the record related entities, such as the trace segment. In some cases, the size of metrics is much less than the record(trace), this would help the shards balance in the ElasticSearch cluster.\nNOTICE, TTL deletion would be affected by these. You should set an extra more dayStep in your TTL. Such as you want to TTL == 30 days and dayStep == 10, you actually need to set TTL = 40;\nSecrets Management File Of ElasticSearch Authentication The value of secretsManagementFile should point to the secrets management file absolute path. The file includes username, password and JKS password of ElasticSearch server in the properties format.\nuser=xxx password=yyy trustStorePass=zzz The major difference between using user, password, trustStorePass configs in the application.yaml file is, the Secrets Management File is being watched by the OAP server. Once it is changed manually or through 3rd party tool, such as Vault, the storage provider will use the new username, password and JKS password to establish the connection and close the old one. If the information exist in the file, the user/password will be overrided.\nAdvanced Configurations For Elasticsearch Index You can add advanced configurations in JSON format to set ElasticSearch index settings by following ElasticSearch doc\nFor example, set translog settings:\nstorage: elasticsearch: # ...... advanced: ${SW_STORAGE_ES_ADVANCED:\u0026#34;{\\\u0026#34;index.translog.durability\\\u0026#34;:\\\u0026#34;request\\\u0026#34;,\\\u0026#34;index.translog.sync_interval\\\u0026#34;:\\\u0026#34;5s\\\u0026#34;}\u0026#34;} Recommended ElasticSearch server-side configurations You could add following config to elasticsearch.yml, set the value based on your env.\n# In tracing scenario, consider to set more than this at least. thread_pool.index.queue_size: 1000 # Only suitable for ElasticSearch 6 thread_pool.write.queue_size: 1000 # Suitable for ElasticSearch 6 and 7 # When you face query error at trace page, remember to check this. index.max_result_window: 1000000 We strongly advice you to read more about these configurations from ElasticSearch official document. This effects the performance of ElasticSearch very much.\nElasticSearch 7 with Zipkin trace extension This implementation shares most of elasticsearch7, just extends to support zipkin span storage. It has all same configs.\nstorage: selector: ${SW_STORAGE:zipkin-elasticsearch7} zipkin-elasticsearch7: nameSpace: ${SW_NAMESPACE:\u0026#34;\u0026#34;} clusterNodes: ${SW_STORAGE_ES_CLUSTER_NODES:localhost:9200} protocol: ${SW_STORAGE_ES_HTTP_PROTOCOL:\u0026#34;http\u0026#34;} user: ${SW_ES_USER:\u0026#34;\u0026#34;} password: ${SW_ES_PASSWORD:\u0026#34;\u0026#34;} indexShardsNumber: ${SW_STORAGE_ES_INDEX_SHARDS_NUMBER:2} indexReplicasNumber: ${SW_STORAGE_ES_INDEX_REPLICAS_NUMBER:0} # Batch process setting, refer to https://www.elastic.co/guide/en/elasticsearch/client/java-api/5.5/java-docs-bulk-processor.html bulkActions: ${SW_STORAGE_ES_BULK_ACTIONS:2000} # Execute the bulk every 2000 requests bulkSize: ${SW_STORAGE_ES_BULK_SIZE:20} # flush the bulk every 20mb flushInterval: ${SW_STORAGE_ES_FLUSH_INTERVAL:10} # flush the bulk every 10 seconds whatever the number of requests concurrentRequests: ${SW_STORAGE_ES_CONCURRENT_REQUESTS:2} # the number of concurrent requests About Namespace When namespace is set, names of all indexes in ElasticSearch will use it as prefix.\nMySQL Active MySQL as storage, set storage provider to mysql.\nNOTICE: MySQL driver is NOT allowed in Apache official distribution and source codes. Please download MySQL driver by yourself. Copy the connection driver jar to oap-libs.\nstorage: selector: ${SW_STORAGE:mysql} mysql: properties: jdbcUrl: ${SW_JDBC_URL:\u0026#34;jdbc:mysql://localhost:3306/swtest\u0026#34;} dataSource.user: ${SW_DATA_SOURCE_USER:root} dataSource.password: ${SW_DATA_SOURCE_PASSWORD:root@1234} dataSource.cachePrepStmts: ${SW_DATA_SOURCE_CACHE_PREP_STMTS:true} dataSource.prepStmtCacheSize: ${SW_DATA_SOURCE_PREP_STMT_CACHE_SQL_SIZE:250} dataSource.prepStmtCacheSqlLimit: ${SW_DATA_SOURCE_PREP_STMT_CACHE_SQL_LIMIT:2048} dataSource.useServerPrepStmts: ${SW_DATA_SOURCE_USE_SERVER_PREP_STMTS:true} metadataQueryMaxSize: ${SW_STORAGE_MYSQL_QUERY_MAX_SIZE:5000} All connection related settings including link url, username and password are in application.yml. Here are some of the settings, please follow HikariCP connection pool document for all the settings.\nTiDB Tested TiDB Server 4.0.8 version and Mysql Client driver 8.0.13 version currently. Active TiDB as storage, set storage provider to tidb.\nstorage: selector: ${SW_STORAGE:tidb} tidb: properties: jdbcUrl: ${SW_JDBC_URL:\u0026#34;jdbc:mysql://localhost:4000/swtest\u0026#34;} dataSource.user: ${SW_DATA_SOURCE_USER:root} dataSource.password: ${SW_DATA_SOURCE_PASSWORD:\u0026#34;\u0026#34;} dataSource.cachePrepStmts: ${SW_DATA_SOURCE_CACHE_PREP_STMTS:true} dataSource.prepStmtCacheSize: ${SW_DATA_SOURCE_PREP_STMT_CACHE_SQL_SIZE:250} dataSource.prepStmtCacheSqlLimit: ${SW_DATA_SOURCE_PREP_STMT_CACHE_SQL_LIMIT:2048} dataSource.useServerPrepStmts: ${SW_DATA_SOURCE_USE_SERVER_PREP_STMTS:true} dataSource.useAffectedRows: ${SW_DATA_SOURCE_USE_AFFECTED_ROWS:true} metadataQueryMaxSize: ${SW_STORAGE_MYSQL_QUERY_MAX_SIZE:5000} maxSizeOfArrayColumn: ${SW_STORAGE_MAX_SIZE_OF_ARRAY_COLUMN:20} numOfSearchableValuesPerTag: ${SW_STORAGE_NUM_OF_SEARCHABLE_VALUES_PER_TAG:2} All connection related settings including link url, username and password are in application.yml. These settings can refer to the configuration of MySQL above.\nInfluxDB InfluxDB storage provides a time-series database as a new storage option.\nstorage: selector: ${SW_STORAGE:influxdb} influxdb: url: ${SW_STORAGE_INFLUXDB_URL:http://localhost:8086} user: ${SW_STORAGE_INFLUXDB_USER:root} password: ${SW_STORAGE_INFLUXDB_PASSWORD:} database: ${SW_STORAGE_INFLUXDB_DATABASE:skywalking} actions: ${SW_STORAGE_INFLUXDB_ACTIONS:1000} # the number of actions to collect duration: ${SW_STORAGE_INFLUXDB_DURATION:1000} # the time to wait at most (milliseconds) fetchTaskLogMaxSize: ${SW_STORAGE_INFLUXDB_FETCH_TASK_LOG_MAX_SIZE:5000} # the max number of fetch task log in a request All connection related settings including link url, username and password are in application.yml. The Metadata storage provider settings can refer to the configuration of H2/MySQL above.\nPostgreSQL PostgreSQL jdbc driver uses version 42.2.18, it supports PostgreSQL 8.2 or newer. Active PostgreSQL as storage, set storage provider to postgresql.\nstorage: selector: ${SW_STORAGE:postgresql} postgresql: properties: jdbcUrl: ${SW_JDBC_URL:\u0026#34;jdbc:postgresql://localhost:5432/skywalking\u0026#34;} dataSource.user: ${SW_DATA_SOURCE_USER:postgres} dataSource.password: ${SW_DATA_SOURCE_PASSWORD:123456} dataSource.cachePrepStmts: ${SW_DATA_SOURCE_CACHE_PREP_STMTS:true} dataSource.prepStmtCacheSize: ${SW_DATA_SOURCE_PREP_STMT_CACHE_SQL_SIZE:250} dataSource.prepStmtCacheSqlLimit: ${SW_DATA_SOURCE_PREP_STMT_CACHE_SQL_LIMIT:2048} dataSource.useServerPrepStmts: ${SW_DATA_SOURCE_USE_SERVER_PREP_STMTS:true} metadataQueryMaxSize: ${SW_STORAGE_MYSQL_QUERY_MAX_SIZE:5000} maxSizeOfArrayColumn: ${SW_STORAGE_MAX_SIZE_OF_ARRAY_COLUMN:20} numOfSearchableValuesPerTag: ${SW_STORAGE_NUM_OF_SEARCHABLE_VALUES_PER_TAG:2} All connection related settings including link url, username and password are in application.yml. Here are some of the settings, please follow HikariCP connection pool document for all the settings.\nMore storage solution extension Follow Storage extension development guide in Project Extensions document in development guide.\n","excerpt":"Backend storage SkyWalking storage is pluggable, we have provided the following storage solutions, …","ref":"/docs/main/v8.6.0/en/setup/backend/backend-storage/","title":"Backend storage"},{"body":"Browser Monitoring Apache SkyWalking Client JS is client-side JavaScript exception and tracing library.\n Provide metrics and error collection to SkyWalking backend. Lightweight, no browser plugin, just a simple JavaScript library. Make browser as a start of whole distributed tracing.  Go to the Client JS official doc to learn more.\nNote, make sure the receiver-browser has been opened, default is ON since 8.2.0.\n","excerpt":"Browser Monitoring Apache SkyWalking Client JS is client-side JavaScript exception and tracing …","ref":"/docs/main/v8.6.0/en/setup/service-agent/browser-agent/","title":"Browser Monitoring"},{"body":"Browser Protocol Browser protocol describes the data format between skywalking-client-js and the backend.\nOverview Browser protocol is defined and provided in gRPC format, and also implemented in HTTP 1.1\nSend performance data and error logs You can send performance data and error logs using the following services:\n BrowserPerfService#collectPerfData for performance data format. BrowserPerfService#collectErrorLogs for error log format.  For error log format, note that:\n BrowserErrorLog#uniqueId should be unique in all distributed environments.  ","excerpt":"Browser Protocol Browser protocol describes the data format between skywalking-client-js and the …","ref":"/docs/main/v8.6.0/en/protocols/browser-protocol/","title":"Browser Protocol"},{"body":"CDS - Configuration Discovery Service CDS - Configuration Discovery Service provides the dynamic configuration for the agent, defined in gRPC.\nConfiguration Format The configuration content includes the service name and their configs. The\nconfigurations: //service name serviceA: // Configurations of service A // Key and Value are determined by the agent side. // Check the agent setup doc for all available configurations. key1: value1 key2: value2 ... serviceB: ... Available key(s) and value(s) in Java Agent. Java agent supports the following dynamic configurations.\n   Config Key Value Description Value Format Example Required Plugin(s)     agent.sample_n_per_3_secs The number of sampled traces per 3 seconds -1 -   agent.ignore_suffix If the operation name of the first span is included in this set, this segment should be ignored. Multiple values should be separated by , .txt,.log -   agent.trace.ignore_path The value is the path that you need to ignore, multiple paths should be separated by , more details /your/path/1/**,/your/path/2/** apm-trace-ignore-plugin   agent.span_limit_per_segment The max number of spans per segment. 300 -     Required plugin(s), the configuration affects only when the required plugins activated.  ","excerpt":"CDS - Configuration Discovery Service CDS - Configuration Discovery Service provides the dynamic …","ref":"/docs/main/v8.6.0/en/setup/service-agent/java-agent/configuration-discovery/","title":"CDS - Configuration Discovery Service"},{"body":"Choosing a receiver Receiver is a defined concept in SkyWalking\u0026rsquo;s backend. All modules which are responsible for receiving telemetry or tracing data from other systems being monitored are all called receivers. If you are looking for the pull mode, take a look at the fetcher document.\nWe have the following receivers, and default implementors are provided in our Apache distribution.\n receiver-trace. gRPC and HTTPRestful services that accept SkyWalking format traces. receiver-register. gRPC and HTTPRestful services that provide service, service instance and endpoint register. service-mesh. gRPC services that accept data from inbound mesh probes. receiver-jvm. gRPC services that accept JVM metrics data. envoy-metric. Envoy metrics_service and ALS(access log service) are supported by this receiver. The OAL script supports all GAUGE type metrics. receiver-profile. gRPC services that accept profile task status and snapshot reporter. receiver-otel. See details. A receiver for analyzing metrics data from OpenTelemetry. receiver-meter. See details. A receiver for analyzing metrics in SkyWalking native meter format. receiver-browser. gRPC services that accept browser performance data and error log. receiver-log. A receiver for native log format. See Log Analyzer for advanced features. configuration-discovery. gRPC services that handle configurationDiscovery. receiver-event. gRPC services that handle events data. receiver-zabbix. See details. Experimental receivers.  receiver_zipkin. See details.    The sample settings of these receivers are by default included in application.yml, and also listed here:\nreceiver-register: selector: ${SW_RECEIVER_REGISTER:default} default: receiver-trace: selector: ${SW_RECEIVER_TRACE:default} default: receiver-jvm: selector: ${SW_RECEIVER_JVM:default} default: service-mesh: selector: ${SW_SERVICE_MESH:default} default: envoy-metric: selector: ${SW_ENVOY_METRIC:default} default: acceptMetricsService: ${SW_ENVOY_METRIC_SERVICE:true} alsHTTPAnalysis: ${SW_ENVOY_METRIC_ALS_HTTP_ANALYSIS:\u0026#34;\u0026#34;} receiver_zipkin: selector: ${SW_RECEIVER_ZIPKIN:-} default: host: ${SW_RECEIVER_ZIPKIN_HOST:0.0.0.0} port: ${SW_RECEIVER_ZIPKIN_PORT:9411} contextPath: ${SW_RECEIVER_ZIPKIN_CONTEXT_PATH:/} jettyMinThreads: ${SW_RECEIVER_ZIPKIN_JETTY_MIN_THREADS:1} jettyMaxThreads: ${SW_RECEIVER_ZIPKIN_JETTY_MAX_THREADS:200} jettyIdleTimeOut: ${SW_RECEIVER_ZIPKIN_JETTY_IDLE_TIMEOUT:30000} jettyAcceptorPriorityDelta: ${SW_RECEIVER_ZIPKIN_JETTY_DELTA:0} jettyAcceptQueueSize: ${SW_RECEIVER_ZIPKIN_QUEUE_SIZE:0} receiver-profile: selector: ${SW_RECEIVER_PROFILE:default} default: receiver-browser: selector: ${SW_RECEIVER_BROWSER:default} default: sampleRate: ${SW_RECEIVER_BROWSER_SAMPLE_RATE:10000} log-analyzer: selector: ${SW_LOG_ANALYZER:default} default: lalFiles: ${SW_LOG_LAL_FILES:default} malFiles: ${SW_LOG_MAL_FILES:\u0026#34;\u0026#34;} configuration-discovery: selector: ${SW_CONFIGURATION_DISCOVERY:default} default: receiver-event: selector: ${SW_RECEIVER_EVENT:default} default: gRPC/HTTP server for receiver By default, all gRPC/HTTP services should be served at core/gRPC and core/rest. But the receiver-sharing-server module allows all receivers to be served at different ip:port, if you set them explicitly.\nreceiver-sharing-server: selector: ${SW_RECEIVER_SHARING_SERVER:default} default: host: ${SW_RECEIVER_JETTY_HOST:0.0.0.0} contextPath: ${SW_RECEIVER_JETTY_CONTEXT_PATH:/} authentication: ${SW_AUTHENTICATION:\u0026#34;\u0026#34;} jettyMinThreads: ${SW_RECEIVER_SHARING_JETTY_MIN_THREADS:1} jettyMaxThreads: ${SW_RECEIVER_SHARING_JETTY_MAX_THREADS:200} jettyIdleTimeOut: ${SW_RECEIVER_SHARING_JETTY_IDLE_TIMEOUT:30000} jettyAcceptorPriorityDelta: ${SW_RECEIVER_SHARING_JETTY_DELTA:0} jettyAcceptQueueSize: ${SW_RECEIVER_SHARING_JETTY_QUEUE_SIZE:0} Note: If you add these settings, make sure that they are not the same as the core module. This is because gRPC/HTTP servers of the core are still used for UI and OAP internal communications.\nOpenTelemetry receiver The OpenTelemetry receiver supports ingesting agent metrics by meter-system. The OAP can load the configuration at bootstrap. If the new configuration is not well-formed, the OAP may fail to start up. The files are located at $CLASSPATH/otel-\u0026lt;handler\u0026gt;-rules. E.g. The oc handler loads rules from $CLASSPATH/otel-oc-rules.\nSupported handlers: * oc: OpenCensus gRPC service handler.\nThe rule file should be in YAML format, defined by the scheme described in prometheus-fetcher. Note: receiver-otel only supports the group, defaultMetricLevel, and metricsRules nodes of the scheme due to its push mode.\nTo activate the oc handler and relevant rules of istio:\nreceiver-otel: selector: ${SW_OTEL_RECEIVER:default} default: enabledHandlers: ${SW_OTEL_RECEIVER_ENABLED_HANDLERS:\u0026#34;oc\u0026#34;} enabledOcRules: ${SW_OTEL_RECEIVER_ENABLED_OC_RULES:\u0026#34;istio-controlplane\u0026#34;} The receiver adds labels with key = node_identifier_host_name and key = node_identifier_pid to the collected data samples, and values from Node.identifier.host_name and Node.identifier.pid defined in OpenCensus Agent Proto, for identification of the metric data.\n   Rule Name Description Configuration File Data Source     istio-controlplane Metrics of Istio control panel otel-oc-rules/istio-controlplane.yaml Istio Control Panel -\u0026gt; OpenTelemetry Collector \u0026ndash;OC format\u0026ndash;\u0026gt; SkyWalking OAP Server   oap Metrics of SkyWalking OAP server itself otel-oc-rules/oap.yaml SkyWalking OAP Server(SelfObservability) -\u0026gt; OpenTelemetry Collector \u0026ndash;OC format\u0026ndash;\u0026gt; SkyWalking OAP Server   vm Metrics of VMs otel-oc-rules/vm.yaml Prometheus node-exporter(VMs) -\u0026gt; OpenTelemetry Collector \u0026ndash;OC format\u0026ndash;\u0026gt; SkyWalking OAP Server   k8s-cluster Metrics of K8s cluster otel-oc-rules/k8s-cluster.yaml K8s kube-state-metrics -\u0026gt; OpenTelemetry Collector \u0026ndash;OC format\u0026ndash;\u0026gt; SkyWalking OAP Server   k8s-node Metrics of K8s cluster otel-oc-rules/k8s-node.yaml cAdvisor \u0026amp; K8s kube-state-metrics -\u0026gt; OpenTelemetry Collector \u0026ndash;OC format\u0026ndash;\u0026gt; SkyWalking OAP Server   k8s-service Metrics of K8s cluster otel-oc-rules/k8s-service.yaml cAdvisor \u0026amp; K8s kube-state-metrics -\u0026gt; OpenTelemetry Collector \u0026ndash;OC format\u0026ndash;\u0026gt; SkyWalking OAP Server    Meter receiver The meter receiver supports accepting the metrics into the meter-system. The OAP can load the configuration at bootstrap.\nThe file is written in YAML format, defined by the scheme described in backend-meter.\nTo activate the default implementation:\nreceiver-meter: selector: ${SW_RECEIVER_METER:default} default: Zipkin receiver The Zipkin receiver makes the OAP server work as an alternative Zipkin server implementation. It supports Zipkin v1/v2 formats through HTTP service. Make sure you use this with SW_STORAGE=zipkin-elasticsearch7 option to activate Zipkin storage implementation. Once this receiver and storage are activated, SkyWalking\u0026rsquo;s native traces would be ignored, and SkyWalking wouldn\u0026rsquo;t analyze topology, metrics, and endpoint dependency from Zipkin\u0026rsquo;s trace.\nUse the following config to activate it.\nreceiver_zipkin: selector: ${SW_RECEIVER_ZIPKIN:-} default: host: ${SW_RECEIVER_ZIPKIN_HOST:0.0.0.0} port: ${SW_RECEIVER_ZIPKIN_PORT:9411} contextPath: ${SW_RECEIVER_ZIPKIN_CONTEXT_PATH:/} jettyMinThreads: ${SW_RECEIVER_ZIPKIN_JETTY_MIN_THREADS:1} jettyMaxThreads: ${SW_RECEIVER_ZIPKIN_JETTY_MAX_THREADS:200} jettyIdleTimeOut: ${SW_RECEIVER_ZIPKIN_JETTY_IDLE_TIMEOUT:30000} jettyAcceptorPriorityDelta: ${SW_RECEIVER_ZIPKIN_JETTY_DELTA:0} jettyAcceptQueueSize: ${SW_RECEIVER_ZIPKIN_QUEUE_SIZE:0} NOTE: Zipkin receiver is only provided in apache-skywalking-apm-es7-x.y.z.tar.gz tar. This requires zipkin-elasticsearch7 storage implementation to be activated. Read this doc to learn about Zipkin as a storage option.\n","excerpt":"Choosing a receiver Receiver is a defined concept in SkyWalking\u0026rsquo;s backend. All modules which …","ref":"/docs/main/v8.6.0/en/setup/backend/backend-receivers/","title":"Choosing a receiver"},{"body":"Cluster Management In many product environments, the backend needs to support high throughput and provide HA to maintain robustness, so you always need cluster management in product env.\nThere are various ways to manage the cluster in the backend. Choose the one that best suits your needs.\n Zookeeper coordinator. Use Zookeeper to let the backend instances detect and communicate with each other. Kubernetes. When the backend clusters are deployed inside Kubernetes, you could make use of this method by using k8s native APIs to manage clusters. Consul. Use Consul as the backend cluster management implementor and coordinate backend instances. Etcd. Use Etcd to coordinate backend instances. Nacos. Use Nacos to coordinate backend instances. In the application.yml file, there are default configurations for the aforementioned coordinators under the section cluster. You can specify any of them in the selector property to enable it.  Zookeeper coordinator Zookeeper is a very common and widely used cluster coordinator. Set the cluster/selector to zookeeper in the yml to enable it.\nRequired Zookeeper version: 3.4+\ncluster: selector: ${SW_CLUSTER:zookeeper} # other configurations  hostPort is the list of zookeeper servers. Format is IP1:PORT1,IP2:PORT2,...,IPn:PORTn enableACL enable Zookeeper ACL to control access to its znode. schema is Zookeeper ACL schemas. expression is a expression of ACL. The format of the expression is specific to the schema. hostPort, baseSleepTimeMs and maxRetries are settings of Zookeeper curator client.  Note:\n If Zookeeper ACL is enabled and /skywalking exists, you must make sure that SkyWalking has CREATE, READ and WRITE permissions. If /skywalking does not exist, it will be created by SkyWalking and all permissions to the specified user will be granted. Simultaneously, znode grants the READ permission to anyone. If you set schema as digest, the password of the expression is set in clear text.  In some cases, the OAP default gRPC host and port in core are not suitable for internal communication among the OAP nodes. The following settings are provided to set the host and port manually, based on your own LAN env.\n internalComHost: The registered host and other OAP nodes use this to communicate with the current node. internalComPort: the registered port and other OAP nodes use this to communicate with the current node.  zookeeper: nameSpace: ${SW_NAMESPACE:\u0026#34;\u0026#34;} hostPort: ${SW_CLUSTER_ZK_HOST_PORT:localhost:2181} #Retry Policy baseSleepTimeMs: ${SW_CLUSTER_ZK_SLEEP_TIME:1000} # initial amount of time to wait between retries maxRetries: ${SW_CLUSTER_ZK_MAX_RETRIES:3} # max number of times to retry internalComHost: 172.10.4.10 internalComPort: 11800 # Enable ACL enableACL: ${SW_ZK_ENABLE_ACL:false} # disable ACL in default schema: ${SW_ZK_SCHEMA:digest} # only support digest schema expression: ${SW_ZK_EXPRESSION:skywalking:skywalking} Kubernetes The require backend clusters are deployed inside Kubernetes. See the guides in Deploy in kubernetes. Set the selector to kubernetes.\ncluster: selector: ${SW_CLUSTER:kubernetes} # other configurations Consul Recently, the Consul system has become more and more popular, and many companies and developers now use Consul as their service discovery solution. Set the cluster/selector to consul in the yml to enable it.\ncluster: selector: ${SW_CLUSTER:consul} # other configurations Same as the Zookeeper coordinator, in some cases, the OAP default gRPC host and port in core are not suitable for internal communication among the OAP nodes. The following settings are provided to set the host and port manually, based on your own LAN env.\n internalComHost: The registed host and other OAP nodes use this to communicate with the current node. internalComPort: The registered port and other OAP nodes use this to communicate with the current node.  Etcd Set the cluster/selector to etcd in the yml to enable it.\ncluster: selector: ${SW_CLUSTER:etcd} # other configurations Same as the Zookeeper coordinator, in some cases, the OAP default gRPC host and port in core are not suitable for internal communication among the oap nodes. The following settings are provided to set the host and port manually, based on your own LAN env.\n internalComHost: The registered host and other OAP nodes use this to communicate with the current node. internalComPort: The registered port and other OAP nodes use this to communicate with the current node.  Nacos Set the cluster/selector to nacos in the yml to enable it.\ncluster: selector: ${SW_CLUSTER:nacos} # other configurations Nacos supports authentication by username or accessKey. Empty means that there is no need for authentication. Extra config is as follows:\nnacos: username: password: accessKey: secretKey: Same as the Zookeeper coordinator, in some cases, the OAP default gRPC host and port in core are not suitable for internal communication among the OAP nodes. The following settings are provided to set the host and port manually, based on your own LAN env.\n internalComHost: The registered host and other OAP nodes use this to communicate with the current node. internalComPort: The registered port and other OAP nodes use this to communicate with the current node.  ","excerpt":"Cluster Management In many product environments, the backend needs to support high throughput and …","ref":"/docs/main/v8.6.0/en/setup/backend/backend-cluster/","title":"Cluster Management"},{"body":"Compatibility with other Java agent bytecode processes Problem   When using the SkyWalking agent, some other agents, such as Arthas, can\u0026rsquo;t work properly. https://github.com/apache/skywalking/pull/4858\n  The retransform classes in the Java agent conflict with the SkyWalking agent, as illustrated in this demo\n  Cause The SkyWalking agent uses ByteBuddy to transform classes when the Java application starts. ByteBuddy generates auxiliary classes with different random names every time.\nWhen another Java agent retransforms the same class, it triggers the SkyWalking agent to enhance the class again. Since the bytecode has been regenerated by ByteBuddy, the fields and imported class names have been modified, and the JVM verifications on class bytecode have failed, the retransform classes would therefore be unsuccessful.\nResolution 1. Enable the class cache feature\nAdd JVM parameters:\n-Dskywalking.agent.is_cache_enhanced_class=true -Dskywalking.agent.class_cache_mode=MEMORY\nOr uncomment the following options in agent.conf:\n# If true, the SkyWalking agent will cache all instrumented classes files to memory or disk files (as determined by the class cache mode), # Allow other Java agents to enhance those classes that are enhanced by the SkyWalking agent. agent.is_cache_enhanced_class = ${SW_AGENT_CACHE_CLASS:false} # The instrumented classes cache mode: MEMORY or FILE # MEMORY: cache class bytes to memory; if there are too many instrumented classes or if their sizes are too large, it may take up more memory # FILE: cache class bytes to user temp folder starts with 'class-cache', and automatically clean up cached class files when the application exits agent.class_cache_mode = ${SW_AGENT_CLASS_CACHE_MODE:MEMORY} If the class cache feature is enabled, save the instrumented class bytecode to memory or a temporary file. When other Java agents retransform the same class, the SkyWalking agent first attempts to load from the cache.\nIf the cached class is found, it will be used directly without regenerating an auxiliary class with a new random name. Then, the process of the subsequent Java agent will not be affected.\n2. Class cache save mode\nWe recommend saving cache classes to memory, if it takes up more memory space. Alternatively, you can use the local file system. Set the class cache mode in one of the folliwng ways:\n-Dskywalking.agent.class_cache_mode=MEMORY : save cache classes to Java memory. -Dskywalking.agent.class_cache_mode=FILE : save cache classes to SkyWalking agent path \u0026lsquo;/class-cache\u0026rsquo;.\nOr modify these options in agent.conf:\nagent.class_cache_mode = ${SW_AGENT_CLASS_CACHE_MODE:MEMORY} agent.class_cache_mode = ${SW_AGENT_CLASS_CACHE_MODE:FILE}\n","excerpt":"Compatibility with other Java agent bytecode processes Problem   When using the SkyWalking agent, …","ref":"/docs/main/v8.6.0/en/faq/compatible-with-other-javaagent-bytecode-processing/","title":"Compatibility with other Java agent bytecode processes"},{"body":"Compiling issues on Mac\u0026rsquo;s M1 chip Problem  When compiling according to How-to-build, The following problems may occur, causing the build to fail.  [ERROR] Failed to execute goal org.xolstice.maven.plugins:protobuf-maven-plugin:0.6.1:compile (grpc-build) on project apm-network: Unable to resolve artifact: Missing: [ERROR] ---------- [ERROR] 1) com.google.protobuf:protoc:exe:osx-aarch_64:3.12.0 [ERROR] [ERROR] Try downloading the file manually from the project website. [ERROR] [ERROR] Then, install it using the command: [ERROR] mvn install:install-file -DgroupId=com.google.protobuf -DartifactId=protoc -Dversion=3.12.0 -Dclassifier=osx-aarch_64 -Dpackaging=exe -Dfile=/path/to/file [ERROR] [ERROR] Alternatively, if you host your own repository you can deploy the file there: [ERROR] mvn deploy:deploy-file -DgroupId=com.google.protobuf -DartifactId=protoc -Dversion=3.12.0 -Dclassifier=osx-aarch_64 -Dpackaging=exe -Dfile=/path/to/file -Durl=[url] -DrepositoryId=[id] [ERROR] [ERROR] Path to dependency: [ERROR] 1) org.apache.skywalking:apm-network:jar:8.4.0-SNAPSHOT [ERROR] 2) com.google.protobuf:protoc:exe:osx-aarch_64:3.12.0 [ERROR] [ERROR] ---------- [ERROR] 1 required artifact is missing. Reason The dependent Protocol Buffers v3.14.0 does not come with an osx-aarch_64 version. You may find the osx-aarch_64 version at the Protocol Buffers Releases link here: https://github.com/protocolbuffers/protobuf/releases. Since Mac\u0026rsquo;s M1 is compatible with the osx-x86_64 version, before this version is available for downloading, you need to manually specify the osx-x86_64 version.\nResolution You may add -Dos.detected.classifier=osx-x86_64 after the original compilation parameters, such as: ./mvnw clean package -DskipTests -Dos.detected.classifier=osx-x86_64. After specifying the version, compile and run normally.\n","excerpt":"Compiling issues on Mac\u0026rsquo;s M1 chip Problem  When compiling according to How-to-build, The …","ref":"/docs/main/v8.6.0/en/faq/how-to-build-with-mac-m1/","title":"Compiling issues on Mac's M1 chip"},{"body":"Component library settings Component library settings are about your own or third-party libraries used in the monitored application.\nIn agent or SDK, regardless of whether the library name is collected as ID or String (literally, e.g. SpringMVC), the collector formats data in ID for better performance and less storage requirements.\nAlso, the collector conjectures the remote service based on the component library. For example: if the component library is MySQL Driver library, then the remote service should be MySQL Server.\nFor these two reasons, the collector requires two parts of settings in this file:\n Component library ID, names and languages. Remote server mapping based on the local library.  All component names and IDs must be defined in this file.\nComponent Library ID Define all names and IDs from component libraries which are used in the monitored application. This uses a two-way mapping strategy. The agent or SDK could use the value (ID) to represent the component name in uplink data.\n Name: the component name used in agent and UI ID: Unique ID. All IDs are reserved once they are released. Languages: Program languages may use this component. Multi languages should be separated by ,.  ID rules  Java and multi languages shared: (0, 3000) .NET Platform reserved: [3000, 4000) Node.js Platform reserved: [4000, 5000) Go reserved: [5000, 6000) Lua reserved: [6000, 7000) Python reserved: [7000, 8000) PHP reserved: [8000, 9000) C++ reserved: [9000, 10000)  Example:\nTomcat: id: 1 languages: Java HttpClient: id: 2 languages: Java,C#,Node.js Dubbo: id: 3 languages: Java H2: id: 4 languages: Java Remote server mapping The remote server will be conjectured by the local component. The mappings are based on names in the component library.\n Key: client component library name Value: server component name  Component-Server-Mappings: Jedis: Redis StackExchange.Redis: Redis Redisson: Redis Lettuce: Redis Zookeeper: Zookeeper SqlClient: SqlServer Npgsql: PostgreSQL MySqlConnector: Mysql EntityFrameworkCore.InMemory: InMemoryDatabase ","excerpt":"Component library settings Component library settings are about your own or third-party libraries …","ref":"/docs/main/v8.6.0/en/guides/component-library-settings/","title":"Component library settings"},{"body":"Configuration Vocabulary Configuration Vocabulary lists all available configurations provided by application.yml.\n   Module Provider Settings Value(s) and Explanation System Environment Variable¹ Default     core default role Option values, Mixed/Receiver/Aggregator. Receiver mode OAP open the service to the agents, analysis and aggregate the results and forward the results for distributed aggregation. Aggregator mode OAP receives data from Mixer and Receiver role OAP nodes, and do 2nd level aggregation. Mixer means being Receiver and Aggregator both. SW_CORE_ROLE Mixed   - - restHost Binding IP of restful service. Services include GraphQL query and HTTP data report SW_CORE_REST_HOST 0.0.0.0   - - restPort Binding port of restful service SW_CORE_REST_PORT 12800   - - restContextPath Web context path of restful service SW_CORE_REST_CONTEXT_PATH /   - - restMinThreads Min threads number of restful service SW_CORE_REST_JETTY_MIN_THREADS 1   - - restMaxThreads Max threads number of restful service SW_CORE_REST_JETTY_MAX_THREADS 200   - - restIdleTimeOut Connector idle timeout in milliseconds of restful service SW_CORE_REST_JETTY_IDLE_TIMEOUT 30000   - - restAcceptorPriorityDelta Thread priority delta to give to acceptor threads of restful service SW_CORE_REST_JETTY_DELTA 0   - - restAcceptQueueSize ServerSocketChannel backlog of restful service SW_CORE_REST_JETTY_QUEUE_SIZE 0   - - gRPCHost Binding IP of gRPC service. Services include gRPC data report and internal communication among OAP nodes SW_CORE_GRPC_HOST 0.0.0.0   - - gRPCPort Binding port of gRPC service SW_CORE_GRPC_PORT 11800   - - gRPCSslEnabled Activate SSL for gRPC service SW_CORE_GRPC_SSL_ENABLED false   - - gRPCSslKeyPath The file path of gRPC SSL key SW_CORE_GRPC_SSL_KEY_PATH -   - - gRPCSslCertChainPath The file path of gRPC SSL cert chain SW_CORE_GRPC_SSL_CERT_CHAIN_PATH -   - - gRPCSslTrustedCAPath The file path of gRPC trusted CA SW_CORE_GRPC_SSL_TRUSTED_CA_PATH -   - - downsampling The activated level of down sampling aggregation  Hour,Day   - - enableDataKeeperExecutor Controller of TTL scheduler. Once disabled, TTL wouldn\u0026rsquo;t work. SW_CORE_ENABLE_DATA_KEEPER_EXECUTOR true   - - dataKeeperExecutePeriod The execution period of TTL scheduler, unit is minute. Execution doesn\u0026rsquo;t mean deleting data. The storage provider could override this, such as ElasticSearch storage. SW_CORE_DATA_KEEPER_EXECUTE_PERIOD 5   - - recordDataTTL The lifecycle of record data. Record data includes traces, top n sampled records, and logs. Unit is day. Minimal value is 2. SW_CORE_RECORD_DATA_TTL 3   - - metricsDataTTL The lifecycle of metrics data, including the metadata. Unit is day. Recommend metricsDataTTL \u0026gt;= recordDataTTL. Minimal value is 2. SW_CORE_METRICS_DATA_TTL 7   - - enableDatabaseSession Cache metrics data for 1 minute to reduce database queries, and if the OAP cluster changes within that minute. SW_CORE_ENABLE_DATABASE_SESSION true   - - topNReportPeriod The execution period of top N sampler, which saves sampled data into the storage. Unit is minute SW_CORE_TOPN_REPORT_PERIOD 10   - - activeExtraModelColumns Append the names of entity, such as service name, into the metrics storage entities. SW_CORE_ACTIVE_EXTRA_MODEL_COLUMNS false   - - serviceNameMaxLength Max length limitation of service name. SW_SERVICE_NAME_MAX_LENGTH 70   - - instanceNameMaxLength Max length limitation of service instance name. The max length of service + instance names should be less than 200. SW_INSTANCE_NAME_MAX_LENGTH 70   - - endpointNameMaxLength Max length limitation of endpoint name. The max length of service + endpoint names should be less than 240. SW_ENDPOINT_NAME_MAX_LENGTH 150   - - searchableTracesTags Define the set of span tag keys, which should be searchable through the GraphQL. Multiple values should be separated through the comma. SW_SEARCHABLE_TAG_KEYS http.method,status_code,db.type,db.instance,mq.queue,mq.topic,mq.broker   - - searchableLogsTags Define the set of log tag keys, which should be searchable through the GraphQL. Multiple values should be separated through the comma. SW_SEARCHABLE_LOGS_TAG_KEYS level   - - searchableAlarmTags Define the set of alarm tag keys, which should be searchable through the GraphQL. Multiple values should be separated through the comma. SW_SEARCHABLE_ALARM_TAG_KEYS level   - - gRPCThreadPoolSize Pool size of gRPC server SW_CORE_GRPC_THREAD_POOL_SIZE CPU core * 4   - - gRPCThreadPoolQueueSize The queue size of gRPC server SW_CORE_GRPC_POOL_QUEUE_SIZE 10000   - - maxConcurrentCallsPerConnection The maximum number of concurrent calls permitted for each incoming connection. Defaults to no limit. SW_CORE_GRPC_MAX_CONCURRENT_CALL -   - - maxMessageSize Sets the maximum message size allowed to be received on the server. Empty means 4 MiB SW_CORE_GRPC_MAX_MESSAGE_SIZE 4M(based on Netty)   - - remoteTimeout Timeout for cluster internal communication, in seconds. - 20   - - maxSizeOfNetworkAddressAlias Max size of network address detected in the be monitored system. - 1_000_000   - - maxPageSizeOfQueryProfileSnapshot The max size in every OAP query for snapshot analysis - 500   - - maxSizeOfAnalyzeProfileSnapshot The max number of snapshots analyzed by OAP - 12000   - - syncThreads The number of threads used to synchronously refresh the metrics data to the storage. SW_CORE_SYNC_THREADS 2   - - maxSyncOperationNum The maximum number of processes supported for each synchronous storage operation. When the number of the flush data is greater than this value, it will be assigned to multiple cores for execution. SW_CORE_MAX_SYNC_OPERATION_NUM 50000   cluster standalone - standalone is not suitable for one node running, no available configuration. - -   - zookeeper nameSpace The namespace, represented by root path, isolates the configurations in the zookeeper. SW_NAMESPACE /, root path   - - hostPort hosts and ports of Zookeeper Cluster SW_CLUSTER_ZK_HOST_PORT localhost:2181   - - baseSleepTimeMs The period of Zookeeper client between two retries. Unit is ms. SW_CLUSTER_ZK_SLEEP_TIME 1000   - - maxRetries The max retry time of re-trying. SW_CLUSTER_ZK_MAX_RETRIES 3   - - enableACL Open ACL by using schema and expression SW_ZK_ENABLE_ACL false   - - schema schema for the authorization SW_ZK_SCHEMA digest   - - expression expression for the authorization SW_ZK_EXPRESSION skywalking:skywalking   - - internalComHost The hostname registered in the Zookeeper for the internal communication of OAP cluster. - -   - - internalComPort The port registered in the Zookeeper for the internal communication of OAP cluster. - -1   - kubernetes namespace Namespace SkyWalking deployed in the k8s SW_CLUSTER_K8S_NAMESPACE default   - - labelSelector Labels used for filtering the OAP deployment in the k8s SW_CLUSTER_K8S_LABEL app=collector,release=skywalking   - - uidEnvName Environment variable name for reading uid. SW_CLUSTER_K8S_UID SKYWALKING_COLLECTOR_UID   - consul serviceName Service name used for SkyWalking cluster. SW_SERVICE_NAME SkyWalking_OAP_Cluster   - - hostPort hosts and ports used of Consul cluster. SW_CLUSTER_CONSUL_HOST_PORT localhost:8500   - - aclToken ALC Token of Consul. Empty string means without ALC token. SW_CLUSTER_CONSUL_ACLTOKEN -   - - internalComHost The hostname registered in the Consul for the internal communication of OAP cluster. - -   - - internalComPort The port registered in the Consul for the internal communication of OAP cluster. - -1   - etcd serviceName Service name used for SkyWalking cluster. SW_SERVICE_NAME SkyWalking_OAP_Cluster   - - hostPort hosts and ports used of etcd cluster. SW_CLUSTER_ETCD_HOST_PORT localhost:2379   - - isSSL Open SSL for the connection between SkyWalking and etcd cluster. - -   - - internalComHost The hostname registered in the etcd for the internal communication of OAP cluster. - -   - - internalComPort The port registered in the etcd for the internal communication of OAP cluster. - -1   - Nacos serviceName Service name used for SkyWalking cluster. SW_SERVICE_NAME SkyWalking_OAP_Cluster   - - hostPort hosts and ports used of Nacos cluster. SW_CLUSTER_NACOS_HOST_PORT localhost:8848   - - namespace Namespace used by SkyWalking node coordination. SW_CLUSTER_NACOS_NAMESPACE public   - - internalComHost The hostname registered in the Nacos for the internal communication of OAP cluster. - -   - - internalComPort The port registered in the Nacos for the internal communication of OAP cluster. - -1   - - username Nacos Auth username SW_CLUSTER_NACOS_USERNAME -   - - password Nacos Auth password SW_CLUSTER_NACOS_PASSWORD -   - - accessKey Nacos Auth accessKey SW_CLUSTER_NACOS_ACCESSKEY -   - - secretKey Nacos Auth secretKey SW_CLUSTER_NACOS_SECRETKEY -   storage elasticsearch - ElasticSearch 6 storage implementation - -   - - nameSpace Prefix of indexes created and used by SkyWalking. SW_NAMESPACE -   - - clusterNodes ElasticSearch cluster nodes for client connection. SW_STORAGE_ES_CLUSTER_NODES localhost   - - protocol HTTP or HTTPs. SW_STORAGE_ES_HTTP_PROTOCOL HTTP   - - user User name of ElasticSearch cluster SW_ES_USER -   - - password Password of ElasticSearch cluster SW_ES_PASSWORD -   - - trustStorePath Trust JKS file path. Only work when user name and password opened SW_STORAGE_ES_SSL_JKS_PATH -   - - trustStorePass Trust JKS file password. Only work when user name and password opened SW_STORAGE_ES_SSL_JKS_PASS -   - - secretsManagementFile Secrets management file in the properties format includes the username, password, which are managed by 3rd party tool. Provide the capability to update them in the runtime. SW_ES_SECRETS_MANAGEMENT_FILE -   - - dayStep Represent the number of days in the one minute/hour/day index. SW_STORAGE_DAY_STEP 1   - - indexShardsNumber Shard number of new indexes SW_STORAGE_ES_INDEX_SHARDS_NUMBER 1   - - indexReplicasNumber Replicas number of new indexes SW_STORAGE_ES_INDEX_REPLICAS_NUMBER 0   - - superDatasetDayStep Represent the number of days in the super size dataset record index, the default value is the same as dayStep when the value is less than 0. SW_SUPERDATASET_STORAGE_DAY_STEP -1   - - superDatasetIndexShardsFactor Super data set has been defined in the codes, such as trace segments. This factor provides more shards for the super data set, shards number = indexShardsNumber * superDatasetIndexShardsFactor. Also, this factor effects Zipkin and Jaeger traces. SW_STORAGE_ES_SUPER_DATASET_INDEX_SHARDS_FACTOR 5   - - superDatasetIndexReplicasNumber Represent the replicas number in the super size dataset record index. SW_STORAGE_ES_SUPER_DATASET_INDEX_REPLICAS_NUMBER 0   - - bulkActions Async bulk size of the record data batch execution. SW_STORAGE_ES_BULK_ACTIONS 1000   - - flushInterval Period of flush, no matter bulkActions reached or not. Unit is second. SW_STORAGE_ES_FLUSH_INTERVAL 10   - - concurrentRequests The number of concurrent requests allowed to be executed. SW_STORAGE_ES_CONCURRENT_REQUESTS 2   - - resultWindowMaxSize The max size of dataset when OAP loading cache, such as network alias. SW_STORAGE_ES_QUERY_MAX_WINDOW_SIZE 10000   - - metadataQueryMaxSize The max size of metadata per query. SW_STORAGE_ES_QUERY_MAX_SIZE 5000   - - segmentQueryMaxSize The max size of trace segments per query. SW_STORAGE_ES_QUERY_SEGMENT_SIZE 200   - - profileTaskQueryMaxSize The max size of profile task per query. SW_STORAGE_ES_QUERY_PROFILE_TASK_SIZE 200   - - advanced All settings of ElasticSearch index creation. The value should be in JSON format SW_STORAGE_ES_ADVANCED -   - elasticsearch7 - ElasticSearch 7 storage implementation - -   - - nameSpace Prefix of indexes created and used by SkyWalking. SW_NAMESPACE -   - - clusterNodes ElasticSearch cluster nodes for client connection. SW_STORAGE_ES_CLUSTER_NODES localhost   - - protocol HTTP or HTTPs. SW_STORAGE_ES_HTTP_PROTOCOL HTTP   - - user User name of ElasticSearch cluster SW_ES_USER -   - - password Password of ElasticSearch cluster SW_ES_PASSWORD -   - - trustStorePath Trust JKS file path. Only work when user name and password opened SW_STORAGE_ES_SSL_JKS_PATH -   - - trustStorePass Trust JKS file password. Only work when user name and password opened SW_STORAGE_ES_SSL_JKS_PASS -   - - secretsManagementFile Secrets management file in the properties format includes the username, password, which are managed by 3rd party tool. Provide the capability to update them in the runtime. SW_ES_SECRETS_MANAGEMENT_FILE -   - - dayStep Represent the number of days in the one minute/hour/day index. SW_STORAGE_DAY_STEP 1   - - indexShardsNumber Shard number of new indexes SW_STORAGE_ES_INDEX_SHARDS_NUMBER 1   - - indexReplicasNumber Replicas number of new indexes SW_STORAGE_ES_INDEX_REPLICAS_NUMBER 0   - - superDatasetDayStep Represent the number of days in the super size dataset record index, the default value is the same as dayStep when the value is less than 0. SW_SUPERDATASET_STORAGE_DAY_STEP -1   - - superDatasetIndexShardsFactor Super data set has been defined in the codes, such as trace segments. This factor provides more shards for the super data set, shards number = indexShardsNumber * superDatasetIndexShardsFactor. Also, this factor effects Zipkin and Jaeger traces. SW_STORAGE_ES_SUPER_DATASET_INDEX_SHARDS_FACTOR 5   - - superDatasetIndexReplicasNumber Represent the replicas number in the super size dataset record index. SW_STORAGE_ES_SUPER_DATASET_INDEX_REPLICAS_NUMBER 0   - - bulkActions Async bulk size of the record data batch execution. SW_STORAGE_ES_BULK_ACTIONS 1000   - - syncBulkActions Sync bulk size of the metrics data batch execution. SW_STORAGE_ES_SYNC_BULK_ACTIONS 50000   - - flushInterval Period of flush, no matter bulkActions reached or not. Unit is second. SW_STORAGE_ES_FLUSH_INTERVAL 10   - - concurrentRequests The number of concurrent requests allowed to be executed. SW_STORAGE_ES_CONCURRENT_REQUESTS 2   - - resultWindowMaxSize The max size of dataset when OAP loading cache, such as network alias. SW_STORAGE_ES_QUERY_MAX_WINDOW_SIZE 10000   - - metadataQueryMaxSize The max size of metadata per query. SW_STORAGE_ES_QUERY_MAX_SIZE 5000   - - segmentQueryMaxSize The max size of trace segments per query. SW_STORAGE_ES_QUERY_SEGMENT_SIZE 200   - - profileTaskQueryMaxSize The max size of profile task per query. SW_STORAGE_ES_QUERY_PROFILE_TASK_SIZE 200   - - advanced All settings of ElasticSearch index creation. The value should be in JSON format SW_STORAGE_ES_ADVANCED -   - h2 - H2 storage is designed for demonstration and running in short term(1-2 hours) only - -   - - driver H2 JDBC driver. SW_STORAGE_H2_DRIVER org.h2.jdbcx.JdbcDataSource   - - url H2 connection URL. Default is H2 memory mode SW_STORAGE_H2_URL jdbc:h2:mem:skywalking-oap-db   - - user User name of H2 database. SW_STORAGE_H2_USER sa   - - password Password of H2 database. - -   - - metadataQueryMaxSize The max size of metadata per query. SW_STORAGE_H2_QUERY_MAX_SIZE 5000   - - maxSizeOfArrayColumn Some entities, such as trace segment, include the logic column with multiple values. In the H2, we use multiple physical columns to host the values, such as, Change column_a with values [1,2,3,4,5] to column_a_0 = 1, column_a_1 = 2, column_a_2 = 3 , column_a_3 = 4, column_a_4 = 5 SW_STORAGE_MAX_SIZE_OF_ARRAY_COLUMN 20   - - numOfSearchableValuesPerTag In a trace segment, it includes multiple spans with multiple tags. Different spans could have same tag keys, such as multiple HTTP exit spans all have their own http.method tag. This configuration set the limitation of max num of values for the same tag key. SW_STORAGE_NUM_OF_SEARCHABLE_VALUES_PER_TAG 2   - mysql - MySQL Storage. The MySQL JDBC Driver is not in the dist, please copy it into oap-lib folder manually - -   - - properties Hikari connection pool configurations - Listed in the application.yaml.   - - metadataQueryMaxSize The max size of metadata per query. SW_STORAGE_MYSQL_QUERY_MAX_SIZE 5000   - - maxSizeOfArrayColumn Some entities, such as trace segment, include the logic column with multiple values. In the MySQL, we use multiple physical columns to host the values, such as, Change column_a with values [1,2,3,4,5] to column_a_0 = 1, column_a_1 = 2, column_a_2 = 3 , column_a_3 = 4, column_a_4 = 5 SW_STORAGE_MAX_SIZE_OF_ARRAY_COLUMN 20   - - numOfSearchableValuesPerTag In a trace segment, it includes multiple spans with multiple tags. Different spans could have same tag keys, such as multiple HTTP exit spans all have their own http.method tag. This configuration set the limitation of max num of values for the same tag key. SW_STORAGE_NUM_OF_SEARCHABLE_VALUES_PER_TAG 2   - postgresql - PostgreSQL storage. - -   - - properties Hikari connection pool configurations - Listed in the application.yaml.   - - metadataQueryMaxSize The max size of metadata per query. SW_STORAGE_MYSQL_QUERY_MAX_SIZE 5000   - - maxSizeOfArrayColumn Some entities, such as trace segment, include the logic column with multiple values. In the PostgreSQL, we use multiple physical columns to host the values, such as, Change column_a with values [1,2,3,4,5] to column_a_0 = 1, column_a_1 = 2, column_a_2 = 3 , column_a_3 = 4, column_a_4 = 5 SW_STORAGE_MAX_SIZE_OF_ARRAY_COLUMN 20   - - numOfSearchableValuesPerTag In a trace segment, it includes multiple spans with multiple tags. Different spans could have same tag keys, such as multiple HTTP exit spans all have their own http.method tag. This configuration set the limitation of max num of values for the same tag key. SW_STORAGE_NUM_OF_SEARCHABLE_VALUES_PER_TAG 2   - influxdb - InfluxDB storage. - -   - - url InfluxDB connection URL. SW_STORAGE_INFLUXDB_URL http://localhost:8086   - - user User name of InfluxDB. SW_STORAGE_INFLUXDB_USER root   - - password Password of InfluxDB. SW_STORAGE_INFLUXDB_PASSWORD -   - - database Database of InfluxDB. SW_STORAGE_INFLUXDB_DATABASE skywalking   - - actions The number of actions to collect. SW_STORAGE_INFLUXDB_ACTIONS 1000   - - duration The time to wait at most (milliseconds). SW_STORAGE_INFLUXDB_DURATION 1000   - - batchEnabled If true, write points with batch api. SW_STORAGE_INFLUXDB_BATCH_ENABLED true   - - fetchTaskLogMaxSize The max number of fetch task log in a request. SW_STORAGE_INFLUXDB_FETCH_TASK_LOG_MAX_SIZE 5000   - - connectionResponseFormat The response format of connection to influxDB, cannot be anything but MSGPACK or JSON. SW_STORAGE_INFLUXDB_CONNECTION_RESPONSE_FORMAT MSGPACK   agent-analyzer default Agent Analyzer. SW_AGENT_ANALYZER default    - - sampleRate Sampling rate for receiving trace. The precision is 1/10000. 10000 means 100% sample in default. SW_TRACE_SAMPLE_RATE 10000   - - slowDBAccessThreshold The slow database access thresholds. Unit ms. SW_SLOW_DB_THRESHOLD default:200,mongodb:100   - - forceSampleErrorSegment When sampling mechanism activated, this config would make the error status segment sampled, ignoring the sampling rate. SW_FORCE_SAMPLE_ERROR_SEGMENT true   - - segmentStatusAnalysisStrategy Determine the final segment status from the status of spans. Available values are FROM_SPAN_STATUS , FROM_ENTRY_SPAN and FROM_FIRST_SPAN. FROM_SPAN_STATUS represents the segment status would be error if any span is in error status. FROM_ENTRY_SPAN means the segment status would be determined by the status of entry spans only. FROM_FIRST_SPAN means the segment status would be determined by the status of the first span only. SW_SEGMENT_STATUS_ANALYSIS_STRATEGY FROM_SPAN_STATUS   - - noUpstreamRealAddressAgents Exit spans with the component in the list would not generate the client-side instance relation metrics. As some tracing plugins can\u0026rsquo;t collect the real peer ip address, such as Nginx-LUA and Envoy. SW_NO_UPSTREAM_REAL_ADDRESS 6000,9000   - - slowTraceSegmentThreshold Setting this threshold about the latency would make the slow trace segments sampled if they cost more time, even the sampling mechanism activated. The default value is -1, which means would not sample slow traces. Unit, millisecond. SW_SLOW_TRACE_SEGMENT_THRESHOLD -1   - - meterAnalyzerActiveFiles Which files could be meter analyzed, files split by \u0026ldquo;,\u0026rdquo; SW_METER_ANALYZER_ACTIVE_FILES    receiver-sharing-server default Sharing server provides new gRPC and restful servers for data collection. Ana make the servers in the core module working for internal communication only. - -    - - restHost Binding IP of restful service. Services include GraphQL query and HTTP data report SW_RECEIVER_SHARING_REST_HOST -   - - restPort Binding port of restful service SW_RECEIVER_SHARING_REST_PORT -   - - restContextPath Web context path of restful service SW_RECEIVER_SHARING_REST_CONTEXT_PATH -   - - restMinThreads Min threads number of restful service SW_RECEIVER_SHARING_JETTY_MIN_THREADS 1   - - restMaxThreads Max threads number of restful service SW_RECEIVER_SHARING_JETTY_MAX_THREADS 200   - - restIdleTimeOut Connector idle timeout in milliseconds of restful service SW_RECEIVER_SHARING_JETTY_IDLE_TIMEOUT 30000   - - restAcceptorPriorityDelta Thread priority delta to give to acceptor threads of restful service SW_RECEIVER_SHARING_JETTY_DELTA 0   - - restAcceptQueueSize ServerSocketChannel backlog of restful service SW_RECEIVER_SHARING_JETTY_QUEUE_SIZE 0   - - gRPCHost Binding IP of gRPC service. Services include gRPC data report and internal communication among OAP nodes SW_RECEIVER_GRPC_HOST 0.0.0.0. Not Activated   - - gRPCPort Binding port of gRPC service SW_RECEIVER_GRPC_PORT Not Activated   - - gRPCThreadPoolSize Pool size of gRPC server SW_RECEIVER_GRPC_THREAD_POOL_SIZE CPU core * 4   - - gRPCThreadPoolQueueSize The queue size of gRPC server SW_RECEIVER_GRPC_POOL_QUEUE_SIZE 10000   - - gRPCSslEnabled Activate SSL for gRPC service SW_RECEIVER_GRPC_SSL_ENABLED false   - - gRPCSslKeyPath The file path of gRPC SSL key SW_RECEIVER_GRPC_SSL_KEY_PATH -   - - gRPCSslCertChainPath The file path of gRPC SSL cert chain SW_RECEIVER_GRPC_SSL_CERT_CHAIN_PATH -   - - maxConcurrentCallsPerConnection The maximum number of concurrent calls permitted for each incoming connection. Defaults to no limit. SW_RECEIVER_GRPC_MAX_CONCURRENT_CALL -   - - authentication The token text for the authentication. Work for gRPC connection only. Once this is set, the client is required to use the same token. SW_AUTHENTICATION -   log-analyzer default Log Analyzer. SW_LOG_ANALYZER default    - - lalFiles The LAL configuration file names (without file extension) to be activated. Read LAL for more details. SW_LOG_LAL_FILES default   - - malFiles The MAL configuration file names (without file extension) to be activated. Read LAL for more details. SW_LOG_MAL_FILES \u0026quot;\u0026quot;   event-analyzer default Event Analyzer. SW_EVENT_ANALYZER default    receiver-register default Read receiver doc for more details - -    receiver-trace default Read receiver doc for more details - -    receiver-jvm default Read receiver doc for more details - -    receiver-clr default Read receiver doc for more details - -    receiver-profile default Read receiver doc for more details - -    receiver-zabbix default Read receiver doc for more details - -    - - port Exported tcp port, Zabbix agent could connect and transport data SW_RECEIVER_ZABBIX_PORT 10051   - - host Bind to host SW_RECEIVER_ZABBIX_HOST 0.0.0.0   - - activeFiles Enable config when receive agent request SW_RECEIVER_ZABBIX_ACTIVE_FILES agent   service-mesh default Read receiver doc for more details - -    envoy-metric default Read receiver doc for more details - -    - - acceptMetricsService Open Envoy Metrics Service analysis SW_ENVOY_METRIC_SERVICE true   - - alsHTTPAnalysis Open Envoy HTTP Access Log Service analysis. Value = k8s-mesh means open the analysis SW_ENVOY_METRIC_ALS_HTTP_ANALYSIS -   - - alsTCPAnalysis Open Envoy TCP Access Log Service analysis. Value = k8s-mesh means open the analysis SW_ENVOY_METRIC_ALS_TCP_ANALYSIS -   - - k8sServiceNameRule k8sServiceNameRule allows you to customize the service name in ALS via Kubernetes metadata, the available variables are pod, service, e.g., you can use ${service.metadata.name}-${pod.metadata.labels.version} to append the version number to the service name. Be careful, when using environment variables to pass this configuration, use single quotes('') to avoid it being evaluated by the shell. -    receiver-otel default Read receiver doc for more details - -    - - enabledHandlers Enabled handlers for otel SW_OTEL_RECEIVER_ENABLED_HANDLERS -   - - enabledOcRules Enabled metric rules for OC handler SW_OTEL_RECEIVER_ENABLED_OC_RULES -   receiver_zipkin default Read receiver doc - -    - - restHost Binding IP of restful service. SW_RECEIVER_ZIPKIN_HOST 0.0.0.0   - - restPort Binding port of restful service SW_RECEIVER_ZIPKIN_PORT 9411   - - restContextPath Web context path of restful service SW_RECEIVER_ZIPKIN_CONTEXT_PATH /   receiver_jaeger default Read receiver doc - -    - - gRPCHost Binding IP of gRPC service. Services include gRPC data report and internal communication among OAP nodes SW_RECEIVER_JAEGER_HOST -   - - gRPCPort Binding port of gRPC service SW_RECEIVER_JAEGER_PORT -   - - gRPCThreadPoolSize Pool size of gRPC server - CPU core * 4   - - gRPCThreadPoolQueueSize The queue size of gRPC server - 10000   - - maxConcurrentCallsPerConnection The maximum number of concurrent calls permitted for each incoming connection. Defaults to no limit. - -   - - maxMessageSize Sets the maximum message size allowed to be received on the server. Empty means 4 MiB - 4M(based on Netty)   prometheus-fetcher default Read fetcher doc for more details - -    - - enabledRules Enable rules. SW_PROMETHEUS_FETCHER_ENABLED_RULES self   - - maxConvertWorker The maximize meter convert worker. SW_PROMETHEUS_FETCHER_NUM_CONVERT_WORKER -1(by default, half the number of CPU core(s))   kafka-fetcher default Read fetcher doc for more details - -    - - bootstrapServers A list of host/port pairs to use for establishing the initial connection to the Kafka cluster. SW_KAFKA_FETCHER_SERVERS localhost:9092   - - namespace namespace aims to isolate multi OAP cluster when using the same Kafka cluster.if you set a namespace for Kafka fetcher, OAP will add a prefix to topic name. you should also set namespace in agent.config, the property named SW_NAMESPACE -   - - groupId A unique string that identifies the consumer group this consumer belongs to. - skywalking-consumer   - - consumePartitions Which PartitionId(s) of the topics assign to the OAP server. If more than one, is separated by commas. SW_KAFKA_FETCHER_CONSUME_PARTITIONS -   - - isSharding it was true when OAP Server in cluster. SW_KAFKA_FETCHER_IS_SHARDING false   - - createTopicIfNotExist If true, create the Kafka topic when it does not exist. - true   - - partitions The number of partitions for the topic being created. SW_KAFKA_FETCHER_PARTITIONS 3   - - enableMeterSystem To enable to fetch and handle Meter System data. SW_KAFKA_FETCHER_ENABLE_METER_SYSTEM false   - - enableNativeProtoLog To enable to fetch and handle native proto log data. SW_KAFKA_FETCHER_ENABLE_NATIVE_PROTO_LOG false   - - enableNativeJsonLog To enable to fetch and handle native json log data. SW_KAFKA_FETCHER_ENABLE_NATIVE_JSON_LOG false   - - replicationFactor The replication factor for each partition in the topic being created. SW_KAFKA_FETCHER_PARTITIONS_FACTOR 2   - - kafkaHandlerThreadPoolSize Pool size of kafka message handler executor. SW_KAFKA_HANDLER_THREAD_POOL_SIZE CPU core * 2   - - kafkaHandlerThreadPoolQueueSize The queue size of kafka message handler executor. SW_KAFKA_HANDLER_THREAD_POOL_QUEUE_SIZE 10000   - - topicNameOfMeters Specifying Kafka topic name for Meter system data. - skywalking-meters   - - topicNameOfMetrics Specifying Kafka topic name for JVM Metrics data. - skywalking-metrics   - - topicNameOfProfiling Specifying Kafka topic name for Profiling data. - skywalking-profilings   - - topicNameOfTracingSegments Specifying Kafka topic name for Tracing data. - skywalking-segments   - - topicNameOfManagements Specifying Kafka topic name for service instance reporting and registering. - skywalking-managements   - - topicNameOfLogs Specifying Kafka topic name for native proto log data. - skywalking-logs   - - topicNameOfJsonLogs Specifying Kafka topic name for native json log data. - skywalking-logs-json   receiver-browser default Read receiver doc for more details - - -   - - sampleRate Sampling rate for receiving trace. The precision is 1/10000. 10000 means 100% sample in default. SW_RECEIVER_BROWSER_SAMPLE_RATE 10000   query graphql - GraphQL query implementation -    - - path Root path of GraphQL query and mutation. SW_QUERY_GRAPHQL_PATH /graphql   alarm default - Read alarm doc for more details. -    telemetry - - Read telemetry doc for more details. -    - none - No op implementation -    - prometheus host Binding host for Prometheus server fetching data SW_TELEMETRY_PROMETHEUS_HOST 0.0.0.0   - - port Binding port for Prometheus server fetching data SW_TELEMETRY_PROMETHEUS_PORT 1234   configuration - - Read dynamic configuration doc for more details. -    - grpc host DCS server binding hostname SW_DCS_SERVER_HOST -   - - port DCS server binding port SW_DCS_SERVER_PORT 80   - - clusterName Cluster name when reading latest configuration from DSC server. SW_DCS_CLUSTER_NAME SkyWalking   - - period The period of OAP reading data from DSC server. Unit is second. SW_DCS_PERIOD 20   - apollo apolloMeta apollo.meta in Apollo SW_CONFIG_APOLLO http://106.12.25.204:8080   - - apolloCluster apollo.cluster in Apollo SW_CONFIG_APOLLO_CLUSTER default   - - apolloEnv env in Apollo SW_CONFIG_APOLLO_ENV -   - - appId app.id in Apollo SW_CONFIG_APOLLO_APP_ID skywalking   - - period The period of data sync. Unit is second. SW_CONFIG_APOLLO_PERIOD 60   - zookeeper nameSpace The namespace, represented by root path, isolates the configurations in the zookeeper. SW_CONFIG_ZK_NAMESPACE /, root path   - - hostPort hosts and ports of Zookeeper Cluster SW_CONFIG_ZK_HOST_PORT localhost:2181   - - baseSleepTimeMs The period of Zookeeper client between two retries. Unit is ms. SW_CONFIG_ZK_BASE_SLEEP_TIME_MS 1000   - - maxRetries The max retry time of re-trying. SW_CONFIG_ZK_MAX_RETRIES 3   - - period The period of data sync. Unit is second. SW_CONFIG_ZK_PERIOD 60   - etcd clusterName Service name used for SkyWalking cluster. SW_CONFIG_ETCD_CLUSTER_NAME default   - - serverAddr hosts and ports used of etcd cluster. SW_CONFIG_ETCD_SERVER_ADDR localhost:2379   - - group Additional prefix of the configuration key SW_CONFIG_ETCD_GROUP skywalking   - - period The period of data sync. Unit is second. SW_CONFIG_ZK_PERIOD 60   - consul hostPort hosts and ports used of Consul cluster. SW_CONFIG_CONSUL_HOST_AND_PORTS localhost:8500   - - aclToken ALC Token of Consul. Empty string means without ALC token. SW_CONFIG_CONSUL_ACL_TOKEN -   - - period The period of data sync. Unit is second. SW_CONFIG_CONSUL_PERIOD 60   - k8s-configmap namespace Deployment namespace of the config map. SW_CLUSTER_K8S_NAMESPACE default   - - labelSelector Labels used for locating configmap. SW_CLUSTER_K8S_LABEL app=collector,release=skywalking   - - period The period of data sync. Unit is second. SW_CONFIG_ZK_PERIOD 60   - nacos serverAddr Nacos Server Host SW_CONFIG_NACOS_SERVER_ADDR 127.0.0.1   - - port Nacos Server Port SW_CONFIG_NACOS_SERVER_PORT 8848   - - group Nacos Configuration namespace SW_CONFIG_NACOS_SERVER_NAMESPACE -   - - period The period of data sync. Unit is second. SW_CONFIG_CONFIG_NACOS_PERIOD 60   - - username Nacos Auth username SW_CONFIG_NACOS_USERNAME -   - - password Nacos Auth password SW_CONFIG_NACOS_PASSWORD -   - - accessKey Nacos Auth accessKey SW_CONFIG_NACOS_ACCESSKEY -   - - secretKey Nacos Auth secretKey SW_CONFIG_NACOS_SECRETKEY -   exporter grpc targetHost The host of target grpc server for receiving export data. SW_EXPORTER_GRPC_HOST 127.0.0.1   - - targetPort The port of target grpc server for receiving export data. SW_EXPORTER_GRPC_PORT 9870   health-checker default checkIntervalSeconds The period of check OAP internal health status. Unit is second. SW_HEALTH_CHECKER_INTERVAL_SECONDS 5   configuration-discovery default disableMessageDigest If true, agent receives the latest configuration every time even without change. In default, OAP uses SHA512 message digest mechanism to detect changes of configuration. SW_DISABLE_MESSAGE_DIGEST false   receiver-event default Read receiver doc for more details - -     Notice ¹ System Environment Variable name could be declared and changed in the application.yml. The names listed here, are just provided in the default application.yml file.\n","excerpt":"Configuration Vocabulary Configuration Vocabulary lists all available configurations provided by …","ref":"/docs/main/v8.6.0/en/setup/backend/configuration-vocabulary/","title":"Configuration Vocabulary"},{"body":" Dependency the toolkit, such as using maven or gradle  \u0026lt;dependency\u0026gt; \u0026lt;groupId\u0026gt;org.apache.skywalking\u0026lt;/groupId\u0026gt; \u0026lt;artifactId\u0026gt;apm-toolkit-log4j-1.x\u0026lt;/artifactId\u0026gt; \u0026lt;version\u0026gt;{project.release.version}\u0026lt;/version\u0026gt; \u0026lt;/dependency\u0026gt; Print trace ID in your logs  Config a layout  log4j.appender.CONSOLE.layout=org.apache.skywalking.apm.toolkit.log.log4j.v1.x.TraceIdPatternLayout  set %T in layout.ConversionPattern ( In 2.0-2016, you should use %x, Why change? )  log4j.appender.CONSOLE.layout.ConversionPattern=%d [%T] %-5p %c{1}:%L - %m%n  When you use -javaagent to active the SkyWalking tracer, log4j will output traceId, if it existed. If the tracer is inactive, the output will be TID: N/A.  Print SkyWalking context in your logs   Your only need to replace pattern %T with %T{SW_CTX}.\n  When you use -javaagent to active the SkyWalking tracer, log4j will output SW_CTX: [$serviceName,$instanceName,$traceId,$traceSegmentId,$spanId], if it existed. If the tracer is inactive, the output will be SW_CTX: N/A.\n  gRPC reporter The gRPC report could forward the collected logs to SkyWalking OAP server, or SkyWalking Satellite sidecar. Trace id, segment id, and span id will attach to logs automatically. You don\u0026rsquo;t need to change the layout.\n Add GRPCLogClientAppender in log4j.properties  log4j.rootLogger=INFO,CustomAppender log4j.appender.CustomAppender=org.apache.skywalking.apm.toolkit.log.log4j.v1.x.log.GRPCLogClientAppender log4j.appender.CustomAppender.layout=org.apache.log4j.PatternLayout log4j.appender.CustomAppender.layout.ConversionPattern=[%t] %-5p %c %x - %m%n  Add config of the plugin or use default  plugin.toolkit.log.grpc.reporter.server_host=${SW_GRPC_LOG_SERVER_HOST:127.0.0.1} plugin.toolkit.log.grpc.reporter.server_port=${SW_GRPC_LOG_SERVER_PORT:11800} plugin.toolkit.log.grpc.reporter.max_message_size=${SW_GRPC_LOG_MAX_MESSAGE_SIZE:10485760} plugin.toolkit.log.grpc.reporter.upstream_timeout=${SW_GRPC_LOG_GRPC_UPSTREAM_TIMEOUT:30} ","excerpt":"Dependency the toolkit, such as using maven or gradle  \u0026lt;dependency\u0026gt; …","ref":"/docs/main/v8.6.0/en/setup/service-agent/java-agent/application-toolkit-log4j-1.x/","title":"Dependency the toolkit, such as using maven or gradle"},{"body":" Dependency the toolkit, such as using maven or gradle  \u0026lt;dependency\u0026gt; \u0026lt;groupId\u0026gt;org.apache.skywalking\u0026lt;/groupId\u0026gt; \u0026lt;artifactId\u0026gt;apm-toolkit-log4j-2.x\u0026lt;/artifactId\u0026gt; \u0026lt;version\u0026gt;{project.release.version}\u0026lt;/version\u0026gt; \u0026lt;/dependency\u0026gt; Print trace ID in your logs  Config the [%traceId] pattern in your log4j2.xml  \u0026lt;Appenders\u0026gt; \u0026lt;Console name=\u0026#34;Console\u0026#34; target=\u0026#34;SYSTEM_OUT\u0026#34;\u0026gt; \u0026lt;PatternLayout pattern=\u0026#34;%d [%traceId] %-5p %c{1}:%L - %m%n\u0026#34;/\u0026gt; \u0026lt;/Console\u0026gt; \u0026lt;/Appenders\u0026gt;  Support log4j2 AsyncRoot , No additional configuration is required. Refer to the demo of log4j2.xml below. For details: Log4j2 Async Loggers  \u0026lt;Configuration\u0026gt; \u0026lt;Appenders\u0026gt; \u0026lt;Console name=\u0026#34;Console\u0026#34; target=\u0026#34;SYSTEM_OUT\u0026#34;\u0026gt; \u0026lt;PatternLayout pattern=\u0026#34;%d [%traceId] %-5p %c{1}:%L - %m%n\u0026#34;/\u0026gt; \u0026lt;/Console\u0026gt; \u0026lt;/Appenders\u0026gt; \u0026lt;Loggers\u0026gt; \u0026lt;AsyncRoot level=\u0026#34;INFO\u0026#34;\u0026gt; \u0026lt;AppenderRef ref=\u0026#34;Console\u0026#34;/\u0026gt; \u0026lt;/AsyncRoot\u0026gt; \u0026lt;/Loggers\u0026gt; \u0026lt;/Configuration\u0026gt;   Support log4j2 AsyncAppender , No additional configuration is required. Refer to the demo of log4j2.xml below.\nFor details: All Loggers Async\nLog4j-2.9 and higher require disruptor-3.3.4.jar or higher on the classpath. Prior to Log4j-2.9, disruptor-3.0.0.jar or higher was required. This is simplest to configure and gives the best performance. To make all loggers asynchronous, add the disruptor jar to the classpath and set the system property log4j2.contextSelector to org.apache.logging.log4j.core.async.AsyncLoggerContextSelector.\n\u0026lt;Configuration status=\u0026#34;WARN\u0026#34;\u0026gt; \u0026lt;Appenders\u0026gt; \u0026lt;!-- Async Loggers will auto-flush in batches, so switch off immediateFlush. --\u0026gt; \u0026lt;RandomAccessFile name=\u0026#34;RandomAccessFile\u0026#34; fileName=\u0026#34;async.log\u0026#34; immediateFlush=\u0026#34;false\u0026#34; append=\u0026#34;false\u0026#34;\u0026gt; \u0026lt;PatternLayout\u0026gt; \u0026lt;Pattern\u0026gt;%d %p %c{1.} [%t] [%traceId] %m %ex%n\u0026lt;/Pattern\u0026gt; \u0026lt;/PatternLayout\u0026gt; \u0026lt;/RandomAccessFile\u0026gt; \u0026lt;/Appenders\u0026gt; \u0026lt;Loggers\u0026gt; \u0026lt;Root level=\u0026#34;info\u0026#34; includeLocation=\u0026#34;false\u0026#34;\u0026gt; \u0026lt;AppenderRef ref=\u0026#34;RandomAccessFile\u0026#34;/\u0026gt; \u0026lt;/Root\u0026gt; \u0026lt;/Loggers\u0026gt; \u0026lt;/Configuration\u0026gt; For details: Mixed Sync \u0026amp; Async\nLog4j-2.9 and higher require disruptor-3.3.4.jar or higher on the classpath. Prior to Log4j-2.9, disruptor-3.0.0.jar or higher was required. There is no need to set system property Log4jContextSelector to any value.\n\u0026lt;Configuration status=\u0026#34;WARN\u0026#34;\u0026gt; \u0026lt;Appenders\u0026gt; \u0026lt;!-- Async Loggers will auto-flush in batches, so switch off immediateFlush. --\u0026gt; \u0026lt;RandomAccessFile name=\u0026#34;RandomAccessFile\u0026#34; fileName=\u0026#34;asyncWithLocation.log\u0026#34; immediateFlush=\u0026#34;false\u0026#34; append=\u0026#34;false\u0026#34;\u0026gt; \u0026lt;PatternLayout\u0026gt; \u0026lt;Pattern\u0026gt;%d %p %class{1.} [%t] [%traceId] %location %m %ex%n\u0026lt;/Pattern\u0026gt; \u0026lt;/PatternLayout\u0026gt; \u0026lt;/RandomAccessFile\u0026gt; \u0026lt;/Appenders\u0026gt; \u0026lt;Loggers\u0026gt; \u0026lt;!-- pattern layout actually uses location, so we need to include it --\u0026gt; \u0026lt;AsyncLogger name=\u0026#34;com.foo.Bar\u0026#34; level=\u0026#34;trace\u0026#34; includeLocation=\u0026#34;true\u0026#34;\u0026gt; \u0026lt;AppenderRef ref=\u0026#34;RandomAccessFile\u0026#34;/\u0026gt; \u0026lt;/AsyncLogger\u0026gt; \u0026lt;Root level=\u0026#34;info\u0026#34; includeLocation=\u0026#34;true\u0026#34;\u0026gt; \u0026lt;AppenderRef ref=\u0026#34;RandomAccessFile\u0026#34;/\u0026gt; \u0026lt;/Root\u0026gt; \u0026lt;/Loggers\u0026gt; \u0026lt;/Configuration\u0026gt;   Support log4j2 AsyncAppender, For details: Log4j2 AsyncAppender\n  \u0026lt;Configuration\u0026gt; \u0026lt;Appenders\u0026gt; \u0026lt;Console name=\u0026#34;Console\u0026#34; target=\u0026#34;SYSTEM_OUT\u0026#34;\u0026gt; \u0026lt;PatternLayout pattern=\u0026#34;%d [%traceId] %-5p %c{1}:%L - %m%n\u0026#34;/\u0026gt; \u0026lt;/Console\u0026gt; \u0026lt;Async name=\u0026#34;Async\u0026#34;\u0026gt; \u0026lt;AppenderRef ref=\u0026#34;Console\u0026#34;/\u0026gt; \u0026lt;/Async\u0026gt; \u0026lt;/Appenders\u0026gt; \u0026lt;Loggers\u0026gt; \u0026lt;Root level=\u0026#34;INFO\u0026#34;\u0026gt; \u0026lt;AppenderRef ref=\u0026#34;Async\u0026#34;/\u0026gt; \u0026lt;/Root\u0026gt; \u0026lt;/Loggers\u0026gt; \u0026lt;/Configuration\u0026gt;  When you use -javaagent to active the SkyWalking tracer, log4j2 will output traceId, if it existed. If the tracer is inactive, the output will be TID: N/A.  Print SkyWalking context in your logs   Your only need to replace pattern %traceId with %sw_ctx.\n  When you use -javaagent to active the SkyWalking tracer, log4j2 will output SW_CTX: [$serviceName,$instanceName,$traceId,$traceSegmentId,$spanId], if it existed. If the tracer is inactive, the output will be SW_CTX: N/A.\n  gRPC reporter The gRPC report could forward the collected logs to SkyWalking OAP server, or SkyWalking Satellite sidecar. Trace id, segment id, and span id will attach to logs automatically. You don\u0026rsquo;t need to change the layout.\n Add GRPCLogClientAppender in log4j2.xml  \u0026lt;GRPCLogClientAppender name=\u0026#34;grpc-log\u0026#34;\u0026gt; \u0026lt;PatternLayout pattern=\u0026#34;%d{HH:mm:ss.SSS} [%t] %-5level %logger{36} - %msg%n\u0026#34;/\u0026gt; \u0026lt;/GRPCLogClientAppender\u0026gt;  Add config of the plugin or use default  plugin.toolkit.log.grpc.reporter.server_host=${SW_GRPC_LOG_SERVER_HOST:127.0.0.1} plugin.toolkit.log.grpc.reporter.server_port=${SW_GRPC_LOG_SERVER_PORT:11800} plugin.toolkit.log.grpc.reporter.max_message_size=${SW_GRPC_LOG_MAX_MESSAGE_SIZE:10485760} plugin.toolkit.log.grpc.reporter.upstream_timeout=${SW_GRPC_LOG_GRPC_UPSTREAM_TIMEOUT:30} Transmitting un-formatted messages The log4j 2.x gRPC reporter supports transmitting logs as formatted or un-formatted. Transmitting formatted data is the default but can be disabled by adding the following to the agent config:\nplugin.toolkit.log.transmit_formatted=false The above will result in the content field being used for the log pattern with additional log tags of argument.0, argument.1, and so on representing each logged argument as well as an additional exception tag which is only present if a throwable is also logged.\nFor example, the following code:\nlog.info(\u0026#34;{} {} {}\u0026#34;, 1, 2, 3); Will result in:\n{ \u0026#34;content\u0026#34;: \u0026#34;{} {} {}\u0026#34;, \u0026#34;tags\u0026#34;: [ { \u0026#34;key\u0026#34;: \u0026#34;argument.0\u0026#34;, \u0026#34;value\u0026#34;: \u0026#34;1\u0026#34; }, { \u0026#34;key\u0026#34;: \u0026#34;argument.1\u0026#34;, \u0026#34;value\u0026#34;: \u0026#34;2\u0026#34; }, { \u0026#34;key\u0026#34;: \u0026#34;argument.2\u0026#34;, \u0026#34;value\u0026#34;: \u0026#34;3\u0026#34; } ] } ","excerpt":"Dependency the toolkit, such as using maven or gradle  \u0026lt;dependency\u0026gt; …","ref":"/docs/main/v8.6.0/en/setup/service-agent/java-agent/application-toolkit-log4j-2.x/","title":"Dependency the toolkit, such as using maven or gradle"},{"body":" Dependency the toolkit, such as using maven or gradle  \u0026lt;dependency\u0026gt; \u0026lt;groupId\u0026gt;org.apache.skywalking\u0026lt;/groupId\u0026gt; \u0026lt;artifactId\u0026gt;apm-toolkit-meter\u0026lt;/artifactId\u0026gt; \u0026lt;version\u0026gt;${skywalking.version}\u0026lt;/version\u0026gt; \u0026lt;/dependency\u0026gt; If you\u0026rsquo;re using Spring sleuth, you could use Spring Sleuth Setup.\n Counter API represents a single monotonically increasing counter, automatic collect data and report to backend.  import org.apache.skywalking.apm.toolkit.meter.MeterFactory; Counter counter = MeterFactory.counter(meterName).tag(\u0026#34;tagKey\u0026#34;, \u0026#34;tagValue\u0026#34;).mode(Counter.Mode.INCREMENT).build(); counter.increment(1d);  MeterFactory.counter Create a new counter builder with the meter name. Counter.Builder.tag(String key, String value) Mark a tag key/value pair. Counter.Builder.mode(Counter.Mode mode) Change the counter mode, RATE mode means reporting rate to the backend. Counter.Builder.build() Build a new Counter which is collected and reported to the backend. Counter.increment(double count) Increment count to the Counter, It could be a positive value.   Gauge API represents a single numerical value.  import org.apache.skywalking.apm.toolkit.meter.MeterFactory; ThreadPoolExecutor threadPool = ...; Gauge gauge = MeterFactory.gauge(meterName, () -\u0026gt; threadPool.getActiveCount()).tag(\u0026#34;tagKey\u0026#34;, \u0026#34;tagValue\u0026#34;).build();  MeterFactory.gauge(String name, Supplier\u0026lt;Double\u0026gt; getter) Create a new gauge builder with the meter name and supplier function, this function need to return a double value. Gauge.Builder.tag(String key, String value) Mark a tag key/value pair. Gauge.Builder.build() Build a new Gauge which is collected and reported to the backend.   Histogram API represents a summary sample observations with customize buckets.  import org.apache.skywalking.apm.toolkit.meter.MeterFactory; Histogram histogram = MeterFactory.histogram(\u0026#34;test\u0026#34;).tag(\u0026#34;tagKey\u0026#34;, \u0026#34;tagValue\u0026#34;).steps(Arrays.asList(1, 5, 10)).minValue(0).build(); histogram.addValue(3);  MeterFactory.histogram(String name) Create a new histogram builder with the meter name. Histogram.Builder.tag(String key, String value) Mark a tag key/value pair. Histogram.Builder.steps(List\u0026lt;Double\u0026gt; steps) Set up the max values of every histogram buckets. Histogram.Builder.minValue(double value) Set up the minimal value of this histogram, default is 0. Histogram.Builder.build() Build a new Histogram which is collected and reported to the backend. Histogram.addValue(double value) Add value into the histogram, automatically analyze what bucket count needs to be increment. rule: count into [step1, step2).  ","excerpt":"Dependency the toolkit, such as using maven or gradle  \u0026lt;dependency\u0026gt; …","ref":"/docs/main/v8.6.0/en/setup/service-agent/java-agent/application-toolkit-meter/","title":"Dependency the toolkit, such as using maven or gradle"},{"body":" Dependency the toolkit, such as using maven or gradle  \u0026lt;dependency\u0026gt; \u0026lt;groupId\u0026gt;org.apache.skywalking\u0026lt;/groupId\u0026gt; \u0026lt;artifactId\u0026gt;apm-toolkit-micrometer-registry\u0026lt;/artifactId\u0026gt; \u0026lt;version\u0026gt;${skywalking.version}\u0026lt;/version\u0026gt; \u0026lt;/dependency\u0026gt;  Using org.apache.skywalking.apm.meter.micrometer.SkywalkingMeterRegistry as the registry, it could forward the MicroMeter collected metrics to OAP server.  import org.apache.skywalking.apm.meter.micrometer.SkywalkingMeterRegistry; SkywalkingMeterRegistry registry = new SkywalkingMeterRegistry(); // If you has some counter want to rate by agent side SkywalkingConfig config = new SkywalkingConfig(Arrays.asList(\u0026#34;test_rate_counter\u0026#34;)); new SkywalkingMeterRegistry(config); // Also you could using composite registry to combine multiple meter registry, such as collect to Skywalking and prometheus CompositeMeterRegistry compositeRegistry = new CompositeMeterRegistry(); compositeRegistry.add(new PrometheusMeterRegistry(PrometheusConfig.DEFAULT)); compositeRegistry.add(new SkywalkingMeterRegistry());   Using snake case as the naming convention. Such as test.meter will be send to test_meter.\n  Using Millisecond as the time unit.\n  Adapt micrometer data convention.\n     Micrometer data type Transform to meter name Skywalking data type Description     Counter Counter name Counter Same with counter   Gauges Gauges name Gauges Same with gauges   Timer Timer name + \u0026ldquo;_count\u0026rdquo; Counter Execute finished count    Timer name + \u0026ldquo;_sum\u0026rdquo; Counter Total execute finished duration    Timer name + \u0026ldquo;_max\u0026rdquo; Gauges Max duration of execute finished time    Timer name + \u0026ldquo;_histogram\u0026rdquo; Histogram Histogram of execute finished duration   LongTaskTimer Timer name + \u0026ldquo;_active_count\u0026rdquo; Gauges Executing task count    Timer name + \u0026ldquo;_duration_sum\u0026rdquo; Counter All of executing task sum duration    Timer name + \u0026ldquo;_max\u0026rdquo; Counter Current longest running task execute duration   Function Timer Timer name + \u0026ldquo;_count\u0026rdquo; Gauges Execute finished timer count    Timer name + \u0026ldquo;_sum\u0026rdquo; Gauges Execute finished timer total duration   Function Counter Counter name Counter Custom counter value   Distribution summary Summary name + \u0026ldquo;_count\u0026rdquo; Counter Total record count    Summary name + \u0026ldquo;_sum\u0026rdquo; Counter Total record amount sum    Summary name + \u0026ldquo;_max\u0026rdquo; Gauges Max record amount    Summary name + \u0026ldquo;_histogram\u0026rdquo; Gauges Histogram of the amount     Not Adapt data convention.     Micrometer data type Data type     LongTaskTimer Histogram    ","excerpt":"Dependency the toolkit, such as using maven or gradle  \u0026lt;dependency\u0026gt; …","ref":"/docs/main/v8.6.0/en/setup/service-agent/java-agent/application-toolkit-micrometer/","title":"Dependency the toolkit, such as using maven or gradle"},{"body":" Dependency the toolkit, such as using maven or gradle  \u0026lt;dependency\u0026gt; \u0026lt;groupId\u0026gt;org.apache.skywalking\u0026lt;/groupId\u0026gt; \u0026lt;artifactId\u0026gt;apm-toolkit-trace\u0026lt;/artifactId\u0026gt; \u0026lt;version\u0026gt;${skywalking.version}\u0026lt;/version\u0026gt; \u0026lt;/dependency\u0026gt;  Use TraceContext.traceId() API to obtain traceId.  import TraceContext; ... modelAndView.addObject(\u0026#34;traceId\u0026#34;, TraceContext.traceId());  Use TraceContext.segmentId() API to obtain segmentId.  import TraceContext; ... modelAndView.addObject(\u0026#34;segmentId\u0026#34;, TraceContext.segmentId());  Use TraceContext.spanId() API to obtain spanId.  import TraceContext; ... modelAndView.addObject(\u0026#34;spanId\u0026#34;, TraceContext.spanId()); Sample codes only\n  Add @Trace to any method you want to trace. After that, you can see the span in the Stack.\n  Methods annotated with @Tag will try to tag the current active span with the given key (Tag#key()) and (Tag#value()), if there is no active span at all, this annotation takes no effect. @Tag can be repeated, and can be used in companion with @Trace, see examples below. The value of Tag is the same as what are supported in Customize Enhance Trace.\n  Add custom tag in the context of traced method, ActiveSpan.tag(\u0026quot;key\u0026quot;, \u0026quot;val\u0026quot;).\n  ActiveSpan.error() Mark the current span as error status.\n  ActiveSpan.error(String errorMsg) Mark the current span as error status with a message.\n  ActiveSpan.error(Throwable throwable) Mark the current span as error status with a Throwable.\n  ActiveSpan.debug(String debugMsg) Add a debug level log message in the current span.\n  ActiveSpan.info(String infoMsg) Add an info level log message in the current span.\n  ActiveSpan.setOperationName(String operationName) Customize an operation name.\n  ActiveSpan.tag(\u0026#34;my_tag\u0026#34;, \u0026#34;my_value\u0026#34;); ActiveSpan.error(); ActiveSpan.error(\u0026#34;Test-Error-Reason\u0026#34;); ActiveSpan.error(new RuntimeException(\u0026#34;Test-Error-Throwable\u0026#34;)); ActiveSpan.info(\u0026#34;Test-Info-Msg\u0026#34;); ActiveSpan.debug(\u0026#34;Test-debug-Msg\u0026#34;); /** * The codes below will generate a span, * and two types of tags, one type tag: keys are `tag1` and `tag2`, values are the passed-in parameters, respectively, the other type tag: keys are `username` and `age`, values are the return value in User, respectively */ @Trace @Tag(key = \u0026#34;tag1\u0026#34;, value = \u0026#34;arg[0]\u0026#34;) @Tag(key = \u0026#34;tag2\u0026#34;, value = \u0026#34;arg[1]\u0026#34;) @Tag(key = \u0026#34;username\u0026#34;, value = \u0026#34;returnedObj.username\u0026#34;) @Tag(key = \u0026#34;age\u0026#34;, value = \u0026#34;returnedObj.age\u0026#34;) public User methodYouWantToTrace(String param1, String param2) { // ActiveSpan.setOperationName(\u0026#34;Customize your own operation name, if this is an entry span, this would be an endpoint name\u0026#34;);  // ... }  Use TraceContext.putCorrelation() API to put custom data in tracing context.  Optional\u0026lt;String\u0026gt; previous = TraceContext.putCorrelation(\u0026#34;customKey\u0026#34;, \u0026#34;customValue\u0026#34;); CorrelationContext will remove the item when the value is null or empty.\n Use TraceContext.getCorrelation() API to get custom data.  Optional\u0026lt;String\u0026gt; value = TraceContext.getCorrelation(\u0026#34;customKey\u0026#34;); CorrelationContext configuration descriptions could be found in the agent configuration documentation, with correlation. as the prefix.\n","excerpt":"Dependency the toolkit, such as using maven or gradle  \u0026lt;dependency\u0026gt; …","ref":"/docs/main/v8.6.0/en/setup/service-agent/java-agent/application-toolkit-trace/","title":"Dependency the toolkit, such as using maven or gradle"},{"body":" Dependency the toolkit, such as using maven or gradle  \u0026lt;dependency\u0026gt; \u0026lt;groupId\u0026gt;org.apache.skywalking\u0026lt;/groupId\u0026gt; \u0026lt;artifactId\u0026gt;apm-toolkit-opentracing\u0026lt;/artifactId\u0026gt; \u0026lt;version\u0026gt;{project.release.version}\u0026lt;/version\u0026gt; \u0026lt;/dependency\u0026gt;  Use our OpenTracing tracer implementation  Tracer tracer = new SkywalkingTracer(); Tracer.SpanBuilder spanBuilder = tracer.buildSpan(\u0026#34;/yourApplication/yourService\u0026#34;); ","excerpt":" Dependency the toolkit, such as using maven or gradle  \u0026lt;dependency\u0026gt; …","ref":"/docs/main/v8.6.0/en/setup/service-agent/java-agent/opentracing/","title":"Dependency the toolkit, such as using maven or gradle"},{"body":"Deploy SkyWalking backend and UI in kubernetes Follow instructions in the deploying SkyWalking backend to Kubernetes cluster to deploy oap and ui to a kubernetes cluster.\nPlease read the Readme file.\n","excerpt":"Deploy SkyWalking backend and UI in kubernetes Follow instructions in the deploying SkyWalking …","ref":"/docs/main/v8.6.0/en/setup/backend/backend-k8s/","title":"Deploy SkyWalking backend and UI in kubernetes"},{"body":"Design Goals This document outlines the core design goals for the SkyWalking project.\n  Maintaining Observability. Regardless of the deployment method of the target system, SkyWalking provides an integration solution for it to maintain observability. Based on this, SkyWalking provides multiple runtime forms and probes.\n  Topology, Metrics and Trace Together. The first step to understanding a distributed system is the topology map. It visualizes the entire complex system in an easy-to-read layout. Under the topology, the OSS personnel have higher requirements in terms of the metrics for service, instance, endpoint and calls. Traces are in the form of detailed logs to make sense of those metrics. For example, when the endpoint latency becomes long, you want to see the slowest the trace to find out why. So you can see, they are from big picture to details, they are all needed. SkyWalking integrates and provides a lot of features to make this possible and easy understand.\n  Light Weight. There two parts of light weight are needed. (1) In probe, we just depend on network communication framework, prefer gRPC. By that, the probe should be as small as possible, to avoid the library conflicts and the payload of VM, such as permsize requirement in JVM. (2) As an observability platform, it is secondary and third level system in your project environment. So we are using our own light weight framework to build the backend core. Then you don\u0026rsquo;t need to deploy big data tech platform and maintain them. SkyWalking should be simple in tech stack.\n  Pluggable. SkyWalking core team provides many default implementations, but definitely it is not enough, and also don\u0026rsquo;t fit every scenario. So, we provide a lot of features for being pluggable.\n  Portability. SkyWalking can run in multiple environments, including: (1) Use traditional register center like eureka. (2) Use RPC framework including service discovery, like Spring Cloud, Apache Dubbo. (3) Use Service Mesh in modern infrastructure. (4) Use cloud services. (5) Across cloud deployment. SkyWalking should run well in all of these cases.\n  Interoperability. The observability landscape is so vast that it is virtually impossible for SkyWalking to support all systems, even with the support of its community. Currently, it supports interoperability with other OSS systems, especially probes, such as Zipkin, Jaeger, OpenTracing, and OpenCensus. It is very important to end users that SkyWalking has the ability to accept and read these data formats, since the users are not required to switch their libraries.\n  What is next?  See probe Introduction to learn about SkyWalking\u0026rsquo;s probe groups. From backend overview, you can understand what the backend does after it receives probe data. If you want to customize the UI, start with the UI overview document.  ","excerpt":"Design Goals This document outlines the core design goals for the SkyWalking project.\n  Maintaining …","ref":"/docs/main/v8.6.0/en/concepts-and-designs/project-goals/","title":"Design Goals"},{"body":"Disable plugins Delete or remove the specific libraries / jars in skywalking-agent/plugins/*.jar\n+-- skywalking-agent +-- activations apm-toolkit-log4j-1.x-activation.jar apm-toolkit-log4j-2.x-activation.jar apm-toolkit-logback-1.x-activation.jar ... +-- config agent.config +-- plugins apm-dubbo-plugin.jar apm-feign-default-http-9.x.jar apm-httpClient-4.x-plugin.jar ..... skywalking-agent.jar ","excerpt":"Disable plugins Delete or remove the specific libraries / jars in skywalking-agent/plugins/*.jar\n+-- …","ref":"/docs/main/v8.6.0/en/setup/service-agent/java-agent/how-to-disable-plugin/","title":"Disable plugins"},{"body":"Docker This section introduces how to build your Java application image on top of this image.\nFROMapache/skywalking-java-agent:8.5.0-jdk8# ... build your java applicationYou can start your Java application with CMD or ENTRYPOINT, but you don\u0026rsquo;t need to care about the Java options to enable SkyWalking agent, it should be adopted automatically.\nKubernetes This section introduces how to use this image as sidecar of Kubernetes service.\nIn Kubernetes scenarios, you can also use this agent image as a sidecar.\napiVersion: v1 kind: Pod metadata: name: agent-as-sidecar spec: restartPolicy: Never volumes: - name: skywalking-agent emptyDir: { } containers: - name: agent-container image: apache/skywalking-java-agent:8.4.0-alpine volumeMounts: - name: skywalking-agent mountPath: /agent command: [ \u0026#34;/bin/sh\u0026#34; ] args: [ \u0026#34;-c\u0026#34;, \u0026#34;cp -R /skywalking/agent /agent/\u0026#34; ] - name: app-container image: springio/gs-spring-boot-docker volumeMounts: - name: skywalking-agent mountPath: /skywalking env: - name: JAVA_TOOL_OPTIONS value: \u0026#34;-javaagent:/skywalking/agent/skywalking-agent.jar\u0026#34; ","excerpt":"Docker This section introduces how to build your Java application image on top of this image. …","ref":"/docs/main/v8.6.0/en/setup/service-agent/java-agent/containerization/","title":"Docker"},{"body":"Dynamic Configuration SkyWalking Configurations mostly are set through application.yml and OS system environment variables. At the same time, some of them are supporting dynamic settings from upstream management system.\nRight now, SkyWalking supports following dynamic configurations.\n   Config Key Value Description Value Format Example     agent-analyzer.default.slowDBAccessThreshold Thresholds of slow Database statement, override receiver-trace/default/slowDBAccessThreshold of application.yml. default:200,mongodb:50   agent-analyzer.default.uninstrumentedGateways The uninstrumented gateways, override gateways.yml. same as gateways.yml   alarm.default.alarm-settings The alarm settings, will override alarm-settings.yml. same as alarm-settings.yml   core.default.apdexThreshold The apdex threshold settings, will override service-apdex-threshold.yml. same as service-apdex-threshold.yml   core.default.endpoint-name-grouping The endpoint name grouping setting, will override endpoint-name-grouping.yml. same as endpoint-name-grouping.yml   agent-analyzer.default.sampleRate Trace sampling , override receiver-trace/default/sampleRate of application.yml. 10000   agent-analyzer.default.slowTraceSegmentThreshold Setting this threshold about the latency would make the slow trace segments sampled if they cost more time, even the sampling mechanism activated. The default value is -1, which means would not sample slow traces. Unit, millisecond. override receiver-trace/default/slowTraceSegmentThreshold of application.yml. -1   configuration-discovery.default.agentConfigurations The ConfigurationDiscovery settings look at configuration-discovery.md    This feature depends on upstream service, so it is DISABLED by default.\nconfiguration: selector: ${SW_CONFIGURATION:none} none: grpc: host: ${SW_DCS_SERVER_HOST:\u0026#34;\u0026#34;} port: ${SW_DCS_SERVER_PORT:80} clusterName: ${SW_DCS_CLUSTER_NAME:SkyWalking} period: ${SW_DCS_PERIOD:20} # ... other implementations Dynamic Configuration Service, DCS Dynamic Configuration Service is a gRPC service, which requires the upstream system implemented. The SkyWalking OAP fetches the configuration from the implementation(any system), after you open this implementation like this.\nconfiguration: selector: ${SW_CONFIGURATION:grpc} grpc: host: ${SW_DCS_SERVER_HOST:\u0026#34;\u0026#34;} port: ${SW_DCS_SERVER_PORT:80} clusterName: ${SW_DCS_CLUSTER_NAME:SkyWalking} period: ${SW_DCS_PERIOD:20} Dynamic Configuration Zookeeper Implementation Zookeeper is also supported as DCC(Dynamic Configuration Center), to use it, please configure as follows:\nconfiguration: selector: ${SW_CONFIGURATION:zookeeper} zookeeper: period: ${SW_CONFIG_ZK_PERIOD:60} # Unit seconds, sync period. Default fetch every 60 seconds. nameSpace: ${SW_CONFIG_ZK_NAMESPACE:/default} hostPort: ${SW_CONFIG_ZK_HOST_PORT:localhost:2181} # Retry Policy baseSleepTimeMs: ${SW_CONFIG_ZK_BASE_SLEEP_TIME_MS:1000} # initial amount of time to wait between retries maxRetries: ${SW_CONFIG_ZK_MAX_RETRIES:3} # max number of times to retry The nameSpace is the ZooKeeper path. The config key and value are the properties of the namespace folder.\nDynamic Configuration Etcd Implementation Etcd is also supported as DCC(Dynamic Configuration Center), to use it, please configure as follows:\nconfiguration: selector: ${SW_CONFIGURATION:etcd} etcd: period: ${SW_CONFIG_ETCD_PERIOD:60} # Unit seconds, sync period. Default fetch every 60 seconds. group: ${SW_CONFIG_ETCD_GROUP:skywalking} serverAddr: ${SW_CONFIG_ETCD_SERVER_ADDR:localhost:2379} clusterName: ${SW_CONFIG_ETCD_CLUSTER_NAME:default} Dynamic Configuration Consul Implementation Consul is also supported as DCC(Dynamic Configuration Center), to use it, please configure as follows:\nconfiguration: selector: ${SW_CONFIGURATION:consul} consul: # Consul host and ports, separated by comma, e.g. 1.2.3.4:8500,2.3.4.5:8500 hostAndPorts: ${SW_CONFIG_CONSUL_HOST_AND_PORTS:1.2.3.4:8500} # Sync period in seconds. Defaults to 60 seconds. period: ${SW_CONFIG_CONSUL_PERIOD:1} # Consul aclToken aclToken: ${SW_CONFIG_CONSUL_ACL_TOKEN:\u0026#34;\u0026#34;} Dynamic Configuration Apollo Implementation Apollo is also supported as DCC(Dynamic Configuration Center), to use it, just configured as follows:\nconfiguration: selector: ${SW_CONFIGURATION:apollo} apollo: apolloMeta: ${SW_CONFIG_APOLLO:http://106.12.25.204:8080} apolloCluster: ${SW_CONFIG_APOLLO_CLUSTER:default} apolloEnv: ${SW_CONFIG_APOLLO_ENV:\u0026#34;\u0026#34;} appId: ${SW_CONFIG_APOLLO_APP_ID:skywalking} period: ${SW_CONFIG_APOLLO_PERIOD:5} Dynamic Configuration Kuberbetes Configmap Implementation configmap is also supported as DCC(Dynamic Configuration Center), to use it, just configured as follows:\nconfiguration: selector: ${SW_CONFIGURATION:k8s-configmap} # [example] (../../../../oap-server/server-configuration/configuration-k8s-configmap/src/test/resources/skywalking-dynamic-configmap.example.yaml) k8s-configmap: # Sync period in seconds. Defaults to 60 seconds. period: ${SW_CONFIG_CONFIGMAP_PERIOD:60} # Which namespace is confiigmap deployed in. namespace: ${SW_CLUSTER_K8S_NAMESPACE:default} # Labelselector is used to locate specific configmap labelSelector: ${SW_CLUSTER_K8S_LABEL:app=collector,release=skywalking} Dynamic Configuration Nacos Implementation Nacos is also supported as DCC(Dynamic Configuration Center), to use it, please configure as follows:\nconfiguration: selector: ${SW_CONFIGURATION:nacos} nacos: # Nacos Server Host serverAddr: ${SW_CONFIG_NACOS_SERVER_ADDR:127.0.0.1} # Nacos Server Port port: ${SW_CONFIG_NACOS_SERVER_PORT:8848} # Nacos Configuration Group group: ${SW_CONFIG_NACOS_SERVER_GROUP:skywalking} # Nacos Configuration namespace namespace: ${SW_CONFIG_NACOS_SERVER_NAMESPACE:} # Unit seconds, sync period. Default fetch every 60 seconds. period: ${SW_CONFIG_NACOS_PERIOD:60} # the name of current cluster, set the name if you want to upstream system known. clusterName: ${SW_CONFIG_NACOS_CLUSTER_NAME:default} ","excerpt":"Dynamic Configuration SkyWalking Configurations mostly are set through application.yml and OS system …","ref":"/docs/main/v8.6.0/en/setup/backend/dynamic-config/","title":"Dynamic Configuration"},{"body":"ElasticSearch Some new users may encounter the following issues:\n The performance of ElasticSearch is not as good as expected. For instance, the latest data cannot be accessed after some time.  Or\n ERROR CODE 429.   Suppressed: org.elasticsearch.client.ResponseException: method [POST], host [http://127.0.0.1:9200], URI [/service_instance_inventory/type/6_tcc-app-gateway-77b98ff6ff-crblx.cards_0_0/_update?refresh=true\u0026amp;timeout=1m], status line [HTTP/1.1 429 Too Many Requests] {\u0026quot;error\u0026quot;:{\u0026quot;root_cause\u0026quot;:[{\u0026quot;type\u0026quot;:\u0026quot;remote_transport_exception\u0026quot;,\u0026quot;reason\u0026quot;:\u0026quot;[elasticsearch-0][10.16.9.130:9300][indices:data/write/update[s]]\u0026quot;}],\u0026quot;type\u0026quot;:\u0026quot;es_rejected_execution_exception\u0026quot;,\u0026quot;reason\u0026quot;:\u0026quot;rejected execution of org.elasticsearch.transport.TransportService$7@19a5cf02 on EsThreadPoolExecutor[name = elasticsearch-0/write, queue capacity = 200, org.elasticsearch.common.util.concurrent.EsThreadPoolExecutor@389297ad[Running, pool size = 2, active threads = 2, queued tasks = 200, completed tasks = 147611]]\u0026quot;},\u0026quot;status\u0026quot;:429} at org.elasticsearch.client.RestClient$SyncResponseListener.get(RestClient.java:705) ~[elasticsearch-rest-client-6.3.2.jar:6.3.2] at org.elasticsearch.client.RestClient.performRequest(RestClient.java:235) ~[elasticsearch-rest-client-6.3.2.jar:6.3.2] at org.elasticsearch.client.RestClient.performRequest(RestClient.java:198) ~[elasticsearch-rest-client-6.3.2.jar:6.3.2] at org.elasticsearch.client.RestHighLevelClient.performRequest(RestHighLevelClient.java:522) ~[elasticsearch You could add the following config to elasticsearch.yml, and set the value based on your environment variable.\n# In the case of tracing, consider setting a value higher than this. thread_pool.index.queue_size: 1000 thread_pool.write.queue_size: 1000 # When you face query error at trace page, remember to check this. index.max_result_window: 1000000 For more information, see ElasticSearch\u0026rsquo;s official documentation.\n","excerpt":"ElasticSearch Some new users may encounter the following issues:\n The performance of ElasticSearch …","ref":"/docs/main/v8.6.0/en/faq/es-server-faq/","title":"ElasticSearch"},{"body":"Events SkyWalking already supports the three pillars of observability, namely logs, metrics, and traces. In reality, a production system experiences many other events that may affect the performance of the system, such as upgrading, rebooting, chaos testing, etc. Although some of these events are reflected in the logs, many others are not. Hence, SkyWalking provides a more native way to collect these events. This doc details how SkyWalking collects events and what events look like in SkyWalking.\nHow to Report Events The SkyWalking backend supports three protocols to collect events: gRPC, HTTP, and Kafka. Any agent or CLI that implements one of these protocols can report events to SkyWalking. Currently, the officially supported clients to report events are:\n Java Agent Toolkit: Using the Java agent toolkit to report events within the applications. SkyWalking CLI: Using the CLI to report events from the command line interface. Kubernetes Event Exporter: Deploying an event exporter to refine and report Kubernetes events.  Event Definitions An event contains the following fields. The definitions of event can be found at the protocol repo.\nUUID Unique ID of the event. Since an event may span a long period of time, the UUID is necessary to associate the start time with the end time of the same event.\nSource The source object on which the event occurs. In SkyWalking, the object is typically a service, service instance, etc.\nName Name of the event. For example, Start, Stop, Crash, Reboot, Upgrade, etc.\nType Type of the event. This field is friendly for UI visualization, where events of type Normal are considered normal operations, while Error is considered unexpected operations, such as Crash events. Marking them with different colors allows us to more easily identify them.\nMessage The detail of the event that describes why this event happened. This should be a one-line message that briefly describes why the event is reported. Examples of an Upgrade event may be something like Upgrade from ${from_version} to ${to_version}. It\u0026rsquo;s NOT recommended to include the detailed logs of this event, such as the exception stack trace.\nParameters The parameters in the message field. This is a simple \u0026lt;string,string\u0026gt; map.\nStart Time The start time of the event. This field is mandatory when an event occurs.\nEnd Time The end time of the event. This field may be empty if the event has not ended yet, otherwise there should be a valid timestamp after startTime.\nNOTE: When reporting an event, you typically call the report function twice, the first time for starting of the event and the second time for ending of the event, both with the same UUID. There are also cases where you would already have both the start time and end time. For example, when exporting events from a third-party system, the start time and end time are already known so you may simply call the report function once.\nHow to Configure Alarms for Events Events are derived from metrics, and can be the source to trigger alarms. For example, if a specific event occurs for a certain times in a period, alarms can be triggered and sent.\nEvery event has a default value = 1, when n events with the same name are reported, they are aggregated into value = n as follows.\nEvent{name=Unhealthy, source={service=A,instance=a}, ...} Event{name=Unhealthy, source={service=A,instance=a}, ...} Event{name=Unhealthy, source={service=A,instance=a}, ...} Event{name=Unhealthy, source={service=A,instance=a}, ...} Event{name=Unhealthy, source={service=A,instance=a}, ...} Event{name=Unhealthy, source={service=A,instance=a}, ...} will be aggregated into\nEvent{name=Unhealthy, source={service=A,instance=a}, ...} \u0026lt;value = 6\u0026gt; so you can configure the following alarm rule to trigger alarm when Unhealthy event occurs more than 5 times within 10 minutes.\nrules: unhealthy_event_rule: metrics-name: Unhealthy # Healthiness check is usually a scheduled task, # they may be unhealthy for the first few times, # and can be unhealthy occasionally due to network jitter, # please adjust the threshold as per your actual situation. threshold: 5 op: \u0026#34;\u0026gt;\u0026#34; period: 10 count: 1 message: Service instance has been unhealthy for 10 minutes For more alarm configuration details, please refer to the alarm doc.\nNote that the Unhealthy event above is only for demonstration, they are not detected by default in SkyWalking, however, you can use the methods in How to Report Events to report this kind of events.\nKnown Events    Name Type When Where     Start Normal When your Java Application starts with SkyWalking Agent installed, the Start Event will be created. Reported from SkyWalking agent.   Shutdown Normal When your Java Application stops with SkyWalking Agent installed, the Shutdown Event will be created. Reported from SkyWalking agent.   Alarm Error When the Alarm is triggered, the corresponding Alarm Event will is created. Reported from internal SkyWalking OAP.    The following events are all reported by Kubernetes Event Exporter, in order to see these events, please make sure you have deployed the exporter.\n   Name Type When Where     Killing Normal When the Kubernetes Pod is being killing. Reporter by Kubernetes Event Exporter.   Pulling Normal When a docker image is being pulled for deployment. Reporter by Kubernetes Event Exporter.   Pulled Normal When a docker image is pulled for deployment. Reporter by Kubernetes Event Exporter.   Created Normal When a container inside a Pod is created. Reporter by Kubernetes Event Exporter.   Started Normal When a container inside a Pod is started. Reporter by Kubernetes Event Exporter.   Unhealthy Error When the readiness probe failed. Reporter by Kubernetes Event Exporter.    The complete event lists can be found in the Kubernetes codebase, please note that not all the events are supported by the exporter for now.\n","excerpt":"Events SkyWalking already supports the three pillars of observability, namely logs, metrics, and …","ref":"/docs/main/v8.6.0/en/concepts-and-designs/event/","title":"Events"},{"body":"Exporter tool for profile raw data When visualization doesn\u0026rsquo;t work well on the official UI, users may submit issue reports. This tool helps users package the original profile data to assist the community in locating the issues in the users' cases. NOTE: This report includes the class name, method name, line number, etc. Before making your submission, please make sure that the security of your system wouldn\u0026rsquo;t be compromised.\nExport using command line  Set the storage in the tools/profile-exporter/application.yml file based on your use case. Prepare the data  Profile task ID: Profile task ID Trace ID: Trace ID of the profile error Export dir: Directory exported by the data   Enter the Skywalking root path Execute shell command bash tools/profile-exporter/profile_exporter.sh --taskid={profileTaskId} --traceid={traceId} {exportDir}  The file {traceId}.tar.gz will be generated after executing shell.  Exported data content  basic.yml: Contains the complete information of the profiled segments in the trace. snapshot.data: All monitored thread snapshot data in the current segment.  Report profile issues  Provide exported data generated from this tool. Provide the operation name and the mode of analysis (including/excluding child span) for the span. Issue description. (It would be great if you could provide UI screenshots.)  ","excerpt":"Exporter tool for profile raw data When visualization doesn\u0026rsquo;t work well on the official UI, …","ref":"/docs/main/v8.6.0/en/guides/backend-profile-export/","title":"Exporter tool for profile raw data"},{"body":"Extend storage SkyWalking has already provided several storage solutions. In this document, you could learn how to easily implement a new storage.\nDefine your storage provider  Define class extension org.apache.skywalking.oap.server.library.module.ModuleProvider. Set this provider targeting to storage module.  @Override public Class\u0026lt;? extends ModuleDefine\u0026gt; module() { return StorageModule.class; } Implement all DAOs Here\u0026rsquo;s a list of all DAO interfaces in storage:\n  IServiceInventoryCacheDAO\n  IServiceInstanceInventoryCacheDAO\n  IEndpointInventoryCacheDAO\n  INetworkAddressInventoryCacheDAO\n  IBatchDAO\n  StorageDAO\n  IRegisterLockDAO\n  ITopologyQueryDAO\n  IMetricsQueryDAO\n  ITraceQueryDAO\n  IMetadataQueryDAO\n  IAggregationQueryDAO\n  IAlarmQueryDAO\n  IHistoryDeleteDAO\n  IMetricsDAO\n  IRecordDAO\n  IRegisterDAO\n  ILogQueryDAO\n  ITopNRecordsQueryDAO\n  IBrowserLogQueryDAO\n  IProfileTaskQueryDAO\n  IProfileTaskLogQueryDAO\n  IProfileThreadSnapshotQueryDAO\n  UITemplateManagementDAO\n  Register all service implementations In public void prepare(), use this#registerServiceImplementation method to register and bind with your implementation of the above interfaces.\nExample org.apache.skywalking.oap.server.storage.plugin.elasticsearch.StorageModuleElasticsearchProvider and org.apache.skywalking.oap.server.storage.plugin.jdbc.mysql.MySQLStorageProvider are good examples.\nRedistribution with new storage implementation To implement the storage, you don\u0026rsquo;t have to clone the main repo. Simply use our Apache releases. Take a look at SkyAPM/SkyWalking-With-Es5x-Storage repo, SkyWalking v6 redistribution with ElasticSearch 5 TCP connection storage implementation.\n","excerpt":"Extend storage SkyWalking has already provided several storage solutions. In this document, you …","ref":"/docs/main/v8.6.0/en/guides/storage-extention/","title":"Extend storage"},{"body":"FAQs These are known and frequently asked questions about SkyWalking. We welcome you to contribute here.\nDesign  Why doesn\u0026rsquo;t SkyWalking involve MQ in its architecture?  Compiling  Protoc plugin fails in maven build Required items could not be found when importing project into Eclipse Maven compilation failure with error such as python2 not found Compiling issues on Mac\u0026rsquo;s M1 chip  Runtime  Version 8.x+ upgrade Why do metrics indexes with Hour and Day precisions stop updating after upgrade to 7.x? Version 6.x upgrade Why are there only traces in UI? Tracing doesn\u0026rsquo;t work on the Kafka consumer end Agent or collector version upgrade, 3.x -\u0026gt; 5.0.0-alpha EnhanceRequireObjectCache class cast exception ElasticSearch server performance issues, including ERROR CODE:429 IllegalStateException when installing Java agent on WebSphere 7 \u0026ldquo;FORBIDDEN/12/index read-only / allow delete (api)\u0026rdquo; appears in the log No data shown and backend replies with \u0026ldquo;Variable \u0026lsquo;serviceId\u0026rsquo; has coerced Null value for NonNull type \u0026lsquo;ID!'\u0026quot; Unexpected endpoint register warning after 6.6.0 Use the profile exporter tool if the profile analysis is not right Compatibility with other javaagent bytecode processes Java agent memory leak when enhancing Worker thread at Thread Pool Thrift plugin  UI  What is VNode? And why does SkyWalking have that?  ","excerpt":"FAQs These are known and frequently asked questions about SkyWalking. We welcome you to contribute …","ref":"/docs/main/v8.6.0/en/faq/readme/","title":"FAQs"},{"body":"Group Parameterized Endpoints In most cases, the endpoint should be detected automatically through the language agents, service mesh observability solution, or configuration of meter system.\nThere are some special cases, especially when people use REST style URI, the application codes put the parameter in the endpoint name, such as putting order id in the URI, like /prod/ORDER123 and /prod/ORDER123. But logically, people expect they could have an endpoint name like prod/{order-id}. This is the feature of parameterized endpoint grouping designed for.\nCurrent, user could set up grouping rules through the static YAML file, named endpoint-name-grouping.yml, or use Dynamic Configuration to initial and update the endpoint grouping rule.\nConfiguration Format No matter in static local file or dynamic configuration value, they are sharing the same YAML format.\ngrouping: # Endpoint of the service would follow the following rules - service-name: serviceA rules: # Logic name when the regex expression matched. - endpoint-name: /prod/{id} regex: \\/prod\\/.+ ","excerpt":"Group Parameterized Endpoints In most cases, the endpoint should be detected automatically through …","ref":"/docs/main/v8.6.0/en/setup/backend/endpoint-grouping-rules/","title":"Group Parameterized Endpoints"},{"body":"Guides There are many ways you can contribute to the SkyWalking community.\n Go through our documents, and point out or fix a problem. Translate the documents into other languages. Download our releases, try to monitor your applications, and provide feedback to us. Read our source codes. For details, reach out to us. If you find any bugs, submit an issue. You can also try to fix it. Find help wanted issues. This is a good place for you to start. Submit an issue or start a discussion at GitHub issue. See all mail list discussions at website list review. If you are already a SkyWalking committer, you can log in and use the mail list in the browser mode. Otherwise, subscribe following the step below. Issue reports and discussions may also take place via dev@skywalking.apache.org. Mail to dev-subscribe@skywalking.apache.org, and follow the instructions in the reply to subscribe to the mail list.  Contact Us All of the following channels are open to the community.\n Submit an issue Mail list: dev@skywalking.apache.org. Mail to dev-subscribe@skywalking.apache.org. Follow the instructions in the reply to subscribe to the mail list. Gitter QQ Group: 392443393  Become an official Apache SkyWalking Committer The PMC assesses the contributions of every contributor, including their code contributions. It also promotes, votes on, and invites new committers and PMC members according to the Apache guides. See Become official Apache SkyWalking Committer for more details.\nFor code developer For developers, the starting point is the Compiling Guide. It guides developers on how to build the project in local and set up the environment.\nIntegration Tests After setting up the environment and writing your codes, to facilitate integration with the SkyWalking project, you\u0026rsquo;ll need to run tests locally to verify that your codes would not break any existing features, as well as write some unit test (UT) codes to verify that the new codes would work well. This will prevent them from being broken by future contributors. If the new codes involve other components or libraries, you should also write integration tests (IT).\nSkyWalking leverages the plugin maven-surefire-plugin to run the UTs and uses maven-failsafe-plugin to run the ITs. maven-surefire-plugin excludes ITs (whose class name starts with IT) and leaves them for maven-failsafe-plugin to run, which is bound to the verify goal and CI-with-IT profile. Therefore, to run the UTs, try ./mvnw clean test, which only runs the UTs but not the ITs.\nIf you would like to run the ITs, please activate the CI-with-IT profile as well as the the profiles of the modules whose ITs you want to run. E.g. if you would like to run the ITs in oap-server, try ./mvnw -Pbackend,CI-with-IT clean verify, and if you would like to run all the ITs, simply run ./mvnw -Pall,CI-with-IT clean verify.\nPlease be advised that if you\u0026rsquo;re writing integration tests, name it with the pattern IT* so they would only run with the CI-with-IT profile.\nEnd to End Tests (E2E) Since version 6.3.0, we have introduced more automatic tests to perform software quality assurance. E2E is an integral part of it.\n End-to-end testing is a methodology used to test whether the flow of an application is performing as designed from start to finish. The purpose of carrying out end-to-end tests is to identify system dependencies and to ensure that the right information is passed between various system components and systems.\n The E2E test involves some/all of the OAP server, storage, coordinator, webapp, and the instrumented services, all of which are orchestrated by docker-compose. Besides, there is a test controller (JUnit test) running outside of the container that sends traffic to the instrumented service, and then verifies the corresponding results after those requests have been made through GraphQL API of the SkyWalking Web App.\nBefore you take the following steps, please set the SkyWalking version sw.version in the pom.xml so that you can build it in your local IDE. Make sure not to check this change into the codebase. However, if you prefer to build it in the command line interface with ./mvnw, you can simply use property -Dsw.version=x.y.z without modifying pom.xml.\nWriting E2E Cases  Set up the environment in IntelliJ IDEA  The E2E test is a separate project under the SkyWalking root directory and the IDEA cannot recognize it by default. Right click on the file test/e2e/pom.xml and click Add as Maven Project. We recommend opening the directory skywalking/test/e2e in a separate IDE window for better experience, since there may be shaded classes issues.\n Orchestrate the components  The goal of the E2E tests is to test the SkyWalking project as a whole, including the OAP server, storage, coordinator, webapp, and even the frontend UI (not for now), on the single node mode as well as the cluster mode. Therefore, the first step is to determine what case we are going to verify, and orchestrate the components.\nTo make the orchestration process easier, we\u0026rsquo;re using a docker-compose that provides a simple file format (docker-compose.yml) for orchestrating the required containers, and offers an opportunity to define the dependencies of the components.\nFollow these steps:\n Decide what (and how many) containers will be needed. For example, for cluster testing, you\u0026rsquo;ll need \u0026gt; 2 OAP nodes, coordinators (e.g. zookeeper), storage (e.g. ElasticSearch), and instrumented services; Define the containers in docker-compose.yml, and carefully specify the dependencies, starting orders, and most importantly, link them together, e.g. set the correct OAP address on the agent end, and set the correct coordinator address in OAP, etc. Write (or hopefully reuse) the test codes to verify that the results are correct.  As for the final step, we have a user-friendly framework to help you get started more quickly. This framework provides the annotation @DockerCompose(\u0026quot;docker-compose.yml\u0026quot;) to load/parse and start up all the containers in the proper order. @ContainerHost/@ContainerPort obtains the real host/port of the container. @ContainerHostAndPort obtains both. @DockerContainer obtains the running container.\n Write test controller  Put it simply, test controllers are tests that can be bound to the maven integration-test/verify phase. They send design requests to the instrumented services, and anticipate corresponding traces/metrics/metadata from the SkyWalking webapp GraphQL API.\nIn the test framework, we provide a TrafficController that periodically sends traffic data to the instrumented services. You can simply enable it by providing a url and traffic data. Refer to this.\n Troubleshooting  We expose all logs from all containers to the stdout in the non-CI (local) mode, but save and upload them to the GitHub server. You can download them (only when the tests have failed) at \u0026ldquo;Artifacts/Download artifacts/logs\u0026rdquo; (see top right) for debugging.\nNOTE: Please verify the newly-added E2E test case locally first. However, if you find that it has passed locally but failed in the PR check status, make sure that all the updated/newly-added files (especially those in the submodules) are committed and included in the PR, or reset the git HEAD to the remote and verify locally again.\nE2E local remote debugging When the E2E test is executed locally, if any test case fails, the E2E local remote debugging function can be used to quickly troubleshoot the bug.\nProject Extensions The SkyWalking project supports various extensions of existing features. If you are interesting in writing extensions, read the following guides.\n Java agent plugin development guide. This guides you in developing SkyWalking agent plugins to support more frameworks. Developers for both open source and private plugins should read this. If you would like to build a new probe or plugin in any language, please read the Component library definition and extension document. Storage extension development guide. Potential contributors can learn how to build a new storage implementor in addition to the official one. Customize analysis using OAL scripts. OAL scripts are located in config/oal/*.oal. You could modify them and reboot the OAP server. Read Observability Analysis Language Introduction to learn more about OAL scripts. Source and scope extension for new metrics. For analysis of a new metric which SkyWalking hasn\u0026rsquo;t yet provided. Add a new receiver, rather than choosing an existing receiver. You would most likely have to add a new source and scope. To learn how to do this, read the document.  UI developer Our UI consists of static pages and the web container.\n RocketBot UI is SkyWalking\u0026rsquo;s primary UI since the 6.1 release. It is built with vue + typescript. Learn more at the rocketbot repository. Web container source codes are in the apm-webapp module. This is a simple zuul proxy which hosts static resources and sends GraphQL query requests to the backend. Legacy UI repository is retained, but not included in SkyWalking releases since 6.0.0-GA.  OAP backend dependency management  This section is only applicable to dependencies of the backend module.\n As one of the Top Level Projects of The Apache Software Foundation (ASF), SkyWalking must follow the ASF 3RD PARTY LICENSE POLICY. So if you\u0026rsquo;re adding new dependencies to the project, you should make sure that the new dependencies would not break the policy, and add their LICENSE and NOTICE to the project.\nWe have a simple script to help you make sure that you haven\u0026rsquo;t missed out any new dependencies:\n Build a distribution package and unzip/untar it to folder dist. Run the script in the root directory. It will print out all new dependencies. Check the LICENSE and NOTICE of those dependencies to make sure that they can be included in an ASF project. Add them to the apm-dist/release-docs/{LICENSE,NOTICE} file. Add the names of these dependencies to the tools/dependencies/known-oap-backend-dependencies.txt file (in alphabetical order). check-LICENSE.sh should pass in the next run.  Profile The performance profile is an enhancement feature in the APM system. We use thread dump to estimate the method execution time, rather than adding multiple local spans. In this way, the cost would be significantly reduced compared to using distributed tracing to locate the slow method. This feature is suitable in the production environment. The following documents are key to understanding the essential parts of this feature.\n Profile data report protocol is provided through gRPC, just like other traces and JVM data. Thread dump merging mechanism introduces the merging mechanism. This mechanism helps end users understand profile reports. Exporter tool of profile raw data guides you on how to package the original profile data for issue reports when the visualization doesn\u0026rsquo;t work well on the official UI.  Release If you\u0026rsquo;re a committer, read the Apache Release Guide to learn about how to create an official Apache version release in accordance with avoid Apache\u0026rsquo;s rules. As long as you keep our LICENSE and NOTICE, the Apache license allows everyone to redistribute.\n","excerpt":"Guides There are many ways you can contribute to the SkyWalking community.\n Go through our …","ref":"/docs/main/v8.6.0/en/guides/readme/","title":"Guides"},{"body":"Health Check Health check intends to provide a unique approach to check the health status of the OAP server. It includes the health status of modules, GraphQL, and gRPC services readiness.\nHealth Checker Module. The Health Checker module helps observe the health status of modules. You may activate it as follows:\nhealth-checker: selector: ${SW_HEALTH_CHECKER:default} default: checkIntervalSeconds: ${SW_HEALTH_CHECKER_INTERVAL_SECONDS:5} Note: The telemetry module should be enabled at the same time. This means that the provider should not be - and none.\nAfter that, we can check the OAP server health status by querying GraphQL:\nquery{ checkHealth{ score details } } If the OAP server is healthy, the response should be\n{ \u0026#34;data\u0026#34;: { \u0026#34;checkHealth\u0026#34;: { \u0026#34;score\u0026#34;: 0, \u0026#34;details\u0026#34;: \u0026#34;\u0026#34; } } } If some modules are unhealthy (e.g. storage H2 is down), then the result may look as follows:\n{ \u0026#34;data\u0026#34;: { \u0026#34;checkHealth\u0026#34;: { \u0026#34;score\u0026#34;: 1, \u0026#34;details\u0026#34;: \u0026#34;storage_h2,\u0026#34; } } } Refer to checkHealth query for more details.\nThe readiness of GraphQL and gRPC Use the query above to check the readiness of GraphQL.\nOAP has implemented the gRPC Health Checking Protocol. You may use the grpc-health-probe or any other tools to check the health of OAP gRPC services.\nCLI tool Please follow the CLI doc to get the health status score directly through the checkhealth command.\n","excerpt":"Health Check Health check intends to provide a unique approach to check the health status of the OAP …","ref":"/docs/main/v8.6.0/en/setup/backend/backend-health-check/","title":"Health Check"},{"body":"How to build a project This document will help you compile and build a project in your maven and set your IDE.\nBuilding the Project Since we are using Git submodule, we do not recommend using the GitHub tag or release page to download source codes for compiling.\nMaven behind the Proxy If you need to execute build behind the proxy, edit the .mvn/jvm.config and set the follow properties:\n-Dhttp.proxyHost=proxy_ip -Dhttp.proxyPort=proxy_port -Dhttps.proxyHost=proxy_ip -Dhttps.proxyPort=proxy_port -Dhttp.proxyUser=username -Dhttp.proxyPassword=password Building from GitHub   Prepare git, JDK8+, and Maven 3.6+.\n  Clone the project.\nIf you want to build a release from source codes, set a tag name by using git clone -b [tag_name] ... while cloning.\ngit clone --recurse-submodules https://github.com/apache/skywalking.git cd skywalking/ OR git clone https://github.com/apache/skywalking.git cd skywalking/ git submodule init git submodule update   Run ./mvnw clean package -DskipTests\n  All packages are in /dist (.tar.gz for Linux and .zip for Windows).\n  Building from Apache source code release  What is the Apache source code release?  For each official Apache release, there is a complete and independent source code tar, which includes all source codes. You could download it from SkyWalking Apache download page. There is no requirement related to git when compiling this. Just follow these steps.\n Prepare JDK8+ and Maven 3.6+. Run ./mvnw clean package -DskipTests. All packages are in /dist.(.tar.gz for Linux and .zip for Windows).  Advanced compiling SkyWalking is a complex maven project that has many modules. Therefore, the time to compile may be a bit longer than usual. If you just want to recompile part of the project, you have the following options:\n Compile agent and package   ./mvnw package -Pagent,dist\n or\n make build.agent\n If you intend to compile a single plugin, such as one in the dev stage, you could\n cd plugin_module_dir \u0026amp; mvn clean package\n  Compile backend and package   ./mvnw package -Pbackend,dist\n or\n make build.backend\n  Compile UI and package   ./mvnw package -Pui,dist\n or\n make build.ui\n Building docker images You can build docker images of backend and ui with Makefile located in root folder.\nRefer to Build docker image for more details.\nSetting up your IntelliJ IDEA NOTE: If you clone the codes from GitHub, please make sure that you have finished steps 1 to 3 in section Build from GitHub. If you download the source codes from the official website of SkyWalking, please make sure that you have followed the steps in section Build from Apache source code release.\n Import the project as a maven project. Run ./mvnw compile -Dmaven.test.skip=true to compile project and generate source codes. The reason is that we use gRPC and protobuf. Set Generated Source Codes folders.  grpc-java and java folders in apm-protocol/apm-network/target/generated-sources/protobuf grpc-java and java folders in oap-server/server-core/target/generated-sources/protobuf grpc-java and java folders in oap-server/server-receiver-plugin/receiver-proto/target/generated-sources/fbs grpc-java and java folders in oap-server/server-receiver-plugin/receiver-proto/target/generated-sources/protobuf grpc-java and java folders in oap-server/exporter/target/generated-sources/protobuf grpc-java and java folders in oap-server/server-configuration/grpc-configuration-sync/target/generated-sources/protobuf grpc-java and java folders in oap-server/server-alarm-plugin/target/generated-sources/protobuf antlr4 folder in oap-server/oal-grammar/target/generated-sources    ","excerpt":"How to build a project This document will help you compile and build a project in your maven and set …","ref":"/docs/main/v8.6.0/en/guides/how-to-build/","title":"How to build a project"},{"body":"How to enable Kafka Reporter The Kafka reporter plugin support report traces, JVM metrics, Instance Properties, and profiled snapshots to Kafka cluster, which is disabled in default. Move the jar of the plugin, kafka-reporter-plugin-x.y.z.jar, from agent/optional-reporter-plugins to agent/plugins for activating.\nNotice, currently, the agent still needs to configure GRPC receiver for delivering the task of profiling. In other words, the following configure cannot be omitted.\n# Backend service addresses. collector.backend_service=${SW_AGENT_COLLECTOR_BACKEND_SERVICES:127.0.0.1:11800} # Kafka producer configuration plugin.kafka.bootstrap_servers=${SW_KAFKA_BOOTSTRAP_SERVERS:localhost:9092} plugin.kafka.producer_config[delivery.timeout.ms]=12000 plugin.kafka.get_topic_timeout=${SW_GET_TOPIC_TIMEOUT:10} Kafka reporter plugin support to customize all configurations of listed in here.\nBefore you activated the Kafka reporter, you have to make sure that Kafka fetcher has been opened in service.\n","excerpt":"How to enable Kafka Reporter The Kafka reporter plugin support report traces, JVM metrics, Instance …","ref":"/docs/main/v8.6.0/en/setup/service-agent/java-agent/how-to-enable-kafka-reporter/","title":"How to enable Kafka Reporter"},{"body":"How to tolerate custom exceptions In some codes, the exception is being used as a way of controlling business flow. Skywalking provides 2 ways to tolerate an exception which is traced in a span.\n Set the names of exception classes in the agent config Use our annotation in the codes.  Set the names of exception classes in the agent config The property named \u0026ldquo;statuscheck.ignored_exceptions\u0026rdquo; is used to set up class names in the agent configuration file. if the exception listed here are detected in the agent, the agent core would flag the related span as the error status.\nDemo   A custom exception.\n TestNamedMatchException  package org.apache.skywalking.apm.agent.core.context.status; public class TestNamedMatchException extends RuntimeException { public TestNamedMatchException() { } public TestNamedMatchException(final String message) { super(message); } ... }  TestHierarchyMatchException  package org.apache.skywalking.apm.agent.core.context.status; public class TestHierarchyMatchException extends TestNamedMatchException { public TestHierarchyMatchException() { } public TestHierarchyMatchException(final String message) { super(message); } ... }   When the above exceptions traced in some spans, the status is like the following.\n   The traced exception Final span status     org.apache.skywalking.apm.agent.core.context.status.TestNamedMatchException true   org.apache.skywalking.apm.agent.core.context.status.TestHierarchyMatchException true      After set these class names through \u0026ldquo;statuscheck.ignored_exceptions\u0026rdquo;, the status of spans would be changed.\nstatuscheck.ignored_exceptions=org.apache.skywalking.apm.agent.core.context.status.TestNamedMatchException    The traced exception Final span status     org.apache.skywalking.apm.agent.core.context.status.TestNamedMatchException false   org.apache.skywalking.apm.agent.core.context.status.TestHierarchyMatchException false      Use our annotation in the codes. If an exception has the @IgnoredException annotation, the exception wouldn\u0026rsquo;t be marked as error status when tracing. Because the annotation supports inheritance, also affects the subclasses.\nDependency  Dependency the toolkit, such as using maven or gradle. Since 8.2.0.  \u0026lt;dependency\u0026gt; \u0026lt;groupId\u0026gt;org.apache.skywalking\u0026lt;/groupId\u0026gt; \u0026lt;artifactId\u0026gt;apm-toolkit-log4j-1.x\u0026lt;/artifactId\u0026gt; \u0026lt;version\u0026gt;{project.release.version}\u0026lt;/version\u0026gt; \u0026lt;/dependency\u0026gt; Demo   A custom exception.\npackage org.apache.skywalking.apm.agent.core.context.status; public class TestAnnotatedException extends RuntimeException { public TestAnnotatedException() { } public TestAnnotatedException(final String message) { super(message); } ... }   When the above exception traced in some spans, the status is like the following.\n   The traced exception Final span status     org.apache.skywalking.apm.agent.core.context.status.TestAnnotatedException true      However, when the exception annotated with the annotation, the status would be changed.\npackage org.apache.skywalking.apm.agent.core.context.status; @IgnoredException public class TestAnnotatedException extends RuntimeException { public TestAnnotatedException() { } public TestAnnotatedException(final String message) { super(message); } ... }    The traced exception Final span status     org.apache.skywalking.apm.agent.core.context.status.TestAnnotatedException false      Recursive check Due to the wrapper nature of Java exceptions, sometimes users need recursive checking. Skywalking also supports it. Typically, we don\u0026rsquo;t recommend setting this more than 10, which could cause a performance issue. Negative value and 0 would be ignored, which means all exceptions would make the span tagged in error status.\n statuscheck.max_recursive_depth=${SW_STATUSCHECK_MAX_RECURSIVE_DEPTH:1} ","excerpt":"How to tolerate custom exceptions In some codes, the exception is being used as a way of controlling …","ref":"/docs/main/v8.6.0/en/setup/service-agent/java-agent/how-to-tolerate-exceptions/","title":"How to tolerate custom exceptions"},{"body":"HTTP API Protocol HTTP API Protocol defines the API data format, including API request and response data format. They use the HTTP1.1 wrapper of the official SkyWalking Browser Protocol. Read it for more details.\nPerformance Data Report Detailed information about data format can be found in BrowserPerf.proto.\nPOST http://localhost:12800/browser/perfData Send a performance data object in JSON format.\nInput:\n{ \u0026#34;service\u0026#34;: \u0026#34;web\u0026#34;, \u0026#34;serviceVersion\u0026#34;: \u0026#34;v0.0.1\u0026#34;, \u0026#34;pagePath\u0026#34;: \u0026#34;/index.html\u0026#34;, \u0026#34;redirectTime\u0026#34;: 10, \u0026#34;dnsTime\u0026#34;: 10, \u0026#34;ttfbTime\u0026#34;: 10, \u0026#34;tcpTime\u0026#34;: 10, \u0026#34;transTime\u0026#34;: 10, \u0026#34;domAnalysisTime\u0026#34;: 10, \u0026#34;fptTime\u0026#34;: 10, \u0026#34;domReadyTime\u0026#34;: 10, \u0026#34;loadPageTime\u0026#34;: 10, \u0026#34;resTime\u0026#34;: 10, \u0026#34;sslTime\u0026#34;: 10, \u0026#34;ttlTime\u0026#34;: 10, \u0026#34;firstPackTime\u0026#34;: 10, \u0026#34;fmpTime\u0026#34;: 10 } OutPut:\nHttp Status: 204\nError Log Report Detailed information about data format can be found in BrowserPerf.proto.\nPOST http://localhost:12800/browser/errorLogs Send an error log object list in JSON format.\nInput:\n[ { \u0026#34;uniqueId\u0026#34;: \u0026#34;55ec6178-3fb7-43ef-899c-a26944407b01\u0026#34;, \u0026#34;service\u0026#34;: \u0026#34;web\u0026#34;, \u0026#34;serviceVersion\u0026#34;: \u0026#34;v0.0.1\u0026#34;, \u0026#34;pagePath\u0026#34;: \u0026#34;/index.html\u0026#34;, \u0026#34;category\u0026#34;: \u0026#34;ajax\u0026#34;, \u0026#34;message\u0026#34;: \u0026#34;error\u0026#34;, \u0026#34;line\u0026#34;: 1, \u0026#34;col\u0026#34;: 1, \u0026#34;stack\u0026#34;: \u0026#34;error\u0026#34;, \u0026#34;errorUrl\u0026#34;: \u0026#34;/index.html\u0026#34; }, { \u0026#34;uniqueId\u0026#34;: \u0026#34;55ec6178-3fb7-43ef-899c-a26944407b02\u0026#34;, \u0026#34;service\u0026#34;: \u0026#34;web\u0026#34;, \u0026#34;serviceVersion\u0026#34;: \u0026#34;v0.0.1\u0026#34;, \u0026#34;pagePath\u0026#34;: \u0026#34;/index.html\u0026#34;, \u0026#34;category\u0026#34;: \u0026#34;ajax\u0026#34;, \u0026#34;message\u0026#34;: \u0026#34;error\u0026#34;, \u0026#34;line\u0026#34;: 1, \u0026#34;col\u0026#34;: 1, \u0026#34;stack\u0026#34;: \u0026#34;error\u0026#34;, \u0026#34;errorUrl\u0026#34;: \u0026#34;/index.html\u0026#34; } ] OutPut:\nHttp Status: 204\nPOST http://localhost:12800/browser/errorLog Send a single error log object in JSON format.\nInput:\n{ \u0026#34;uniqueId\u0026#34;: \u0026#34;55ec6178-3fb7-43ef-899c-a26944407b01\u0026#34;, \u0026#34;service\u0026#34;: \u0026#34;web\u0026#34;, \u0026#34;serviceVersion\u0026#34;: \u0026#34;v0.0.1\u0026#34;, \u0026#34;pagePath\u0026#34;: \u0026#34;/index.html\u0026#34;, \u0026#34;category\u0026#34;: \u0026#34;ajax\u0026#34;, \u0026#34;message\u0026#34;: \u0026#34;error\u0026#34;, \u0026#34;line\u0026#34;: 1, \u0026#34;col\u0026#34;: 1, \u0026#34;stack\u0026#34;: \u0026#34;error\u0026#34;, \u0026#34;errorUrl\u0026#34;: \u0026#34;/index.html\u0026#34; } OutPut:\nHttp Status: 204\n","excerpt":"HTTP API Protocol HTTP API Protocol defines the API data format, including API request and response …","ref":"/docs/main/v8.6.0/en/protocols/browser-http-api-protocol/","title":"HTTP API Protocol"},{"body":"HTTP API Protocol HTTP API Protocol defines the API data format, including API request and response data format. They use the HTTP1.1 wrapper of the official SkyWalking Trace Data Protocol v3. Read it for more details.\nInstance Management Detailed information about data format can be found in Instance Management.\n Report service instance properties   POST http://localhost:12800/v3/management/reportProperties\n Input:\n{ \u0026#34;service\u0026#34;: \u0026#34;User Service Name\u0026#34;, \u0026#34;serviceInstance\u0026#34;: \u0026#34;User Service Instance Name\u0026#34;, \u0026#34;properties\u0026#34;: [{ \u0026#34;language\u0026#34;: \u0026#34;Lua\u0026#34; }] } Output JSON Array:\n{}  Service instance ping   POST http://localhost:12800/v3/management/keepAlive\n Input:\n{ \u0026#34;service\u0026#34;: \u0026#34;User Service Name\u0026#34;, \u0026#34;serviceInstance\u0026#34;: \u0026#34;User Service Instance Name\u0026#34; } OutPut:\n{} Trace Report Detailed information about data format can be found in Instance Management. There are two ways to report segment data: one segment per request or segment array in bulk mode.\nPOST http://localhost:12800/v3/segment Send a single segment object in JSON format.\nInput:\n{ \u0026#34;traceId\u0026#34;: \u0026#34;a12ff60b-5807-463b-a1f8-fb1c8608219e\u0026#34;, \u0026#34;serviceInstance\u0026#34;: \u0026#34;User_Service_Instance_Name\u0026#34;, \u0026#34;spans\u0026#34;: [{ \u0026#34;operationName\u0026#34;: \u0026#34;/ingress\u0026#34;, \u0026#34;startTime\u0026#34;: 1588664577013, \u0026#34;endTime\u0026#34;: 1588664577028, \u0026#34;spanType\u0026#34;: \u0026#34;Exit\u0026#34;, \u0026#34;spanId\u0026#34;: 1, \u0026#34;isError\u0026#34;: false, \u0026#34;parentSpanId\u0026#34;: 0, \u0026#34;componentId\u0026#34;: 6000, \u0026#34;peer\u0026#34;: \u0026#34;upstream service\u0026#34;, \u0026#34;spanLayer\u0026#34;: \u0026#34;Http\u0026#34; }, { \u0026#34;operationName\u0026#34;: \u0026#34;/ingress\u0026#34;, \u0026#34;startTime\u0026#34;: 1588664577013, \u0026#34;tags\u0026#34;: [{ \u0026#34;key\u0026#34;: \u0026#34;http.method\u0026#34;, \u0026#34;value\u0026#34;: \u0026#34;GET\u0026#34; }, { \u0026#34;key\u0026#34;: \u0026#34;http.params\u0026#34;, \u0026#34;value\u0026#34;: \u0026#34;http://localhost/ingress\u0026#34; }], \u0026#34;endTime\u0026#34;: 1588664577028, \u0026#34;spanType\u0026#34;: \u0026#34;Entry\u0026#34;, \u0026#34;spanId\u0026#34;: 0, \u0026#34;parentSpanId\u0026#34;: -1, \u0026#34;isError\u0026#34;: false, \u0026#34;spanLayer\u0026#34;: \u0026#34;Http\u0026#34;, \u0026#34;componentId\u0026#34;: 6000 }], \u0026#34;service\u0026#34;: \u0026#34;User_Service_Name\u0026#34;, \u0026#34;traceSegmentId\u0026#34;: \u0026#34;a12ff60b-5807-463b-a1f8-fb1c8608219e\u0026#34; } OutPut:\nPOST http://localhost:12800/v3/segments Send a segment object list in JSON format.\nInput:\n[{ \u0026#34;traceId\u0026#34;: \u0026#34;a12ff60b-5807-463b-a1f8-fb1c8608219e\u0026#34;, \u0026#34;serviceInstance\u0026#34;: \u0026#34;User_Service_Instance_Name\u0026#34;, \u0026#34;spans\u0026#34;: [{ \u0026#34;operationName\u0026#34;: \u0026#34;/ingress\u0026#34;, \u0026#34;startTime\u0026#34;: 1588664577013, \u0026#34;endTime\u0026#34;: 1588664577028, \u0026#34;spanType\u0026#34;: \u0026#34;Exit\u0026#34;, \u0026#34;spanId\u0026#34;: 1, \u0026#34;isError\u0026#34;: false, \u0026#34;parentSpanId\u0026#34;: 0, \u0026#34;componentId\u0026#34;: 6000, \u0026#34;peer\u0026#34;: \u0026#34;upstream service\u0026#34;, \u0026#34;spanLayer\u0026#34;: \u0026#34;Http\u0026#34; }, { \u0026#34;operationName\u0026#34;: \u0026#34;/ingress\u0026#34;, \u0026#34;startTime\u0026#34;: 1588664577013, \u0026#34;tags\u0026#34;: [{ \u0026#34;key\u0026#34;: \u0026#34;http.method\u0026#34;, \u0026#34;value\u0026#34;: \u0026#34;GET\u0026#34; }, { \u0026#34;key\u0026#34;: \u0026#34;http.params\u0026#34;, \u0026#34;value\u0026#34;: \u0026#34;http://localhost/ingress\u0026#34; }], \u0026#34;endTime\u0026#34;: 1588664577028, \u0026#34;spanType\u0026#34;: \u0026#34;Entry\u0026#34;, \u0026#34;spanId\u0026#34;: 0, \u0026#34;parentSpanId\u0026#34;: -1, \u0026#34;isError\u0026#34;: false, \u0026#34;spanLayer\u0026#34;: \u0026#34;Http\u0026#34;, \u0026#34;componentId\u0026#34;: 6000 }], \u0026#34;service\u0026#34;: \u0026#34;User_Service_Name\u0026#34;, \u0026#34;traceSegmentId\u0026#34;: \u0026#34;a12ff60b-5807-463b-a1f8-fb1c8608219e\u0026#34; }, { \u0026#34;traceId\u0026#34;: \u0026#34;f956699e-5106-4ea3-95e5-da748c55bac1\u0026#34;, \u0026#34;serviceInstance\u0026#34;: \u0026#34;User_Service_Instance_Name\u0026#34;, \u0026#34;spans\u0026#34;: [{ \u0026#34;operationName\u0026#34;: \u0026#34;/ingress\u0026#34;, \u0026#34;startTime\u0026#34;: 1588664577250, \u0026#34;endTime\u0026#34;: 1588664577250, \u0026#34;spanType\u0026#34;: \u0026#34;Exit\u0026#34;, \u0026#34;spanId\u0026#34;: 1, \u0026#34;isError\u0026#34;: false, \u0026#34;parentSpanId\u0026#34;: 0, \u0026#34;componentId\u0026#34;: 6000, \u0026#34;peer\u0026#34;: \u0026#34;upstream service\u0026#34;, \u0026#34;spanLayer\u0026#34;: \u0026#34;Http\u0026#34; }, { \u0026#34;operationName\u0026#34;: \u0026#34;/ingress\u0026#34;, \u0026#34;startTime\u0026#34;: 1588664577250, \u0026#34;tags\u0026#34;: [{ \u0026#34;key\u0026#34;: \u0026#34;http.method\u0026#34;, \u0026#34;value\u0026#34;: \u0026#34;GET\u0026#34; }, { \u0026#34;key\u0026#34;: \u0026#34;http.params\u0026#34;, \u0026#34;value\u0026#34;: \u0026#34;http://localhost/ingress\u0026#34; }], \u0026#34;endTime\u0026#34;: 1588664577250, \u0026#34;spanType\u0026#34;: \u0026#34;Entry\u0026#34;, \u0026#34;spanId\u0026#34;: 0, \u0026#34;parentSpanId\u0026#34;: -1, \u0026#34;isError\u0026#34;: false, \u0026#34;spanLayer\u0026#34;: \u0026#34;Http\u0026#34;, \u0026#34;componentId\u0026#34;: 6000 }], \u0026#34;service\u0026#34;: \u0026#34;User_Service_Name\u0026#34;, \u0026#34;traceSegmentId\u0026#34;: \u0026#34;f956699e-5106-4ea3-95e5-da748c55bac1\u0026#34; }] OutPut:\n","excerpt":"HTTP API Protocol HTTP API Protocol defines the API data format, including API request and response …","ref":"/docs/main/v8.6.0/en/protocols/http-api-protocol/","title":"HTTP API Protocol"},{"body":"IllegalStateException when installing Java agent on WebSphere This issue was found in our community discussion and feedback. A user installed the SkyWalking Java agent on WebSphere 7.0.0.11 and ibm jdk 1.8_20160719 and 1.7.0_20150407, and experienced the following error logs:\nWARN 2019-05-09 17:01:35:905 SkywalkingAgent-1-GRPCChannelManager-0 ProtectiveShieldMatcher : Byte-buddy occurs exception when match type. java.lang.IllegalStateException: Cannot resolve type description for java.security.PrivilegedAction at org.apache.skywalking.apm.dependencies.net.bytebuddy.pool.TypePool$Resolution$Illegal.resolve(TypePool.java:144) at org.apache.skywalking.apm.dependencies.net.bytebuddy.pool.TypePool$Default$WithLazyResolution$LazyTypeDescription.delegate(TypePool.java:1392) at org.apache.skywalking.apm.dependencies.net.bytebuddy.description.type.TypeDescription$AbstractBase$OfSimpleType$WithDelegation.getInterfaces(TypeDescription.java:8016) at org.apache.skywalking.apm.dependencies.net.bytebuddy.description.type.TypeDescription$Generic$OfNonGenericType.getInterfaces(TypeDescription.java:3621) at org.apache.skywalking.apm.dependencies.net.bytebuddy.matcher.HasSuperTypeMatcher.hasInterface(HasSuperTypeMatcher.java:53) at org.apache.skywalking.apm.dependencies.net.bytebuddy.matcher.HasSuperTypeMatcher.hasInterface(HasSuperTypeMatcher.java:54) at org.apache.skywalking.apm.dependencies.net.bytebuddy.matcher.HasSuperTypeMatcher.matches(HasSuperTypeMatcher.java:38) at org.apache.skywalking.apm.dependencies.net.bytebuddy.matcher.HasSuperTypeMatcher.matches(HasSuperTypeMatcher.java:15) at org.apache.skywalking.apm.dependencies.net.bytebuddy.matcher.ElementMatcher$Junction$Conjunction.matches(ElementMatcher.java:107) at org.apache.skywalking.apm.dependencies.net.bytebuddy.matcher.ElementMatcher$Junction$Disjunction.matches(ElementMatcher.java:147) at org.apache.skywalking.apm.dependencies.net.bytebuddy.matcher.ElementMatcher$Junction$Disjunction.matches(ElementMatcher.java:147) at org.apache.skywalking.apm.dependencies.net.bytebuddy.matcher.ElementMatcher$Junction$Disjunction.matches(ElementMatcher.java:147) at org.apache.skywalking.apm.dependencies.net.bytebuddy.matcher.ElementMatcher$Junction$Disjunction.matches(ElementMatcher.java:147) at org.apache.skywalking.apm.dependencies.net.bytebuddy.matcher.ElementMatcher$Junction$Disjunction.matches(ElementMatcher.java:147) ... The exception occured because access grant was required in WebSphere. Simply follow these steps:\n Set the agent\u0026rsquo;s owner to the owner of WebSphere. Add \u0026ldquo;grant codeBase \u0026ldquo;file:${agent_dir}/-\u0026rdquo; { permission java.security.AllPermission; };\u0026rdquo; in the file of \u0026ldquo;server.policy\u0026rdquo;.  ","excerpt":"IllegalStateException when installing Java agent on WebSphere This issue was found in our community …","ref":"/docs/main/v8.6.0/en/faq/install_agent_on_websphere/","title":"IllegalStateException when installing Java agent on WebSphere"},{"body":"Init mode The SkyWalking backend supports multiple storage implementors. Most of them would automatically initialize the storage, such as Elastic Search or Database, when the backend starts up at first.\nBut there may be some unexpected events that may occur with the storage, such as When multiple Elastic Search indexes are created concurrently, these backend instances would start up at the same time., When there is a change, the APIs of Elastic Search would be blocked without reporting any exception. This often happens on container management platforms, such as k8s.\nThis is where you need the Init mode startup.\nSolution Only one single instance should run in the Init mode before other instances start up. And this instance will exit graciously after all initialization steps are done.\nUse oapServiceInit.sh/oapServiceInit.bat to start up backend. You should see the following logs:\n 2018-11-09 23:04:39,465 - org.apache.skywalking.oap.server.starter.OAPServerStartUp -2214 [main] INFO [] - OAP starts up in init mode successfully, exit now\u0026hellip;\n Kubernetes Initialization in this mode would be included in our Kubernetes scripts and Helm.\n","excerpt":"Init mode The SkyWalking backend supports multiple storage implementors. Most of them would …","ref":"/docs/main/v8.6.0/en/setup/backend/backend-init-mode/","title":"Init mode"},{"body":"IP and port setting The backend uses IP and port binding in order to allow the OS to have multiple IPs. The binding/listening IP and port are specified by the core module\ncore: default: restHost: 0.0.0.0 restPort: 12800 restContextPath: / gRPCHost: 0.0.0.0 gRPCPort: 11800 There are two IP/port pairs for gRPC and HTTP REST services.\n Most agents and probes use gRPC service for better performance and code readability. Some agents use REST service, because gRPC may be not supported in that language. The UI uses REST service, but the data is always in GraphQL format.  Note IP binding For users who are not familiar with IP binding, note that once IP binding is complete, the client could only use this IP to access the service. For example, if 172.09.13.28 is bound, even if you are in this machine, you must use 172.09.13.28, rather than 127.0.0.1 or localhost, to access the service.\nModule provider specified IP and port The IP and port in the core module are provided by default. But it is common for some module providers, such as receiver modules, to provide other IP and port settings.\n","excerpt":"IP and port setting The backend uses IP and port binding in order to allow the OS to have multiple …","ref":"/docs/main/v8.6.0/en/setup/backend/backend-ip-port/","title":"IP and port setting"},{"body":"JVM Metrics Service Abstract Uplink the JVM metrics, including PermSize, HeapSize, CPU, Memory, etc., every second.\ngRPC service define\n","excerpt":"JVM Metrics Service Abstract Uplink the JVM metrics, including PermSize, HeapSize, CPU, Memory, …","ref":"/docs/main/v8.6.0/en/protocols/jvm-protocol/","title":"JVM Metrics Service"},{"body":"Language agents in Service   Java agent. Introduces how to install java agent to your service, without any impact in your code.\n  LUA agent. Introduce how to install the lua agent in Nginx + LUA module or OpenResty.\n  Python Agent. Introduce how to install the Python Agent in a Python service.\n  Node.js agent. Introduce how to install the NodeJS Agent in a NodeJS service.\n  The following agents and SDKs are compatible with the SkyWalking\u0026rsquo;s data formats and network protocols, but are maintained by 3rd-parties. You can go to their project repositories for additional info about guides and releases.\n  SkyAPM .NET Core agent. See .NET Core agent project document for more details.\n  SkyAPM Node.js agent. See Node.js server side agent project document for more details.\n  SkyAPM PHP agent. See PHP agent project document for more details.\n  SkyAPM Go SDK. See go2sky project document for more details.\n  SkyAPM C++ SDK. See cpp2sky project document for more details.\n  ","excerpt":"Language agents in Service   Java agent. Introduces how to install java agent to your service, …","ref":"/docs/main/v8.6.0/en/setup/service-agent/server-agents/","title":"Language agents in Service"},{"body":"Locate agent config file by system property Supported version 5.0.0-RC+\nWhat is Locate agent config file by system property ？ In Default. The agent will try to locate agent.config, which should be in the /config dictionary of agent package. If User sets the specified agent config file through system properties, The agent will try to load file from there. By the way, This function has no conflict with Setting Override\nOverride priority The specified agent config \u0026gt; The default agent config\nHow to use The content formats of the specified config must be same as the default config.\nUsing System.Properties(-D) to set the specified config path\n-Dskywalking_config=/path/to/agent.config /path/to/agent.config is the absolute path of the specified config file\n","excerpt":"Locate agent config file by system property Supported version 5.0.0-RC+\nWhat is Locate agent config …","ref":"/docs/main/v8.6.0/en/setup/service-agent/java-agent/specified-agent-config/","title":"Locate agent config file by system property"},{"body":"Log Analysis Language Log Analysis Language (LAL) in SkyWalking is essentially a Domain-Specific Language (DSL) to analyze logs. You can use LAL to parse, extract, and save the logs, as well as collaborate the logs with traces (by extracting the trace ID, segment ID and span ID) and metrics (by generating metrics from the logs and sending them to the meter system).\nThe LAL config files are in YAML format, and are located under directory lal. You can set log-analyzer/default/lalFiles in the application.yml file or set environment variable SW_LOG_LAL_FILES to activate specific LAL config files.\nFilter A filter is a group of parser, extractor and sink. Users can use one or more filters to organize their processing logic. Every piece of log will be sent to all filters in an LAL rule. A piece of log sent to the filter is available as property log in the LAL, therefore you can access the log service name via log.service. For all available fields of log, please refer to the protocol definition.\nAll components are executed sequentially in the orders they are declared.\nGlobal Functions Globally available functions may be used them in all components (i.e. parsers, extractors, and sinks) where necessary.\n abort  By default, all components declared are executed no matter what flags (dropped, saved, etc.) have been set. There are cases where you may want the filter chain to stop earlier when specified conditions are met. abort function aborts the remaining filter chain from where it\u0026rsquo;s declared, and all the remaining components won\u0026rsquo;t be executed at all. abort function serves as a fast-fail mechanism in LAL.\nfilter { if (log.service == \u0026#34;TestingService\u0026#34;) { // Don\u0026#39;t waste resources on TestingServices  abort {} // all remaining components won\u0026#39;t be executed at all  } // ... parsers, extractors, sinks } Note that when you put regexp in an if statement, you need to surround the expression with () like regexp(\u0026lt;the expression\u0026gt;), instead of regexp \u0026lt;the expression\u0026gt;.\nParser Parsers are responsible for parsing the raw logs into structured data in SkyWalking for further processing. There are 3 types of parsers at the moment, namely json, yaml, and text.\nWhen a piece of log is parsed, there is a corresponding property available, called parsed, injected by LAL. Property parsed is typically a map, containing all the fields parsed from the raw logs. For example, if the parser is json / yaml, parsed is a map containing all the key-values in the json / yaml; if the parser is text , parsed is a map containing all the captured groups and their values (for regexp and grok).\nAll parsers share the following options:\n   Option Type Description Default Value     abortOnFailure boolean Whether the filter chain should abort if the parser failed to parse / match the logs true    See examples below.\njson filter { json { abortOnFailure true // this is optional because it\u0026#39;s default behaviour  } } yaml filter { yaml { abortOnFailure true // this is optional because it\u0026#39;s default behaviour  } } text For unstructured logs, there are some text parsers for use.\n regexp  regexp parser uses a regular expression (regexp) to parse the logs. It leverages the captured groups of the regexp, all the captured groups can be used later in the extractors or sinks. regexp returns a boolean indicating whether the log matches the pattern or not.\nfilter { text { abortOnFailure true // this is optional because it\u0026#39;s default behaviour  // this is just a demo pattern  regexp \u0026#34;(?\u0026lt;timestamp\u0026gt;\\\\d{8}) (?\u0026lt;thread\u0026gt;\\\\w+) (?\u0026lt;level\u0026gt;\\\\w+) (?\u0026lt;traceId\u0026gt;\\\\w+) (?\u0026lt;msg\u0026gt;.+)\u0026#34; } extractor { tag level: parsed.level // we add a tag called `level` and its value is parsed.level, captured from the regexp above  traceId parsed.traceId // we also extract the trace id from the parsed result, which will be used to associate the log with the trace  } // ... }  grok (TODO)  We\u0026rsquo;re aware of certains performance issues in the grok Java library, and so we\u0026rsquo;re currently conducting investigations and benchmarking. Contributions are welcome.\nExtractor Extractors aim to extract metadata from the logs. The metadata can be a service name, a service instance name, an endpoint name, or even a trace ID, all of which can be associated with the existing traces and metrics.\n service  service extracts the service name from the parsed result, and set it into the LogData, which will be persisted (if not dropped) and is used to associate with traces / metrics.\n instance  instance extracts the service instance name from the parsed result, and set it into the LogData, which will be persisted (if not dropped) and is used to associate with traces / metrics.\n endpoint  endpoint extracts the service instance name from the parsed result, and set it into the LogData, which will be persisted (if not dropped) and is used to associate with traces / metrics.\n traceId  traceId extracts the trace ID from the parsed result, and set it into the LogData, which will be persisted (if not dropped) and is used to associate with traces / metrics.\n segmentId  segmentId extracts the segment ID from the parsed result, and set it into the LogData, which will be persisted (if not dropped) and is used to associate with traces / metrics.\n spanId  spanId extracts the span ID from the parsed result, and set it into the LogData, which will be persisted (if not dropped) and is used to associate with traces / metrics.\n timestamp  timestamp extracts the timestamp from the parsed result, and set it into the LogData, which will be persisted (if not dropped) and is used to associate with traces / metrics.\nThe unit of timestamp is millisecond.\n tag  tag extracts the tags from the parsed result, and set them into the LogData. The form of this extractor should look something like this: tag key1: value, key2: value2. You may use the properties of parsed as both keys and values.\nfilter { // ... parser  extractor { tag level: parsed.level, (parsed.statusCode): parsed.statusMsg tag anotherKey: \u0026#34;anotherConstantValue\u0026#34; } }  metrics  metrics extracts / generates metrics from the logs, and sends the generated metrics to the meter system. You may configure MAL for further analysis of these metrics. The dedicated MAL config files are under directory log-mal-rules, and you can set log-analyzer/default/malFiles to enable configured files.\n# application.yml # ... log-analyzer: selector: ${SW_LOG_ANALYZER:default} default: lalFiles: ${SW_LOG_LAL_FILES:my-lal-config} # files are under \u0026#34;lal\u0026#34; directory malFiles: ${SW_LOG_MAL_FILES:my-lal-mal-config,another-lal-mal-config} # files are under \u0026#34;log-mal-rules\u0026#34; directory Examples are as follows:\nfilter { // ...  extractor { service parsed.serviceName metrics { name \u0026#34;log_count\u0026#34; timestamp parsed.timestamp labels level: parsed.level, service: parsed.service, instance: parsed.instance value 1 } metrics { name \u0026#34;http_response_time\u0026#34; timestamp parsed.timestamp labels status_code: parsed.statusCode, service: parsed.service, instance: parsed.instance value parsed.duration } } // ... } The extractor above generates a metrics named log_count, with tag key level and value 1. After that, you can configure MAL rules to calculate the log count grouping by logging level like this:\n# ... other configurations of MAL metrics: - name: log_count_debug exp: log_count.tagEqual(\u0026#39;level\u0026#39;, \u0026#39;DEBUG\u0026#39;).sum([\u0026#39;service\u0026#39;, \u0026#39;instance\u0026#39;]).increase(\u0026#39;PT1M\u0026#39;) - name: log_count_error exp: log_count.tagEqual(\u0026#39;level\u0026#39;, \u0026#39;ERROR\u0026#39;).sum([\u0026#39;service\u0026#39;, \u0026#39;instance\u0026#39;]).increase(\u0026#39;PT1M\u0026#39;) The other metrics generated is http_response_time, so you can configure MAL rules to generate more useful metrics like percentiles.\n# ... other configurations of MAL metrics: - name: response_time_percentile exp: http_response_time.sum([\u0026#39;le\u0026#39;, \u0026#39;service\u0026#39;, \u0026#39;instance\u0026#39;]).increase(\u0026#39;PT5M\u0026#39;).histogram().histogram_percentile([50,70,90,99]) Sink Sinks are the persistent layer of the LAL. By default, all the logs of each filter are persisted into the storage. However, some mechanisms allow you to selectively save some logs, or even drop all the logs after you\u0026rsquo;ve extracted useful information, such as metrics.\nSampler Sampler allows you to save the logs in a sampling manner. Currently, the sampling strategy rateLimit is supported. We welcome contributions on more sampling strategies. If multiple samplers are specified, the last one determines the final sampling result. See examples in Enforcer.\nrateLimit samples n logs at a maximum rate of 1 second. rateLimit(\u0026quot;SamplerID\u0026quot;) requires an ID for the sampler. Sampler declarations with the same ID share the same sampler instance, thus sharing the same qps and resetting logic.\nExamples:\nfilter { // ... parser  sink { sampler { if (parsed.service == \u0026#34;ImportantApp\u0026#34;) { rateLimit(\u0026#34;ImportantAppSampler\u0026#34;) { qps 30 // samples 30 pieces of logs every second for service \u0026#34;ImportantApp\u0026#34;  } } else { rateLimit(\u0026#34;OtherSampler\u0026#34;) { qps 3 // samples 3 pieces of logs every second for other services than \u0026#34;ImportantApp\u0026#34;  } } } } } Dropper Dropper is a special sink, meaning that all logs are dropped without any exception. This is useful when you want to drop debugging logs.\nfilter { // ... parser  sink { if (parsed.level == \u0026#34;DEBUG\u0026#34;) { dropper {} } else { sampler { // ... configs  } } } } Or if you have multiple filters, some of which are for extracting metrics, only one of them has to be persisted.\nfilter { // filter A: this is for persistence  // ... parser  sink { sampler { // .. sampler configs  } } } filter { // filter B:  // ... extractors to generate many metrics  extractors { metrics { // ... metrics  } } sink { dropper {} // drop all logs because they have been saved in \u0026#34;filter A\u0026#34; above.  } } Enforcer Enforcer is another special sink that forcibly samples the log. A typical use case of enforcer is when you have configured a sampler and want to save some logs forcibly, such as to save error logs even if the sampling mechanism has been configured.\nfilter { // ... parser  sink { sampler { // ... sampler configs  } if (parserd.level == \u0026#34;ERROR\u0026#34; || parsed.userId == \u0026#34;TestingUserId\u0026#34;) { // sample error logs or testing users\u0026#39; logs (userId == \u0026#34;TestingUserId\u0026#34;) even if the sampling strategy is configured  enforcer { } } } } You can use enforcer and dropper to simulate a probabilistic sampler like this.\nfilter { // ... parser  sink { sampler { // simulate a probabilistic sampler with sampler rate 30% (not accurate though)  if (Math.abs(Math.random()) \u0026gt; 0.3) { enforcer {} } else { dropper {} } } } } ","excerpt":"Log Analysis Language Log Analysis Language (LAL) in SkyWalking is essentially a Domain-Specific …","ref":"/docs/main/v8.6.0/en/concepts-and-designs/lal/","title":"Log Analysis Language"},{"body":"Log Collecting And Analysis Collecting There are various ways to collect logs from application.\nLog files collector You can use Filebeat 、Fluentd to collect file logs including to use Kafka MQ to transport native-json format logs. When use this, need to open kafka-fetcher and enable configs enableNativeJsonLog.\nCollector config examples:\n filebeat.yml fluentd.conf  Java agent\u0026rsquo;s toolkits Java agent provides toolkit for log4j, log4j2, logback to report logs through gRPC with automatic injected trace context.\nSkyWalking Satellite sidecar is a recommended proxy/side to forward logs including to use Kafka MQ to transport logs. When use this, need to open kafka-fetcher and enable configs enableNativeProtoLog.\nJava agent provides toolkit for log4j, log4j2, logback to report logs through files with automatic injected trace context.\nLog framework config examples:\n log4j1.x fileAppender log4j2.x fileAppender logback fileAppender  Log Analyzer Log analyzer of OAP server supports native log data. OAP could use Log Analysis Language to structurize log content through parse, extract, and save logs. Also the analyzer leverages Meter Analysis Language Engine for further metrics calculation.\nlog-analyzer: selector: ${SW_LOG_ANALYZER:default} default: lalFiles: ${SW_LOG_LAL_FILES:default} malFiles: ${SW_LOG_MAL_FILES:\u0026#34;\u0026#34;} Read Log Analysis Language documentation to learn log structurize and metrics analysis.\n","excerpt":"Log Collecting And Analysis Collecting There are various ways to collect logs from application.\nLog …","ref":"/docs/main/v8.6.0/en/setup/backend/log-analyzer/","title":"Log Collecting And Analysis"},{"body":"Log Data Protocol Report log data via protocol.\nNative Proto Protocol Report native-proto format log via gRPC.\ngRPC service define\nNative Kafka Protocol Report native-json format log via kafka.\nJson log record example:\n{ \u0026#34;timestamp\u0026#34;:1618161813371, \u0026#34;service\u0026#34;:\u0026#34;Your_ApplicationName\u0026#34;, \u0026#34;serviceInstance\u0026#34;:\u0026#34;3a5b8da5a5ba40c0b192e91b5c80f1a8@192.168.1.8\u0026#34;, \u0026#34;traceContext\u0026#34;:{ \u0026#34;traceId\u0026#34;:\u0026#34;ddd92f52207c468e9cd03ddd107cd530.69.16181331190470001\u0026#34;, \u0026#34;spanId\u0026#34;:\u0026#34;0\u0026#34;, \u0026#34;traceSegmentId\u0026#34;:\u0026#34;ddd92f52207c468e9cd03ddd107cd530.69.16181331190470000\u0026#34; }, \u0026#34;tags\u0026#34;:{ \u0026#34;data\u0026#34;:[ { \u0026#34;key\u0026#34;:\u0026#34;level\u0026#34;, \u0026#34;value\u0026#34;:\u0026#34;INFO\u0026#34; }, { \u0026#34;key\u0026#34;:\u0026#34;logger\u0026#34;, \u0026#34;value\u0026#34;:\u0026#34;com.example.MyLogger\u0026#34; } ] }, \u0026#34;body\u0026#34;:{ \u0026#34;text\u0026#34;:{ \u0026#34;text\u0026#34;:\u0026#34;log message\u0026#34; } } } HTTP API Report json format logs via HTTP API, the endpoint is http://:12800/v3/logs.\nJson log record example:\n[ { \u0026#34;timestamp\u0026#34;: 1618161813371, \u0026#34;service\u0026#34;: \u0026#34;Your_ApplicationName\u0026#34;, \u0026#34;serviceInstance\u0026#34;: \u0026#34;3a5b8da5a5ba40c0b192e91b5c80f1a8@192.168.1.8\u0026#34;, \u0026#34;traceContext\u0026#34;: { \u0026#34;traceId\u0026#34;: \u0026#34;ddd92f52207c468e9cd03ddd107cd530.69.16181331190470001\u0026#34;, \u0026#34;spanId\u0026#34;: \u0026#34;0\u0026#34;, \u0026#34;traceSegmentId\u0026#34;: \u0026#34;ddd92f52207c468e9cd03ddd107cd530.69.16181331190470000\u0026#34; }, \u0026#34;tags\u0026#34;: { \u0026#34;data\u0026#34;: [ { \u0026#34;key\u0026#34;: \u0026#34;level\u0026#34;, \u0026#34;value\u0026#34;: \u0026#34;INFO\u0026#34; }, { \u0026#34;key\u0026#34;: \u0026#34;logger\u0026#34;, \u0026#34;value\u0026#34;: \u0026#34;com.example.MyLogger\u0026#34; } ] }, \u0026#34;body\u0026#34;: { \u0026#34;text\u0026#34;: { \u0026#34;text\u0026#34;: \u0026#34;log message\u0026#34; } } } ] ","excerpt":"Log Data Protocol Report log data via protocol.\nNative Proto Protocol Report native-proto format log …","ref":"/docs/main/v8.6.0/en/protocols/log-data-protocol/","title":"Log Data Protocol"},{"body":"logback plugin  Dependency the toolkit, such as using maven or gradle  \u0026lt;dependency\u0026gt; \u0026lt;groupId\u0026gt;org.apache.skywalking\u0026lt;/groupId\u0026gt; \u0026lt;artifactId\u0026gt;apm-toolkit-logback-1.x\u0026lt;/artifactId\u0026gt; \u0026lt;version\u0026gt;{project.release.version}\u0026lt;/version\u0026gt; \u0026lt;/dependency\u0026gt; Print trace ID in your logs  set %tid in Pattern section of logback.xml  \u0026lt;appender name=\u0026#34;STDOUT\u0026#34; class=\u0026#34;ch.qos.logback.core.ConsoleAppender\u0026#34;\u0026gt; \u0026lt;encoder class=\u0026#34;ch.qos.logback.core.encoder.LayoutWrappingEncoder\u0026#34;\u0026gt; \u0026lt;layout class=\u0026#34;org.apache.skywalking.apm.toolkit.log.logback.v1.x.TraceIdPatternLogbackLayout\u0026#34;\u0026gt; \u0026lt;Pattern\u0026gt;%d{yyyy-MM-dd HH:mm:ss.SSS} [%tid] [%thread] %-5level %logger{36} -%msg%n\u0026lt;/Pattern\u0026gt; \u0026lt;/layout\u0026gt; \u0026lt;/encoder\u0026gt; \u0026lt;/appender\u0026gt;  with the MDC, set %X{tid} in Pattern section of logback.xml  \u0026lt;appender name=\u0026#34;STDOUT\u0026#34; class=\u0026#34;ch.qos.logback.core.ConsoleAppender\u0026#34;\u0026gt; \u0026lt;encoder class=\u0026#34;ch.qos.logback.core.encoder.LayoutWrappingEncoder\u0026#34;\u0026gt; \u0026lt;layout class=\u0026#34;org.apache.skywalking.apm.toolkit.log.logback.v1.x.mdc.TraceIdMDCPatternLogbackLayout\u0026#34;\u0026gt; \u0026lt;Pattern\u0026gt;%d{yyyy-MM-dd HH:mm:ss.SSS} [%X{tid}] [%thread] %-5level %logger{36} -%msg%n\u0026lt;/Pattern\u0026gt; \u0026lt;/layout\u0026gt; \u0026lt;/encoder\u0026gt; \u0026lt;/appender\u0026gt;  Support logback AsyncAppender(MDC also support), No additional configuration is required. Refer to the demo of logback.xml below. For details: Logback AsyncAppender  \u0026lt;configuration scan=\u0026#34;true\u0026#34; scanPeriod=\u0026#34; 5 seconds\u0026#34;\u0026gt; \u0026lt;appender name=\u0026#34;STDOUT\u0026#34; class=\u0026#34;ch.qos.logback.core.ConsoleAppender\u0026#34;\u0026gt; \u0026lt;encoder class=\u0026#34;ch.qos.logback.core.encoder.LayoutWrappingEncoder\u0026#34;\u0026gt; \u0026lt;layout class=\u0026#34;org.apache.skywalking.apm.toolkit.log.logback.v1.x.mdc.TraceIdMDCPatternLogbackLayout\u0026#34;\u0026gt; \u0026lt;Pattern\u0026gt;%d{yyyy-MM-dd HH:mm:ss.SSS} [%X{tid}] [%thread] %-5level %logger{36} -%msg%n\u0026lt;/Pattern\u0026gt; \u0026lt;/layout\u0026gt; \u0026lt;/encoder\u0026gt; \u0026lt;/appender\u0026gt; \u0026lt;appender name=\u0026#34;ASYNC\u0026#34; class=\u0026#34;ch.qos.logback.classic.AsyncAppender\u0026#34;\u0026gt; \u0026lt;discardingThreshold\u0026gt;0\u0026lt;/discardingThreshold\u0026gt; \u0026lt;queueSize\u0026gt;1024\u0026lt;/queueSize\u0026gt; \u0026lt;neverBlock\u0026gt;true\u0026lt;/neverBlock\u0026gt; \u0026lt;appender-ref ref=\u0026#34;STDOUT\u0026#34;/\u0026gt; \u0026lt;/appender\u0026gt; \u0026lt;root level=\u0026#34;INFO\u0026#34;\u0026gt; \u0026lt;appender-ref ref=\u0026#34;ASYNC\u0026#34;/\u0026gt; \u0026lt;/root\u0026gt; \u0026lt;/configuration\u0026gt;  When you use -javaagent to active the SkyWalking tracer, logback will output traceId, if it existed. If the tracer is inactive, the output will be TID: N/A.  Print SkyWalking context in your logs   Your only need to replace pattern %tid or %X{tid]} with %sw_ctx or %X{sw_ctx}.\n  When you use -javaagent to active the SkyWalking tracer, logback will output SW_CTX: [$serviceName,$instanceName,$traceId,$traceSegmentId,$spanId], if it existed. If the tracer is inactive, the output will be SW_CTX: N/A.\n  logstash logback plugin  Dependency the toolkit, such as using maven or gradle  \u0026lt;dependency\u0026gt; \u0026lt;groupId\u0026gt;org.apache.skywalking\u0026lt;/groupId\u0026gt; \u0026lt;artifactId\u0026gt;apm-toolkit-logback-1.x\u0026lt;/artifactId\u0026gt; \u0026lt;version\u0026gt;${skywalking.version}\u0026lt;/version\u0026gt; \u0026lt;/dependency\u0026gt;  set LogstashEncoder of logback.xml  \u0026lt;encoder charset=\u0026#34;UTF-8\u0026#34; class=\u0026#34;net.logstash.logback.encoder.LogstashEncoder\u0026#34;\u0026gt; \u0026lt;!-- add TID(traceId) field --\u0026gt; \u0026lt;provider class=\u0026#34;org.apache.skywalking.apm.toolkit.log.logback.v1.x.logstash.TraceIdJsonProvider\u0026#34;\u0026gt; \u0026lt;/provider\u0026gt; \u0026lt;!-- add SW_CTX(SkyWalking context) field --\u0026gt; \u0026lt;provider class=\u0026#34;org.apache.skywalking.apm.toolkit.log.logback.v1.x.logstash.SkyWalkingContextJsonProvider\u0026#34;\u0026gt; \u0026lt;/provider\u0026gt; \u0026lt;/encoder\u0026gt;  set LoggingEventCompositeJsonEncoder of logstash in logback-spring.xml for custom json format  1.add converter for %tid or %sw_ctx as child of  node\n\u0026lt;!-- add converter for %tid --\u0026gt; \u0026lt;conversionRule conversionWord=\u0026#34;tid\u0026#34; converterClass=\u0026#34;org.apache.skywalking.apm.toolkit.log.logback.v1.x.LogbackPatternConverter\u0026#34;/\u0026gt; \u0026lt;!-- add converter for %sw_ctx --\u0026gt; \u0026lt;conversionRule conversionWord=\u0026#34;sw_ctx\u0026#34; converterClass=\u0026#34;org.apache.skywalking.apm.toolkit.log.logback.v1.x.LogbackSkyWalkingContextPatternConverter\u0026#34;/\u0026gt; 2.add json encoder for custom json format\n\u0026lt;encoder class=\u0026#34;net.logstash.logback.encoder.LoggingEventCompositeJsonEncoder\u0026#34;\u0026gt; \u0026lt;providers\u0026gt; \u0026lt;timestamp\u0026gt; \u0026lt;timeZone\u0026gt;UTC\u0026lt;/timeZone\u0026gt; \u0026lt;/timestamp\u0026gt; \u0026lt;pattern\u0026gt; \u0026lt;pattern\u0026gt; { \u0026#34;level\u0026#34;: \u0026#34;%level\u0026#34;, \u0026#34;tid\u0026#34;: \u0026#34;%tid\u0026#34;, \u0026#34;skyWalkingContext\u0026#34;: \u0026#34;%sw_ctx\u0026#34;, \u0026#34;thread\u0026#34;: \u0026#34;%thread\u0026#34;, \u0026#34;class\u0026#34;: \u0026#34;%logger{1.}:%L\u0026#34;, \u0026#34;message\u0026#34;: \u0026#34;%message\u0026#34;, \u0026#34;stackTrace\u0026#34;: \u0026#34;%exception{10}\u0026#34; } \u0026lt;/pattern\u0026gt; \u0026lt;/pattern\u0026gt; \u0026lt;/providers\u0026gt; \u0026lt;/encoder\u0026gt; gRPC reporter The gRPC reporter could forward the collected logs to SkyWalking OAP server, or SkyWalking Satellite sidecar. Trace id, segment id, and span id will attach to logs automatically. There is no need to modify existing layouts.\n Add GRPCLogClientAppender in logback.xml  \u0026lt;appender name=\u0026#34;grpc-log\u0026#34; class=\u0026#34;org.apache.skywalking.apm.toolkit.log.logback.v1.x.log.GRPCLogClientAppender\u0026#34;\u0026gt; \u0026lt;encoder class=\u0026#34;ch.qos.logback.core.encoder.LayoutWrappingEncoder\u0026#34;\u0026gt; \u0026lt;layout class=\u0026#34;org.apache.skywalking.apm.toolkit.log.logback.v1.x.mdc.TraceIdMDCPatternLogbackLayout\u0026#34;\u0026gt; \u0026lt;Pattern\u0026gt;%d{yyyy-MM-dd HH:mm:ss.SSS} [%X{tid}] [%thread] %-5level %logger{36} -%msg%n\u0026lt;/Pattern\u0026gt; \u0026lt;/layout\u0026gt; \u0026lt;/encoder\u0026gt; \u0026lt;/appender\u0026gt;  Add config of the plugin or use default  plugin.toolkit.log.grpc.reporter.server_host=${SW_GRPC_LOG_SERVER_HOST:127.0.0.1} plugin.toolkit.log.grpc.reporter.server_port=${SW_GRPC_LOG_SERVER_PORT:11800} plugin.toolkit.log.grpc.reporter.max_message_size=${SW_GRPC_LOG_MAX_MESSAGE_SIZE:10485760} plugin.toolkit.log.grpc.reporter.upstream_timeout=${SW_GRPC_LOG_GRPC_UPSTREAM_TIMEOUT:30} Transmitting un-formatted messages The logback 1.x gRPC reporter supports transmitting logs as formatted or un-formatted. Transmitting formatted data is the default but can be disabled by adding the following to the agent config:\nplugin.toolkit.log.transmit_formatted=false The above will result in the content field being used for the log pattern with additional log tags of argument.0, argument.1, and so on representing each logged argument as well as an additional exception tag which is only present if a throwable is also logged.\nFor example, the following code:\nlog.info(\u0026#34;{} {} {}\u0026#34;, 1, 2, 3); Will result in:\n{ \u0026#34;content\u0026#34;: \u0026#34;{} {} {}\u0026#34;, \u0026#34;tags\u0026#34;: [ { \u0026#34;key\u0026#34;: \u0026#34;argument.0\u0026#34;, \u0026#34;value\u0026#34;: \u0026#34;1\u0026#34; }, { \u0026#34;key\u0026#34;: \u0026#34;argument.1\u0026#34;, \u0026#34;value\u0026#34;: \u0026#34;2\u0026#34; }, { \u0026#34;key\u0026#34;: \u0026#34;argument.2\u0026#34;, \u0026#34;value\u0026#34;: \u0026#34;3\u0026#34; } ] } ","excerpt":"logback plugin  Dependency the toolkit, such as using maven or gradle  \u0026lt;dependency\u0026gt; …","ref":"/docs/main/v8.6.0/en/setup/service-agent/java-agent/application-toolkit-logback-1.x/","title":"logback plugin"},{"body":"Manual instrument SDK Our incredible community has contributed to the manual instrument SDK.\n Go2Sky. Go SDK follows the SkyWalking format. C++. C++ SDK follows the SkyWalking format.  What are the SkyWalking format and the propagation protocols? See these protocols in protocols document.\nEnvoy tracer Envoy has its internal tracer implementation for SkyWalking. Read SkyWalking Tracer doc and SkyWalking tracing sandbox for more details.\n","excerpt":"Manual instrument SDK Our incredible community has contributed to the manual instrument SDK. …","ref":"/docs/main/v8.6.0/en/concepts-and-designs/manual-sdk/","title":"Manual instrument SDK"},{"body":"Meter Analysis Language The meter system provides a functional analysis language called MAL (Meter Analysis Language) that lets users analyze and aggregate meter data in the OAP streaming system. The result of an expression can either be ingested by the agent analyzer, or the OC/Prometheus analyzer.\nLanguage data type In MAL, an expression or sub-expression can evaluate to one of the following two types:\n Sample family: A set of samples (metrics) containing a range of metrics whose names are identical. Scalar: A simple numeric value that supports integer/long and floating/double.  Sample family A set of samples, which acts as the basic unit in MAL. For example:\ninstance_trace_count The sample family above may contain the following samples which are provided by external modules, such as the agent analyzer:\ninstance_trace_count{region=\u0026quot;us-west\u0026quot;,az=\u0026quot;az-1\u0026quot;} 100 instance_trace_count{region=\u0026quot;us-east\u0026quot;,az=\u0026quot;az-3\u0026quot;} 20 instance_trace_count{region=\u0026quot;asia-north\u0026quot;,az=\u0026quot;az-1\u0026quot;} 33 Tag filter MAL supports four type operations to filter samples in a sample family:\n tagEqual: Filter tags exactly equal to the string provided. tagNotEqual: Filter tags not equal to the string provided. tagMatch: Filter tags that regex-match the string provided. tagNotMatch: Filter labels that do not regex-match the string provided.  For example, this filters all instance_trace_count samples for us-west and asia-north region and az-1 az:\ninstance_trace_count.tagMatch(\u0026quot;region\u0026quot;, \u0026quot;us-west|asia-north\u0026quot;).tagEqual(\u0026quot;az\u0026quot;, \u0026quot;az-1\u0026quot;) Value filter MAL supports six type operations to filter samples in a sample family by value:\n valueEqual: Filter values exactly equal to the value provided. valueNotEqual: Filter values equal to the value provided. valueGreater: Filter values greater than the value provided. valueGreaterEqual: Filter values greater than or equal to the value provided. valueLess: Filter values less than the value provided. valueLessEqual: Filter values less than or equal to the value provided.  For example, this filters all instance_trace_count samples for values \u0026gt;= 33:\ninstance_trace_count.valueGreaterEqual(33) Tag manipulator MAL allows tag manipulators to change (i.e. add/delete/update) tags and their values.\nK8s MAL supports using the metadata of K8s to manipulate the tags and their values. This feature requires authorizing the OAP Server to access K8s\u0026rsquo;s API Server.\nretagByK8sMeta retagByK8sMeta(newLabelName, K8sRetagType, existingLabelName, namespaceLabelName). Add a new tag to the sample family based on the value of an existing label. Provide several internal converting types, including\n K8sRetagType.Pod2Service  Add a tag to the sample using service as the key, $serviceName.$namespace as the value, and according to the given value of the tag key, which represents the name of a pod.\nFor example:\ncontainer_cpu_usage_seconds_total{namespace=default, container=my-nginx, cpu=total, pod=my-nginx-5dc4865748-mbczh} 2 Expression:\ncontainer_cpu_usage_seconds_total.retagByK8sMeta('service' , K8sRetagType.Pod2Service , 'pod' , 'namespace') Output:\ncontainer_cpu_usage_seconds_total{namespace=default, container=my-nginx, cpu=total, pod=my-nginx-5dc4865748-mbczh, service='nginx-service.default'} 2 Binary operators The following binary arithmetic operators are available in MAL:\n + (addition) - (subtraction) * (multiplication) / (division)  Binary operators are defined between scalar/scalar, sampleFamily/scalar and sampleFamily/sampleFamily value pairs.\nBetween two scalars: they evaluate to another scalar that is the result of the operator being applied to both scalar operands:\n1 + 2 Between a sample family and a scalar, the operator is applied to the value of every sample in the sample family. For example:\ninstance_trace_count + 2 or\n2 + instance_trace_count results in\ninstance_trace_count{region=\u0026quot;us-west\u0026quot;,az=\u0026quot;az-1\u0026quot;} 102 // 100 + 2 instance_trace_count{region=\u0026quot;us-east\u0026quot;,az=\u0026quot;az-3\u0026quot;} 22 // 20 + 2 instance_trace_count{region=\u0026quot;asia-north\u0026quot;,az=\u0026quot;az-1\u0026quot;} 35 // 33 + 2 Between two sample families, a binary operator is applied to each sample in the sample family on the left and its matching sample in the sample family on the right. A new sample family with empty name will be generated. Only the matched tags will be reserved. Samples with no matching samples in the sample family on the right will not be found in the result.\nAnother sample family instance_trace_analysis_error_count is\ninstance_trace_analysis_error_count{region=\u0026quot;us-west\u0026quot;,az=\u0026quot;az-1\u0026quot;} 20 instance_trace_analysis_error_count{region=\u0026quot;asia-north\u0026quot;,az=\u0026quot;az-1\u0026quot;} 11 Example expression:\ninstance_trace_analysis_error_count / instance_trace_count This returns a resulting sample family containing the error rate of trace analysis. Samples with region us-west and az az-3 have no match and will not show up in the result:\n{region=\u0026quot;us-west\u0026quot;,az=\u0026quot;az-1\u0026quot;} 0.8 // 20 / 100 {region=\u0026quot;asia-north\u0026quot;,az=\u0026quot;az-1\u0026quot;} 0.3333 // 11 / 33 Aggregation Operation Sample family supports the following aggregation operations that can be used to aggregate the samples of a single sample family, resulting in a new sample family having fewer samples (sometimes having just a single sample) with aggregated values:\n sum (calculate sum over dimensions) min (select minimum over dimensions) max (select maximum over dimensions) avg (calculate the average over dimensions)  These operations can be used to aggregate overall label dimensions or preserve distinct dimensions by inputting by parameter.\n\u0026lt;aggr-op\u0026gt;(by: \u0026lt;tag1, tag2, ...\u0026gt;) Example expression:\ninstance_trace_count.sum(by: ['az']) will output the following result:\ninstance_trace_count{az=\u0026quot;az-1\u0026quot;} 133 // 100 + 33 instance_trace_count{az=\u0026quot;az-3\u0026quot;} 20 Function Duraton is a textual representation of a time range. The formats accepted are based on the ISO-8601 duration format {@code PnDTnHnMn.nS} where a day is regarded as exactly 24 hours.\nExamples:\n \u0026ldquo;PT20.345S\u0026rdquo; \u0026ndash; parses as \u0026ldquo;20.345 seconds\u0026rdquo; \u0026ldquo;PT15M\u0026rdquo; \u0026ndash; parses as \u0026ldquo;15 minutes\u0026rdquo; (where a minute is 60 seconds) \u0026ldquo;PT10H\u0026rdquo; \u0026ndash; parses as \u0026ldquo;10 hours\u0026rdquo; (where an hour is 3600 seconds) \u0026ldquo;P2D\u0026rdquo; \u0026ndash; parses as \u0026ldquo;2 days\u0026rdquo; (where a day is 24 hours or 86400 seconds) \u0026ldquo;P2DT3H4M\u0026rdquo; \u0026ndash; parses as \u0026ldquo;2 days, 3 hours and 4 minutes\u0026rdquo; \u0026ldquo;P-6H3M\u0026rdquo; \u0026ndash; parses as \u0026ldquo;-6 hours and +3 minutes\u0026rdquo; \u0026ldquo;-P6H3M\u0026rdquo; \u0026ndash; parses as \u0026ldquo;-6 hours and -3 minutes\u0026rdquo; \u0026ldquo;-P-6H+3M\u0026rdquo; \u0026ndash; parses as \u0026ldquo;+6 hours and -3 minutes\u0026rdquo;  increase increase(Duration): Calculates the increase in the time range.\nrate rate(Duration): Calculates the per-second average rate of increase in the time range.\nirate irate(): Calculates the per-second instant rate of increase in the time range.\ntag tag({allTags -\u0026gt; }): Updates tags of samples. User can add, drop, rename and update tags.\nhistogram histogram(le: '\u0026lt;the tag name of le\u0026gt;'): Transforms less-based histogram buckets to meter system histogram buckets. le parameter represents the tag name of the bucket.\nhistogram_percentile histogram_percentile([\u0026lt;p scalar\u0026gt;]). Represents the meter-system to calculate the p-percentile (0 ≤ p ≤ 100) from the buckets.\ntime time(): Returns the number of seconds since January 1, 1970 UTC.\nDown Sampling Operation MAL should instruct meter-system on how to downsample for metrics. It doesn\u0026rsquo;t only refer to aggregate raw samples to minute level, but also expresses data from minute in higher levels, such as hour and day.\nDown sampling function is called downsampling in MAL, and it accepts the following types:\n AVG SUM LATEST MIN (TODO) MAX (TODO) MEAN (TODO) COUNT (TODO)  The default type is AVG.\nIf users want to get the latest time from last_server_state_sync_time_in_seconds:\nlast_server_state_sync_time_in_seconds.tagEqual('production', 'catalog').downsampling(LATEST) Metric level function There are three levels in metric: service, instance and endpoint. They extract level relevant labels from metric labels, then informs the meter-system the level to which this metric belongs.\n servcie([svc_label1, svc_label2...]) extracts service level labels from the array argument. instance([svc_label1, svc_label2...], [ins_label1, ins_label2...]) extracts service level labels from the first array argument, extracts instance level labels from the second array argument. endpoint([svc_label1, svc_label2...], [ep_label1, ep_label2...]) extracts service level labels from the first array argument, extracts endpoint level labels from the second array argument.  More Examples Please refer to OAP Self-Observability\n","excerpt":"Meter Analysis Language The meter system provides a functional analysis language called MAL (Meter …","ref":"/docs/main/v8.6.0/en/concepts-and-designs/mal/","title":"Meter Analysis Language"},{"body":"Meter receiver The meter receiver accepts the metrics of meter protocol into the meter system.\nModule definition receiver-meter: selector: ${SW_RECEIVER_METER:default} default: In Kafka Fetcher, follow these configurations to enable it.\nkafka-fetcher: selector: ${SW_KAFKA_FETCHER:default} default: bootstrapServers: ${SW_KAFKA_FETCHER_SERVERS:localhost:9092} enableMeterSystem: ${SW_KAFKA_FETCHER_ENABLE_METER_SYSTEM:true} Configuration file The meter receiver is configured via a configuration file. The configuration file defines everything related to receiving from agents, as well as which rule files to load.\nThe OAP can load the configuration at bootstrap. If the new configuration is not well-formed, the OAP may fail to start up. The files are located at $CLASSPATH/meter-analyzer-config.\nThe file is written in YAML format, defined by the scheme described below. Brackets indicate that a parameter is optional.\nAn example can be found here. If you\u0026rsquo;re using Spring Sleuth, see Spring Sleuth Setup.\nMeters configuration # expSuffix is appended to all expression in this file. expSuffix: \u0026lt;string\u0026gt; # insert metricPrefix into metric name: \u0026lt;metricPrefix\u0026gt;_\u0026lt;raw_metric_name\u0026gt; metricPrefix: \u0026lt;string\u0026gt; # Metrics rule allow you to recompute queries. metricsRules: # The name of rule, which combinates with a prefix \u0026#39;meter_\u0026#39; as the index/table name in storage. name: \u0026lt;string\u0026gt; # MAL expression. exp: \u0026lt;string\u0026gt; For more information on MAL, please refer to mal.md\nrate, irate, and increase Although we support the rate, irate, increase functions in the backend, we still recommend users to consider using client-side APIs to run these functions. The reasons are as follows:\n The OAP has to set up caches to calculate the values. Once the agent reconnects to another OAP instance, the time windows of rate calculation break. This leads to inaccurate results.  ","excerpt":"Meter receiver The meter receiver accepts the metrics of meter protocol into the meter system. …","ref":"/docs/main/v8.6.0/en/setup/backend/backend-meter/","title":"Meter receiver"},{"body":"Meter System Meter system is another streaming calculation mode designed for metrics data. In the OAL, there are clear Scope Definitions, including definitions for native objects. Meter system is focused on the data type itself, and provides a more flexible approach to the end user in defining the scope entity.\nThe meter system is open to different receivers and fetchers in the backend, see the backend setup document for more details.\nEvery metric is declared in the meter system to include the following attributes:\n Metrics Name. A globally unique name to avoid overlapping between the OAL variable names. Function Name. The function used for this metric, namely distributed aggregation, value calculation or down sampling calculation based on the function implementation. Further, the data structure is determined by the function as well, such as function Avg is for Long. Scope Type. Unlike within the OAL, there are plenty of logic scope definitions. In the meter system, only type is required. Type values include service, instance, and endpoint, just as we have described in the Overview section. The values of scope entity name, such as service name, are required when metrics data are generated with the metrics data values.  NOTE: The metrics must be declared in the bootstrap stage, and there must be no change to runtime.\nThe Meter System supports the following binding functions:\n avg. Calculates the avg value for every entity under the same metrics name. histogram. Aggregates the counts in the configurable buckets. Buckets are configurable but must be assigned in the declaration stage. percentile. See percentile in WIKI. Unlike the OAL, we provide 50/75/90/95/99 by default. In the meter system function, the percentile function accepts several ranks, which should be in the (0, 100) range.  ","excerpt":"Meter System Meter system is another streaming calculation mode designed for metrics data. In the …","ref":"/docs/main/v8.6.0/en/concepts-and-designs/meter/","title":"Meter System"},{"body":"Metrics Exporter SkyWalking provides basic and most important metrics aggregation, alarm and analysis. In real world, people may want to forward the data to their 3rd party system, for deeper analysis or anything else. Metrics Exporter makes that possible.\nMetrics exporter is an independent module, you need manually active it.\nRight now, we provide the following exporters\n gRPC exporter  gRPC exporter gRPC exporter uses SkyWalking native exporter service definition. Here is proto definition.\nservice MetricExportService { rpc export (stream ExportMetricValue) returns (ExportResponse) { } rpc subscription (SubscriptionReq) returns (SubscriptionsResp) { }}message ExportMetricValue { string metricName = 1; string entityName = 2; string entityId = 3; ValueType type = 4; int64 timeBucket = 5; int64 longValue = 6; double doubleValue = 7; repeated int64 longValues = 8;}message SubscriptionsResp { repeated SubscriptionMetric metrics = 1;}message SubscriptionMetric { string metricName = 1; EventType eventType = 2;}enum ValueType { LONG = 0; DOUBLE = 1; MULTI_LONG = 2;}enum EventType { // The metrics aggregated in this bulk, not include the existing persistent data.  INCREMENT = 0; // Final result of the metrics at this moment.  TOTAL = 1;}message SubscriptionReq {}message ExportResponse {}To active the exporter, you should add this into your application.yml\nexporter: grpc: targetHost: 127.0.0.1 targetPort: 9870  targetHost:targetPort is the expected target service address. You could set any gRPC server to receive the data. Target gRPC service needs to be standby, otherwise, the OAP starts up failure.  For target exporter service subscription implementation Return the expected metrics name list with event type(increment or total), all the names must match the OAL/MAL script definition. Return empty list, if you want to export all metrics in increment event type.\nexport implementation Stream service, all subscribed metrics will be sent to here, based on OAP core schedule. Also, if the OAP deployed as cluster, then this method will be called concurrently. For metrics value, you need follow #type to choose #longValue or #doubleValue.\n","excerpt":"Metrics Exporter SkyWalking provides basic and most important metrics aggregation, alarm and …","ref":"/docs/main/v8.6.0/en/setup/backend/metrics-exporter/","title":"Metrics Exporter"},{"body":"Namespace Background SkyWalking is a monitoring tool, which collects metrics from a distributed system. In the real world, a very large distributed system includes hundreds of services, thousands of service instances. In that case, most likely, more than one group, even more than one company are maintaining and monitoring the distributed system. Each one of them takes charge of different parts, don\u0026rsquo;t want or shouldn\u0026rsquo;t share there metrics.\nNamespace is the proposal from this.It is used for tracing and monitoring isolation.\nSet the namespace Set agent.namespace in agent config # The agent namespace # agent.namespace=default-namespace The default value of agent.namespace is empty.\nInfluence The default header key of SkyWalking is sw8, more in this document. After agent.namespace is set, the key changes to namespace-sw8.\nThe across process propagation chain breaks, when the two sides are using different namespace.\n","excerpt":"Namespace Background SkyWalking is a monitoring tool, which collects metrics from a distributed …","ref":"/docs/main/v8.6.0/en/setup/service-agent/java-agent/namespace/","title":"Namespace"},{"body":"Observability Analysis Language OAL(Observability Analysis Language) serves to analyze incoming data in streaming mode.\nOAL focuses on metrics in Service, Service Instance and Endpoint. Therefore, the language is easy to learn and use.\nSince 6.3, the OAL engine is embedded in OAP server runtime as oal-rt(OAL Runtime). OAL scripts are now found in the /config folder, and users could simply change and reboot the server to run them. However, the OAL script is a compiled language, and the OAL Runtime generates java codes dynamically.\nYou can open set SW_OAL_ENGINE_DEBUG=Y at system env to see which classes are generated.\nGrammar Scripts should be named *.oal\n// Declare the metrics. METRICS_NAME = from(SCOPE.(* | [FIELD][,FIELD ...])) [.filter(FIELD OP [INT | STRING])] .FUNCTION([PARAM][, PARAM ...]) // Disable hard code disable(METRICS_NAME); Scope Primary SCOPEs are All, Service, ServiceInstance, Endpoint, ServiceRelation, ServiceInstanceRelation, and EndpointRelation. There are also some secondary scopes which belong to a primary scope.\nSee Scope Definitions, where you can find all existing Scopes and Fields.\nFilter Use filter to build conditions for the value of fields by using field name and expression.\nThe expressions support linking by and, or and (...). The OPs support ==, !=, \u0026gt;, \u0026lt;, \u0026gt;=, \u0026lt;=, in [...] ,like %..., like ...% , like %...% , contain and not contain, with type detection based on field type. In the event of incompatibility, compile or code generation errors may be triggered.\nAggregation Function The default functions are provided by the SkyWalking OAP core, and it is possible to implement additional functions.\nFunctions provided\n longAvg. The avg of all input per scope entity. The input field must be a long.   instance_jvm_memory_max = from(ServiceInstanceJVMMemory.max).longAvg();\n In this case, the input represents the request of each ServiceInstanceJVMMemory scope, and avg is based on field max.\n doubleAvg. The avg of all input per scope entity. The input field must be a double.   instance_jvm_cpu = from(ServiceInstanceJVMCPU.usePercent).doubleAvg();\n In this case, the input represents the request of each ServiceInstanceJVMCPU scope, and avg is based on field usePercent.\n percent. The number or ratio is expressed as a fraction of 100, where the input matches with the condition.   endpoint_percent = from(Endpoint.*).percent(status == true);\n In this case, all input represents requests of each endpoint, and the condition is endpoint.status == true.\n rate. The rate expressed is as a fraction of 100, where the input matches with the condition.   browser_app_error_rate = from(BrowserAppTraffic.*).rate(trafficCategory == BrowserAppTrafficCategory.FIRST_ERROR, trafficCategory == BrowserAppTrafficCategory.NORMAL);\n In this case, all input represents requests of each browser app traffic, the numerator condition is trafficCategory == BrowserAppTrafficCategory.FIRST_ERROR and denominator condition is trafficCategory == BrowserAppTrafficCategory.NORMAL. Parameter (1) is the numerator condition. Parameter (2) is the denominator condition.\n count. The sum of calls per scope entity.   service_calls_sum = from(Service.*).count();\n In this case, the number of calls of each service.\n histogram. See Heatmap in WIKI.   all_heatmap = from(All.latency).histogram(100, 20);\n In this case, the thermodynamic heatmap of all incoming requests. Parameter (1) is the precision of latency calculation, such as in the above case, where 113ms and 193ms are considered the same in the 101-200ms group. Parameter (2) is the group amount. In the above case, 21(param value + 1) groups are 0-100ms, 101-200ms, \u0026hellip; 1901-2000ms, 2000+ms\n apdex. See Apdex in WIKI.   service_apdex = from(Service.latency).apdex(name, status);\n In this case, the apdex score of each service. Parameter (1) is the service name, which reflects the Apdex threshold value loaded from service-apdex-threshold.yml in the config folder. Parameter (2) is the status of this request. The status(success/failure) reflects the Apdex calculation.\n p99, p95, p90, p75, p50. See percentile in WIKI.   all_percentile = from(All.latency).percentile(10);\n percentile is the first multiple-value metric, which has been introduced since 7.0.0. As a metric with multiple values, it could be queried through the getMultipleLinearIntValues GraphQL query. In this case, see p99, p95, p90, p75, and p50 of all incoming requests. The parameter is precise to a latency at p99, such as in the above case, and 120ms and 124ms are considered to produce the same response time. Before 7.0.0, p99, p95, p90, p75, p50 func(s) are used to calculate metrics separately. They are still supported in 7.x, but they are no longer recommended and are not included in the current official OAL script.\n all_p99 = from(All.latency).p99(10);\n In this case, the p99 value of all incoming requests. The parameter is precise to a latency at p99, such as in the above case, and 120ms and 124ms are considered to produce the same response time.\nMetrics name The metrics name for storage implementor, alarm and query modules. The type inference is supported by core.\nGroup All metrics data will be grouped by Scope.ID and min-level TimeBucket.\n In the Endpoint scope, the Scope.ID is same as the Endpoint ID (i.e. the unique ID based on service and its endpoint).  Disable Disable is an advanced statement in OAL, which is only used in certain cases. Some of the aggregation and metrics are defined through core hard codes. Examples include segment and top_n_database_statement. This disable statement is designed to render them inactive. By default, none of them are disabled.\nNOTICE, all disable statements should be in oal/disable.oal script file.\nExamples // Calculate p99 of both Endpoint1 and Endpoint2 endpoint_p99 = from(Endpoint.latency).filter(name in (\u0026quot;Endpoint1\u0026quot;, \u0026quot;Endpoint2\u0026quot;)).summary(0.99) // Calculate p99 of Endpoint name started with `serv` serv_Endpoint_p99 = from(Endpoint.latency).filter(name like \u0026quot;serv%\u0026quot;).summary(0.99) // Calculate the avg response time of each Endpoint endpoint_avg = from(Endpoint.latency).avg() // Calculate the p50, p75, p90, p95 and p99 of each Endpoint by 50 ms steps. endpoint_percentile = from(Endpoint.latency).percentile(10) // Calculate the percent of response status is true, for each service. endpoint_success = from(Endpoint.*).filter(status == true).percent() // Calculate the sum of response code in [404, 500, 503], for each service. endpoint_abnormal = from(Endpoint.*).filter(responseCode in [404, 500, 503]).count() // Calculate the sum of request type in [RequestType.RPC, RequestType.gRPC], for each service. endpoint_rpc_calls_sum = from(Endpoint.*).filter(type in [RequestType.RPC, RequestType.gRPC]).count() // Calculate the sum of endpoint name in [\u0026quot;/v1\u0026quot;, \u0026quot;/v2\u0026quot;], for each service. endpoint_url_sum = from(Endpoint.*).filter(name in [\u0026quot;/v1\u0026quot;, \u0026quot;/v2\u0026quot;]).count() // Calculate the sum of calls for each service. endpoint_calls = from(Endpoint.*).count() // Calculate the CPM with the GET method for each service.The value is made up with `tagKey:tagValue`. service_cpm_http_get = from(Service.*).filter(tags contain \u0026quot;http.method:GET\u0026quot;).cpm() // Calculate the CPM with the HTTP method except for the GET method for each service.The value is made up with `tagKey:tagValue`. service_cpm_http_other = from(Service.*).filter(tags not contain \u0026quot;http.method:GET\u0026quot;).cpm() disable(segment); disable(endpoint_relation_server_side); disable(top_n_database_statement); ","excerpt":"Observability Analysis Language OAL(Observability Analysis Language) serves to analyze incoming data …","ref":"/docs/main/v8.6.0/en/concepts-and-designs/oal/","title":"Observability Analysis Language"},{"body":"Observability Analysis Platform SkyWalking is an Observability Analysis Platform that provides full observability to services running in both brown and green zones, as well as services using a hybrid model.\nCapabilities SkyWalking covers all 3 areas of observability, including, Tracing, Metrics and Logging.\n Tracing. SkyWalking native data formats, including Zipkin v1 and v2, as well as Jaeger. Metrics. SkyWalking integrates with Service Mesh platforms, such as Istio, Envoy, and Linkerd, to build observability into the data panel or control panel. Also, SkyWalking native agents can run in the metrics mode, which greatly improves performances. Logging. Includes logs collected from disk or through network. Native agents could bind the tracing context with logs automatically, or use SkyWalking to bind the trace and log through the text content.  There are 3 powerful and native language engines designed to analyze observability data from the above areas.\n Observability Analysis Language processes native traces and service mesh data. Meter Analysis Language is responsible for metrics calculation for native meter data, and adopts a stable and widely used metrics system, such as Prometheus and OpenTelemetry. Log Analysis Language focuses on log contents and collaborate with Meter Analysis Language.  ","excerpt":"Observability Analysis Platform SkyWalking is an Observability Analysis Platform that provides full …","ref":"/docs/main/v8.6.0/en/concepts-and-designs/backend-overview/","title":"Observability Analysis Platform"},{"body":"Observe Service Mesh through ALS Envoy Access Log Service (ALS) provides full logs about RPC routed, including HTTP and TCP.\nBackground The solution was initialized and firstly implemented by Sheng Wu, Hongtao Gao, Lizan Zhou, and Dhi Aurrahman at 17 May. 2019, and was presented on KubeCon China 2019. Here is the recorded video.\nSkyWalking is the first open source project introducing this ALS based solution to the world. This provides a new way with very low payload to service mesh, but the same observability.\nEnable ALS and SkyWalking Receiver You need the following steps to set up ALS.\n  Enable envoyAccessLogService in ProxyConfig and set the ALS address to where SkyWalking OAP listens. On Istio version 1.6.0+, if Istio is installed with demo profile, you can enable ALS with command:\nistioctl manifest apply \\  --set profile=demo \\  --set meshConfig.enableEnvoyAccessLogService=true \\  --set meshConfig.defaultConfig.envoyAccessLogService.address=\u0026lt;skywalking-oap.skywalking.svc:11800\u0026gt; Note: Replace \u0026lt;skywalking-oap.skywalking.svc:11800\u0026gt; with the real address where SkyWalking OAP is deployed.\n  Activate SkyWalking Envoy Receiver. This is activated by default.\n  Choose an ALS analyzer. There are two available analyzers, k8s-mesh and mx-mesh for both HTTP access logs and TCP access logs. Set the system environment variable SW_ENVOY_METRIC_ALS_HTTP_ANALYSIS and SW_ENVOY_METRIC_ALS_TCP_ANALYSIS such as SW_ENVOY_METRIC_ALS_HTTP_ANALYSIS=mx-mesh, SW_ENVOY_METRIC_ALS_TCP_ANALYSIS=mx-mesh or in the application.yaml to activate the analyzer. For more about the analyzers, see SkyWalking ALS Analyzers\nenvoy-metric: selector: ${SW_ENVOY_METRIC:default} default: acceptMetricsService: ${SW_ENVOY_METRIC_SERVICE:true} alsHTTPAnalysis: ${SW_ENVOY_METRIC_ALS_HTTP_ANALYSIS:\u0026#34;\u0026#34;} # Setting the system env variable would override this.  alsTCPAnalysis: ${SW_ENVOY_METRIC_ALS_TCP_ANALYSIS:\u0026#34;\u0026#34;} To use multiple analyzers as a fallback，please use , to concatenate.\n  Example Here\u0026rsquo;s an example to install Istio and deploy SkyWalking by Helm chart.\nistioctl install \\  --set profile=demo \\  --set meshConfig.enableEnvoyAccessLogService=true \\  --set meshConfig.defaultConfig.envoyAccessLogService.address=skywalking-oap.istio-system:11800 git clone https://github.com/apache/skywalking-kubernetes.git cd skywalking-kubernetes/chart helm repo add elastic https://helm.elastic.co helm dep up skywalking helm install 8.1.0 skywalking -n istio-system \\  --set oap.env.SW_ENVOY_METRIC_ALS_HTTP_ANALYSIS=mx-mesh \\  --set oap.env.SW_ENVOY_METRIC_ALS_TCP_ANALYSIS=mx-mesh \\  --set fullnameOverride=skywalking \\  --set oap.envoy.als.enabled=true You can use kubectl -n istio-system logs -l app=skywalking | grep \u0026quot;K8sALSServiceMeshHTTPAnalysis\u0026quot; to ensure OAP ALS mx-mesh analyzer has been activated.\nSkyWalking ALS Analyzers There are several available analyzers, k8s-mesh, mx-mesh and persistence, you can specify one or more analyzers to analyze the access logs. When multiple analyzers are specified, it acts as a fast-success mechanism: SkyWalking loops over the analyzers and use it to analyze the logs, once there is an analyzer that is able to produce a result, it stops the loop.\nk8s-mesh k8s-mesh uses the metadata from Kubernetes cluster, hence in this analyzer OAP needs access roles to Pod, Service, and Endpoints.\nThe blog illustrates the detail of how it works, and a step-by-step tutorial to apply it into the bookinfo application.\nmx-mesh mx-mesh uses the Envoy metadata exchange mechanism to get the service name, etc., this analyzer requires Istio to enable the metadata exchange plugin (you can enable it by --set values.telemetry.v2.enabled=true, or if you\u0026rsquo;re using Istio 1.7+ and installing it with profile demo/preview, it should be enabled then).\nThe blog illustrates the detail of how it works, and a step-by-step tutorial to apply it into the Online Boutique system.\npersistence persistence analyzer adapts the Envoy access log format to SkyWalking\u0026rsquo;s native log format , and forwards the formatted logs to LAL, where you can configure persistent conditions, such as sampler, only persist error logs, etc. SkyWalking provides a default configuration file envoy-als.yaml that you can adjust as per your needs. Please make sure to activate this rule via adding the rule name envoy-als into config item log-analyzer/default/lalFiles (or environment variable SW_LOG_LAL_FILES, e.g. SW_LOG_LAL_FILES=envoy-als).\nAttention: because persistence analyzer also needs a mechanism to map the logs into responding services, hence, you need to configure at least one of k8s-mesh or mx-mesh as its antecedent so that persistence analyzer knows which service the logs belong to. For example, you should set envoy-metric/default/alsHTTPAnalysis (or environment variable SW_ENVOY_METRIC_ALS_HTTP_ANALYSIS) to something like k8s-mesh,persistence, mx-mesh,persistence or mx-mesh,k8s-mesh,persistence.\n","excerpt":"Observe Service Mesh through ALS Envoy Access Log Service (ALS) provides full logs about RPC routed, …","ref":"/docs/main/v8.6.0/en/setup/envoy/als_setting/","title":"Observe Service Mesh through ALS"},{"body":"Official OAL script First, read the OAL introduction.\nFrom 8.0.0, you may find the OAL script at /config/oal/*.oal of the SkyWalking dist. You could change it, such as by adding filter conditions or new metrics. Then, reboot the OAP server and it will come into effect.\nAll metrics named in this script may be used in alarm and UI query.\nNote: If you try to add or remove certain metrics, there is a possibility that the UI would break. You should only do this when you plan to build your own UI based on the customization analysis core.\n","excerpt":"Official OAL script First, read the OAL introduction.\nFrom 8.0.0, you may find the OAL script at …","ref":"/docs/main/v8.6.0/en/guides/backend-oal-scripts/","title":"Official OAL script"},{"body":"Open Fetcher Fetcher is a concept in SkyWalking backend. When reading data from target systems, the pull mode is more suitable than the receiver. This mode is typically found in metrics SDKs, such as Prometheus.\nPrometheus Fetcher Suppose you want to enable some metric-custom.yaml files stored at fetcher-prom-rules, append its name to enabledRules of prometheus-fetcher as follows:\nprometheus-fetcher: selector: ${SW_PROMETHEUS_FETCHER:default} default: enabledRules: ${SW_PROMETHEUS_FETCHER_ENABLED_RULES:\u0026#34;self,metric-custom\u0026#34;} Configuration file Prometheus fetcher is configured via a configuration file. The configuration file defines everything related to fetching services and their instances, as well as which rule files to load.\nThe OAP can load the configuration at bootstrap. If the new configuration is not well-formed, the OAP fails to start up. The files are located at $CLASSPATH/fetcher-prom-rules.\nThe file is written in YAML format, defined by the scheme described below. Brackets indicate that a parameter is optional.\nA full example can be found here\nGeneric placeholders are defined as follows:\n \u0026lt;duration\u0026gt;: This is parsed into a textual representation of a duration. The formats accepted are based on the ISO-8601 duration format PnDTnHnMn.nS with days considered to be exactly 24 hours. \u0026lt;labelname\u0026gt;: A string matching the regular expression [a-zA-Z_][a-zA-Z0-9_]*. \u0026lt;labelvalue\u0026gt;: A string of unicode characters. \u0026lt;host\u0026gt;: A valid string consisting of a hostname or IP followed by an optional port number. \u0026lt;path\u0026gt;: A valid URL path. \u0026lt;string\u0026gt;: A regular string.  # How frequently to fetch targets. fetcherInterval: \u0026lt;duration\u0026gt; # Per-fetch timeout when fetching this target. fetcherTimeout: \u0026lt;duration\u0026gt; # The HTTP resource path on which to fetch metrics from targets. metricsPath: \u0026lt;path\u0026gt; #Statically configured targets. staticConfig: # The targets specified by the static config. targets: [ - \u0026lt;target\u0026gt; ] # Labels assigned to all metrics fetched from the targets. labels: [ \u0026lt;labelname\u0026gt;: \u0026lt;labelvalue\u0026gt; ... ] # expSuffix is appended to all expression in this file. expSuffix: \u0026lt;string\u0026gt; # insert metricPrefix into metric name: \u0026lt;metricPrefix\u0026gt;_\u0026lt;raw_metric_name\u0026gt; metricPrefix: \u0026lt;string\u0026gt; # Metrics rule allow you to recompute queries. metricsRules: [ - \u0026lt;metric_rules\u0026gt; ]  # The url of target exporter. the format should be complied with \u0026#34;java.net.URI\u0026#34; url: \u0026lt;string\u0026gt; # The path of root CA file. sslCaFilePath: \u0026lt;string\u0026gt; \u0026lt;metric_rules\u0026gt; # The name of rule, which combinates with a prefix \u0026#39;meter_\u0026#39; as the index/table name in storage. name: \u0026lt;string\u0026gt; # MAL expression. exp: \u0026lt;string\u0026gt; To know more about MAL, please refer to mal.md\nKafka Fetcher The Kafka Fetcher pulls messages from the Kafka Broker to learn about what agent is delivered. Check the agent documentation for details. Typically, tracing segments, service/instance properties, JVM metrics, and meter system data are supported. Kafka Fetcher can work with gRPC/HTTP Receivers at the same time for adopting different transport protocols.\nKafka Fetcher is disabled by default. To enable it, configure as follows.\nNamespace aims to isolate multi OAP cluster when using the same Kafka cluster. If you set a namespace for Kafka fetcher, the OAP will add a prefix to topic name. You should also set namespace in the property named plugin.kafka.namespace in agent.config.\nkafka-fetcher: selector: ${SW_KAFKA_FETCHER:default} default: bootstrapServers: ${SW_KAFKA_FETCHER_SERVERS:localhost:9092} namespace: ${SW_NAMESPACE:\u0026#34;\u0026#34;} skywalking-segments, skywalking-metrics, skywalking-profile, skywalking-managements, skywalking-meters, skywalking-logs and skywalking-logs-json topics are required by kafka-fetcher. If they do not exist, Kafka Fetcher will create them by default. Also, you can create them by yourself before the OAP server starts.\nWhen using the OAP server automatic creation mechanism, you could modify the number of partitions and replications of the topics using the following configurations:\nkafka-fetcher: selector: ${SW_KAFKA_FETCHER:default} default: bootstrapServers: ${SW_KAFKA_FETCHER_SERVERS:localhost:9092} namespace: ${SW_NAMESPACE:\u0026#34;\u0026#34;} partitions: ${SW_KAFKA_FETCHER_PARTITIONS:3} replicationFactor: ${SW_KAFKA_FETCHER_PARTITIONS_FACTOR:2} enableMeterSystem: ${SW_KAFKA_FETCHER_ENABLE_METER_SYSTEM:false} isSharding: ${SW_KAFKA_FETCHER_IS_SHARDING:false} consumePartitions: ${SW_KAFKA_FETCHER_CONSUME_PARTITIONS:\u0026#34;\u0026#34;} In the cluster mode, all topics have the same number of partitions. Set \u0026quot;isSharding\u0026quot; to \u0026quot;true\u0026quot; and assign the partitions to consume for the OAP server. Use commas to separate multiple partitions for the OAP server.\nThe Kafka Fetcher allows you to configure all the Kafka producers listed here in property kafkaConsumerConfig. For example:\nkafka-fetcher: selector: ${SW_KAFKA_FETCHER:default} default: bootstrapServers: ${SW_KAFKA_FETCHER_SERVERS:localhost:9092} namespace: ${SW_NAMESPACE:\u0026#34;\u0026#34;} partitions: ${SW_KAFKA_FETCHER_PARTITIONS:3} replicationFactor: ${SW_KAFKA_FETCHER_PARTITIONS_FACTOR:2} enableMeterSystem: ${SW_KAFKA_FETCHER_ENABLE_METER_SYSTEM:false} isSharding: ${SW_KAFKA_FETCHER_IS_SHARDING:true} consumePartitions: ${SW_KAFKA_FETCHER_CONSUME_PARTITIONS:1,3,5} kafkaConsumerConfig: enable.auto.commit: true ... When using Kafka MirrorMaker 2.0 to replicate topics between Kafka clusters, you can set the source Kafka Cluster alias (mm2SourceAlias) and separator (mm2SourceSeparator) according to your Kafka MirrorMaker config.\nkafka-fetcher: selector: ${SW_KAFKA_FETCHER:default} default: bootstrapServers: ${SW_KAFKA_FETCHER_SERVERS:localhost:9092} namespace: ${SW_NAMESPACE:\u0026#34;\u0026#34;} partitions: ${SW_KAFKA_FETCHER_PARTITIONS:3} replicationFactor: ${SW_KAFKA_FETCHER_PARTITIONS_FACTOR:2} enableMeterSystem: ${SW_KAFKA_FETCHER_ENABLE_METER_SYSTEM:false} isSharding: ${SW_KAFKA_FETCHER_IS_SHARDING:true} consumePartitions: ${SW_KAFKA_FETCHER_CONSUME_PARTITIONS:1,3,5} mm2SourceAlias: ${SW_KAFKA_MM2_SOURCE_ALIAS:\u0026#34;\u0026#34;} mm2SourceSeparator: ${SW_KAFKA_MM2_SOURCE_SEPARATOR:\u0026#34;\u0026#34;} kafkaConsumerConfig: enable.auto.commit: true ... ","excerpt":"Open Fetcher Fetcher is a concept in SkyWalking backend. When reading data from target systems, the …","ref":"/docs/main/v8.6.0/en/setup/backend/backend-fetcher/","title":"Open Fetcher"},{"body":"Oracle and Resin plugins These plugins can\u0026rsquo;t be provided in Apache release because of Oracle and Resin Licenses. If you want to know details, please read Apache license legal document\nDue to license incompatibilities/restrictions these plugins are hosted and released in 3rd part repository, go to OpenSkywalking java plugin extension repository to get these.\n","excerpt":"Oracle and Resin plugins These plugins can\u0026rsquo;t be provided in Apache release because of Oracle …","ref":"/docs/main/v8.6.0/en/setup/service-agent/java-agent/agent-optional-plugins/oracle-resin-plugins/","title":"Oracle and Resin plugins"},{"body":"Overview SkyWalking is an open source observability platform used to collect, analyze, aggregate and visualize data from services and cloud native infrastructures. SkyWalking provides an easy way to maintain a clear view of your distributed systems, even across Clouds. It is a modern APM, specially designed for cloud native, container based distributed systems.\nWhy use SkyWalking? SkyWalking provides solutions for observing and monitoring distributed systems, in many different scenarios. First of all, like traditional approaches, SkyWalking provides auto instrument agents for services, such as Java, C#, Node.js, Go, PHP and Nginx LUA. (with calls out for Python and C++ SDK contributions). In multi-language, continuously deployed environments, cloud native infrastructures grow more powerful but also more complex. SkyWalking\u0026rsquo;s service mesh receiver allows SkyWalking to receive telemetry data from service mesh frameworks such as Istio/Envoy and Linkerd, allowing users to understand the entire distributed system.\nSkyWalking provides observability capabilities for service(s), service instance(s), endpoint(s). The terms Service, Instance and Endpoint are used everywhere today, so it is worth defining their specific meanings in the context of SkyWalking:\n Service. Represents a set/group of workloads which provide the same behaviours for incoming requests. You can define the service name when you are using instrument agents or SDKs. SkyWalking can also use the name you define in platforms such as Istio. Service Instance. Each individual workload in the Service group is known as an instance. Like pods in Kubernetes, it doesn\u0026rsquo;t need to be a single OS process, however, if you are using instrument agents, an instance is actually a real OS process. Endpoint. A path in a service for incoming requests, such as an HTTP URI path or a gRPC service class + method signature.  SkyWalking allows users to understand the topology relationship between Services and Endpoints, to view the metrics of every Service/Service Instance/Endpoint and to set alarm rules.\nIn addition, you can integrate\n Other distributed tracing using SkyWalking native agents and SDKs with Zipkin, Jaeger and OpenCensus. Other metrics systems, such as Prometheus, Sleuth(Micrometer), OpenTelemetry.  Architecture SkyWalking is logically split into four parts: Probes, Platform backend, Storage and UI.\n Probes collect data and reformat them for SkyWalking requirements (different probes support different sources). Platform backend supports data aggregation, analysis and streaming process covers traces, metrics, and logs. Storage houses SkyWalking data through an open/plugable interface. You can choose an existing implementation, such as ElasticSearch, H2, MySQL, TiDB, InfluxDB, or implement your own. Patches for new storage implementors welcome! UI is a highly customizable web based interface allowing SkyWalking end users to visualize and manage SkyWalking data.  What is next?  Learn SkyWalking\u0026rsquo;s Project Goals FAQ, Why doesn\u0026rsquo;t SkyWalking involve MQ in the architecture in default?  ","excerpt":"Overview SkyWalking is an open source observability platform used to collect, analyze, aggregate and …","ref":"/docs/main/v8.6.0/en/concepts-and-designs/overview/","title":"Overview"},{"body":"Plugin automatic test framework The plugin test framework is designed to verify the function and compatibility of plugins. As there are dozens of plugins and hundreds of versions that need to be verified, it is impossible to do it manually. The test framework uses container-based tech stack and requires a set of real services with the agents installed. Then, the test mock OAP backend runs to check the segments data sent from agents.\nEvery plugin maintained in the main repo requires corresponding test cases as well as matching versions in the supported list doc.\nEnvironment Requirements  MacOS/Linux JDK 8+ Docker Docker Compose  Case Base Image Introduction The test framework provides JVM-container and Tomcat-container base images including JDK8 and JDK14. You can choose the best one for your test case. If both are suitable for your case, JVM-container is preferred.\nJVM-container Image Introduction JVM-container uses openjdk:8 as the base image. JVM-container supports JDK14, which inherits openjdk:14. The test case project must be packaged as project-name.zip, including startup.sh and uber jar, by using mvn clean package.\nTake the following test projects as examples:\n sofarpc-scenario is a single project case. webflux-scenario is a case including multiple projects. jdk14-with-gson-scenario is a single project case with JDK14.  Tomcat-container Image Introduction Tomcat-container uses tomcat:8.5.57-jdk8-openjdk or tomcat:8.5.57-jdk14-openjdk as the base image. The test case project must be packaged as project-name.war by using mvn package.\nTake the following test project as an example\n spring-4.3.x-scenario  Test project hierarchical structure The test case is an independent maven project, and it must be packaged as a war tar ball or zip file, depending on the chosen base image. Also, two external accessible endpoints usually two URLs) are required.\nAll test case codes should be in the org.apache.skywalking.apm.testcase.* package. If there are some codes expected to be instrumented, then the classes could be in the test.org.apache.skywalking.apm.testcase.* package.\nJVM-container test project hierarchical structure\n[plugin-scenario] |- [bin] |- startup.sh |- [config] |- expectedData.yaml |- [src] |- [main] |- ... |- [resource] |- log4j2.xml |- pom.xml |- configuration.yaml |- support-version.list [] = directory Tomcat-container test project hierarchical structure\n[plugin-scenario] |- [config] |- expectedData.yaml |- [src] |- [main] |- ... |- [resource] |- log4j2.xml |- [webapp] |- [WEB-INF] |- web.xml |- pom.xml |- configuration.yaml |- support-version.list [] = directory Test case configuration files The following files are required in every test case.\n   File Name Descriptions     configuration.yml Declare the basic case information, including case name, entrance endpoints, mode, and dependencies.   expectedData.yaml Describe the expected segmentItems.   support-version.list List the target versions for this case.   startup.sh JVM-container only. This is not required when using Tomcat-container.    * support-version.list format requires every line for a single version (contains only the last version number of each minor version). You may use # to comment out this version.\nconfiguration.yml    Field description     type Image type, options, jvm, or tomcat. Required.   entryService The entrance endpoint (URL) for test case access. Required. (HTTP Method: GET)   healthCheck The health check endpoint (URL) for test case access. Required. (HTTP Method: HEAD)   startScript Path of the start up script. Required in type: jvm only.   runningMode Running mode with the optional plugin, options, default(default), with_optional, or with_bootstrap.   withPlugins Plugin selector rule, e.g.:apm-spring-annotation-plugin-*.jar. Required for runningMode=with_optional or runningMode=with_bootstrap.   environment Same as docker-compose#environment.   depends_on Same as docker-compose#depends_on.   dependencies Same as docker-compose#services, image, links, hostname, environment and depends_on are supported.    Note:, docker-compose activates only when dependencies is blank.\nrunningMode option description.\n   Option description     default Activate all plugins in plugin folder like the official distribution agent.   with_optional Activate default and plugins in optional-plugin by the give selector.   with_bootstrap Activate default and plugins in bootstrap-plugin by the give selector.    with_optional/with_bootstrap supports multiple selectors, separated by ;.\nFile Format\ntype: entryService: healthCheck: startScript: runningMode: withPlugins: environment: ... depends_on: ... dependencies: service1: image: hostname: expose: ... environment: ... depends_on: ... links: ... entrypoint: ... healthcheck: ...  dependencies support docker compose healthcheck. But the format is a little different. We need to have - as the start of every config item, and describe it as a string line.  For example, in the official document, the health check is:\nhealthcheck: test: [\u0026#34;CMD\u0026#34;, \u0026#34;curl\u0026#34;, \u0026#34;-f\u0026#34;, \u0026#34;http://localhost\u0026#34;] interval: 1m30s timeout: 10s retries: 3 start_period: 40s Here you should write:\nhealthcheck: - \u0026#39;test: [\u0026#34;CMD\u0026#34;, \u0026#34;curl\u0026#34;, \u0026#34;-f\u0026#34;, \u0026#34;http://localhost\u0026#34;]\u0026#39; - \u0026#34;interval: 1m30s\u0026#34; - \u0026#34;timeout: 10s\u0026#34; - \u0026#34;retries: 3\u0026#34; - \u0026#34;start_period: 40s\u0026#34; In some cases, the dependency service (usually a third-party server like the SolrJ server) is required to keep the same version as the client lib version, which is defined as ${test.framework.version} in pom. You may use ${CASE_SERVER_IMAGE_VERSION} as the version number, which will be changed in the test for each version.\n It does not support resource related configurations, such as volumes, ports, and ulimits. The reason for this is that in test scenarios, no mapping is required for any port to the host VM, or to mount any folder.\n Take the following test cases as examples:\n dubbo-2.7.x with JVM-container jetty with JVM-container gateway with runningMode canal with docker-compose  expectedData.yaml Operator for number\n   Operator Description     nq Not equal   eq Equal(default)   ge Greater than or equal   gt Greater than    Operator for String\n   Operator Description     not null Not null   null Null or empty String   eq Equal(default)    Expected Data Format Of The Segment\nsegmentItems: - serviceName: SERVICE_NAME(string) segmentSize: SEGMENT_SIZE(int) segments: - segmentId: SEGMENT_ID(string) spans: ...    Field Description     serviceName Service Name.   segmentSize The number of segments is expected.   segmentId Trace ID.   spans Segment span list. In the next section, you will learn how to describe each span.    Expected Data Format Of The Span\nNote: The order of span list should follow the order of the span finish time.\noperationName: OPERATION_NAME(string) parentSpanId: PARENT_SPAN_ID(int) spanId: SPAN_ID(int) startTime: START_TIME(int) endTime: END_TIME(int) isError: IS_ERROR(string: true, false) spanLayer: SPAN_LAYER(string: DB, RPC_FRAMEWORK, HTTP, MQ, CACHE) spanType: SPAN_TYPE(string: Exit, Entry, Local) componentId: COMPONENT_ID(int) tags: - {key: TAG_KEY(string), value: TAG_VALUE(string)} ... logs: - {key: LOG_KEY(string), value: LOG_VALUE(string)} ... peer: PEER(string) refs: - { traceId: TRACE_ID(string), parentTraceSegmentId: PARENT_TRACE_SEGMENT_ID(string), parentSpanId: PARENT_SPAN_ID(int), parentService: PARENT_SERVICE(string), parentServiceInstance: PARENT_SERVICE_INSTANCE(string), parentEndpoint: PARENT_ENDPOINT_NAME(string), networkAddress: NETWORK_ADDRESS(string), refType: REF_TYPE(string: CrossProcess, CrossThread) } ...    Field Description     operationName Span Operation Name.   parentSpanId Parent span ID. Note: The parent span ID of the first span should be -1.   spanId Span ID. Note: Start from 0.   startTime Span start time. It is impossible to get the accurate time, not 0 should be enough.   endTime Span finish time. It is impossible to get the accurate time, not 0 should be enough.   isError Span status, true or false.   componentId Component id for your plugin.   tags Span tag list. Notice, Keep in the same order as the plugin coded.   logs Span log list. Notice, Keep in the same order as the plugin coded.   SpanLayer Options, DB, RPC_FRAMEWORK, HTTP, MQ, CACHE.   SpanType Span type, options, Exit, Entry or Local.   peer Remote network address, IP + port mostly. For exit span, this should be required.    The verify description for SegmentRef\n   Field Description     traceId    parentTraceSegmentId Parent SegmentId, pointing to the segment id in the parent segment.   parentSpanId Parent SpanID, pointing to the span id in the parent segment.   parentService The service of parent/downstream service name.   parentServiceInstance The instance of parent/downstream service instance name.   parentEndpoint The endpoint of parent/downstream service.   networkAddress The peer value of parent exit span.   refType Ref type, options, CrossProcess or CrossThread.    Expected Data Format Of The Meter Items\nmeterItems: - serviceName: SERVICE_NAME(string) meterSize: METER_SIZE(int) meters: - ...    Field Description     serviceName Service Name.   meterSize The number of meters is expected.   meters meter list. Follow the next section to see how to describe every meter.    Expected Data Format Of The Meter\nmeterId: name: NAME(string) tags: - {name: TAG_NAME(string), value: TAG_VALUE(string)} singleValue: SINGLE_VALUE(double) histogramBuckets: - HISTOGRAM_BUCKET(double) ... The verify description for MeterId\n   Field Description     name meter name.   tags meter tags.   tags.name tag name.   tags.value tag value.   singleValue counter or gauge value. Using condition operate of the number to validate, such as gt, ge. If current meter is histogram, don\u0026rsquo;t need to write this field.   histogramBuckets histogram bucket. The bucket list must be ordered. The tool assert at least one bucket of the histogram having nonzero count. If current meter is counter or gauge, don\u0026rsquo;t need to write this field.    startup.sh This script provide a start point to JVM based service, most of them starts by a java -jar, with some variables. The following system environment variables are available in the shell.\n   Variable Description     agent_opts Agent plugin opts, check the detail in plugin doc or the same opt added in this PR.   SCENARIO_NAME Service name. Default same as the case folder name   SCENARIO_VERSION Version   SCENARIO_ENTRY_SERVICE Entrance URL to access this service   SCENARIO_HEALTH_CHECK_URL Health check URL     ${agent_opts} is required to add into your java -jar command, which including the parameter injected by test framework, and make agent installed. All other parameters should be added after ${agent_opts}.\n The test framework will set the service name as the test case folder name by default, but in some cases, there are more than one test projects are required to run in different service codes, could set it explicitly like the following example.\nExample\nhome=\u0026#34;$(cd \u0026#34;$(dirname $0)\u0026#34;; pwd)\u0026#34; java -jar ${agent_opts} \u0026#34;-Dskywalking.agent.service_name=jettyserver-scenario\u0026#34; ${home}/../libs/jettyserver-scenario.jar \u0026amp; sleep 1 java -jar ${agent_opts} \u0026#34;-Dskywalking.agent.service_name=jettyclient-scenario\u0026#34; ${home}/../libs/jettyclient-scenario.jar \u0026amp;  Only set this or use other skywalking options when it is really necessary.\n Take the following test cases as examples\n undertow webflux  Best Practices How To Use The Archetype To Create A Test Case Project We provided archetypes and a script to make creating a project easier. It creates a completed project of a test case. So that we only need to focus on cases. First, we can use followed command to get usage about the script.\nbash ${SKYWALKING_HOME}/test/plugin/generator.sh\nThen, runs and generates a project, named by scenario_name, in ./scenarios.\nRecommendations for pom \u0026lt;properties\u0026gt; \u0026lt;!-- Provide and use this property in the pom. --\u0026gt; \u0026lt;!-- This version should match the library version, --\u0026gt; \u0026lt;!-- in this case, http components lib version 4.3. --\u0026gt; \u0026lt;test.framework.version\u0026gt;4.3\u0026lt;/test.framework.version\u0026gt; \u0026lt;/properties\u0026gt; \u0026lt;dependencies\u0026gt; \u0026lt;dependency\u0026gt; \u0026lt;groupId\u0026gt;org.apache.httpcomponents\u0026lt;/groupId\u0026gt; \u0026lt;artifactId\u0026gt;httpclient\u0026lt;/artifactId\u0026gt; \u0026lt;version\u0026gt;${test.framework.version}\u0026lt;/version\u0026gt; \u0026lt;/dependency\u0026gt; ... \u0026lt;/dependencies\u0026gt; \u0026lt;build\u0026gt; \u0026lt;!-- Set the package final name as same as the test case folder case. --\u0026gt; \u0026lt;finalName\u0026gt;httpclient-4.3.x-scenario\u0026lt;/finalName\u0026gt; .... \u0026lt;/build\u0026gt; How To Implement Heartbeat Service Heartbeat service is designed for checking the service available status. This service is a simple HTTP service, returning 200 means the target service is ready. Then the traffic generator will access the entry service and verify the expected data. User should consider to use this service to detect such as whether the dependent services are ready, especially when dependent services are database or cluster.\nNotice, because heartbeat service could be traced fully or partially, so, segmentSize in expectedData.yaml should use ge as the operator, and don\u0026rsquo;t include the segments of heartbeat service in the expected segment data.\nThe example Process of Writing Tracing Expected Data Expected data file, expectedData.yaml, include SegmentItems part.\nWe are using the HttpClient plugin to show how to write the expected data.\nThere are two key points of testing\n Whether is HttpClient span created. Whether the ContextCarrier created correctly, and propagates across processes.  +-------------+ +------------------+ +-------------------------+ | Browser | | Case Servlet | | ContextPropagateServlet | | | | | | | +-----|-------+ +---------|--------+ +------------|------------+ | | | | | | | WebHttp +-+ | +------------------------\u0026gt; |-| HttpClient +-+ | |--------------------------------\u0026gt; |-| | |-| |-| | |-| |-| | |-| \u0026lt;--------------------------------| | |-| +-+ | \u0026lt;--------------------------| | | +-+ | | | | | | | | | | | | | + + + segmentItems By following the flow of HttpClient case, there should be two segments created.\n Segment represents the CaseServlet access. Let\u0026rsquo;s name it as SegmentA. Segment represents the ContextPropagateServlet access. Let\u0026rsquo;s name it as SegmentB.  segmentItems: - serviceName: httpclient-case segmentSize: ge 2 # Could have more than one health check segments, because, the dependency is not standby. Because Tomcat plugin is a default plugin of SkyWalking, so, in SegmentA, there are two spans\n Tomcat entry span HttpClient exit span  SegmentA span list should like following\n- segmentId: not null spans: - operationName: /httpclient-case/case/context-propagate parentSpanId: 0 spanId: 1 startTime: nq 0 endTime: nq 0 isError: false spanLayer: Http spanType: Exit componentId: eq 2 tags: - {key: url, value: \u0026#39;http://127.0.0.1:8080/httpclient-case/case/context-propagate\u0026#39;} - {key: http.method, value: GET} logs: [] peer: 127.0.0.1:8080 - operationName: /httpclient-case/case/httpclient parentSpanId: -1 spanId: 0 startTime: nq 0 endTime: nq 0 spanLayer: Http isError: false spanType: Entry componentId: 1 tags: - {key: url, value: \u0026#39;http://localhost:{SERVER_OUTPUT_PORT}/httpclient-case/case/httpclient\u0026#39;} - {key: http.method, value: GET} logs: [] peer: null SegmentB should only have one Tomcat entry span, but includes the Ref pointing to SegmentA.\nSegmentB span list should like following\n- segmentId: not null spans: - operationName: /httpclient-case/case/context-propagate parentSpanId: -1 spanId: 0 tags: - {key: url, value: \u0026#39;http://127.0.0.1:8080/httpclient-case/case/context-propagate\u0026#39;} - {key: http.method, value: GET} logs: [] startTime: nq 0 endTime: nq 0 spanLayer: Http isError: false spanType: Entry componentId: 1 peer: null refs: - {parentEndpoint: /httpclient-case/case/httpclient, networkAddress: \u0026#39;localhost:8080\u0026#39;, refType: CrossProcess, parentSpanId: 1, parentTraceSegmentId: not null, parentServiceInstance: not null, parentService: not null, traceId: not null} The example Process of Writing Meter Expected Data Expected data file, expectedData.yaml, include MeterItems part.\nWe are using the toolkit plugin to demonstrate how to write the expected data. When write the meter plugin, the expected data file keeps the same.\nThere is one key point of testing\n Build a meter and operate it.  Such as Counter:\nMeterFactory.counter(\u0026#34;test_counter\u0026#34;).tag(\u0026#34;ck1\u0026#34;, \u0026#34;cv1\u0026#34;).build().increment(1d); MeterFactory.histogram(\u0026#34;test_histogram\u0026#34;).tag(\u0026#34;hk1\u0026#34;, \u0026#34;hv1\u0026#34;).steps(1d, 5d, 10d).build().addValue(2d); +-------------+ +------------------+ | Plugin | | Agent core | | | | | +-----|-------+ +---------|--------+ | | | | | Build or operate +-+ +------------------------\u0026gt; |-| | |-] | |-| | |-| | |-| | |-| | \u0026lt;--------------------------| | +-+ | | | | | | | | + + meterItems By following the flow of the toolkit case, there should be two meters created.\n Meter test_counter created from MeterFactory#counter. Let\u0026rsquo;s name it as MeterA. Meter test_histogram created from MeterFactory#histogram. Let\u0026rsquo;s name it as MeterB.  meterItems: - serviceName: toolkit-case meterSize: 2 They\u0026rsquo;re showing two kinds of meter, MeterA has a single value, MeterB has a histogram value.\nMeterA should like following, counter and gauge use the same data format.\n- meterId: name: test_counter tags: - {name: ck1, value: cv1} singleValue: gt 0 MeterB should like following.\n- meterId: name: test_histogram tags: - {name: hk1, value: hv1} histogramBuckets: - 0.0 - 1.0 - 5.0 - 10.0 Local Test and Pull Request To The Upstream First of all, the test case project could be compiled successfully, with right project structure and be able to deploy. The developer should test the start script could run in Linux/MacOS, and entryService/health services are able to provide the response.\nYou could run test by using following commands\ncd ${SKYWALKING_HOME} bash ./test/plugin/run.sh -f ${scenario_name} Notice，if codes in ./apm-sniffer have been changed, no matter because your change or git update， please recompile the skywalking-agent. Because the test framework will use the existing skywalking-agent folder, rather than recompiling it every time.\nUse ${SKYWALKING_HOME}/test/plugin/run.sh -h to know more command options.\nIf the local test passed, then you could add it to .github/workflows/plugins-test.\u0026lt;n\u0026gt;.yaml file, which will drive the tests running on the GitHub Actions of official SkyWalking repository. Based on your plugin\u0026rsquo;s name, please add the test case into file .github/workflows/plugins-test.\u0026lt;n\u0026gt;.yaml, by alphabetical orders.\nEvery test case is a GitHub Actions Job. Please use the scenario directory name as the case name, mostly you\u0026rsquo;ll just need to decide which file (plugins-test.\u0026lt;n\u0026gt;.yaml) to add your test case, and simply put one line (as follows) in it, take the existed cases as examples. You can run python3 tools/select-group.py to see which file contains the least cases and add your cases into it, in order to balance the running time of each group.\nIf a test case required to run in JDK 14 environment, please add you test case into file plugins-jdk14-test.\u0026lt;n\u0026gt;.yaml.\njobs: PluginsTest: name: Plugin runs-on: ubuntu-latest timeout-minutes: 90 strategy: fail-fast: true matrix: case: # ... - \u0026lt;your scenario test directory name\u0026gt; # ... ","excerpt":"Plugin automatic test framework The plugin test framework is designed to verify the function and …","ref":"/docs/main/v8.6.0/en/guides/plugin-test/","title":"Plugin automatic test framework"},{"body":"Plugin Development Guide This document describes how to understand, develop and contribute a plugin.\nThere are 2 kinds of plugin:\n Tracing plugin. Follow the distributed tracing concept to collect spans with tags and logs. Meter plugin. Collect numeric metrics in Counter, Gauge, and Histogram formats.  We also provide the plugin test tool to verify the data collected and reported by the plugin. If you plan to contribute any plugin to our main repo, the data would be verified by this tool too.\nTracing plugin Concepts Span The span is an important and recognized concept in the distributed tracing system. Learn about the span from the Google Dapper Paper and OpenTracing\nSkyWalking has supported OpenTracing and OpenTracing-Java API since 2017. Our concepts of the span are similar to that of the Google Dapper Paper and OpenTracing. We have also extended the span.\nThere are three types of span:\n1.1 EntrySpan The EntrySpan represents a service provider. It is also an endpoint on the server end. As an APM system, our target is the application servers. Therefore, almost all the services and MQ-consumers are EntrySpan.\n1.2 LocalSpan The LocalSpan represents a normal Java method that does not concern remote services. It is neither a MQ producer/consumer nor a service (e.g. HTTP service) provider/consumer.\n1.3 ExitSpan The ExitSpan represents a client of service or MQ-producer. It is named the LeafSpan in the early versions of SkyWalking. For example, accessing DB through JDBC and reading Redis/Memcached are classified as an ExitSpan.\nContextCarrier In order to implement distributed tracing, cross-process tracing has to be bound, and the context must propagate across the process. This is where the ContextCarrier comes in.\nHere are the steps on how to use the ContextCarrier in an A-\u0026gt;B distributed call.\n Create a new and empty ContextCarrier on the client end. Create an ExitSpan by ContextManager#createExitSpan or use ContextManager#inject to initalize the ContextCarrier. Place all items of ContextCarrier into heads (e.g. HTTP HEAD), attachments (e.g. Dubbo RPC framework) or messages (e.g. Kafka). The ContextCarrier propagates to the server end through the service call. On the server end, obtain all items from the heads, attachments or messages. Create an EntrySpan by ContextManager#createEntrySpan or use ContextManager#extract to bind the client and server ends.  See the following examples, where we use the Apache HTTPComponent client plugin and Tomcat 7 server plugin:\n Using the Apache HTTPComponent client plugin on the client end  span = ContextManager.createExitSpan(\u0026#34;/span/operation/name\u0026#34;, contextCarrier, \u0026#34;ip:port\u0026#34;); CarrierItem next = contextCarrier.items(); while (next.hasNext()) { next = next.next(); httpRequest.setHeader(next.getHeadKey(), next.getHeadValue()); } Using the Tomcat 7 server plugin on the server end  ContextCarrier contextCarrier = new ContextCarrier(); CarrierItem next = contextCarrier.items(); while (next.hasNext()) { next = next.next(); next.setHeadValue(request.getHeader(next.getHeadKey())); } span = ContextManager.createEntrySpan(“/span/operation/name”, contextCarrier); ContextSnapshot Besides cross-process tracing, cross-thread tracing has to be supported as well. For instance, both async process (in-memory MQ) and batch process are common in Java. Cross-process and cross-thread tracing are very similar in that they both require propagating context, except that cross-thread tracing does not require serialization.\nHere are the three steps on cross-thread propagation:\n Use ContextManager#capture to get the ContextSnapshot object. Let the sub-thread access the ContextSnapshot through method arguments or being carried by existing arguments Use ContextManager#continued in sub-thread.  Core APIs ContextManager ContextManager provides all major and primary APIs.\n Create EntrySpan  public static AbstractSpan createEntrySpan(String endpointName, ContextCarrier carrier) Create EntrySpan according to the operation name (e.g. service name, uri) and ContextCarrier.\nCreate LocalSpan  public static AbstractSpan createLocalSpan(String endpointName) Create LocalSpan according to the operation name (e.g. full method signature).\nCreate ExitSpan  public static AbstractSpan createExitSpan(String endpointName, ContextCarrier carrier, String remotePeer) Create ExitSpan according to the operation name (e.g. service name, uri) and the new ContextCarrier and peer address (e.g. ip+port, hostname+port).\nAbstractSpan /** * Set the component id, which defines in {@link ComponentsDefine} * * @param component * @return the span for chaining. */ AbstractSpan setComponent(Component component); AbstractSpan setLayer(SpanLayer layer); /** * Set a key:value tag on the Span. * * @return this Span instance, for chaining */ AbstractSpan tag(String key, String value); /** * Record an exception event of the current walltime timestamp. * * @param t any subclass of {@link Throwable}, which occurs in this span. * @return the Span, for chaining */ AbstractSpan log(Throwable t); AbstractSpan errorOccurred(); /** * Record an event at a specific timestamp. * * @param timestamp The explicit timestamp for the log record. * @param event the events * @return the Span, for chaining */ AbstractSpan log(long timestamp, Map\u0026lt;String, ?\u0026gt; event); /** * Sets the string name for the logical operation this span represents. * * @return this Span instance, for chaining */ AbstractSpan setOperationName(String endpointName); Besides setting the operation name, tags and logs, two attributes must be set, namely the component and layer. This is especially important for the EntrySpan and ExitSpan.\nSpanLayer is the type of span. There are 5 values:\n UNKNOWN (default) DB RPC_FRAMEWORK (designed for the RPC framework, rather than an ordinary HTTP call) HTTP MQ  Component IDs are defined and reserved by the SkyWalking project. For extension of the component name/ID, please follow the component library definitions and extensions document.\nSpecial Span Tags All tags are available in the trace view. Meanwhile, in the OAP backend analysis, some special tags or tag combinations provide other advanced features.\nTag key status_code The value should be an integer. The response code of OAL entities corresponds to this value.\nTag keys db.statement and db.type. The value of db.statement should be a string that represents the database statement, such as SQL, or [No statement]/+span#operationName if the value is empty. When the exit span contains this tag, OAP samples the slow statements based on agent-analyzer/default/maxSlowSQLLength. The threshold of slow statement is defined in accordance with agent-analyzer/default/slowDBAccessThreshold\nExtension logic endpoint: Tag key x-le The logic endpoint is a concept that doesn\u0026rsquo;t represent a real RPC call, but requires the statistic. The value of x-le should be in JSON format. There are two options:\n Define a separated logic endpoint. Provide its own endpoint name, latency and status. Suitable for entry and local span.  { \u0026#34;name\u0026#34;: \u0026#34;GraphQL-service\u0026#34;, \u0026#34;latency\u0026#34;: 100, \u0026#34;status\u0026#34;: true } Declare the current local span representing a logic endpoint.  { \u0026#34;logic-span\u0026#34;: true } Advanced APIs Async Span APIs There is a set of advanced APIs in Span which is specifically designed for async use cases. When tags, logs, and attributes (including end time) of the span need to be set in another thread, you should use these APIs.\n/** * The span finish at current tracing context, but the current span is still alive, until {@link #asyncFinish} * called. * * This method must be called\u0026lt;br/\u0026gt; * 1. In original thread(tracing context). * 2. Current span is active span. * * During alive, tags, logs and attributes of the span could be changed, in any thread. * * The execution times of {@link #prepareForAsync} and {@link #asyncFinish()} must match. * * @return the current span */ AbstractSpan prepareForAsync(); /** * Notify the span, it could be finished. * * The execution times of {@link #prepareForAsync} and {@link #asyncFinish()} must match. * * @return the current span */ AbstractSpan asyncFinish();  Call #prepareForAsync in the original context. Run ContextManager#stopSpan in the original context when your job in the current thread is complete. Propagate the span to any other thread. Once the above steps are all set, call #asyncFinish in any thread. When #prepareForAsync is complete for all spans, the tracing context will be finished and will report to the backend (based on the count of API execution).  Develop a plugin Abstract The basic method to trace is to intercept a Java method, by using byte code manipulation tech and AOP concept. SkyWalking has packaged the byte code manipulation tech and tracing context propagation, so you simply have to define the intercept point (a.k.a. aspect pointcut in Spring).\nIntercept SkyWalking provides two common definitions to intercept constructor, instance method and class method.\nv1 APIs  Extend ClassInstanceMethodsEnhancePluginDefine to define constructor intercept points and instance method intercept points. Extend ClassStaticMethodsEnhancePluginDefine to define class method intercept points.  Of course, you can extend ClassEnhancePluginDefine to set all intercept points, although it is uncommon to do so.\nv2 APIs v2 APIs provide an enhanced interceptor, which could propagate context through MIC(MethodInvocationContext).\n Extend ClassInstanceMethodsEnhancePluginDefineV2 to define constructor intercept points and instance method intercept points. Extend ClassStaticMethodsEnhancePluginDefineV2 to define class method intercept points.  Of course, you can extend ClassEnhancePluginDefineV2 to set all intercept points, although it is uncommon to do so.\nImplement plugin See the following demonstration on how to implement a plugin by extending ClassInstanceMethodsEnhancePluginDefine.\n Define the target class name.  protected abstract ClassMatch enhanceClass(); ClassMatch represents how to match the target classes. There are 4 ways:\n byName: Based on the full class names (package name + . + class name). byClassAnnotationMatch: Depends on whether there are certain annotations in the target classes. byMethodAnnotationMatch: Depends on whether there are certain annotations in the methods of the target classes. byHierarchyMatch: Based on the parent classes or interfaces of the target classes.  Attention:\n Never use ThirdPartyClass.class in the instrumentation definitions, such as takesArguments(ThirdPartyClass.class), or byName(ThirdPartyClass.class.getName()), because of the fact that ThirdPartyClass dose not necessarily exist in the target application and this will break the agent; we have import checks to assist in checking this in CI, but it doesn\u0026rsquo;t cover all scenarios of this limitation, so never try to work around this limitation by something like using full-qualified-class-name (FQCN), i.e. takesArguments(full.qualified.ThirdPartyClass.class) and byName(full.qualified.ThirdPartyClass.class.getName()) will pass the CI check, but are still invalid in the agent codes. Therefore, Use Full Qualified Class Name String Literature Instead. Even if you are perfectly sure that the class to be intercepted exists in the target application (such as JDK classes), still, do not use *.class.getName() to get the class String name. We recommend you to use a literal string. This is to avoid ClassLoader issues. by*AnnotationMatch does not support inherited annotations. We do not recommend using byHierarchyMatch unless necessary. Using it may trigger the interception of many unexcepted methods, which would cause performance issues.  Example：\n@Override protected ClassMatch enhanceClassName() { return byName(\u0026#34;org.apache.catalina.core.StandardEngineValve\u0026#34;);\t}\tDefine an instance method intercept point.  public InstanceMethodsInterceptPoint[] getInstanceMethodsInterceptPoints(); public interface InstanceMethodsInterceptPoint { /** * class instance methods matcher. * * @return methods matcher */ ElementMatcher\u0026lt;MethodDescription\u0026gt; getMethodsMatcher(); /** * @return represents a class name, the class instance must instanceof InstanceMethodsAroundInterceptor. */ String getMethodsInterceptor(); boolean isOverrideArgs(); } You may also use Matcher to set the target methods. Return true in isOverrideArgs, if you want to change the argument ref in interceptor.\nThe following sections will tell you how to implement the interceptor.\nAdd plugin definition into the skywalking-plugin.def file.  tomcat-7.x/8.x=TomcatInstrumentation  Set up witnessClasses and/or witnessMethods if the instrumentation has to be activated in specific versions.\nExample:\n// The plugin is activated only when the foo.Bar class exists. @Override protected String[] witnessClasses() { return new String[] { \u0026#34;foo.Bar\u0026#34; }; } // The plugin is activated only when the foo.Bar#hello method exists. @Override protected List\u0026lt;WitnessMethod\u0026gt; witnessMethods() { List\u0026lt;WitnessMethod\u0026gt; witnessMethodList = new ArrayList\u0026lt;\u0026gt;(); WitnessMethod witnessMethod = new WitnessMethod(\u0026#34;foo.Bar\u0026#34;, ElementMatchers.named(\u0026#34;hello\u0026#34;)); witnessMethodList.add(witnessMethod); return witnessMethodList; } For more examples, see WitnessTest.java\n  Implement an interceptor As an interceptor for an instance method, it has to implement org.apache.skywalking.apm.agent.core.plugin.interceptor.enhance.InstanceMethodsAroundInterceptor\n/** * A interceptor, which intercept method\u0026#39;s invocation. The target methods will be defined in {@link * ClassEnhancePluginDefine}\u0026#39;s subclass, most likely in {@link ClassInstanceMethodsEnhancePluginDefine} */ public interface InstanceMethodsAroundInterceptor { /** * called before target method invocation. * * @param result change this result, if you want to truncate the method. * @throws Throwable */ void beforeMethod(EnhancedInstance objInst, Method method, Object[] allArguments, Class\u0026lt;?\u0026gt;[] argumentsTypes, MethodInterceptResult result) throws Throwable; /** * called after target method invocation. Even method\u0026#39;s invocation triggers an exception. * * @param ret the method\u0026#39;s original return value. * @return the method\u0026#39;s actual return value. * @throws Throwable */ Object afterMethod(EnhancedInstance objInst, Method method, Object[] allArguments, Class\u0026lt;?\u0026gt;[] argumentsTypes, Object ret) throws Throwable; /** * called when occur exception. * * @param t the exception occur. */ void handleMethodException(EnhancedInstance objInst, Method method, Object[] allArguments, Class\u0026lt;?\u0026gt;[] argumentsTypes, Throwable t); } Use the core APIs before and after calling the method, as well as during exception handling.\nV2 APIs The interceptor of V2 API uses MethodInvocationContext context to replace the MethodInterceptResult result in the beforeMethod, and be added as a new parameter in afterMethod and handleMethodException.\nMethodInvocationContext context is only shared in one time execution, and safe to use when face concurrency execution.\n/** * A v2 interceptor, which intercept method\u0026#39;s invocation. The target methods will be defined in {@link * ClassEnhancePluginDefineV2}\u0026#39;s subclass, most likely in {@link ClassInstanceMethodsEnhancePluginDefine} */ public interface InstanceMethodsAroundInterceptorV2 { /** * called before target method invocation. * * @param context the method invocation context including result context. */ void beforeMethod(EnhancedInstance objInst, Method method, Object[] allArguments, Class\u0026lt;?\u0026gt;[] argumentsTypes, MethodInvocationContext context) throws Throwable; /** * called after target method invocation. Even method\u0026#39;s invocation triggers an exception. * * @param ret the method\u0026#39;s original return value. May be null if the method triggers an exception. * @return the method\u0026#39;s actual return value. */ Object afterMethod(EnhancedInstance objInst, Method method, Object[] allArguments, Class\u0026lt;?\u0026gt;[] argumentsTypes, Object ret, MethodInvocationContext context) throws Throwable; /** * called when occur exception. * * @param t the exception occur. */ void handleMethodException(EnhancedInstance objInst, Method method, Object[] allArguments, Class\u0026lt;?\u0026gt;[] argumentsTypes, Throwable t, MethodInvocationContext context); } Bootstrap class instrumentation. SkyWalking has packaged the bootstrap instrumentation in the agent core. You can easily implement it by declaring it in the instrumentation definition.\nOverride the public boolean isBootstrapInstrumentation() and return true. Such as\npublic class URLInstrumentation extends ClassEnhancePluginDefine { private static String CLASS_NAME = \u0026#34;java.net.URL\u0026#34;; @Override protected ClassMatch enhanceClass() { return byName(CLASS_NAME); } @Override public ConstructorInterceptPoint[] getConstructorsInterceptPoints() { return new ConstructorInterceptPoint[] { new ConstructorInterceptPoint() { @Override public ElementMatcher\u0026lt;MethodDescription\u0026gt; getConstructorMatcher() { return any(); } @Override public String getConstructorInterceptor() { return \u0026#34;org.apache.skywalking.apm.plugin.jre.httpurlconnection.Interceptor2\u0026#34;; } } }; } @Override public InstanceMethodsInterceptPoint[] getInstanceMethodsInterceptPoints() { return new InstanceMethodsInterceptPoint[0]; } @Override public StaticMethodsInterceptPoint[] getStaticMethodsInterceptPoints() { return new StaticMethodsInterceptPoint[0]; } @Override public boolean isBootstrapInstrumentation() { return true; } } ClassEnhancePluginDefineV2 is provided in v2 APIs, #isBootstrapInstrumentation works too.\nNOTE: Bootstrap instrumentation should be used only where necessary. During its actual execution, it mostly affects the JRE core(rt.jar). Defining it other than where necessary could lead to unexpected results or side effects.\nProvide custom config for the plugin The config could provide different behaviours based on the configurations. The SkyWalking plugin mechanism provides the configuration injection and initialization system in the agent core.\nEvery plugin could declare one or more classes to represent the config by using @PluginConfig annotation. The agent core could initialize this class' static field through System environments, System properties, and agent.config static file.\nThe #root() method in the @PluginConfig annotation requires declaring the root class for the initialization process. Typically, SkyWalking prefers to use nested inner static classes for the hierarchy of the configuration. We recommend using Plugin/plugin-name/config-key as the nested classes structure of the config class.\nNOTE: because of the Java ClassLoader mechanism, the @PluginConfig annotation should be added on the real class used in the interceptor codes.\nIn the following example, @PluginConfig(root = SpringMVCPluginConfig.class) indicates that initialization should start with using SpringMVCPluginConfig as the root. Then, the config key of the attribute USE_QUALIFIED_NAME_AS_ENDPOINT_NAME should be plugin.springmvc.use_qualified_name_as_endpoint_name.\npublic class SpringMVCPluginConfig { public static class Plugin { // NOTE, if move this annotation on the `Plugin` or `SpringMVCPluginConfig` class, it no longer has any effect.  @PluginConfig(root = SpringMVCPluginConfig.class) public static class SpringMVC { /** * If true, the fully qualified method name will be used as the endpoint name instead of the request URL, * default is false. */ public static boolean USE_QUALIFIED_NAME_AS_ENDPOINT_NAME = false; /** * This config item controls that whether the SpringMVC plugin should collect the parameters of the * request. */ public static boolean COLLECT_HTTP_PARAMS = false; } @PluginConfig(root = SpringMVCPluginConfig.class) public static class Http { /** * When either {@link Plugin.SpringMVC#COLLECT_HTTP_PARAMS} is enabled, how many characters to keep and send * to the OAP backend, use negative values to keep and send the complete parameters, NB. this config item is * added for the sake of performance */ public static int HTTP_PARAMS_LENGTH_THRESHOLD = 1024; } } } Meter Plugin Java agent plugin could use meter APIs to collect metrics for backend analysis.\n Counter API represents a single monotonically increasing counter which automatically collects data and reports to the backend.  import org.apache.skywalking.apm.agent.core.meter.MeterFactory; Counter counter = MeterFactory.counter(meterName).tag(\u0026#34;tagKey\u0026#34;, \u0026#34;tagValue\u0026#34;).mode(Counter.Mode.INCREMENT).build(); counter.increment(1d);  MeterFactory.counter creates a new counter builder with the meter name. Counter.Builder.tag(String key, String value) marks a tag key/value pair. Counter.Builder.mode(Counter.Mode mode) changes the counter mode. RATE mode means the reporting rate to the backend. Counter.Builder.build() builds a new Counter which is collected and reported to the backend. Counter.increment(double count) increment counts to the Counter. It could be a positive value.   Gauge API represents a single numerical value.  import org.apache.skywalking.apm.agent.core.meter.MeterFactory; ThreadPoolExecutor threadPool = ...; Gauge gauge = MeterFactory.gauge(meterName, () -\u0026gt; threadPool.getActiveCount()).tag(\u0026#34;tagKey\u0026#34;, \u0026#34;tagValue\u0026#34;).build();  MeterFactory.gauge(String name, Supplier\u0026lt;Double\u0026gt; getter) creates a new gauge builder with the meter name and supplier function. This function must return a double value. Gauge.Builder.tag(String key, String value) marks a tag key/value pair. Gauge.Builder.build() builds a new Gauge which is collected and reported to the backend.   Histogram API represents a summary sample observations with customized buckets.  import org.apache.skywalking.apm.agent.core.meter.MeterFactory; Histogram histogram = MeterFactory.histogram(\u0026#34;test\u0026#34;).tag(\u0026#34;tagKey\u0026#34;, \u0026#34;tagValue\u0026#34;).steps(Arrays.asList(1, 5, 10)).minValue(0).build(); histogram.addValue(3);  MeterFactory.histogram(String name) creates a new histogram builder with the meter name. Histogram.Builder.tag(String key, String value) marks a tag key/value pair. Histogram.Builder.steps(List\u0026lt;Double\u0026gt; steps) sets up the max values of every histogram buckets. Histogram.Builder.minValue(double value) sets up the minimal value of this histogram. Default is 0. Histogram.Builder.build() builds a new Histogram which is collected and reported to the backend. Histogram.addValue(double value) adds value into the histogram, and automatically analyzes what bucket count needs to be incremented. Rule: count into [step1, step2).  Plugin Test Tool The Apache SkyWalking Agent Test Tool Suite is an incredibly useful test tool suite that is available in a wide variety of agent languages. It includes the mock collector and validator. The mock collector is a SkyWalking receiver, like the OAP server.\nYou could learn how to use this tool to test the plugin in this doc. This is a must if you want to contribute plugins to the SkyWalking official repo.\nContribute plugins to the Apache SkyWalking repository We welcome everyone to contribute their plugins.\nPlease follow these steps:\n Submit an issue for your plugin, including any supported versions. Create sub modules under apm-sniffer/apm-sdk-plugin or apm-sniffer/optional-plugins, and the name should include supported library name and versions. Follow this guide to develop. Make sure comments and test cases are provided. Develop and test. Provide the automatic test cases. Learn how to write the plugin test case from this doc Send a pull request and ask for review. The plugin committers will approve your plugins, plugin CI-with-IT, e2e, and the plugin tests will be passed. The plugin is accepted by SkyWalking.  ","excerpt":"Plugin Development Guide This document describes how to understand, develop and contribute a plugin. …","ref":"/docs/main/v8.6.0/en/guides/java-plugin-development-guide/","title":"Plugin Development Guide"},{"body":"Probe Introduction In SkyWalking, probe means an agent or SDK library integrated into a target system that takes charge of collecting telemetry data, including tracing and metrics. Depending on the target system tech stack, there are very different ways how the probe performs such tasks. But ultimately, they all work towards the same goal — to collect and reformat data, and then to send them to the backend.\nOn a high level, there are three typical categories in all SkyWalking probes.\n  Language based native agent. These agents run in target service user spaces, such as a part of user codes. For example, the SkyWalking Java agent uses the -javaagent command line argument to manipulate codes in runtime, where manipulate means to change and inject user\u0026rsquo;s codes. Another kind of agents uses certain hook or intercept mechanism provided by target libraries. As you can see, these agents are based on languages and libraries.\n  Service Mesh probes. Service Mesh probes collect data from sidecar, control panel in service mesh or proxy. In the old days, proxy is only used as an ingress of the whole cluster, but with the Service Mesh and sidecar, we can now perform observability functions.\n  3rd-party instrument library. SkyWalking accepts many widely used instrument libraries data formats. It analyzes the data, transfers it to SkyWalking\u0026rsquo;s formats of trace, metrics or both. This feature starts with accepting Zipkin span data. See Receiver for other tracers for more information.\n  You don\u0026rsquo;t need to use Language based native agent and Service Mesh probe at the same time, since they both serve to collect metrics data. Otherwise, your system will suffer twice the payload, and the analytic numbers will be doubled.\nThere are several recommended ways on how to use these probes:\n Use Language based native agent only. Use 3rd-party instrument library only, like the Zipkin instrument ecosystem. Use Service Mesh probe only. Use Service Mesh probe with Language based native agent or 3rd-party instrument library in tracing status. (Advanced usage)  What is the meaning of in tracing status?\nBy default, Language based native agent and 3rd-party instrument library both send distributed traces to the backend, where analyses and aggregation on those traces are performed. In tracing status means that the backend considers these traces as something like logs. In other words, the backend saves them, and builds the links between traces and metrics, like which endpoint and service does the trace belong?.\nWhat is next?  Learn more about the probes supported by SkyWalking in Service auto instrument agent, Manual instrument SDK, Service Mesh probe and Zipkin receiver. After understanding how the probe works, see the backend overview for more on analysis and persistence.  ","excerpt":"Probe Introduction In SkyWalking, probe means an agent or SDK library integrated into a target …","ref":"/docs/main/v8.6.0/en/concepts-and-designs/probe-introduction/","title":"Probe Introduction"},{"body":"Problem When you start your application with the skywalking agent, you may find this exception in your agent log which means that EnhanceRequireObjectCache cannot be casted to EnhanceRequireObjectCache. For example:\nERROR 2018-05-07 21:31:24 InstMethodsInter : class[class org.springframework.web.method.HandlerMethod] after method[getBean] intercept failure java.lang.ClassCastException: org.apache.skywalking.apm.plugin.spring.mvc.commons.EnhanceRequireObjectCache cannot be cast to org.apache.skywalking.apm.plugin.spring.mvc.commons.EnhanceRequireObjectCache at org.apache.skywalking.apm.plugin.spring.mvc.commons.interceptor.GetBeanInterceptor.afterMethod(GetBeanInterceptor.java:45) at org.apache.skywalking.apm.agent.core.plugin.interceptor.enhance.InstMethodsInter.intercept(InstMethodsInter.java:105) at org.springframework.web.method.HandlerMethod.getBean(HandlerMethod.java) at org.springframework.web.servlet.handler.AbstractHandlerMethodExceptionResolver.shouldApplyTo(AbstractHandlerMethodExceptionResolver.java:47) at org.springframework.web.servlet.handler.AbstractHandlerExceptionResolver.resolveException(AbstractHandlerExceptionResolver.java:131) at org.springframework.web.servlet.handler.HandlerExceptionResolverComposite.resolveException(HandlerExceptionResolverComposite.java:76) ... Reason This exception may be caused by hot deployment tools (spring-boot-devtool) or otherwise, which changes the classloader in runtime.\nResolution  This error does not occur under the production environment, since developer tools are automatically disabled: See spring-boot-devtools. If you would like to debug in your development environment as usual, you should temporarily remove such hot deployment package in your lib path.  ","excerpt":"Problem When you start your application with the skywalking agent, you may find this exception in …","ref":"/docs/main/v8.6.0/en/faq/enhancerequireobjectcache-cast-exception/","title":"Problem"},{"body":"Problem  When importing the SkyWalking project to Eclipse, the following errors may occur:   Software being installed: Checkstyle configuration plugin for M2Eclipse 1.0.0.201705301746 (com.basistech.m2e.code.quality.checkstyle.feature.feature.group 1.0.0.201705301746) Missing requirement: Checkstyle configuration plugin for M2Eclipse 1.0.0.201705301746 (com.basistech.m2e.code.quality.checkstyle.feature.feature.group 1.0.0.201705301746) requires \u0026lsquo;net.sf.eclipsecs.core 5.2.0\u0026rsquo; but it could not be found\n Reason The Eclipse Checkstyle Plug-in has not been installed.\nResolution Download the plug-in at the link here: https://sourceforge.net/projects/eclipse-cs/?source=typ_redirect Eclipse Checkstyle Plug-in version 8.7.0.201801131309 is required. Plug-in notification: The Eclipse Checkstyle plug-in integrates the Checkstyle Java code auditor into the Eclipse IDE. The plug-in provides real-time feedback to the user on rule violations, including checking against coding style and error-prone code constructs.\n","excerpt":"Problem  When importing the SkyWalking project to Eclipse, the following errors may occur: …","ref":"/docs/main/v8.6.0/en/faq/import-project-eclipse-requireitems-exception/","title":"Problem"},{"body":"Problem Tracing doesn\u0026rsquo;t work on the Kafka consumer end.\nReason The kafka client is responsible for pulling messages from the brokers, after which the data will be processed by user-defined codes. However, only the poll action can be traced by the plug-in and the subsequent data processing work inevitably goes beyond the scope of the trace context. Thus, in order to complete tracing on the client end, manual instrumentation is required, i.e. the poll action and the processing action should be wrapped manually.\nResolve For a native Kafka client, please use the Application Toolkit libraries to do the manual instrumentation, with the help of the @KafkaPollAndInvoke annotation in apm-toolkit-kafka or with OpenTracing API. If you\u0026rsquo;re using spring-kafka 1.3.x, 2.2.x or above, you can easily trace the consumer end without further configuration.\n","excerpt":"Problem Tracing doesn\u0026rsquo;t work on the Kafka consumer end.\nReason The kafka client is responsible …","ref":"/docs/main/v8.6.0/en/faq/kafka-plugin/","title":"Problem"},{"body":"Problem When using a thread pool, TraceSegment data in a thread cannot be reported and there are memory data that cannot be recycled (memory leaks).\nExample ThreadPoolTaskExecutor executor = new ThreadPoolTaskExecutor(); executor.setThreadFactory(r -\u0026gt; new Thread(RunnableWrapper.of(r))); Reason  Worker threads are enhanced when using the thread pool. Based on the design of the SkyWalking Java Agent, when tracing a cross thread, you must enhance the task thread.  Resolution   When using Thread Schedule Framework: See SkyWalking Thread Schedule Framework at SkyWalking Java agent supported list, such as Spring FrameWork @Async, which can implement tracing without any modification.\n  When using Custom Thread Pool: Enhance the task thread with the following code.\n  ExecutorService executorService = Executors.newFixedThreadPool(1); executorService.execute(RunnableWrapper.of(new Runnable() { @Override public void run() { //your code  } })); See across thread solution APIs for more use cases.\n","excerpt":"Problem When using a thread pool, TraceSegment data in a thread cannot be reported and there are …","ref":"/docs/main/v8.6.0/en/faq/memory-leak-enhance-worker-thread/","title":"Problem"},{"body":"Problem  In maven build, the following error may occur with the protoc-plugin:  [ERROR] Failed to execute goal org.xolstice.maven.plugins:protobuf-maven-plugin:0.5.0:compile-custom (default) on project apm-network: Unable to copy the file to \\skywalking\\apm-network\\target\\protoc-plugins: \\skywalking\\apm-network\\target\\protoc-plugins\\protoc-3.3.0-linux-x86_64.exe (The process cannot access the file because it is being used by another process) -\u0026gt; [Help 1] Reason  The Protobuf compiler is dependent on the glibc. However, glibc has not been installed, or there is an old version already installed in the system.  Resolution  Install or upgrade to the latest version of the glibc library. Under the container environment, the latest glibc version of the alpine system is recommended. Please refer to http://www.gnu.org/software/libc/documentation.html.  ","excerpt":"Problem  In maven build, the following error may occur with the protoc-plugin:  [ERROR] Failed to …","ref":"/docs/main/v8.6.0/en/faq/protoc-plugin-fails-when-build/","title":"Problem"},{"body":"Problem The message with Field ID, 8888, must be reserved.\nReason Because Thrift cannot carry metadata to transport Trace Header in the original API, we transport them by wrapping TProtocolFactory.\nThrift allows us to append any additional fields in the message even if the receiver doesn\u0026rsquo;t deal with them. Those data will be skipped and left unread. Based on this, the 8888th field of the message is used to store Trace Header (or metadata) and to transport them. That means the message with Field ID, 8888, must be reserved.\nResolution Avoid using the Field(ID is 8888) in your application.\n","excerpt":"Problem The message with Field ID, 8888, must be reserved.\nReason Because Thrift cannot carry …","ref":"/docs/main/v8.6.0/en/faq/thrift-plugin/","title":"Problem"},{"body":"Problem  There is no abnormal log in Agent log and Collector log. The traces can be seen, but no other information is available in UI.  Reason The operating system where the monitored system is located is not set as the current time zone, causing statistics collection time points to deviate.\nResolution Make sure the time is synchronized between collector servers and monitored application servers.\n","excerpt":"Problem  There is no abnormal log in Agent log and Collector log. The traces can be seen, but no …","ref":"/docs/main/v8.6.0/en/faq/why-have-traces-no-others/","title":"Problem"},{"body":"Problem： Maven compilation failure with error such as Error: not found: python2 When you compile the project via Maven, it fails at module apm-webapp and the following error occurs.\nPay attention to keywords such as node-sass and Error: not found: python2.\n[INFO] \u0026gt; node-sass@4.11.0 postinstall C:\\XXX\\skywalking\\skywalking-ui\\node_modules\\node-sass [INFO] \u0026gt; node scripts/build.js [ERROR] gyp verb check python checking for Python executable \u0026quot;python2\u0026quot; in the PATH [ERROR] gyp verb `which` failed Error: not found: python2 [ERROR] gyp verb `which` failed at getNotFoundError (C:\\XXX\\skywalking\\skywalking-ui\\node_modules\\which\\which.js:13:12) [ERROR] gyp verb `which` failed at F (C:\\XXX\\skywalking\\skywalking-ui\\node_modules\\which\\which.js:68:19) [ERROR] gyp verb `which` failed at E (C:\\XXX\\skywalking\\skywalking-ui\\node_modules\\which\\which.js:80:29) [ERROR] gyp verb `which` failed at C:\\XXX\\skywalking\\skywalking-ui\\node_modules\\which\\which.js:89:16 [ERROR] gyp verb `which` failed at C:\\XXX\\skywalking\\skywalking-ui\\node_modules\\isexe\\index.js:42:5 [ERROR] gyp verb `which` failed at C:\\XXX\\skywalking\\skywalking-ui\\node_modules\\isexe\\windows.js:36:5 [ERROR] gyp verb `which` failed at FSReqWrap.oncomplete (fs.js:152:21) [ERROR] gyp verb `which` failed code: 'ENOENT' } [ERROR] gyp verb check python checking for Python executable \u0026quot;python\u0026quot; in the PATH [ERROR] gyp verb `which` succeeded python C:\\Users\\XXX\\AppData\\Local\\Programs\\Python\\Python37\\python.EXE [ERROR] gyp ERR! configure error [ERROR] gyp ERR! stack Error: Command failed: C:\\Users\\XXX\\AppData\\Local\\Programs\\Python\\Python37\\python.EXE -c import sys; print \u0026quot;%s.%s.%s\u0026quot; % sys.version_info[:3]; [ERROR] gyp ERR! stack File \u0026quot;\u0026lt;string\u0026gt;\u0026quot;, line 1 [ERROR] gyp ERR! stack import sys; print \u0026quot;%s.%s.%s\u0026quot; % sys.version_info[:3]; [ERROR] gyp ERR! stack ^ [ERROR] gyp ERR! stack SyntaxError: invalid syntax [ERROR] gyp ERR! stack [ERROR] gyp ERR! stack at ChildProcess.exithandler (child_process.js:275:12) [ERROR] gyp ERR! stack at emitTwo (events.js:126:13) [ERROR] gyp ERR! stack at ChildProcess.emit (events.js:214:7) [ERROR] gyp ERR! stack at maybeClose (internal/child_process.js:925:16) [ERROR] gyp ERR! stack at Process.ChildProcess._handle.onexit (internal/child_process.js:209:5) [ERROR] gyp ERR! System Windows_NT 10.0.17134 ...... [INFO] server-starter-es7 ................................. SUCCESS [ 11.657 s] [INFO] apm-webapp ......................................... FAILURE [ 25.857 s] [INFO] apache-skywalking-apm .............................. SKIPPED [INFO] apache-skywalking-apm-es7 .......................... SKIPPED Reason The error has nothing to do with SkyWalking.\nAccording to the issue here (https://github.com/sass/node-sass/issues/1176), if you live in countries where requesting resources from GitHub and npmjs.org runs slow, some precompiled binaries for dependency node-sass would fail to be downloaded during npm install, and npm would try to compile them itself. That\u0026rsquo;s why python2 is needed.\nResolution 1. Use mirror. For instance, if you\u0026rsquo;re in China, please edit skywalking\\apm-webapp\\pom.xml as follows. Find\n\u0026lt;configuration\u0026gt; \u0026lt;arguments\u0026gt;install --registry=https://registry.npmjs.org/\u0026lt;/arguments\u0026gt; \u0026lt;/configuration\u0026gt; Replace it with\n\u0026lt;configuration\u0026gt; \u0026lt;arguments\u0026gt;install --registry=https://registry.npm.taobao.org/ --sass_binary_site=https://npm.taobao.org/mirrors/node-sass/\u0026lt;/arguments\u0026gt; \u0026lt;/configuration\u0026gt; 2. Get a sufficiently powerful VPN. ","excerpt":"Problem： Maven compilation failure with error such as Error: not found: python2 When you compile the …","ref":"/docs/main/v8.6.0/en/faq/maven-compile-npm-failure/","title":"Problem： Maven compilation failure with error such as `Error： not found： python2`"},{"body":"Protocols There are two different types of protocols.\n  Probe Protocol. It includes descriptions and definitions on how agents send collected metrics data and traces, as well as the format of each entity.\n  Query Protocol. The backend enables the query function in SkyWalking\u0026rsquo;s own UI and other UIs. These queries are based on GraphQL.\n  Probe Protocols They also related to the probe group. For more information, see Concepts and Designs. These groups are language-based native agent protocol, service mesh protocol and 3rd-party instrument protocol.\nLanguage-based native agent protocol There are two types of protocols that help language agents work in distributed environments.\n Cross Process Propagation Headers Protocol and Cross Process Correlation Headers Protocol come in in-wire data format. Agent/SDK usually uses HTTP/MQ/HTTP2 headers to carry the data with the RPC request. The remote agent will receive this in the request handler, and bind the context with this specific request. Trace Data Protocol is in out-of-wire data format. Agent/SDK uses this to send traces and metrics to SkyWalking or other compatible backends.  Cross Process Propagation Headers Protocol v3 has been the new protocol for in-wire context propagation since the version 8.0.0 release.\nCross Process Correlation Headers Protocol v1 is a new in-wire context propagation protocol which is additional and optional. Please read SkyWalking language agents documentation to see whether it is supported. This protocol defines the data format of transporting custom data with Cross Process Propagation Headers Protocol. It has been supported by the SkyWalking javaagent since 8.0.0,\nSkyWalking Trace Data Protocol v3 defines the communication method and format between the agent and backend.\nSkyWalking Log Data Protocol defines the communication method and format between the agent and backend.\nBrowser probe protocol The browser probe, such as skywalking-client-js, could use this protocol to send data to the backend. This service is provided by gRPC.\nSkyWalking Browser Protocol defines the communication method and format between skywalking-client-js and backend.\nService Mesh probe protocol The probe in sidecar or proxy could use this protocol to send data to the backend. This service provided by gRPC requires the following key information:\n Service Name or ID on both sides. Service Instance Name or ID on both sides. Endpoint. URI in HTTP, service method full signature in gRPC. Latency. In milliseconds. Response code in HTTP Status. Success or fail. Protocol. HTTP, gRPC DetectPoint. In Service Mesh sidecar, client or server. In normal L7 proxy, value is proxy.  Events Report Protocol The protocol is used to report events to the backend. The doc introduces the definition of an event, and the protocol repository defines gRPC services and message formats of events.\n3rd-party instrument protocol 3rd-party instrument protocols are not defined by SkyWalking. They are just protocols/formats with which SkyWalking is compatible, and SkyWalking could receive them from their existing libraries. SkyWalking starts with supporting Zipkin v1, v2 data formats.\nThe backend has a modular design, so it is very easy to extend a new receiver to support a new protocol/format.\nQuery Protocol The query protocol follows GraphQL grammar, and provides data query capabilities, which depends on your analysis metrics. Read query protocol doc for more details.\n","excerpt":"Protocols There are two different types of protocols.\n  Probe Protocol. It includes descriptions and …","ref":"/docs/main/v8.6.0/en/protocols/readme/","title":"Protocols"},{"body":"Query Protocol Query Protocol defines a set of APIs in GraphQL grammar to provide data query and interactive capabilities with SkyWalking native visualization tool or 3rd party system, including Web UI, CLI or private system.\nQuery protocol official repository, https://github.com/apache/skywalking-query-protocol.\nMetadata Metadata contains concise information on all services and their instances, endpoints, etc. under monitoring. You may query the metadata in different ways.\nextend type Query { getGlobalBrief(duration: Duration!): ClusterBrief # Normal service related metainfo  getAllServices(duration: Duration!): [Service!]! searchServices(duration: Duration!, keyword: String!): [Service!]! searchService(serviceCode: String!): Service # Fetch all services of Browser type getAllBrowserServices(duration: Duration!): [Service!]! # Service intance query getServiceInstances(duration: Duration!, serviceId: ID!): [ServiceInstance!]! # Endpoint query # Consider there are huge numbers of endpoint, # must use endpoint owner\u0026#39;s service id, keyword and limit filter to do query. searchEndpoint(keyword: String!, serviceId: ID!, limit: Int!): [Endpoint!]! getEndpointInfo(endpointId: ID!): EndpointInfo # Database related meta info. getAllDatabases(duration: Duration!): [Database!]! getTimeInfo: TimeInfo } Topology The topology and dependency graphs among services, instances and endpoints. Includes direct relationships or global maps.\nextend type Query { # Query the global topology getGlobalTopology(duration: Duration!): Topology # Query the topology, based on the given service getServiceTopology(serviceId: ID!, duration: Duration!): Topology # Query the topology, based on the given services. # `#getServiceTopology` could be replaced by this. getServicesTopology(serviceIds: [ID!]!, duration: Duration!): Topology # Query the instance topology, based on the given clientServiceId and serverServiceId getServiceInstanceTopology(clientServiceId: ID!, serverServiceId: ID!, duration: Duration!): ServiceInstanceTopology # Query the topology, based on the given endpoint getEndpointTopology(endpointId: ID!, duration: Duration!): Topology # v2 of getEndpointTopology getEndpointDependencies(endpointId: ID!, duration: Duration!): EndpointTopology } Metrics Metrics query targets all objects defined in OAL script and MAL. You may obtain the metrics data in linear or thermodynamic matrix formats based on the aggregation functions in script.\nV2 APIs Provide Metrics V2 query APIs since 8.0.0, including metadata, single/multiple values, heatmap, and sampled records metrics.\nextend type Query { # Metrics definition metadata query. Response the metrics type which determines the suitable query methods. typeOfMetrics(name: String!): MetricsType! # Get the list of all available metrics in the current OAP server. # Param, regex, could be used to filter the metrics by name. listMetrics(regex: String): [MetricDefinition!]! # Read metrics single value in the duration of required metrics readMetricsValue(condition: MetricsCondition!, duration: Duration!): Long! # Read time-series values in the duration of required metrics readMetricsValues(condition: MetricsCondition!, duration: Duration!): MetricsValues! # Read entity list of required metrics and parent entity type. sortMetrics(condition: TopNCondition!, duration: Duration!): [SelectedRecord!]! # Read value in the given time duration, usually as a linear. # labels: the labels you need to query. readLabeledMetricsValues(condition: MetricsCondition!, labels: [String!]!, duration: Duration!): [MetricsValues!]! # Heatmap is bucket based value statistic result. readHeatMap(condition: MetricsCondition!, duration: Duration!): HeatMap # Read the sampled records # TopNCondition#scope is not required. readSampledRecords(condition: TopNCondition!, duration: Duration!): [SelectedRecord!]! } V1 APIs 3 types of metrics can be queried. V1 APIs were introduced since 6.x. Now they are a shell to V2 APIs.\n Single value. Most default metrics are in single value. getValues and getLinearIntValues are suitable for this purpose. Multiple value. A metric defined in OAL includes multiple value calculations. Use getMultipleLinearIntValues to obtain all values. percentile is a typical multiple value function in OAL. Heatmap value. Read Heatmap in WIKI for details. thermodynamic is the only OAL function. Use getThermodynamic to get the values.  extend type Query { getValues(metric: BatchMetricConditions!, duration: Duration!): IntValues getLinearIntValues(metric: MetricCondition!, duration: Duration!): IntValues # Query the type of metrics including multiple values, and format them as multiple linears. # The seq of these multiple lines base on the calculation func in OAL # Such as, should us this to query the result of func percentile(50,75,90,95,99) in OAL, # then five lines will be responsed, p50 is the first element of return value. getMultipleLinearIntValues(metric: MetricCondition!, numOfLinear: Int!, duration: Duration!): [IntValues!]! getThermodynamic(metric: MetricCondition!, duration: Duration!): Thermodynamic } Metrics are defined in the config/oal/*.oal files.\nAggregation Aggregation query means that the metrics data need a secondary aggregation at query stage, which causes the query interfaces to have some different arguments. A typical example of aggregation query is the TopN list of services. Metrics stream aggregation simply calculates the metrics values of each service, but the expected list requires ordering metrics data by their values.\nAggregation query is for single value metrics only.\n# The aggregation query is different with the metric query. # All aggregation queries require backend or/and storage do aggregation in query time. extend type Query { # TopN is an aggregation query. getServiceTopN(name: String!, topN: Int!, duration: Duration!, order: Order!): [TopNEntity!]! getAllServiceInstanceTopN(name: String!, topN: Int!, duration: Duration!, order: Order!): [TopNEntity!]! getServiceInstanceTopN(serviceId: ID!, name: String!, topN: Int!, duration: Duration!, order: Order!): [TopNEntity!]! getAllEndpointTopN(name: String!, topN: Int!, duration: Duration!, order: Order!): [TopNEntity!]! getEndpointTopN(serviceId: ID!, name: String!, topN: Int!, duration: Duration!, order: Order!): [TopNEntity!]! } Others The following queries are for specific features, including trace, alarm, and profile.\n Trace. Query distributed traces by this. Alarm. Through alarm query, you can find alarm trends and their details.  The actual query GraphQL scripts can be found in the query-protocol folder here.\nCondition Duration Duration is a widely used parameter type as the APM data is time-related. See the following for more details. Step relates to precision.\n# The Duration defines the start and end time for each query operation. # Fields: `start` and `end` # represents the time span. And each of them matches the step. # ref https://www.ietf.org/rfc/rfc3339.txt # The time formats are # `SECOND` step: yyyy-MM-dd HHmmss # `MINUTE` step: yyyy-MM-dd HHmm # `HOUR` step: yyyy-MM-dd HH # `DAY` step: yyyy-MM-dd # `MONTH` step: yyyy-MM # Field: `step` # represents the accurate time point. # e.g. # if step==HOUR , start=2017-11-08 09, end=2017-11-08 19 # then # metrics from the following time points expected # 2017-11-08 9:00 -\u0026gt; 2017-11-08 19:00 # there are 11 time points (hours) in the time span. input Duration { start: String! end: String! step: Step! } enum Step { MONTH DAY HOUR MINUTE SECOND } ","excerpt":"Query Protocol Query Protocol defines a set of APIs in GraphQL grammar to provide data query and …","ref":"/docs/main/v8.6.0/en/protocols/query-protocol/","title":"Query Protocol"},{"body":"Register mechanism is no longer required for local / exit span Since version 6.6.0, SkyWalking has removed the local and exit span registers. If an old java agent (before 6.6.0) is still running, which registers to the 6.6.0+ backend, you will face the following warning message.\nclass=RegisterServiceHandler, message = Unexpected endpoint register, endpoint isn't detected from server side. This will not harm the backend or cause any issues, but serves as a reminder that your agent or other clients should follow the new protocol requirements.\nYou could simply use log4j2.xml to filter this warning message out.\n","excerpt":"Register mechanism is no longer required for local / exit span Since version 6.6.0, SkyWalking has …","ref":"/docs/main/v8.6.0/en/faq/unexpected-endpoint-register/","title":"Register mechanism is no longer required for local / exit span"},{"body":"Scopes and Fields Using the Aggregation Function, the requests will be grouped by time and Group Key(s) in each scope.\nSCOPE All    Name Remarks Group Key Type     name The service name of each request.  string   serviceInstanceName The name of the service instance ID.  string   endpoint The endpoint path of each request.  string   latency The time taken by each request.  int(in ms)   status The success or failure of the request.  bool(true for success)   responseCode The response code of the HTTP response, and if this request is the HTTP call. E.g. 200, 404, 302  int   type The type of each request, such as Database, HTTP, RPC, or gRPC.  enum   tags The labels of each request. Each value is made up by TagKey:TagValue in the segment.  List\u0026lt;String\u0026gt;    SCOPE Service This calculates the metrics data from each request of the service.\n   Name Remarks Group Key Type     name The name of the service.  string   nodeType The kind of node to which the Service or Network address belongs, such as Normal, Database, MQ, or Cache.  enum   serviceInstanceName The name of the service instance ID.  string   endpointName The name of the endpoint, such as a full path of HTTP URI.  string   latency The time taken by each request.  int   status Indicates the success or failure of the request.  bool(true for success)   responseCode The response code of the HTTP response, if this request is an HTTP call.  int   type The type of each request. Such as: Database, HTTP, RPC, gRPC.  enum   tags The labels of each request. Each value is made up by TagKey:TagValue in the segment.  List\u0026lt;String\u0026gt;   sideCar.internalErrorCode The sidecar/gateway proxy internal error code. The value is based on the implementation.  string   tcpInfo.receivedBytes The received bytes of the TCP traffic, if this request is a TCP call.  long   tcpInfo.sentBytes The sent bytes of the TCP traffic, if this request is a TCP call.  long    SCOPE ServiceInstance This calculates the metrics data from each request of the service instance.\n   Name Remarks Group Key Type     name The name of the service instance, such as ip:port@Service Name. Note: Currently, the native agent uses uuid@ipv4 as the instance name, which does not assist in setting up a filter in aggregation.  string   serviceName The name of the service.  string   nodeType The kind of node to which the Service or Network address belongs, such as Normal, Database, MQ, or Cache.  enum   endpointName The name of the endpoint, such as a full path of the HTTP URI.  string   latency The time taken by each request.  int   status Indicates the success or failure of the request.  bool(true for success)   responseCode The response code of HTTP response, if this request is an HTTP call.  int   type The type of each request, such as Database, HTTP, RPC, or gRPC.  enum   tags The labels of each request. Each value is made up by TagKey:TagValue in the segment.  List\u0026lt;String\u0026gt;   sideCar.internalErrorCode The sidecar/gateway proxy internal error code. The value is based on the implementation.  string   tcpInfo.receivedBytes The received bytes of the TCP traffic, if this request is a TCP call.  long   tcpInfo.sentBytes The sent bytes of the TCP traffic, if this request is a TCP call.  long    Secondary scopes of ServiceInstance This calculates the metrics data if the service instance is a JVM and collects through javaagent.\n SCOPE ServiceInstanceJVMCPU     Name Remarks Group Key Type     name The name of the service instance, such as ip:port@Service Name. Note: Currently, the native agent uses uuid@ipv4 as the instance name, which does not assist in setting up a filter in aggregation.  string   serviceName The name of the service.  string   usePercent The percentage of CPU time spent.  double    SCOPE ServiceInstanceJVMMemory     Name Remarks Group Key Type     name The name of the service instance, such as ip:port@Service Name. Note: Currently, the native agent uses uuid@ipv4 as the instance name, which does not assist in setting up a filter in aggregation.  string   serviceName The name of the service.  string   heapStatus Indicates whether the metric has a heap property or not.  bool   init See the JVM documentation.  long   max See the JVM documentation.  long   used See the JVM documentation.  long   committed See the JVM documentation.  long    SCOPE ServiceInstanceJVMMemoryPool     Name Remarks Group Key Type     name The name of the service instance, such as ip:port@Service Name. Note: Currently, the native agent uses uuid@ipv4 as the instance name, which does not assist in setting up a filter in aggregation.  string   serviceName The name of the service.  string   poolType The type may be CODE_CACHE_USAGE, NEWGEN_USAGE, OLDGEN_USAGE, SURVIVOR_USAGE, PERMGEN_USAGE, or METASPACE_USAGE based on different versions of JVM.  enum   init See the JVM documentation.  long   max See the JVM documentation.  long   used See the JVM documentation.  long   committed See the JVM documentation.  long    SCOPE ServiceInstanceJVMGC     Name Remarks Group Key Type     name The name of the service instance, such as ip:port@Service Name. Note: Currently, the native agent uses uuid@ipv4 as the instance name, which does not assist in setting up a filter in aggregation.  string   serviceName The name of the service.  string   phrase Includes both NEW and OLD.  Enum   time The time spent in GC.  long   count The count in GC operations.  long    SCOPE ServiceInstanceJVMThread     Name Remarks Group Key Type     name The name of the service instance, such as ip:port@Service Name. Note: Currently, the native agent uses uuid@ipv4 as the instance name, which does not assist in setting up a filter in aggregation.  string   serviceName The name of the service.  string   liveCount The current number of live threads.  int   daemonCount The current number of daemon threads.  int   peakCount The current number of peak threads.  int    SCOPE Endpoint This calculates the metrics data from each request of the endpoint in the service.\n   Name Remarks Group Key Type     name The name of the endpoint, such as a full path of the HTTP URI.  string   serviceName The name of the service.  string   serviceNodeType The type of node to which the Service or Network address belongs, such as Normal, Database, MQ, or Cache.  enum   serviceInstanceName The name of the service instance ID.  string   latency The time taken by each request.  int   status Indicates the success or failure of the request.  bool(true for success)   responseCode The response code of HTTP response, if this request is an HTTP call.  int   type The type of each request, such as Database, HTTP, RPC, or gRPC.  enum   tags The labels of each request. Each value is made up by TagKey:TagValue in the segment.  List\u0026lt;String\u0026gt;   sideCar.internalErrorCode The sidecar/gateway proxy internal error code. The value is based on the implementation.  string   tcpInfo.receivedBytes The received bytes of the TCP traffic, if this request is a TCP call.  long   tcpInfo.sentBytes The sent bytes of the TCP traffic, if this request is a TCP call.  long    SCOPE ServiceRelation This calculates the metrics data from each request between services.\n   Name Remarks Group Key Type     sourceServiceName The name of the source service.  string   sourceServiceNodeType The type of node to which the Service or Network address belongs, such as Normal, Database, MQ, or Cache.  enum   sourceServiceInstanceName The name of the source service instance.  string   destServiceName The name of the destination service.  string   destServiceNodeType The type of node of to which the Service or Network address belongs.  enum   destServiceInstanceName The name of the destination service instance.  string   endpoint The endpoint used in this call.  string   componentId The ID of component used in this call. yes string   latency The time taken by each request.  int   status Indicates the success or failure of the request.  bool(true for success)   responseCode The response code of HTTP response, if this request is an HTTP call.  int   type The type of each request, such as Database, HTTP, RPC, or gRPC.  enum   detectPoint Where the relation is detected. The value may be client, server, or proxy. yes enum   tlsMode The TLS mode between source and destination services, such as service_relation_mtls_cpm = from(ServiceRelation.*).filter(tlsMode == \u0026quot;mTLS\u0026quot;).cpm()  string   sideCar.internalErrorCode The sidecar/gateway proxy internal error code. The value is based on the implementation.  string   tcpInfo.receivedBytes The received bytes of the TCP traffic, if this request is a TCP call.  long   tcpInfo.sentBytes The sent bytes of the TCP traffic, if this request is a TCP call.  long    SCOPE ServiceInstanceRelation This calculates the metrics data from each request between service instances.\n   Name Remarks Group Key Type     sourceServiceName The name of the source service.  string   sourceServiceNodeType The type of node to which the Service or Network address belongs, such as Normal, Database, MQ, or Cache.  enum   sourceServiceInstanceName The name of the source service instance.  string   destServiceName The name of the destination service.     destServiceNodeType The type of node to which the Service or Network address belongs, such as Normal, Database, MQ, or Cache.  string   destServiceInstanceName The name of the destination service instance.  string   endpoint The endpoint used in this call.  string   componentId The ID of the component used in this call. yes string   latency The time taken by each request.  int   status Indicates the success or failure of the request.  bool(true for success)   responseCode The response code of the HTTP response, if this request is an HTTP call.  int   type The type of each request, such as Database, HTTP, RPC, or gRPC.  enum   detectPoint Where the relation is detected. The value may be client, server, or proxy. yes enum   tlsMode The TLS mode between source and destination service instances, such as service_instance_relation_mtls_cpm = from(ServiceInstanceRelation.*).filter(tlsMode == \u0026quot;mTLS\u0026quot;).cpm()  string   sideCar.internalErrorCode The sidecar/gateway proxy internal error code. The value is based on the implementation.  string   tcpInfo.receivedBytes The received bytes of the TCP traffic, if this request is a TCP call.  long   tcpInfo.sentBytes The sent bytes of the TCP traffic, if this request is a TCP call.  long    SCOPE EndpointRelation This calculates the metrics data of the dependency between endpoints. This relation is hard to detect, and it depends on the tracing library to propagate the previous endpoint. Therefore, the EndpointRelation scope aggregation comes into effect only in services under tracing by SkyWalking native agents, including auto instrument agents (like Java and .NET), OpenCensus SkyWalking exporter implementation, or other tracing context propagation in SkyWalking specification.\n   Name Remarks Group Key Type     endpoint The parent endpoint in the dependency.  string   serviceName The name of the service.  string   serviceNodeType The type of node to which the Service or Network address belongs, such as Normal, Database, MQ, or Cache.  enum   childEndpoint The endpoint used by the parent endpoint in row(1).  string   childServiceName The endpoint used by the parent service in row(1).  string   childServiceNodeType The type of node to which the Service or Network address belongs, such as Normal, Database, MQ, or Cache.  string   childServiceInstanceName The endpoint used by the parent service instance in row(1).  string   rpcLatency The latency of the RPC between the parent endpoint and childEndpoint, excluding the latency caused by the parent endpoint itself.     componentId The ID of the component used in this call. yes string   status Indicates the success or failure of the request.  bool(true for success)   responseCode The response code of the HTTP response, if this request is an HTTP call.  int   type The type of each request, such as Database, HTTP, RPC, or gRPC.  enum   detectPoint Indicates where the relation is detected. The value may be client, server, or proxy. yes enum    SCOPE BrowserAppTraffic This calculates the metrics data from each request of the browser application (browser only).\n   Name Remarks Group Key Type     name The browser application name of each request.  string   count The number of request, which is fixed at 1.  int   trafficCategory The traffic category. The value may be NORMAL, FIRST_ERROR, or ERROR.  enum   errorCategory The error category. The value may be AJAX, RESOURCE, VUE, PROMISE, or UNKNOWN.  enum    SCOPE BrowserAppSingleVersionTraffic This calculates the metrics data from each request of a single version in the browser application (browser only).\n   Name Remarks Group Key Type     name The single version name of each request.  string   serviceName The name of the browser application.  string   count The number of request, which is fixed at 1.  int   trafficCategory The traffic category. The value may be NORMAL, FIRST_ERROR, or ERROR.  enum   errorCategory The error category. The value may be AJAX, RESOURCE, VUE, PROMISE, or UNKNOWN.  enum    SCOPE BrowserAppPageTraffic This calculates the metrics data from each request of the page in the browser application (browser only).\n   Name Remarks Group Key Type     name The page name of each request.  string   serviceName The name of the browser application.  string   count The number of request, which is fixed at 1.  int   trafficCategory The traffic category. The value may be NORMAL, FIRST_ERROR, or ERROR.  enum   errorCategory The error category. The value may be AJAX, RESOURCE, VUE, PROMISE, or UNKNOWN.  enum    SCOPE BrowserAppPagePerf This calculates the metrics data form each request of the page in the browser application (browser only).\n   Name Remarks Group Key Type     name The page name of each request.  string   serviceName The name of the browser application.  string   redirectTime The time taken to redirect.  int(in ms)   dnsTime The DNS query time.  int(in ms)   ttfbTime Time to first byte.  int(in ms)   tcpTime TCP connection time.  int(in ms)   transTime Content transfer time.  int(in ms)   domAnalysisTime Dom parsing time.  int(in ms)   fptTime First paint time or blank screen time.  int(in ms)   domReadyTime Dom ready time.  int(in ms)   loadPageTime Page full load time.  int(in ms)   resTime Synchronous load resources in the page.  int(in ms)   sslTime Only valid for HTTPS.  int(in ms)   ttlTime Time to interact.  int(in ms)   firstPackTime First pack time.  int(in ms)   fmpTime First Meaningful Paint.  int(in ms)    ","excerpt":"Scopes and Fields Using the Aggregation Function, the requests will be grouped by time and Group …","ref":"/docs/main/v8.6.0/en/concepts-and-designs/scope-definitions/","title":"Scopes and Fields"},{"body":"Send Envoy metrics to SkyWalking with / without Istio Envoy defines a gRPC service to emit the metrics, whatever implements this protocol can be used to receive the metrics. SkyWalking has a built-in receiver that implements this protocol so that you can configure Envoy to emit its metrics to SkyWalking.\nAs an APM system, SkyWalking does not only receive and store the metrics emitted by Envoy, it also analyzes the topology of services and service instances.\nAttention: There are two versions of Envoy metrics service protocol up to date, v2 and v3, SkyWalking (8.3.0+) supports both of them.\nConfigure Envoy to send metrics to SkyWalking without Istio Envoy can be used with / without Istio\u0026rsquo;s control. This section introduces how to configure the standalone Envoy to send the metrics to SkyWalking.\nIn order to let Envoy send metrics to SkyWalking, we need to feed Envoy with a configuration which contains stats_sinks that includes envoy.metrics_service. This envoy.metrics_service should be configured as a config.grpc_service entry.\nThe interesting parts of the config is shown in the config below:\nstats_sinks: - name: envoy.metrics_service config: grpc_service: # Note: we can use google_grpc implementation as well. envoy_grpc: cluster_name: service_skywalking static_resources: ... clusters: - name: service_skywalking connect_timeout: 5s type: LOGICAL_DNS http2_protocol_options: {} dns_lookup_family: V4_ONLY lb_policy: ROUND_ROBIN load_assignment: cluster_name: service_skywalking endpoints: - lb_endpoints: - endpoint: address: socket_address: address: skywalking # This is the port where SkyWalking serving the Envoy Metrics Service gRPC stream. port_value: 11800 A more complete static configuration, can be observed here.\nNote that Envoy can also be configured dynamically through xDS Protocol.\nAs mentioned above, SkyWalking also builds the topology of services from the metrics, this is because Envoy also carries the service metadata along with the metrics, to feed the Envoy such metadata, another configuration part is as follows:\nnode: # ... other configs metadata: LABELS: app: test-app NAME: service-instance-name Configure Envoy to send metrics to SkyWalking with Istio Typically, Envoy can be also used under Istio\u0026rsquo;s control, where the configurations are much more simple because Istio composes the configurations for you and sends them to Envoy via xDS Protocol. Istio also automatically injects the metadata such as service name and instance name into the bootstrap configurations.\nUnder this circumstance, emitting the metrics to SyWalking is as simple as adding the option --set meshConfig.defaultConfig.envoyMetricsService.address=\u0026lt;skywalking.address.port.11800\u0026gt; to Istio install command, for example:\nistioctl install -y \\  --set profile=demo `# replace the profile as per your need` \\ --set meshConfig.defaultConfig.envoyMetricsService.address=\u0026lt;skywalking.address.port.11800\u0026gt; # replace \u0026lt;skywalking.address.port.11800\u0026gt; with your actual SkyWalking OAP address If you already have Istio installed, you can use the following command to apply the config without re-installing Istio:\nistioctl manifest install -y \\  --set profile=demo `# replace the profile as per your need` \\ --set meshConfig.defaultConfig.envoyMetricsService.address=\u0026lt;skywalking.address.port.11800\u0026gt; # replace \u0026lt;skywalking.address.port.11800\u0026gt; with your actual SkyWalking OAP address Metrics data Some Envoy statistics are listed in this list. A sample data that contains identifier can be found here, while the metrics only can be observed here.\n","excerpt":"Send Envoy metrics to SkyWalking with / without Istio Envoy defines a gRPC service to emit the …","ref":"/docs/main/v8.6.0/en/setup/envoy/metrics_service_setting/","title":"Send Envoy metrics to SkyWalking with / without Istio"},{"body":"Sending Envoy Metrics to SkyWalking OAP Server Example This is an example of sending Envoy Stats to SkyWalking OAP server through Metric Service.\nRunning the example The example requires docker and docker-compose to be installed in your local. It fetches images from Docker Hub.\nNote that in ths setup, we override the log4j2.xml config to set the org.apache.skywalking.oap.server.receiver.envoy logger level to DEBUG. This enables us to see the messages sent by Envoy to SkyWalking OAP server.\n$ make up $ docker-compose logs -f skywalking $ # Please wait for a moment until SkyWalking is ready and Envoy starts sending the stats. You will see similar messages like the following: skywalking_1 | 2019-08-31 23:57:40,672 - org.apache.skywalking.oap.server.receiver.envoy.MetricServiceGRPCHandler -26870 [grpc-default-executor-0] DEBUG [] - Received msg identifier { skywalking_1 | node { skywalking_1 | id: \u0026quot;ingress\u0026quot; skywalking_1 | cluster: \u0026quot;envoy-proxy\u0026quot; skywalking_1 | metadata { skywalking_1 | fields { skywalking_1 | key: \u0026quot;skywalking\u0026quot; skywalking_1 | value { skywalking_1 | string_value: \u0026quot;iscool\u0026quot; skywalking_1 | } skywalking_1 | } skywalking_1 | fields { skywalking_1 | key: \u0026quot;envoy\u0026quot; skywalking_1 | value { skywalking_1 | string_value: \u0026quot;isawesome\u0026quot; skywalking_1 | } skywalking_1 | } skywalking_1 | } skywalking_1 | locality { skywalking_1 | region: \u0026quot;ap-southeast-1\u0026quot; skywalking_1 | zone: \u0026quot;zone1\u0026quot; skywalking_1 | sub_zone: \u0026quot;subzone1\u0026quot; skywalking_1 | } skywalking_1 | build_version: \u0026quot;e349fb6139e4b7a59a9a359be0ea45dd61e589c5/1.11.1/Clean/RELEASE/BoringSSL\u0026quot; skywalking_1 | } skywalking_1 | } skywalking_1 | envoy_metrics { skywalking_1 | name: \u0026quot;cluster.service_skywalking.update_success\u0026quot; skywalking_1 | type: COUNTER skywalking_1 | metric { skywalking_1 | counter { skywalking_1 | value: 2.0 skywalking_1 | } skywalking_1 | timestamp_ms: 1567295859556 skywalking_1 | } skywalking_1 | } ... $ # To tear down: $ make down ","excerpt":"Sending Envoy Metrics to SkyWalking OAP Server Example This is an example of sending Envoy Stats to …","ref":"/docs/main/v8.6.0/en/setup/envoy/examples/metrics/readme/","title":"Sending Envoy Metrics to SkyWalking OAP Server Example"},{"body":"Service Auto Grouping SkyWalking supports various default and customized dashboard templates. Each template provides the reasonable layout for the services in the particular field. Such as, services with a language agent installed could have different metrics with service detected by the service mesh observability solution, and different with SkyWalking\u0026rsquo;s self-observability metrics dashboard.\nTherefore, since 8.3.0, SkyWalking OAP would generate the group based on this simple naming format.\n${service name} = [${group name}::]${logic name} Once the service name includes double colons(::), the literal string before the colons would be considered as the group name. In the latest GraphQL query, the group name has been provided as an option parameter.\n getAllServices(duration: Duration!, group: String): [Service!]!\n RocketBot UI dashboards(Standard type) support the group name for default and custom configurations.\n","excerpt":"Service Auto Grouping SkyWalking supports various default and customized dashboard templates. Each …","ref":"/docs/main/v8.6.0/en/setup/backend/service-auto-grouping/","title":"Service Auto Grouping"},{"body":"Service Auto Instrument Agent The service auto instrument agent is a subset of language-based native agents. This kind of agents is based on some language-specific features, especially those of a VM-based language.\nWhat does Auto Instrument mean? Many users learned about these agents when they first heard that \u0026ldquo;Not a single line of code has to be changed\u0026rdquo;. SkyWalking used to mention this in its readme page as well. However, this does not reflect the full picture. For end users, it is true that they no longer have to modify their codes in most cases. But it is important to understand that the codes are in fact still modified by the agent, which is usually known as \u0026ldquo;runtime code manipulation\u0026rdquo;. The underlying logic is that the auto instrument agent uses the VM interface for code modification to dynamically add in the instrument code, such as modifying the class in Java through javaagent premain.\nIn fact, although the SkyWalking team has mentioned that most auto instrument agents are VM-based, you may build such tools during compiling time rather than runtime.\nWhat are the limitations? Auto instrument is very helpful, as you may perform auto instrument during compiling time, without having to depend on VM features. But there are also certain limitations that come with it:\n  Higher possibility of in-process propagation in many cases. Many high-level languages, such as Java and .NET, are used for building business systems. Most business logic codes run in the same thread for each request, which causes propagation to be based on thread ID, in order for the stack module to make sure that the context is safe.\n  Only works in certain frameworks or libraries. Since the agents are responsible for modifying the codes during runtime, the codes are already known to the agent plugin developers. There is usually a list of frameworks or libraries supported by this kind of probes. For example, see the SkyWalking Java agent supported list.\n  Cross-thread operations are not always supported. Like what is mentioned above regarding in-process propagation, most codes (especially business codes) run in a single thread per request. But in some other cases, they operate across different threads, such as assigning tasks to other threads, task pools or batch processes. Some languages may even provide coroutine or similar components like Goroutine, which allows developers to run async process with low payload. In such cases, auto instrument will face problems.\n  So, there\u0026rsquo;s nothing mysterious about auto instrument. In short, agent developers write an activation script to make instrument codes work for you. That\u0026rsquo;s it!\nWhat is next? If you want to learn about manual instrument libs in SkyWalking, see the Manual instrument SDK section.\n","excerpt":"Service Auto Instrument Agent The service auto instrument agent is a subset of language-based native …","ref":"/docs/main/v8.6.0/en/concepts-and-designs/service-agent/","title":"Service Auto Instrument Agent"},{"body":"Service Mesh Probe Service Mesh probes use the extendable mechanism provided in the Service Mesh implementor, like Istio.\nWhat is Service Mesh? The following explanation comes from Istio\u0026rsquo;s documentation.\n The term \u0026ldquo;service mesh\u0026rdquo; is often used to describe the networks of microservices that make up such applications and the interactions between them. As a service mesh grows in size and complexity, it can become harder to understand and manage. Its requirements can include discovery, load balancing, failure recovery, metrics, and monitoring, and often more complex operational requirements such as A/B testing, canary releases, rate limiting, access control, and end-to-end authentication.\n Where does the probe collect data from? Istio is a typical Service Mesh design and implementor. It defines Control Panel and Data Panel, which are widely used. Here is the Istio Architecture:\nThe Service Mesh probe can choose to collect data from Data Panel. In Istio, it means collecting telemetry data from Envoy sidecar (Data Panel). The probe collects two telemetry entities from the client end and the server end per request.\nHow does Service Mesh make backend work? In this kind of probes, you can see that there is no trace related to them. So how does the SkyWalking platform manage to work?\nThe Service Mesh probe collects telemetry data from each request, so they know about information such as the source, destination, endpoint, latency and status. From these information, the backend can tell the whole topology map by combining these calls into lines, as well as the metrics of each node through their incoming requests. The backend requests for the same metrics data by parsing the trace data. In short: The Service Mesh metrics work exactly the same way as the metrics that are generated by trace parsers.\n","excerpt":"Service Mesh Probe Service Mesh probes use the extendable mechanism provided in the Service Mesh …","ref":"/docs/main/v8.6.0/en/concepts-and-designs/service-mesh-probe/","title":"Service Mesh Probe"},{"body":"Setting Override SkyWalking backend supports setting overrides by system properties and system environment variables. You could override the settings in application.yml\nSystem properties key rule ModuleName.ProviderName.SettingKey.\n  Example\nOverride restHost in this setting segment\n  core: default: restHost: ${SW_CORE_REST_HOST:0.0.0.0} restPort: ${SW_CORE_REST_PORT:12800} restContextPath: ${SW_CORE_REST_CONTEXT_PATH:/} gRPCHost: ${SW_CORE_GRPC_HOST:0.0.0.0} gRPCPort: ${SW_CORE_GRPC_PORT:11800} Use command arg\n-Dcore.default.restHost=172.0.4.12 System environment variables   Example\nOverride restHost in this setting segment through environment variables\n  core: default: restHost: ${REST_HOST:0.0.0.0} restPort: ${SW_CORE_REST_PORT:12800} restContextPath: ${SW_CORE_REST_CONTEXT_PATH:/} gRPCHost: ${SW_CORE_GRPC_HOST:0.0.0.0} gRPCPort: ${SW_CORE_GRPC_PORT:11800} If the REST_HOST  environment variable exists in your operating system and its value is 172.0.4.12, then the value of restHost here will be overwritten to 172.0.4.12, otherwise, it will be set to 0.0.0.0.\nBy the way, Placeholder nesting is also supported, like ${REST_HOST:${ANOTHER_REST_HOST:127.0.0.1}}. In this case, if the REST_HOST  environment variable not exists, but the REST_ANOTHER_REST_HOSTHOST environment variable exists and its value is 172.0.4.12, then the value of restHost here will be overwritten to 172.0.4.12, otherwise, it will be set to 127.0.0.1.\n","excerpt":"Setting Override SkyWalking backend supports setting overrides by system properties and system …","ref":"/docs/main/v8.6.0/en/setup/backend/backend-setting-override/","title":"Setting Override"},{"body":"Setting Override In default, SkyWalking provide agent.config for agent.\nSetting override means end user can override the settings in these config file, through using system properties or agent options.\nSystem properties Use skywalking. + key in config file as system properties key, to override the value.\n  Why need this prefix?\nThe agent system properties and env share with target application, this prefix can avoid variable conflict.\n  Example\nOverride agent.application_code by this.\n  -Dskywalking.agent.application_code=31200 Agent options Add the properties after the agent path in JVM arguments.\n-javaagent:/path/to/skywalking-agent.jar=[option1]=[value1],[option2]=[value2]   Example\nOverride agent.application_code and logging.level by this.\n  -javaagent:/path/to/skywalking-agent.jar=agent.application_code=31200,logging.level=debug   Special characters\nIf a separator(, or =) in the option or value, it should be wrapped in quotes.\n  -javaagent:/path/to/skywalking-agent.jar=agent.ignore_suffix='.jpg,.jpeg' System environment variables   Example\nOverride agent.application_code and logging.level by this.\n  # The service name in UI agent.service_name=${SW_AGENT_NAME:Your_ApplicationName} # Logging level logging.level=${SW_LOGGING_LEVEL:INFO} If the SW_AGENT_NAME  environment variable exists in your operating system and its value is skywalking-agent-demo, then the value of agent.service_name here will be overwritten to skywalking-agent-demo, otherwise, it will be set to Your_ApplicationName.\nBy the way, Placeholder nesting is also supported, like ${SW_AGENT_NAME:${ANOTHER_AGENT_NAME:Your_ApplicationName}}. In this case, if the SW_AGENT_NAME  environment variable not exists, but the ANOTHER_AGENT_NAME environment variable exists and its value is skywalking-agent-demo, then the value of agent.service_name here will be overwritten to skywalking-agent-demo,otherwise, it will be set to Your_ApplicationName.\nOverride priority Agent Options \u0026gt; System.Properties(-D) \u0026gt; System environment variables \u0026gt; Config file\n","excerpt":"Setting Override In default, SkyWalking provide agent.config for agent.\nSetting override means end …","ref":"/docs/main/v8.6.0/en/setup/service-agent/java-agent/setting-override/","title":"Setting Override"},{"body":"Setup java agent  Agent is available for JDK 8 - 14. Find agent folder in SkyWalking release package Set agent.service_name in config/agent.config. Could be any String in English. Set collector.backend_service in config/agent.config. Default point to 127.0.0.1:11800, only works for local backend. Add -javaagent:/path/to/skywalking-package/agent/skywalking-agent.jar to JVM argument. And make sure to add it before the -jar argument.  The agent release dist is included in Apache official release. New agent package looks like this.\n+-- agent +-- activations apm-toolkit-log4j-1.x-activation.jar apm-toolkit-log4j-2.x-activation.jar apm-toolkit-logback-1.x-activation.jar ... +-- config agent.config +-- plugins apm-dubbo-plugin.jar apm-feign-default-http-9.x.jar apm-httpClient-4.x-plugin.jar ..... +-- optional-plugins apm-gson-2.x-plugin.jar ..... +-- bootstrap-plugins jdk-http-plugin.jar ..... +-- logs skywalking-agent.jar  Start your application.  Supported middleware, framework and library SkyWalking agent has supported various middlewares, frameworks and libraries. Read supported list to get them and supported version. If the plugin is in Optional² catalog, go to optional plugins section to learn how to active it.\n All plugins are in /plugins folder. The plugin jar is active when it is in there. Remove the plugin jar, it disabled. The default logging output folder is /logs.  Install javaagent FAQs  Linux Tomcat 7, Tomcat 8, Tomcat 9\nChange the first line of tomcat/bin/catalina.sh.  CATALINA_OPTS=\u0026#34;$CATALINA_OPTS-javaagent:/path/to/skywalking-agent/skywalking-agent.jar\u0026#34;; export CATALINA_OPTS  Windows Tomcat 7, Tomcat 8, Tomcat 9\nChange the first line of tomcat/bin/catalina.bat.  set \u0026#34;CATALINA_OPTS=-javaagent:/path/to/skywalking-agent/skywalking-agent.jar\u0026#34;  JAR file\nAdd -javaagent argument to command line in which you start your app. eg:  java -javaagent:/path/to/skywalking-agent/skywalking-agent.jar -jar yourApp.jar  Jetty\nModify jetty.sh, add -javaagent argument to command line in which you start your app. eg:  export JAVA_OPTIONS=\u0026#34;${JAVA_OPTIONS}-javaagent:/path/to/skywalking-agent/skywalking-agent.jar\u0026#34; Table of Agent Configuration Properties This is the properties list supported in agent/config/agent.config.\n   property key Description Default     agent.namespace Namespace isolates headers in cross process propagation. The HEADER name will be HeaderName:Namespace. Not set   agent.service_name The service name to represent a logic group providing the same capabilities/logic. Suggestion: set a unique name for every logic service group, service instance nodes share the same code, Max length is 50(UTF-8 char). Optional, once service_name follows \u0026lt;group name\u0026gt;::\u0026lt;logic name\u0026gt; format, OAP server assigns the group name to the service metadata. Your_ApplicationName   agent.sample_n_per_3_secs Negative or zero means off, by default.SAMPLE_N_PER_3_SECS means sampling N TraceSegment in 3 seconds tops. Not set   agent.authentication Authentication active is based on backend setting, see application.yml for more details.For most scenarios, this needs backend extensions, only basic match auth provided in default implementation. Not set   agent.trace_segment_ref_limit_per_span The max number of TraceSegmentRef in a single span to keep memory cost estimatable. 500   agent.span_limit_per_segment The max number of spans in a single segment. Through this config item, SkyWalking keep your application memory cost estimated. 300   agent.ignore_suffix If the operation name of the first span is included in this set, this segment should be ignored. Not set   agent.is_open_debugging_class If true, skywalking agent will save all instrumented classes files in /debugging folder. SkyWalking team may ask for these files in order to resolve compatible problem. Not set   agent.is_cache_enhanced_class If true, SkyWalking agent will cache all instrumented classes files to memory or disk files (decided by class cache mode), allow another java agent to enhance those classes that enhanced by SkyWalking agent. To use some Java diagnostic tools (such as BTrace, Arthas) to diagnose applications or add a custom java agent to enhance classes, you need to enable this feature. Read this FAQ for more details false   agent.class_cache_mode The instrumented classes cache mode: MEMORY or FILE. MEMORY: cache class bytes to memory, if instrumented classes is too many or too large, it may take up more memory. FILE: cache class bytes in /class-cache folder, automatically clean up cached class files when the application exits. MEMORY   agent.instance_name Instance name is the identity of an instance, should be unique in the service. If empty, SkyWalking agent will generate an 32-bit uuid. Default, use UUID@hostname as the instance name. Max length is 50(UTF-8 char) \u0026quot;\u0026quot;   agent.instance_properties[key]=value Add service instance custom properties. Not set   agent.cause_exception_depth How depth the agent goes, when log all cause exceptions. 5   agent.force_reconnection_period  Force reconnection period of grpc, based on grpc_channel_check_interval. 1   agent.operation_name_threshold  The operationName max length, setting this value \u0026gt; 190 is not recommended. 150   agent.keep_tracing Keep tracing even the backend is not available if this value is true. false   agent.force_tls Force open TLS for gRPC channel if this value is true. false   osinfo.ipv4_list_size Limit the length of the ipv4 list size. 10   collector.grpc_channel_check_interval grpc channel status check interval. 30   collector.heartbeat_period agent heartbeat report period. Unit, second. 30   collector.properties_report_period_factor The agent sends the instance properties to the backend every collector.heartbeat_period * collector.properties_report_period_factor seconds 10   collector.backend_service Collector SkyWalking trace receiver service addresses. 127.0.0.1:11800   collector.grpc_upstream_timeout How long grpc client will timeout in sending data to upstream. Unit is second. 30 seconds   collector.get_profile_task_interval Sniffer get profile task list interval. 20   collector.get_agent_dynamic_config_interval Sniffer get agent dynamic config interval 20   collector.dns_period_resolve_active If true, skywalking agent will enable periodically resolving DNS to update receiver service addresses. false   logging.level Log level: TRACE, DEBUG, INFO, WARN, ERROR, OFF. Default is info. INFO   logging.file_name Log file name. skywalking-api.log   logging.output Log output. Default is FILE. Use CONSOLE means output to stdout. FILE   logging.dir Log files directory. Default is blank string, means, use \u0026ldquo;{theSkywalkingAgentJarDir}/logs \u0026quot; to output logs. {theSkywalkingAgentJarDir} is the directory where the skywalking agent jar file is located \u0026quot;\u0026quot;   logging.resolver Logger resolver: PATTERN or JSON. The default is PATTERN, which uses logging.pattern to print traditional text logs. JSON resolver prints logs in JSON format. PATTERN   logging.pattern  Logging format. There are all conversion specifiers: * %level means log level. * %timestamp means now of time with format yyyy-MM-dd HH:mm:ss:SSS.\n* %thread means name of current thread.\n* %msg means some message which user logged. * %class means SimpleName of TargetClass. * %throwable means a throwable which user called. * %agent_name means agent.service_name. Only apply to the PatternLogger. %level %timestamp %thread %class : %msg %throwable   logging.max_file_size The max size of log file. If the size is bigger than this, archive the current file, and write into a new file. 300 * 1024 * 1024   logging.max_history_files The max history log files. When rollover happened, if log files exceed this number,then the oldest file will be delete. Negative or zero means off, by default. -1   statuscheck.ignored_exceptions Listed exceptions would not be treated as an error. Because in some codes, the exception is being used as a way of controlling business flow. \u0026quot;\u0026quot;   statuscheck.max_recursive_depth The max recursive depth when checking the exception traced by the agent. Typically, we don\u0026rsquo;t recommend setting this more than 10, which could cause a performance issue. Negative value and 0 would be ignored, which means all exceptions would make the span tagged in error status. 1   correlation.element_max_number Max element count in the correlation context. 3   correlation.value_max_length Max value length of each element. 128   correlation.auto_tag_keys Tag the span by the key/value in the correlation context, when the keys listed here exist. \u0026quot;\u0026quot;   jvm.buffer_size The buffer size of collected JVM info. 60 * 10   buffer.channel_size The buffer channel size. 5   buffer.buffer_size The buffer size. 300   profile.active If true, skywalking agent will enable profile when user create a new profile task. Otherwise disable profile. true   profile.max_parallel Parallel monitor segment count 5   profile.duration Max monitor segment time(minutes), if current segment monitor time out of limit, then stop it. 10   profile.dump_max_stack_depth Max dump thread stack depth 500   profile.snapshot_transport_buffer_size Snapshot transport to backend buffer size 50   meter.active If true, the agent collects and reports metrics to the backend. true   meter.report_interval Report meters interval. The unit is second 20   meter.max_meter_size Max size of the meter pool 500   plugin.mount Mount the specific folders of the plugins. Plugins in mounted folders would work. plugins,activations   plugin.peer_max_length  Peer maximum description limit. 200   plugin.exclude_plugins  Exclude some plugins define in plugins dir.Plugin names is defined in Agent plugin list \u0026quot;\u0026quot;   plugin.mongodb.trace_param If true, trace all the parameters in MongoDB access, default is false. Only trace the operation, not include parameters. false   plugin.mongodb.filter_length_limit If set to positive number, the WriteRequest.params would be truncated to this length, otherwise it would be completely saved, which may cause performance problem. 256   plugin.elasticsearch.trace_dsl If true, trace all the DSL(Domain Specific Language) in ElasticSearch access, default is false. false   plugin.springmvc.use_qualified_name_as_endpoint_name If true, the fully qualified method name will be used as the endpoint name instead of the request URL, default is false. false   plugin.toolit.use_qualified_name_as_operation_name If true, the fully qualified method name will be used as the operation name instead of the given operation name, default is false. false   plugin.jdbc.trace_sql_parameters If set to true, the parameters of the sql (typically java.sql.PreparedStatement) would be collected. false   plugin.jdbc.sql_parameters_max_length If set to positive number, the db.sql.parameters would be truncated to this length, otherwise it would be completely saved, which may cause performance problem. 512   plugin.jdbc.sql_body_max_length If set to positive number, the db.statement would be truncated to this length, otherwise it would be completely saved, which may cause performance problem. 2048   plugin.solrj.trace_statement If true, trace all the query parameters(include deleteByIds and deleteByQuery) in Solr query request, default is false. false   plugin.solrj.trace_ops_params If true, trace all the operation parameters in Solr request, default is false. false   plugin.light4j.trace_handler_chain If true, trace all middleware/business handlers that are part of the Light4J handler chain for a request. false   plugin.opgroup.* Support operation name customize group rules in different plugins. Read Group rule supported plugins Not set   plugin.springtransaction.simplify_transaction_definition_name If true, the transaction definition name will be simplified. false   plugin.jdkthreading.threading_class_prefixes Threading classes (java.lang.Runnable and java.util.concurrent.Callable) and their subclasses, including anonymous inner classes whose name match any one of the THREADING_CLASS_PREFIXES (splitted by ,) will be instrumented, make sure to only specify as narrow prefixes as what you\u0026rsquo;re expecting to instrument, (java. and javax. will be ignored due to safety issues) Not set   plugin.tomcat.collect_http_params This config item controls that whether the Tomcat plugin should collect the parameters of the request. Also, activate implicitly in the profiled trace. false   plugin.springmvc.collect_http_params This config item controls that whether the SpringMVC plugin should collect the parameters of the request, when your Spring application is based on Tomcat, consider only setting either plugin.tomcat.collect_http_params or plugin.springmvc.collect_http_params. Also, activate implicitly in the profiled trace. false   plugin.httpclient.collect_http_params This config item controls that whether the HttpClient plugin should collect the parameters of the request false   plugin.http.http_params_length_threshold When COLLECT_HTTP_PARAMS is enabled, how many characters to keep and send to the OAP backend, use negative values to keep and send the complete parameters, NB. this config item is added for the sake of performance. 1024   plugin.http.http_headers_length_threshold When include_http_headers declares header names, this threshold controls the length limitation of all header values. use negative values to keep and send the complete headers. Note. this config item is added for the sake of performance. 2048   plugin.http.include_http_headers Set the header names, which should be collected by the plugin. Header name must follow javax.servlet.http definition. Multiple names should be split by comma. ``(No header would be collected) |   plugin.feign.collect_request_body This config item controls that whether the Feign plugin should collect the http body of the request. false   plugin.feign.filter_length_limit When COLLECT_REQUEST_BODY is enabled, how many characters to keep and send to the OAP backend, use negative values to keep and send the complete body. 1024   plugin.feign.supported_content_types_prefix When COLLECT_REQUEST_BODY is enabled and content-type start with SUPPORTED_CONTENT_TYPES_PREFIX, collect the body of the request , multiple paths should be separated by , application/json,text/   plugin.influxdb.trace_influxql If true, trace all the influxql(query and write) in InfluxDB access, default is true. true   plugin.dubbo.collect_consumer_arguments Apache Dubbo consumer collect arguments in RPC call, use Object#toString to collect arguments. false   plugin.dubbo.consumer_arguments_length_threshold When plugin.dubbo.collect_consumer_arguments is true, Arguments of length from the front will to the OAP backend 256   plugin.dubbo.collect_provider_arguments Apache Dubbo provider collect arguments in RPC call, use Object#toString to collect arguments. false   plugin.dubbo.consumer_provider_length_threshold When plugin.dubbo.provider_consumer_arguments is true, Arguments of length from the front will to the OAP backend 256   plugin.kafka.bootstrap_servers A list of host/port pairs to use for establishing the initial connection to the Kafka cluster. localhost:9092   plugin.kafka.get_topic_timeout Timeout period of reading topics from the Kafka server, the unit is second. 10   plugin.kafka.consumer_config Kafka producer configuration.    plugin.kafka.producer_config Kafka producer configuration. Read producer configure to get more details. Check Kafka report doc for more details and examples.    plugin.kafka.topic_meter Specify which Kafka topic name for Meter System data to report to. skywalking_meters   plugin.kafka.topic_metrics Specify which Kafka topic name for JVM metrics data to report to. skywalking_metrics   plugin.kafka.topic_segment Specify which Kafka topic name for traces data to report to. skywalking_segments   plugin.kafka.topic_profilings Specify which Kafka topic name for Thread Profiling snapshot to report to. skywalking_profilings   plugin.kafka.topic_management Specify which Kafka topic name for the register or heartbeat data of Service Instance to report to. skywalking_managements   plugin.kafka.namespace isolate multi OAP server when using same Kafka cluster (final topic name will append namespace before Kafka topics with - ). `` |   plugin.springannotation.classname_match_regex Match spring beans with regular expression for the class name. Multiple expressions could be separated by a comma. This only works when Spring annotation plugin has been activated. All the spring beans tagged with @Bean,@Service,@Dao, or @Repository.   plugin.toolkit.log.transmit_formatted Whether or not to transmit logged data as formatted or un-formatted. true   plugin.toolkit.log.grpc.reporter.server_host Specify which grpc server\u0026rsquo;s host for log data to report to. 127.0.0.1   plugin.toolkit.log.grpc.reporter.server_port Specify which grpc server\u0026rsquo;s port for log data to report to. 11800   plugin.toolkit.log.grpc.reporter.max_message_size Specify the maximum size of log data for grpc client to report to. 10485760   plugin.toolkit.log.grpc.reporter.upstream_timeout How long grpc client will timeout in sending data to upstream. Unit is second. 30 seconds   plugin.lettuce.trace_redis_parameters If set to true, the parameters of Redis commands would be collected by Lettuce agent. false   plugin.lettuce.redis_parameter_max_length If set to positive number and plugin.lettuce.trace_redis_parameters is set to true, Redis command parameters would be collected and truncated to this length. 128    Dynamic Configurations All configurations above are static, if you need to change some agent settings at runtime, please read CDS - Configuration Discovery Service document for more details.\nOptional Plugins Java agent plugins are all pluggable. Optional plugins could be provided in optional-plugins folder under agent or 3rd party repositories. For using these plugins, you need to put the target plugin jar file into /plugins.\nNow, we have the following known optional plugins.\n Plugin of tracing Spring annotation beans Plugin of tracing Oracle and Resin Filter traces through specified endpoint name patterns Plugin of Gson serialization lib in optional plugin folder. Plugin of Zookeeper 3.4.x in optional plugin folder. The reason of being optional plugin is, many business irrelevant traces are generated, which cause extra payload to agents and backends. At the same time, those traces may be just heartbeat(s). Customize enhance Trace methods based on description files, rather than write plugin or change source codes. Plugin of Spring Cloud Gateway 2.1.x in optional plugin folder. Please only active this plugin when you install agent in Spring Gateway. spring-cloud-gateway-2.x-plugin and spring-webflux-5.x-plugin are both required. Plugin of Spring Transaction in optional plugin folder. The reason of being optional plugin is, many local span are generated, which also spend more CPU, memory and network. Plugin of Kotlin coroutine provides the tracing across coroutines automatically. As it will add local spans to all across routines scenarios, Please assess the performance impact. Plugin of quartz-scheduler-2.x in the optional plugin folder. The reason for being an optional plugin is, many task scheduling systems are based on quartz-scheduler, this will cause duplicate tracing and link different sub-tasks as they share the same quartz level trigger, such as ElasticJob. Plugin of spring-webflux-5.x in the optional plugin folder. Please only activate this plugin when you use webflux alone as a web container. If you are using SpringMVC 5 or Spring Gateway, you don\u0026rsquo;t need this plugin. Plugin of mybatis-3.x in optional plugin folder. The reason of being optional plugin is, many local span are generated, which also spend more CPU, memory and network.  Bootstrap class plugins All bootstrap plugins are optional, due to unexpected risk. Bootstrap plugins are provided in bootstrap-plugins folder. For using these plugins, you need to put the target plugin jar file into /plugins.\nNow, we have the following known bootstrap plugins.\n Plugin of JDK HttpURLConnection. Agent is compatible with JDK 1.6+ Plugin of JDK Callable and Runnable. Agent is compatible with JDK 1.6+  The Logic Endpoint In default, all the RPC server-side names as entry spans, such as RESTFul API path and gRPC service name, would be endpoints with metrics. At the same time, SkyWalking introduces the logic endpoint concept, which allows plugins and users to add new endpoints without adding new spans. The following logic endpoints are added automatically by plugins.\n GraphQL Query and Mutation are logic endpoints by using the names of them. Spring\u0026rsquo;s ScheduledMethodRunnable jobs are logic endpoints. The name format is SpringScheduled/${className}/${methodName}. Apache ShardingSphere ElasticJob\u0026rsquo;s jobs are logic endpoints. The name format is ElasticJob/${jobName}. XXLJob\u0026rsquo;s jobs are logic endpoints. The name formats include xxl-job/MethodJob/${className}.${methodName}, xxl-job/ScriptJob/${GlueType}/id/${jobId}, and xxl-job/SimpleJob/${className}. Quartz(optional plugin)\u0026rsquo;s jobs are logic endpoints. the name format is quartz-scheduler/${className}.  User could use the SkyWalking\u0026rsquo;s application toolkits to add the tag into the local span to label the span as a logic endpoint in the analysis stage. The tag is, key=x-le and value = {\u0026quot;logic-span\u0026quot;:true}.\nAdvanced Features  Set the settings through system properties for config file override. Read setting override. Use gRPC TLS to link backend. See open TLS Monitor a big cluster by different SkyWalking services. Use Namespace to isolate the context propagation. Set client token if backend open token authentication. Application Toolkit, are a collection of libraries, provided by SkyWalking APM. Using them, you have a bridge between your application and SkyWalking APM agent.  If you want your codes to interact with SkyWalking agent, including getting trace id, setting tags, propagating custom data etc.. Try SkyWalking manual APIs. If you require customized metrics, try SkyWalking Meter System Toolkit. If you want to continue traces across thread manually, use across thread solution APIs. If you want to forward MicroMeter/Spring Sleuth metrics to Meter System, use SkyWalking MicroMeter Register. If you want to use OpenTracing Java APIs, try SkyWalking OpenTracing compatible tracer. More details you could find at http://opentracing.io If you want to tolerate some exceptions, read tolerate custom exception doc. If you want to print trace context(e.g. traceId) in your logs, or collect logs, choose the log frameworks, log4j, log4j2, logback.   If you want to specify the path of your agent.config file. Read set config file through system properties  Advanced Reporters The advanced report provides an alternative way to submit the agent collected data to the backend. All of them are in the optional-reporter-plugins folder, move the one you needed into the reporter-plugins folder for the activation. Notice, don\u0026rsquo;t try to activate multiple reporters, that could cause unexpected fatal errors.\n Use Kafka to transport the traces, JVM metrics, instance properties, and profiled snapshots to the backend. Read the How to enable Kafka Reporter for more details.  Plugin Development Guide SkyWalking java agent supports plugin to extend the supported list. Please follow our Plugin Development Guide.\n","excerpt":"Setup java agent  Agent is available for JDK 8 - 14. Find agent folder in SkyWalking release package …","ref":"/docs/main/v8.6.0/en/setup/service-agent/java-agent/readme/","title":"Setup java agent"},{"body":"Skywalking Agent List  activemq-5.x armeria-063-084 armeria-085 armeria-086 armeria-098 async-http-client-2.x avro-1.x brpc-java canal-1.x cassandra-java-driver-3.x dbcp-2.x dubbo ehcache-2.x elastic-job-2.x elastic-job-3.x elasticsearch-5.x elasticsearch-6.x elasticsearch-7.x feign-default-http-9.x feign-pathvar-9.x finagle graphql grpc-1.x gson-2.8.x h2-1.x hbase-1.x/2.x httpasyncclient-4.x httpclient-3.x httpclient-4.x hystrix-1.x influxdb-2.x jdk-http-plugin jdk-threading-plugin jedis-2.x jetty-client-9.0 jetty-client-9.x jetty-server-9.x kafka-0.11.x/1.x/2.x kotlin-coroutine lettuce-5.x light4j mariadb-2.x memcache-2.x mongodb-2.x mongodb-3.x mongodb-4.x motan-0.x mybatis-3.x mysql-5.x mysql-6.x mysql-8.x netty-socketio nutz-http-1.x nutz-mvc-annotation-1.x okhttp-3.x okhttp-4.x play-2.x postgresql-8.x pulsar quasar quartz-scheduler-2.x rabbitmq-5.x redisson-3.x resteasy-server-3.x rocketMQ-3.x rocketMQ-4.x servicecomb-0.x servicecomb-1.x sharding-jdbc-1.5.x sharding-sphere-3.x sharding-sphere-4.0.0 sharding-sphere-4.1.0 sharding-sphere-4.x sharding-sphere-4.x-rc3 sofarpc solrj-7.x spring-annotation spring-async-annotation-5.x spring-cloud-feign-1.x spring-cloud-feign-2.x spring-cloud-gateway-2.0.x spring-cloud-gateway-2.1.x spring-concurrent-util-4.x spring-core-patch spring-kafka-1.x spring-kafka-2.x spring-mvc-annotation spring-mvc-annotation-3.x spring-mvc-annotation-4.x spring-mvc-annotation-5.x spring-resttemplate-4.x spring-scheduled-annotation spring-tx spring-webflux-5.x spring-webflux-5.x-webclient spymemcached-2.x struts2-2.x thrift tomcat-7.x/8.x toolkit-counter toolkit-gauge toolkit-histogram toolkit-kafka toolkit-log4j toolkit-log4j2 toolkit-logback toolkit-opentracing toolkit-tag toolkit-trace toolkit-exception undertow-2.x-plugin vertx-core-3.x xxl-job-2.x zookeeper-3.4.x mssql-jtds-1.x mssql-jdbc apache-cxf-3.x jsonrpc4j spring-cloud-gateway-3.x  ","excerpt":"Skywalking Agent List  activemq-5.x armeria-063-084 armeria-085 armeria-086 armeria-098 …","ref":"/docs/main/v8.6.0/en/setup/service-agent/java-agent/plugin-list/","title":"Skywalking Agent List"},{"body":"SkyWalking Cross Process Correlation Headers Protocol  Version 1.0  The Cross Process Correlation Headers Protocol is used to transport custom data by leveraging the capability of Cross Process Propagation Headers Protocol.\nThis is an optional and additional protocol for language tracer implementation. All tracer implementation could consider implementing this. Cross Process Correlation Header key is sw8-correlation. The value is the encoded(key):encoded(value) list with elements splitted by , such as base64(string key):base64(string value),base64(string key2):base64(string value2).\nRecommendations for language APIs The following implementation method is recommended for different language APIs.\n TraceContext#putCorrelation and TraceContext#getCorrelation are recommended to write and read the correlation context, with key/value string. The key should be added if it is absent. The latter writes should override the previous value. The total number of all keys should be less than 3, and the length of each value should be less than 128 bytes. The context should be propagated as well when tracing context is propagated across threads and processes.  ","excerpt":"SkyWalking Cross Process Correlation Headers Protocol  Version 1.0  The Cross Process Correlation …","ref":"/docs/main/v8.6.0/en/protocols/skywalking-cross-process-correlation-headers-protocol-v1/","title":"SkyWalking Cross Process Correlation Headers Protocol"},{"body":"SkyWalking Cross Process Propagation Headers Protocol  Version 3.0  SkyWalking is more akin to an APM system, rather than a common distributed tracing system. SkyWalking\u0026rsquo;s headers are much more complex than those found in a common distributed tracing system. The reason behind their complexity is for better analysis performance of the OAP. You can find many similar mechanisms in other commercial APM systems (some of which are even more complex than ours!).\nAbstract The SkyWalking Cross Process Propagation Headers Protocol v3, also known as the sw8 protocol, is designed for context propagation.\nStandard Header Item The standard header is the minimal requirement for context propagation.\n Header Name: sw8. Header Value: 8 fields split by -. The length of header value must be less than 2k (default).  Example of the value format: XXXXX-XXXXX-XXXX-XXXX\nValues Values must include the following segments, and all string type values are in BASE64 encoding.\n Required:   Sample. 0 or 1. 0 means that the context exists, but it could (and most likely will) be ignored. 1 means this trace needs to be sampled and sent to the backend. Trace ID. String(BASE64 encoded). A literal string that is globally unique. Parent trace segment ID. String(BASE64 encoded). A literal string that is globally unique. Parent span ID. Must be an integer. It begins with 0. This span ID points to the parent span in parent trace segment. Parent service. String(BASE64 encoded). Its length should be no more than 50 UTF-8 characters. Parent service instance. String(BASE64 encoded). Its length should be no more than 50 UTF-8 characters. Parent endpoint. String(BASE64 encoded). The operation name of the first entry span in the parent segment. Its length should be less than 150 UTF-8 characters. Target address of this request used on the client end. String(BASE64 encoded). The network address (not necessarily IP + port) used on the client end to access this target service.   Sample values: 1-TRACEID-SEGMENTID-3-PARENT_SERVICE-PARENT_INSTANCE-PARENT_ENDPOINT-IPPORT  Extension Header Item The extension header item is designed for advanced features. It provides interaction capabilities between the agents deployed in upstream and downstream services.\n Header Name: sw8-x Header Value: Split by -. The fields are extendable.  Values The current value includes fields.\n Tracing Mode. Empty, 0, or 1. Empty or 0 is the default. 1 indicates that all spans generated in this context will skip analysis, spanObject#skipAnalysis=true. This context is propagated to upstream by default, unless it is changed in the tracing process. The timestamp of sending on the client end. This is used in async RPC, such as MQ. Once it is set, the consumer end would calculate the latency between sending and receiving, and tag the latency in the span by using key transmission.latency automatically.  ","excerpt":"SkyWalking Cross Process Propagation Headers Protocol  Version 3.0  SkyWalking is more akin to an …","ref":"/docs/main/v8.6.0/en/protocols/skywalking-cross-process-propagation-headers-protocol-v3/","title":"SkyWalking Cross Process Propagation Headers Protocol"},{"body":"Apache SkyWalking release guide If you\u0026rsquo;re a committer, you can learn how to release SkyWalking in The Apache Way and start the voting process by reading this document.\nSet up your development environment Follow the steps in the Apache maven deployment environment document to set gpg tool and encrypt passwords.\nUse the following block as a template and place it in ~/.m2/settings.xml.\n\u0026lt;settings\u0026gt; ... \u0026lt;servers\u0026gt; \u0026lt;!-- To publish a snapshot of some part of Maven --\u0026gt; \u0026lt;server\u0026gt; \u0026lt;id\u0026gt;apache.snapshots.https\u0026lt;/id\u0026gt; \u0026lt;username\u0026gt; \u0026lt;!-- YOUR APACHE LDAP USERNAME --\u0026gt; \u0026lt;/username\u0026gt; \u0026lt;password\u0026gt; \u0026lt;!-- YOUR APACHE LDAP PASSWORD (encrypted) --\u0026gt; \u0026lt;/password\u0026gt; \u0026lt;/server\u0026gt; \u0026lt;!-- To stage a release of some part of Maven --\u0026gt; \u0026lt;server\u0026gt; \u0026lt;id\u0026gt;apache.releases.https\u0026lt;/id\u0026gt; \u0026lt;username\u0026gt; \u0026lt;!-- YOUR APACHE LDAP USERNAME --\u0026gt; \u0026lt;/username\u0026gt; \u0026lt;password\u0026gt; \u0026lt;!-- YOUR APACHE LDAP PASSWORD (encrypted) --\u0026gt; \u0026lt;/password\u0026gt; \u0026lt;/server\u0026gt; ... \u0026lt;/servers\u0026gt; \u0026lt;/settings\u0026gt; Add your GPG public key  Add your GPG public key into the SkyWalking GPG KEYS file. If you are a committer, use your Apache ID and password to log in this svn, and update the file. Don\u0026rsquo;t override the existing file. Upload your GPG public key to the public GPG site, such as MIT\u0026rsquo;s site. This site should be in the Apache maven staging repository checklist.  Test your settings This step is only for testing purpose. If your env is correctly set, you don\u0026rsquo;t need to check every time.\n./mvnw clean install -Pall (this will build artifacts, sources and sign) Prepare for the release ./mvnw release:clean ./mvnw release:prepare -DautoVersionSubmodules=true -Pall  Set version number as x.y.z, and tag as vx.y.z (The version tag must start with v. You will find out why this is necessary in the next step.)  You could do a GPG signature before preparing for the release. If you need to input the password to sign, and the maven doesn\u0026rsquo;t provide you with the opportunity to do so, this may lead to failure of the release. To resolve this, you may run gpg --sign xxx in any file. This will allow it to remember the password for long enough to prepare for the release.\nStage the release ./mvnw release:perform -DskipTests -Pall  The release will be automatically inserted into a temporary staging repository.  Build and sign the source code package export RELEASE_VERSION=x.y.z (example: RELEASE_VERSION=5.0.0-alpha) cd tools/releasing bash create_source_release.sh This script takes care of the following things:\n Use v + RELEASE_VERSION as tag to clone the codes. Complete git submodule init/update. Exclude all unnecessary files in the target source tar, such as .git, .github, and .gitmodules. See the script for more details. Execute gpg and shasum 512.  apache-skywalking-apm-x.y.z-src.tgz and files ending with .asc and .sha512 may be found in the tools/releasing folder.\nLocate and download the distribution package in Apache Nexus Staging repositories  Use your Apache ID to log in to https://repository.apache.org/. Go to https://repository.apache.org/#stagingRepositories. Search skywalking and find your staging repository. Close the repository and wait for all checks to pass. In this step, your GPG KEYS will be checked. See the set PGP document, if you haven\u0026rsquo;t done it before. Go to {REPO_URL}/org/apache/skywalking/apache-skywalking-apm/x.y.z. Download .tar.gz and .zip and files ending with .asc and .sha1.  Upload to Apache svn  Use your Apache ID to log in to https://dist.apache.org/repos/dist/dev/skywalking/. Create a folder and name it by the release version and round, such as: x.y.z Upload the source code package to the folder with files ending with .asc and .sha512.  Package name: apache-skywalking-x.y.z-src.tar.gz See Section \u0026ldquo;Build and sign the source code package\u0026rdquo; for more details   Upload the distribution package to the folder with files ending with .asc and .sha512.  Package name: apache-skywalking-bin-x.y.z.tar.gz and apache-skywalking-bin-x.y.z.zip See Section \u0026ldquo;Locate and download the distribution package in Apache Nexus Staging repositories\u0026rdquo; for more details. Create a .sha512 package: shasum -a 512 file \u0026gt; file.sha512    Make the internal announcements Send an announcement mail in dev mail list.\nMail title: [ANNOUNCE] SkyWalking x.y.z test build available Mail content: The test build of x.y.z is available. We welcome any comments you may have, and will take all feedback into account if a quality vote is called for this build. Release notes: * https://github.com/apache/skywalking/blob/master/changes/changes-x.y.z.md Release Candidate: * https://dist.apache.org/repos/dist/dev/skywalking/xxxx * sha512 checksums - sha512xxxxyyyzzz apache-skywalking-apm-x.x.x-src.tgz - sha512xxxxyyyzzz apache-skywalking-apm-bin-x.x.x.tar.gz - sha512xxxxyyyzzz apache-skywalking-apm-bin-x.x.x.zip Maven 2 staging repository: * https://repository.apache.org/content/repositories/xxxx/org/apache/skywalking/ Release Tag : * (Git Tag) x.y.z Release CommitID : * https://github.com/apache/skywalking/tree/(Git Commit ID) * Git submodule * skywalking-ui: https://github.com/apache/skywalking-rocketbot-ui/tree/(Git Commit ID) * apm-protocol/apm-network/src/main/proto: https://github.com/apache/skywalking-data-collect-protocol/tree/(Git Commit ID) * oap-server/server-query-plugin/query-graphql-plugin/src/main/resources/query-protocol https://github.com/apache/skywalking-query-protocol/tree/(Git Commit ID) Keys to verify the Release Candidate : * https://dist.apache.org/repos/dist/release/skywalking/KEYS Guide to build the release from source : * https://github.com/apache/skywalking/blob/x.y.z/docs/en/guides/How-to-build.md A vote regarding the quality of this test build will be initiated within the next couple of days. Wait for at least 48 hours for test responses Any PMC member, committer or contributor can test the release features and provide feedback. Based on that, the PMC will decide whether to start the voting process.\nCall a vote in dev Call a vote in dev@skywalking.apache.org\nMail title: [VOTE] Release Apache SkyWalking version x.y.z Mail content: Hi All, This is a call for vote to release Apache SkyWalking version x.y.z. Release notes: * https://github.com/apache/skywalking/blob/master/changes/changes-x.y.z.md Release Candidate: * https://dist.apache.org/repos/dist/dev/skywalking/xxxx * sha512 checksums - sha512xxxxyyyzzz apache-skywalking-apm-x.x.x-src.tgz - sha512xxxxyyyzzz apache-skywalking-apm-bin-x.x.x.tar.gz - sha512xxxxyyyzzz apache-skywalking-apm-bin-x.x.x.zip Maven 2 staging repository: * https://repository.apache.org/content/repositories/xxxx/org/apache/skywalking/ Release Tag : * (Git Tag) x.y.z Release CommitID : * https://github.com/apache/skywalking/tree/(Git Commit ID) * Git submodule * skywalking-ui: https://github.com/apache/skywalking-rocketbot-ui/tree/(Git Commit ID) * apm-protocol/apm-network/src/main/proto: https://github.com/apache/skywalking-data-collect-protocol/tree/(Git Commit ID) * oap-server/server-query-plugin/query-graphql-plugin/src/main/resources/query-protocol https://github.com/apache/skywalking-query-protocol/tree/(Git Commit ID) Keys to verify the Release Candidate : * https://dist.apache.org/repos/dist/release/skywalking/KEYS Guide to build the release from source : * https://github.com/apache/skywalking/blob/x.y.z/docs/en/guides/How-to-build.md Voting will start now (xxxx date) and will remain open for at least 72 hours, Request all PMC members to give their vote. [ ] +1 Release this package. [ ] +0 No opinion. [ ] -1 Do not release this package because.... Vote Check All PMC members and committers should check these before casting +1 votes.\n Features test. All artifacts in staging repository are published with .asc, .md5, and *sha1 files. Source code and distribution package (apache-skywalking-x.y.z-src.tar.gz, apache-skywalking-bin-x.y.z.tar.gz, apache-skywalking-bin-x.y.z.zip) are found in https://dist.apache.org/repos/dist/dev/skywalking/x.y.z with .asc and .sha512. LICENSE and NOTICE are in the source code and distribution package. Check shasum -c apache-skywalking-apm-x.y.z-src.tgz.sha512. Check gpg --verify apache-skywalking-apm-x.y.z-src.tgz.asc apache-skywalking-apm-x.y.z-src.tgz Build a distribution package from the source code package (apache-skywalking-x.y.z-src.tar.gz) by following this doc. Check the Apache License Header. Run docker run --rm -v $(pwd):/github/workspace apache/skywalking-eyes header check. (No binaries in source codes)  The voting process is as follows:\n All PMC member votes are +1 binding, and all other votes are +1 but non-binding. If you obtain at least 3 (+1 binding) votes with more +1 than -1 votes within 72 hours, the release will be approved.  Publish the release  Move source codes tar and distribution packages to https://dist.apache.org/repos/dist/release/skywalking/.  \u0026gt; export SVN_EDITOR=vim \u0026gt; svn mv https://dist.apache.org/repos/dist/dev/skywalking/x.y.z https://dist.apache.org/repos/dist/release/skywalking .... enter your apache password .... Release in the nexus staging repo. Public download source and distribution tar/zip are located in http://www.apache.org/dyn/closer.cgi/skywalking/x.y.z/xxx. The Apache mirror path is the only release information that we publish. Public asc and sha512 are located in https://www.apache.org/dist/skywalking/x.y.z/xxx. Public KEYS point to https://www.apache.org/dist/skywalking/KEYS. Update the website download page. http://skywalking.apache.org/downloads/ . Add a new download source, distribution, sha512, asc, and document links. The links can be found following rules (3) to (6) above. Add a release event on the website homepage and event page. Announce the public release with changelog or key features. Send ANNOUNCE email to dev@skywalking.apache.org, announce@apache.org. The sender should use the Apache email account.  Mail title: [ANNOUNCE] Apache SkyWalking x.y.z released Mail content: Hi all, Apache SkyWalking Team is glad to announce the first release of Apache SkyWalking x.y.z. SkyWalking: APM (application performance monitor) tool for distributed systems, especially designed for microservices, cloud native and container-based (Docker, Kubernetes, Mesos) architectures. This release contains a number of new features, bug fixes and improvements compared to version a.b.c(last release). The notable changes since x.y.z include: (Highlight key changes) 1. ... 2. ... 3. ... Please refer to the change log for the complete list of changes: https://github.com/apache/skywalking/blob/master/changes/changes-x.y.z.md Apache SkyWalking website: http://skywalking.apache.org/ Downloads: http://skywalking.apache.org/downloads/ Twitter: https://twitter.com/ASFSkyWalking SkyWalking Resources: - GitHub: https://github.com/apache/skywalking - Issue: https://github.com/apache/skywalking/issues - Mailing list: dev@skywalkiing.apache.org - Apache SkyWalking Team Clean up the old releases Once the latest release has been published, you should clean up the old releases from the mirror system.\n Update the download links (source, dist, asc, and sha512) on the website to the archive repo (https://archive.apache.org/dist/skywalking). Remove previous releases from https://dist.apache.org/repos/dist/release/skywalking/.  ","excerpt":"Apache SkyWalking release guide If you\u0026rsquo;re a committer, you can learn how to release SkyWalking …","ref":"/docs/main/v8.6.0/en/guides/how-to-release/","title":"SkyWalking release guide"},{"body":"Skywalking with Kotlin coroutine This Plugin provides an auto instrument support plugin for Kotlin coroutine based on context snapshot.\nDescription SkyWalking provide tracing context propagation inside thread. In order to support Kotlin Coroutine, we provide this additional plugin.\nImplementation principle As we know, Kotlin coroutine switches the execution thread by CoroutineDispatcher.\n Create a snapshot of the current context before dispatch the continuation. Then create a coroutine span after thread switched, mark the span continued with the snapshot. Every new span which created in the new thread will be a child of this coroutine span. So we can link those span together in a tracing. After the original runnable executed, we need to stop the coroutine span for cleaning thread state.  Some screenshots Run without the plugin We run a Kotlin coroutine based gRPC server without this coroutine plugin.\nYou can find, the one call (client -\u0026gt; server1 -\u0026gt; server2) has been split two tracing paths.\n Server1 without exit span and server2 tracing path.  Server2 tracing path.   Run with the plugin Without changing codes manually, just install the plugin. We can find the spans be connected together. We can get all info of one client call.\n","excerpt":"Skywalking with Kotlin coroutine This Plugin provides an auto instrument support plugin for Kotlin …","ref":"/docs/main/v8.6.0/en/setup/service-agent/java-agent/agent-optional-plugins/kotlin-coroutine-plugin/","title":"Skywalking with Kotlin coroutine"},{"body":"Slow Database Statement Slow Database statements are significant important to find out the bottleneck of the system, which relied on Database.\nSlow DB statements are based on sampling, right now, the core samples top 50 slowest in every 10 minutes. But duration of those statements must be slower than threshold.\nThe setting format is following, unit is millisecond.\n database-type:thresholdValue,database-type2:thresholdValue2\n Default setting is default:200,mongodb:100. Reserved DB type is default, which be as default threshold for all database types, except set explicitly.\nNotice, the threshold should not be too small, like 1ms. Functionally, it works, but would cost OAP performance issue, if your system statement access time are mostly more than 1ms.\n","excerpt":"Slow Database Statement Slow Database statements are significant important to find out the …","ref":"/docs/main/v8.6.0/en/setup/backend/slow-db-statement/","title":"Slow Database Statement"},{"body":"Source and scope extension for new metrics From the OAL scope introduction, you should already have understood what a scope is. If you would like to create more extensions, you need to have a deeper understanding of what a source is.\nSource and scope are interrelated concepts. Scope declares the ID (int) and name, while source declares the attributes. Follow these steps to create a new source and sccope.\n In the OAP core module, it provides SourceReceiver internal services.  public interface SourceReceiver extends Service { void receive(Source source); } All data of the analysis must be a org.apache.skywalking.oap.server.core.source.Source sub class that is tagged by @SourceType annotation, and included in the org.apache.skywalking package. Then, it can be supported by the OAL script and OAP core.  Take the existing source service as an example.\n@ScopeDeclaration(id = SERVICE_INSTANCE, name = \u0026#34;ServiceInstance\u0026#34;, catalog = SERVICE_INSTANCE_CATALOG_NAME) @ScopeDefaultColumn.VirtualColumnDefinition(fieldName = \u0026#34;entityId\u0026#34;, columnName = \u0026#34;entity_id\u0026#34;, isID = true, type = String.class) public class ServiceInstance extends Source { @Override public int scope() { return DefaultScopeDefine.SERVICE_INSTANCE; } @Override public String getEntityId() { return String.valueOf(id); } @Getter @Setter private int id; @Getter @Setter @ScopeDefaultColumn.DefinedByField(columnName = \u0026#34;service_id\u0026#34;) private int serviceId; @Getter @Setter private String name; @Getter @Setter private String serviceName; @Getter @Setter private String endpointName; @Getter @Setter private int latency; @Getter @Setter private boolean status; @Getter @Setter private int responseCode; @Getter @Setter private RequestType type; }  The scope() method in source returns an ID, which is not a random value. This ID must be declared through the @ScopeDeclaration annotation too. The ID in @ScopeDeclaration and ID in scope() method must be the same for this source.\n  The String getEntityId() method in source requests the return value representing the unique entity to which the scope relates. For example, in this service scope, the ID is the service ID, which represents a particular service, like the Order service. This value is used in the OAL group mechanism.\n  @ScopeDefaultColumn.VirtualColumnDefinition and @ScopeDefaultColumn.DefinedByField are required. All declared fields (virtual/byField) will be pushed into a persistent entity, and maps to lists such as the ElasticSearch index and Database table column. For example, the entity ID and service ID for endpoint and service instance level scope are usually included. Take a reference from all existing scopes. All these fields are detected by OAL Runtime, and are required during query.\n  Add scope name as keyword to OAL grammar definition file, OALLexer.g4, which is at the antlr4 folder of the generate-tool-grammar module.\n  Add scope name as keyword to the parser definition file, OALParser.g4, which is located in the same folder as OALLexer.g4.\n   After finishing these steps, you could build a receiver, which do\n Obtain the original data of the metrics. Build the source, and send to SourceReceiver. Complete your OAL scripts. Repackage the project.  ","excerpt":"Source and scope extension for new metrics From the OAL scope introduction, you should already have …","ref":"/docs/main/v8.6.0/en/guides/source-extension/","title":"Source and scope extension for new metrics"},{"body":"Spring annotation plugin This plugin allows to trace all methods of beans in Spring context, which are annotated with @Bean, @Service, @Component and @Repository.\n Why does this plugin optional?  Tracing all methods in Spring context all creates a lot of spans, which also spend more CPU, memory and network. Of course you want to have spans as many as possible, but please make sure your system payload can support these.\n","excerpt":"Spring annotation plugin This plugin allows to trace all methods of beans in Spring context, which …","ref":"/docs/main/v8.6.0/en/setup/service-agent/java-agent/agent-optional-plugins/spring-annotation-plugin/","title":"Spring annotation plugin"},{"body":"Spring sleuth setup Spring Sleuth provides Spring Boot auto-configuration for distributed tracing. Skywalking integrates it\u0026rsquo;s micrometer part, and it can send metrics to the Skywalking Meter System.\nSet up agent  Add the Micrometer and Skywalking meter registry dependency into project pom.xml file. Also you could found more detail at Toolkit micrometer.  \u0026lt;dependency\u0026gt; \u0026lt;groupId\u0026gt;org.springframework.boot\u0026lt;/groupId\u0026gt; \u0026lt;artifactId\u0026gt;spring-boot-starter-actuator\u0026lt;/artifactId\u0026gt; \u0026lt;/dependency\u0026gt; \u0026lt;dependency\u0026gt; \u0026lt;groupId\u0026gt;org.apache.skywalking\u0026lt;/groupId\u0026gt; \u0026lt;artifactId\u0026gt;apm-toolkit-micrometer-registry\u0026lt;/artifactId\u0026gt; \u0026lt;version\u0026gt;${skywalking.version}\u0026lt;/version\u0026gt; \u0026lt;/dependency\u0026gt; Create the Skywalking meter resgitry into spring bean management.  @Bean SkywalkingMeterRegistry skywalkingMeterRegistry() { // Add rate configs If you need, otherwise using none args construct  SkywalkingConfig config = new SkywalkingConfig(Arrays.asList(\u0026#34;\u0026#34;)); return new SkywalkingMeterRegistry(config); } Set up backend receiver  Make sure enable meter receiver in the applicaiton.yml.  receiver-meter: selector: ${SW_RECEIVER_METER:default} default: Configure the meter config file, It already has the spring sleuth meter config. If you also has some customized meter at the agent side, please read meter document to configure meter.  Add UI dashboard   Open the dashboard view, click edit button to edit the templates.\n  Create a new template. Template type: Standard -\u0026gt; Template Configuration: Spring -\u0026gt; Input the Template Name.\n  Click view button, Finally get the spring sleuth dashboard.\n  Supported meter Supported 3 types information: Application, System, JVM.\n Application: HTTP request count and duration, JDBC max/idle/active connection count, Tomcat session active/reject count. System: CPU system/process usage, OS System load, OS Process file count. JVM: GC pause count and duration, Memory max/used/committed size, Thread peak/live/daemon count, Classes loaded/unloaded count.  ","excerpt":"Spring sleuth setup Spring Sleuth provides Spring Boot auto-configuration for distributed tracing. …","ref":"/docs/main/v8.6.0/en/setup/backend/spring-sleuth-setup/","title":"Spring sleuth setup"},{"body":"Start up mode In different deployment tool, such as k8s, you may need different startup mode. We provide another two optional startup modes.\nDefault mode Default mode. Do initialization works if necessary, start listen and provide service.\nRun /bin/oapService.sh(.bat) to start in this mode. Also when use startup.sh(.bat) to start.\nInit mode In this mode, oap server starts up to do initialization works, then exit. You could use this mode to init your storage, such as ElasticSearch indexes, MySQL and TiDB tables, and init data.\nRun /bin/oapServiceInit.sh(.bat) to start in this mode.\nNo-init mode In this mode, oap server starts up without initialization works, but it waits for ElasticSearch indexes, MySQL and TiDB tables existed, start listen and provide service. Meaning, this oap server expect another oap server to do the initialization.\nRun /bin/oapServiceNoInit.sh(.bat) to start in this mode.\n","excerpt":"Start up mode In different deployment tool, such as k8s, you may need different startup mode. We …","ref":"/docs/main/v8.6.0/en/setup/backend/backend-start-up-mode/","title":"Start up mode"},{"body":"Support custom enhance Here is an optional plugin apm-customize-enhance-plugin\nIntroduce SkyWalking has provided Java agent plugin development guide to help developers to build new plugin.\nThis plugin is not designed for replacement but for user convenience. The behaviour is very similar with @Trace toolkit, but without code change requirement, and more powerful, such as provide tag and log.\nHow to configure Implementing enhancements to custom classes requires two steps.\n Active the plugin, move the optional-plugins/apm-customize-enhance-plugin.jar to plugin/apm-customize-enhance-plugin.jar. Set plugin.customize.enhance_file in agent.config, which targets to rule file, such as /absolute/path/to/customize_enhance.xml. Set enhancement rules in customize_enhance.xml. \u0026lt;?xml version=\u0026#34;1.0\u0026#34; encoding=\u0026#34;UTF-8\u0026#34;?\u0026gt; \u0026lt;enhanced\u0026gt; \u0026lt;class class_name=\u0026#34;test.apache.skywalking.testcase.customize.service.TestService1\u0026#34;\u0026gt; \u0026lt;method method=\u0026#34;staticMethod()\u0026#34; operation_name=\u0026#34;/is_static_method\u0026#34; static=\u0026#34;true\u0026#34;/\u0026gt; \u0026lt;method method=\u0026#34;staticMethod(java.lang.String,int.class,java.util.Map,java.util.List,[Ljava.lang.Object;)\u0026#34; operation_name=\u0026#34;/is_static_method_args\u0026#34; static=\u0026#34;true\u0026#34;\u0026gt; \u0026lt;operation_name_suffix\u0026gt;arg[0]\u0026lt;/operation_name_suffix\u0026gt; \u0026lt;operation_name_suffix\u0026gt;arg[1]\u0026lt;/operation_name_suffix\u0026gt; \u0026lt;operation_name_suffix\u0026gt;arg[3].[0]\u0026lt;/operation_name_suffix\u0026gt; \u0026lt;tag key=\u0026#34;tag_1\u0026#34;\u0026gt;arg[2].[\u0026#39;k1\u0026#39;]\u0026lt;/tag\u0026gt; \u0026lt;tag key=\u0026#34;tag_2\u0026#34;\u0026gt;arg[4].[1]\u0026lt;/tag\u0026gt; \u0026lt;log key=\u0026#34;log_1\u0026#34;\u0026gt;arg[4].[2]\u0026lt;/log\u0026gt; \u0026lt;/method\u0026gt; \u0026lt;method method=\u0026#34;method()\u0026#34; static=\u0026#34;false\u0026#34;/\u0026gt; \u0026lt;method method=\u0026#34;method(java.lang.String,int.class)\u0026#34; operation_name=\u0026#34;/method_2\u0026#34; static=\u0026#34;false\u0026#34;\u0026gt; \u0026lt;operation_name_suffix\u0026gt;arg[0]\u0026lt;/operation_name_suffix\u0026gt; \u0026lt;tag key=\u0026#34;tag_1\u0026#34;\u0026gt;arg[0]\u0026lt;/tag\u0026gt; \u0026lt;log key=\u0026#34;log_1\u0026#34;\u0026gt;arg[1]\u0026lt;/log\u0026gt; \u0026lt;/method\u0026gt; \u0026lt;method method=\u0026#34;method(test.apache.skywalking.testcase.customize.model.Model0,java.lang.String,int.class)\u0026#34; operation_name=\u0026#34;/method_3\u0026#34; static=\u0026#34;false\u0026#34;\u0026gt; \u0026lt;operation_name_suffix\u0026gt;arg[0].id\u0026lt;/operation_name_suffix\u0026gt; \u0026lt;operation_name_suffix\u0026gt;arg[0].model1.name\u0026lt;/operation_name_suffix\u0026gt; \u0026lt;operation_name_suffix\u0026gt;arg[0].model1.getId()\u0026lt;/operation_name_suffix\u0026gt; \u0026lt;tag key=\u0026#34;tag_os\u0026#34;\u0026gt;arg[0].os.[1]\u0026lt;/tag\u0026gt; \u0026lt;log key=\u0026#34;log_map\u0026#34;\u0026gt;arg[0].getM().[\u0026#39;k1\u0026#39;]\u0026lt;/log\u0026gt; \u0026lt;/method\u0026gt; \u0026lt;/class\u0026gt; \u0026lt;class class_name=\u0026#34;test.apache.skywalking.testcase.customize.service.TestService2\u0026#34;\u0026gt; \u0026lt;method method=\u0026#34;staticMethod(java.lang.String,int.class)\u0026#34; operation_name=\u0026#34;/is_2_static_method\u0026#34; static=\u0026#34;true\u0026#34;\u0026gt; \u0026lt;tag key=\u0026#34;tag_2_1\u0026#34;\u0026gt;arg[0]\u0026lt;/tag\u0026gt; \u0026lt;log key=\u0026#34;log_1_1\u0026#34;\u0026gt;arg[1]\u0026lt;/log\u0026gt; \u0026lt;/method\u0026gt; \u0026lt;method method=\u0026#34;method([Ljava.lang.Object;)\u0026#34; operation_name=\u0026#34;/method_4\u0026#34; static=\u0026#34;false\u0026#34;\u0026gt; \u0026lt;tag key=\u0026#34;tag_4_1\u0026#34;\u0026gt;arg[0].[0]\u0026lt;/tag\u0026gt; \u0026lt;/method\u0026gt; \u0026lt;method method=\u0026#34;method(java.util.List,int.class)\u0026#34; operation_name=\u0026#34;/method_5\u0026#34; static=\u0026#34;false\u0026#34;\u0026gt; \u0026lt;tag key=\u0026#34;tag_5_1\u0026#34;\u0026gt;arg[0].[0]\u0026lt;/tag\u0026gt; \u0026lt;log key=\u0026#34;log_5_1\u0026#34;\u0026gt;arg[1]\u0026lt;/log\u0026gt; \u0026lt;/method\u0026gt; \u0026lt;/class\u0026gt; \u0026lt;/enhanced\u0026gt; ``\n   Explanation of the configuration in the file    configuration explanation     class_name The enhanced class   method The interceptor method of the class   operation_name If fill it out, will use it instead of the default operation_name.   operation_name_suffix What it means adding dynamic data after the operation_name.   static Is this method static.   tag Will add a tag in local span. The value of key needs to be represented on the XML node.   log Will add a log in local span. The value of key needs to be represented on the XML node.   arg[x] What it means is to get the input arguments. such as arg[0] is means get first arguments.   .[x] When the parsing object is Array or List, you can use it to get the object at the specified index.   .[\u0026lsquo;key\u0026rsquo;] When the parsing object is Map, you can get the map \u0026lsquo;key\u0026rsquo; through it.      ","excerpt":"Support custom enhance Here is an optional plugin apm-customize-enhance-plugin\nIntroduce SkyWalking …","ref":"/docs/main/v8.6.0/en/setup/service-agent/java-agent/customize-enhance-trace/","title":"Support custom enhance"},{"body":"Support custom trace ignore Here is an optional plugin apm-trace-ignore-plugin\nNotice: Sampling still works when the trace ignores plug-in activation.\nIntroduce  The purpose of this plugin is to filter endpoint which are expected to be ignored by the tracing system. You can setup multiple URL path patterns, The endpoints match these patterns wouldn\u0026rsquo;t be traced. The current matching rules follow Ant Path match style , like /path/*, /path/**, /path/?. Copy apm-trace-ignore-plugin-x.jar to agent/plugins, restarting the agent can effect the plugin.  How to configure There are two ways to configure ignore patterns. Settings through system env has higher priority.\n Set through the system environment variable,you need to add skywalking.trace.ignore_path to the system variables, the value is the path that you need to ignore, multiple paths should be separated by , Copy/agent/optional-plugins/apm-trace-ignore-plugin/apm-trace-ignore-plugin.config to /agent/config/ dir, and add rules to filter traces  trace.ignore_path=/your/path/1/**,/your/path/2/** ","excerpt":"Support custom trace ignore Here is an optional plugin apm-trace-ignore-plugin\nNotice: Sampling …","ref":"/docs/main/v8.6.0/en/setup/service-agent/java-agent/agent-optional-plugins/trace-ignore-plugin/","title":"Support custom trace ignore"},{"body":"Support gRPC SSL transportation for OAP server For OAP communication we are currently using gRPC, a multi-platform RPC framework that uses protocol buffers for message serialization. The nice part about gRPC is that it promotes the use of SSL/TLS to authenticate and encrypt exchanges. Now OAP supports to enable SSL transportation for gRPC receivers.\nYou can follow below steps to enable this feature\nCreating SSL/TLS Certificates It seems like step one is to generate certificates and key files for encrypting communication. I thought this would be fairly straightforward using openssl from the command line.\nUse this script if you are not familiar with how to generate key files.\nWe need below files:\n server.pem a private RSA key to sign and authenticate the public key. It\u0026rsquo;s either a PKCS#8(PEM) or PKCS#1(DER). server.crt self-signed X.509 public keys for distribution. ca.crt a certificate authority public key for a client to validate the server\u0026rsquo;s certificate.  Config OAP server You can enable gRPC SSL by add following lines to application.yml/core/default.\ngRPCSslEnabled: true gRPCSslKeyPath: /path/to/server.pem gRPCSslCertChainPath: /path/to/server.crt gRPCSslTrustedCAPath: /path/to/ca.crt gRPCSslKeyPath and gRPCSslCertChainPath are loaded by OAP server to encrypt the communication. gRPCSslTrustedCAPath helps gRPC client to verify server certificates in cluster mode.\nWhen new files are in place, they can be load dynamically instead of restarting OAP instance.\nIf you enable sharding-server to ingest data from external, add following lines to application.yml/receiver-sharing-server/default:\ngRPCSslEnabled: true gRPCSslKeyPath: /path/to/server.pem gRPCSslCertChainPath: /path/to/server.crt Because sharding-server only receives data from external, so it doesn\u0026rsquo;t need CA at all.\nIf you port to java agent, refer to TLS.md to config java agent to enable TLS.\n","excerpt":"Support gRPC SSL transportation for OAP server For OAP communication we are currently using gRPC, a …","ref":"/docs/main/v8.6.0/en/setup/backend/grpc-ssl/","title":"Support gRPC SSL transportation for OAP server"},{"body":"Support Transport Layer Security (TLS) Transport Layer Security (TLS) is a very common security way when transport data through Internet. In some use cases, end users report the background:\n Target(under monitoring) applications are in a region, which also named VPC, at the same time, the SkyWalking backend is in another region (VPC).\nBecause of that, security requirement is very obvious.\n Authentication Mode Only support no mutual auth.\n Use this script if you are not familiar with how to generate key files. Find ca.crt, and use it at client side Find server.crt ,server.pem and ca.crt. Use them at server side. Please refer to gRPC SSL for more details.  Open and config TLS Agent config   Place ca.crt into /ca folder in agent package. Notice, /ca is not created in distribution, please create it by yourself.\n  Agent open TLS automatically after the /ca/ca.crt file detected.\n  TLS with no CA mode could be activated by this setting.\n  agent.force_tls=${SW_AGENT_FORCE_TLS:false} ","excerpt":"Support Transport Layer Security (TLS) Transport Layer Security (TLS) is a very common security way …","ref":"/docs/main/v8.6.0/en/setup/service-agent/java-agent/tls/","title":"Support Transport Layer Security (TLS)"},{"body":"Telemetry for backend By default, the telemetry is disabled by setting selector to none, like this\ntelemetry: selector: ${SW_TELEMETRY:none} none: prometheus: host: ${SW_TELEMETRY_PROMETHEUS_HOST:0.0.0.0} port: ${SW_TELEMETRY_PROMETHEUS_PORT:1234} sslEnabled: ${SW_TELEMETRY_PROMETHEUS_SSL_ENABLED:false} sslKeyPath: ${SW_TELEMETRY_PROMETHEUS_SSL_KEY_PATH:\u0026#34;\u0026#34;} sslCertChainPath: ${SW_TELEMETRY_PROMETHEUS_SSL_CERT_CHAIN_PATH:\u0026#34;\u0026#34;} but you can set one of prometheus to enable them, for more information, refer to the details below.\nSelf Observability SkyWalking supports to collect telemetry data into OAP backend directly. Users could check them out through UI or GraphQL API then.\nAdding following configuration to enable self-observability related modules.\n Setting up prometheus telemetry.  telemetry: selector: ${SW_TELEMETRY:prometheus} prometheus: host: 127.0.0.1 port: 1543 Setting up prometheus fetcher  prometheus-fetcher: selector: ${SW_PROMETHEUS_FETCHER:default} default: enabledRules: ${SW_PROMETHEUS_FETCHER_ENABLED_RULES:\u0026#34;self\u0026#34;} Make sure config/fetcher-prom-rules/self.yaml exists.  Once you deploy an oap-server cluster, the target host should be replaced with a dedicated IP or hostname. For instances, there are three oap server in your cluster, their host is service1, service2 and service3 respectively. You should update each self.yaml to twist target host.\nservice1:\nfetcherInterval: PT15S fetcherTimeout: PT10S metricsPath: /metrics staticConfig: # targets will be labeled as \u0026#34;instance\u0026#34; targets: - service1:1234 labels: service: oap-server ... service2:\nfetcherInterval: PT15S fetcherTimeout: PT10S metricsPath: /metrics staticConfig: # targets will be labeled as \u0026#34;instance\u0026#34; targets: - service2:1234 labels: service: oap-server ... service3:\nfetcherInterval: PT15S fetcherTimeout: PT10S metricsPath: /metrics staticConfig: # targets will be labeled as \u0026#34;instance\u0026#34; targets: - service3:1234 labels: service: oap-server ...  WARNING, since Apr 21, 2021, Grafana project has been relicensed to AGPL-v3, no as Apache 2.0 anymore. Check the LICENSE details. The following Prometheus + Grafana solution is optional, not a recommendation.\nPrometheus Prometheus is supported as telemetry implementor. By using this, prometheus collects metrics from SkyWalking backend.\nSet prometheus to provider. The endpoint open at http://0.0.0.0:1234/ and http://0.0.0.0:1234/metrics.\ntelemetry: selector: ${SW_TELEMETRY:prometheus} prometheus: Set host and port if needed.\ntelemetry: selector: ${SW_TELEMETRY:prometheus} prometheus: host: 127.0.0.1 port: 1543 Set SSL relevant settings to expose a secure endpoint. Notice private key file and cert chain file could be uploaded once changes are applied to them.\ntelemetry: selector: ${SW_TELEMETRY:prometheus} prometheus: host: 127.0.0.1 port: 1543 sslEnabled: true sslKeyPath: /etc/ssl/key.pem sslCertChainPath: /etc/ssl/cert-chain.pem Grafana Visualization Provide the grafana dashboard settings. Check SkyWalking OAP Cluster Monitor Dashboard config and SkyWalking OAP Instance Monitor Dashboard config.\n","excerpt":"Telemetry for backend By default, the telemetry is disabled by setting selector to none, like this …","ref":"/docs/main/v8.6.0/en/setup/backend/backend-telemetry/","title":"Telemetry for backend"},{"body":"Thread dump merging mechanism The performance profile is an enhancement feature in the APM system. We are using the thread dump to estimate the method execution time, rather than adding multiple local spans. In this way, the resource cost would be much less than using distributed tracing to locate slow method. This feature is suitable in the production environment. This document introduces how thread dumps are merged into the final report as a stack tree(s).\nThread analyst Read data and transform Read the data from the database and convert it to a data structure in gRPC.\nst=\u0026gt;start: Start e=\u0026gt;end: End op1=\u0026gt;operation: Load data using paging op2=\u0026gt;operation: Transform data using parallel st(right)-\u0026gt;op1(right)-\u0026gt;op2 op2(right)-\u0026gt;e Copy the code and paste it into this link to generate flow chart.\n Use the stream to read data by page (50 records per page). Convert the data into gRPC data structures in the form of parallel streams. Merge into a list of data.  Data analysis Use the group-by and collector modes in the Java parallel stream to group according to the first stack element in the database records, and use the collector to perform data aggregation. Generate a multi-root tree.\nst=\u0026gt;start: Start e=\u0026gt;end: End op1=\u0026gt;operation: Group by first stack element sup=\u0026gt;operation: Generate empty stack tree acc=\u0026gt;operation: Accumulator data to stack tree com=\u0026gt;operation: Combine stack trees fin=\u0026gt;operation: Calculate durations and build result st(right)-\u0026gt;op1-\u0026gt;sup(right)-\u0026gt;acc acc(right)-\u0026gt;com(right)-\u0026gt;fin-\u0026gt;e Copy the code and paste it into this link to generate a flow chart.\n Group by first stack element: Use the first level element in each stack to group, ensuring that the stacks have the same root node. Generate empty stack tree: Generate multiple top-level empty trees to prepare for the following steps. The reason for generating multiple top-level trees is that original data can be added in parallel without generating locks. Accumulator data to stack tree: Add every thread dump into the generated trees.  Iterate through each element in the thread dump to find if there is any child element with the same code signature and same stack depth in the parent element. If not, add this element. Keep the dump sequences and timestamps in each nodes from the source.   Combine stack trees: Combine all trees structures into one by using the same rules as the Accumulator.  Use LDR to traverse the tree node. Use the Stack data structure to avoid recursive calls. Each stack element represents the node that needs to be merged. The task of merging two nodes is to merge the list of children nodes. If they have the same code signature and same parents, save the dump sequences and timestamps in this node. Otherwise, the node needs to be added into the target node as a new child.   Calculate durations and build result: Calculate relevant statistics and generate response.  Use the same traversal node logic as in the Combine stack trees step. Convert to a GraphQL data structure, and put all nodes into a list for subsequent duration calculations. Calculate each node\u0026rsquo;s duration in parallel. For each node, sort the sequences. If there are two continuous sequences, the duration should add the duration of these two seq\u0026rsquo;s timestamp. Calculate each node execution in parallel. For each node, the duration of the current node should deduct the time consumed by all children.    Profile data debuggiing Please follow the exporter tool to package profile data. Unzip the profile data and use analyzer main function to run it.\n","excerpt":"Thread dump merging mechanism The performance profile is an enhancement feature in the APM system. …","ref":"/docs/main/v8.6.0/en/guides/backend-profile/","title":"Thread dump merging mechanism"},{"body":"Token Authentication Supported version 7.0.0+\nWhy need token authentication after we have TLS? TLS is about transport security, which makes sure the network can be trusted. The token authentication is about monitoring application data can be trusted.\nToken In current version, Token is considered as a simple string.\nSet Token  Set token in agent.config file  # Authentication active is based on backend setting, see application.yml for more details. agent.authentication = ${SW_AGENT_AUTHENTICATION:xxxx} Set token in application.yml file  ······ receiver-sharing-server: default: authentication: ${SW_AUTHENTICATION:\u0026#34;\u0026#34;} ······ Authentication fails The Skywalking OAP verifies every request from agent, only allows requests whose token matches the one configured in application.yml.\nIf the token is not right, you will see the following log in agent\norg.apache.skywalking.apm.dependencies.io.grpc.StatusRuntimeException: PERMISSION_DENIED FAQ Can I use token authentication instead of TLS? No, you shouldn\u0026rsquo;t. In tech way, you can of course, but token and TLS are used for untrusted network env. In that circumstance, TLS has higher priority than this. Token can be trusted only under TLS protection.Token can be stolen easily if you send it through a non-TLS network.\nDo you support other authentication mechanisms? Such as ak/sk? For now, no. But we appreciate someone contributes this feature.\n","excerpt":"Token Authentication Supported version 7.0.0+\nWhy need token authentication after we have TLS? TLS …","ref":"/docs/main/v8.6.0/en/setup/backend/backend-token-auth/","title":"Token Authentication"},{"body":"Token Authentication Token In current version, Token is considered as a simple string.\nSet Token Set token in agent.config file\n# Authentication active is based on backend setting, see application.yml for more details. agent.authentication = xxxx Meanwhile, open the backend token authentication.\nAuthentication fails The Collector verifies every request from agent, allowed only the token match.\nIf the token is not right, you will see the following log in agent\norg.apache.skywalking.apm.dependencies.io.grpc.StatusRuntimeException: PERMISSION_DENIED FAQ Can I use token authentication instead of TLS? No, you shouldn\u0026rsquo;t. In tech way, you can of course, but token and TLS are used for untrusted network env. In that circumstance, TLS has higher priority than this. Token can be trusted only under TLS protection.Token can be stolen easily if you send it through a non-TLS network.\nDo you support other authentication mechanisms? Such as ak/sk? For now, no. But we appreciate someone contributes this feature.\n","excerpt":"Token Authentication Token In current version, Token is considered as a simple string.\nSet Token Set …","ref":"/docs/main/v8.6.0/en/setup/service-agent/java-agent/token-auth/","title":"Token Authentication"},{"body":"trace cross thread  Dependency the toolkit, such as using maven or gradle  \u0026lt;dependency\u0026gt; \u0026lt;groupId\u0026gt;org.apache.skywalking\u0026lt;/groupId\u0026gt; \u0026lt;artifactId\u0026gt;apm-toolkit-trace\u0026lt;/artifactId\u0026gt; \u0026lt;version\u0026gt;${skywalking.version}\u0026lt;/version\u0026gt; \u0026lt;/dependency\u0026gt;  usage 1.  @TraceCrossThread public static class MyCallable\u0026lt;String\u0026gt; implements Callable\u0026lt;String\u0026gt; { @Override public String call() throws Exception { return null; } } ... ExecutorService executorService = Executors.newFixedThreadPool(1); executorService.submit(new MyCallable());  usage 2.  ExecutorService executorService = Executors.newFixedThreadPool(1); executorService.submit(CallableWrapper.of(new Callable\u0026lt;String\u0026gt;() { @Override public String call() throws Exception { return null; } })); or\nExecutorService executorService = Executors.newFixedThreadPool(1); executorService.execute(RunnableWrapper.of(new Runnable() { @Override public void run() { //your code  } }));  usage 3.  @TraceCrossThread public class MySupplier\u0026lt;String\u0026gt; implements Supplier\u0026lt;String\u0026gt; { @Override public String get() { return null; } } ... CompletableFuture.supplyAsync(new MySupplier\u0026lt;String\u0026gt;()); or\nCompletableFuture.supplyAsync(SupplierWrapper.of(()-\u0026gt;{ return \u0026#34;SupplierWrapper\u0026#34;; })).thenAccept(System.out::println); Sample codes only\n","excerpt":"trace cross thread  Dependency the toolkit, such as using maven or gradle  \u0026lt;dependency\u0026gt; …","ref":"/docs/main/v8.6.0/en/setup/service-agent/java-agent/application-toolkit-trace-cross-thread/","title":"trace cross thread"},{"body":"Trace Data Protocol v3 Trace Data Protocol describes the data format between SkyWalking agent/sniffer and backend.\nOverview Trace data protocol is defined and provided in gRPC format, also implemented in HTTP 1.1\nReport service instance status   Service Instance Properties Service instance contains more information than just a name. Once the agent wants to report this, use ManagementService#reportInstanceProperties service to provide a string-key/string-value pair list as the parameter. language of target instance is expected at least.\n  Service Ping Service instance should keep alive with the backend. The agent should set a scheduler using ManagementService#keepAlive service every minute.\n  Send trace and metrics After you have the service ID and service instance ID ready, you could send traces and metrics. Now we have\n TraceSegmentReportService#collect for the SkyWalking native trace format JVMMetricReportService#collect for the SkyWalking native jvm format  For trace format, note that:\n The segment is a unique concept in SkyWalking. It should include all spans for each request in a single OS process, which is usually a single language-based thread. There are three types of spans.    EntrySpan EntrySpan represents a service provider, which is also the endpoint on the server end. As an APM system, SkyWalking targets the application servers. Therefore, almost all the services and MQ-consumers are EntrySpans.\n  LocalSpan LocalSpan represents a typical Java method which is not related to remote services. It is neither a MQ producer/consumer nor a provider/consumer of a service (e.g. HTTP service).\n  ExitSpan ExitSpan represents a client of service or MQ-producer. It is known as the LeafSpan in the early stages of SkyWalking. For example, accessing DB by JDBC, and reading Redis/Memcached are classified as ExitSpans.\n   Cross-thread/process span parent information is called \u0026ldquo;reference\u0026rdquo;. Reference carries the trace ID, segment ID, span ID, service name, service instance name, endpoint name, and target address used on the client end (note: this is not required in cross-thread operations) of this request in the parent. See Cross Process Propagation Headers Protocol v3 for more details.\n  Span#skipAnalysis may be TRUE, if this span doesn\u0026rsquo;t require backend analysis.\n  ","excerpt":"Trace Data Protocol v3 Trace Data Protocol describes the data format between SkyWalking …","ref":"/docs/main/v8.6.0/en/protocols/trace-data-protocol-v3/","title":"Trace Data Protocol v3"},{"body":"Trace Sampling at server side When we run a distributed tracing system, the trace bring us detailed info, but cost a lot at storage. Open server side trace sampling mechanism, the metrics of service, service instance, endpoint and topology are all accurate as before, but only don\u0026rsquo;t save all the traces into storage.\nOf course, even you open sampling, the traces will be kept as consistent as possible. Consistent means, once the trace segments have been collected and reported by agents, the backend would do their best to don\u0026rsquo;t break the trace. See Recommendation to understand why we called it as consistent as possible and do their best to don't break the trace.\nSet the sample rate In agent-analyzer module, you will find sampleRate setting.\nagent-analyzer: default: ... sampleRate: ${SW_TRACE_SAMPLE_RATE:10000} # The sample rate precision is 1/10000. 10000 means 100% sample in default. forceSampleErrorSegment: ${SW_FORCE_SAMPLE_ERROR_SEGMENT:true} # When sampling mechanism activated, this config would make the error status segment sampled, ignoring the sampling rate. slowTraceSegmentThreshold: ${SW_SLOW_TRACE_SEGMENT_THRESHOLD:-1} # Setting this threshold about the latency would make the slow trace segments sampled if they cost more time, even the sampling mechanism activated. The default value is `-1`, which means would not sample slow traces. Unit, millisecond. sampleRate is for you to set sample rate to this backend. The sample rate precision is 1/10000. 10000 means 100% sample in default.\nforceSampleErrorSegment is for you to save all error segments when sampling mechanism actived. When sampling mechanism activated, this config would make the error status segment sampled, ignoring the sampling rate.\nslowTraceSegmentThreshold is for you to save all slow trace segments when sampling mechanism actived. Setting this threshold about the latency would make the slow trace segments sampled if they cost more time, even the sampling mechanism activated. The default value is -1, which means would not sample slow traces. Unit, millisecond.\nRecommendation You could set different backend instances with different sampleRate values, but we recommend you to set the same.\nWhen you set the rate different, let\u0026rsquo;s say\n Backend-InstanceA.sampleRate = 35 Backend-InstanceB.sampleRate = 55  And we assume the agents reported all trace segments to backend, Then the 35% traces in the global will be collected and saved in storage consistent/complete, with all spans. 20% trace segments, which reported to Backend-InstanceB, will saved in storage, maybe miss some trace segments, because they are reported to Backend-InstanceA and ignored.\nNote When you open sampling, the actual sample rate could be over sampleRate. Because currently, all error/slow segments will be saved, meanwhile, the upstream and downstream may not be sampled. This feature is going to make sure you could have the error/slow stacks and segments, but don\u0026rsquo;t guarantee you would have the whole trace.\nAlso, the side effect would be, if most of the accesses are fail/slow, the sampling rate would be closing to 100%, which could crash the backend or storage clusters.\n","excerpt":"Trace Sampling at server side When we run a distributed tracing system, the trace bring us detailed …","ref":"/docs/main/v8.6.0/en/setup/backend/trace-sampling/","title":"Trace Sampling at server side"},{"body":"Tracing and Tracing based Metrics Analyze Plugins The following plugins provide the distributed tracing capability, and the OAP backend would analyze the topology and metrics based on the tracing data.\n HTTP Server  Tomcat 7 Tomcat 8 Tomcat 9 Spring Boot Web 4.x Spring MVC 3.x, 4.x 5.x with servlet 3.x Nutz Web Framework 1.x Struts2 MVC 2.3.x -\u0026gt; 2.5.x Resin 3 (Optional¹) Resin 4 (Optional¹) Jetty Server 9 Spring WebFlux 5.x (Optional¹) Undertow 1.3.0.Final -\u0026gt; 2.0.27.Final RESTEasy 3.1.0.Final -\u0026gt; 3.7.0.Final Play Framework 2.6.x -\u0026gt; 2.8.x Light4J Microservices Framework 1.6.x -\u0026gt; 2.x Netty SocketIO 1.x   HTTP Client  Feign 9.x Netflix Spring Cloud Feign 1.1.x -\u0026gt; 2.x Okhttp 3.x -\u0026gt; 4.x Apache httpcomponent HttpClient 2.0 -\u0026gt; 3.1, 4.2, 4.3 Spring RestTemplete 4.x Jetty Client 9 Apache httpcomponent AsyncClient 4.x AsyncHttpClient 2.x JRE HttpURLConnection (Optional²)   HTTP Gateway  Spring Cloud Gateway 2.0.2.RELEASE -\u0026gt; 3.x (Optional²)   JDBC  Mysql Driver 5.x, 6.x, 8.x Oracle Driver (Optional¹) H2 Driver 1.3.x -\u0026gt; 1.4.x Sharding-JDBC 1.5.x ShardingSphere 3.0.0, 4.0.0-RC1, 4.0.0, 4.0.1, 4.1.0, 4.1.1 PostgreSQL Driver 8.x, 9.x, 42.x Mariadb Driver 2.x, 1.8 InfluxDB 2.5 -\u0026gt; 2.17 Mssql-Jtds 1.x Mssql-jdbc 6.x -\u0026gt; 8.x   RPC Frameworks  Dubbo 2.5.4 -\u0026gt; 2.6.0 Dubbox 2.8.4 Apache Dubbo 2.7.0 Motan 0.2.x -\u0026gt; 1.1.0 gRPC 1.x Apache ServiceComb Java Chassis 0.1 -\u0026gt; 0.5,1.x SOFARPC 5.4.0 Armeria 0.63.0 -\u0026gt; 0.98.0 Apache Avro 1.7.0 - 1.8.x Finagle 6.44.0 -\u0026gt; 20.1.0 (6.25.0 -\u0026gt; 6.44.0 not tested) Brpc-Java 2.3.7 -\u0026gt; 2.5.3 Thrift 0.10.0 -\u0026gt; 0.12.0 Apache CXF 3.x   MQ  RocketMQ 4.x Kafka 0.11.0.0 -\u0026gt; 2.8.0 Spring-Kafka Spring Kafka Consumer 1.3.x -\u0026gt; 2.3.x (2.0.x and 2.1.x not tested and not recommended by the official document) ActiveMQ 5.10.0 -\u0026gt; 5.15.4 RabbitMQ 5.x Pulsar 2.2.x -\u0026gt; 2.4.x Aliyun ONS 1.x (Optional¹)   NoSQL  Redis  Jedis 2.x Redisson Easy Java Redis client 3.5.2+ Lettuce 5.x   MongoDB Java Driver 2.13-2.14, 3.4.0-3.12.7, 4.0.0-4.1.0 Memcached Client  Spymemcached 2.x Xmemcached 2.x   Elasticsearch  transport-client 5.2.x-5.6.x transport-client 6.7.1-6.8.4 transport-client 7.0.0-7.5.2 rest-high-level-client 6.7.1-6.8.4 rest-high-level-client 7.0.0-7.5.2   Solr  SolrJ 7.x   Cassandra 3.x  cassandra-java-driver 3.7.0-3.7.2   HBase  hbase-client HTable 1.0.0-2.4.2     Service Discovery  Netflix Eureka   Distributed Coordination  Zookeeper 3.4.x (Optional² \u0026amp; Except 3.4.4)   Spring Ecosystem  Spring Bean annotations(@Bean, @Service, @Component, @Repository) 3.x and 4.x (Optional²) Spring Core Async SuccessCallback/FailureCallback/ListenableFutureCallback 4.x Spring Transaction 4.x and 5.x (Optional²)   Hystrix: Latency and Fault Tolerance for Distributed Systems 1.4.20 -\u0026gt; 1.5.18 Scheduler  Elastic Job 2.x Apache ShardingSphere-Elasticjob 3.0.0-alpha Spring @Scheduled 3.1+ Quartz Scheduler 2.x (Optional²) XXL Job 2.x   OpenTracing community supported Canal: Alibaba mysql database binlog incremental subscription \u0026amp; consumer components 1.0.25 -\u0026gt; 1.1.2 JSON  GSON 2.8.x (Optional²)   Vert.x Ecosystem  Vert.x Eventbus 3.2+ Vert.x Web 3.x   Thread Schedule Framework  Spring @Async 4.x and 5.x Quasar 0.7.x JRE Callable and Runnable (Optional²)   Cache  Ehcache 2.x   Kotlin  Coroutine 1.0.1 -\u0026gt; 1.3.x (Optional²)   GraphQL  Graphql 8.0 -\u0026gt; 15.x   Pool  Apache Commons DBCP 2.x   Logging Framework  log4j 2.x log4j2 1.2.x logback 1.2.x   ORM  MyBatis 3.4.x -\u0026gt; 3.5.x    Meter Plugins The meter plugin provides the advanced metrics collections, which are not a part of tracing.\n ¹Due to license incompatibilities/restrictions these plugins are hosted and released in 3rd part repository, go to SkyAPM java plugin extension repository to get these.\n²These plugins affect the performance or must be used under some conditions, from experiences. So only released in /optional-plugins or /bootstrap-plugins, copy to /plugins in order to make them work.\n","excerpt":"Tracing and Tracing based Metrics Analyze Plugins The following plugins provide the distributed …","ref":"/docs/main/v8.6.0/en/setup/service-agent/java-agent/supported-list/","title":"Tracing and Tracing based Metrics Analyze Plugins"},{"body":"TTL In SkyWalking, there are two types of observability data, besides metadata.\n Record, including trace and alarm. Maybe log in the future. Metric, including such as percentile, heat map, success rate, cpm(rpm) etc.  You have following settings for different types.\n# Set a timeout on metrics data. After the timeout has expired, the metrics data will automatically be deleted. recordDataTTL: ${SW_CORE_RECORD_DATA_TTL:3} # Unit is day metricsDataTTL: ${SW_CORE_METRICS_DATA_TTL:7} # Unit is day  recordDataTTL affects Record data, including tracing and alarm. metricsDataTTL affects all metrics, including service, instance, endpoint metrics and topology map metrics.  ","excerpt":"TTL In SkyWalking, there are two types of observability data, besides metadata.\n Record, including …","ref":"/docs/main/v8.6.0/en/setup/backend/ttl/","title":"TTL"},{"body":"UI SkyWalking UI distribution is already included in our Apache official release.\nStartup Startup script is also in /bin/webappService.sh(.bat). UI runs as an OS Java process, powered-by Zuul.\nSettings Setting file of UI is webapp/webapp.yml in distribution package. It is constituted by three parts.\n Listening port. Backend connect info.  server: port: 8080 collector: path: /graphql ribbon: ReadTimeout: 10000 # Point to all backend\u0026#39;s restHost:restPort, split by ,  listOfServers: 10.2.34.1:12800,10.2.34.2:12800 ","excerpt":"UI SkyWalking UI distribution is already included in our Apache official release.\nStartup Startup …","ref":"/docs/main/v8.6.0/en/setup/backend/ui-setup/","title":"UI"},{"body":"UI Introduction SkyWalking official UI provides the default and powerful visualization capabilities for SkyWalking observing distributed cluster.\nThe latest introduction video could be found on the Youtube\n\nSkyWalking dashboard includes the following part.\n Feature Tab Selector Zone. The key features are list there. The more details will be introduced below. Reload Zone. Control the reload mechanism, including reload periodically or manually. Time Selector Zone. Control the timezone and time range. And a Chinese/English switch button here, default, the UI uses the browser language setting. We also welcome to contribute more languages.  Dashboard Dashboard provide metrics of service, service instance and endpoint. There are a few metrics terms you need to understand\n Throughput CPM , represents calls per minute. Apdex score, Read Apdex in WIKI Response Time Percentile, including p99, p95, p90, p75, p50. Read percentile in WIKI SLA, represents the successful rate. For HTTP, it means the rate of 200 response code.  Service, Instance and Dashboard selector could reload manually rather than reload the whole page. NOTICE, the Reload Zone wouldn\u0026rsquo;t reload these selectors.\nTwo default dashboards are provided to visualize the metrics of service and database.\nUser could click the lock button left aside the Service/Instance/Endpoint Reload button to custom your own dashboard.\nCustom Dashboard Users could customize the dashboard. The default dashboards are provided through the default templates located in /ui-initialized-templates folders.\nThe template file follows this format.\ntemplates: - name: template name # The unique name # The type includes DASHBOARD, TOPOLOGY_INSTANCE, TOPOLOGY_ENDPOINT. # DASHBOARD type templates could have multiple definitions, by using different names. # TOPOLOGY_INSTANCE, TOPOLOGY_ENDPOINT type templates should be defined once,  # as they are used in the topology page only. type: \u0026#34;DASHBOARD\u0026#34; # Custom the dashboard or create a new one on the UI, set the metrics as you like in the edit mode. # Then, you could export this configuration through the page and add it here. configuration: |-[ { \u0026#34;name\u0026#34;:\u0026#34;Spring Sleuth\u0026#34;, \u0026#34;type\u0026#34;:\u0026#34;service\u0026#34;, \u0026#34;children\u0026#34;:[ { \u0026#34;name\u0026#34;:\u0026#34;Sleuth\u0026#34;, \u0026#34;children\u0026#34;: [{ \u0026#34;width\u0026#34;: \u0026#34;3\u0026#34;, \u0026#34;title\u0026#34;: \u0026#34;HTTP Request\u0026#34;, \u0026#34;height\u0026#34;: \u0026#34;200\u0026#34;, \u0026#34;entityType\u0026#34;: \u0026#34;ServiceInstance\u0026#34;, \u0026#34;independentSelector\u0026#34;: false, \u0026#34;metricType\u0026#34;: \u0026#34;REGULAR_VALUE\u0026#34;, \u0026#34;metricName\u0026#34;: \u0026#34;meter_http_server_requests_count\u0026#34;, \u0026#34;queryMetricType\u0026#34;: \u0026#34;readMetricsValues\u0026#34;, \u0026#34;chartType\u0026#34;: \u0026#34;ChartLine\u0026#34;, \u0026#34;unit\u0026#34;: \u0026#34;Count\u0026#34; } ... ] } ] } ] # Activated means this templates added into the UI page automatically. # False means providing a basic template, user needs to add it manually on the page. activated: false # True means wouldn\u0026#39;t show up on the dashboard. Only keeps the definition in the storage. disabled: false NOTE, UI initialized templates would only be initialized if there is no template in the storage has the same name. Check the entity named as ui_template in your storage.\nTopology Topology map shows the relationship among the services and instances with metrics.\n Topology shows the default global topology including all services. Service Selector provides 2 level selectors, service group list and service name list. The group name is separated from the service name if it follows \u0026lt;group name\u0026gt;::\u0026lt;logic name\u0026gt; format. Topology map is available for single group, single service, or global(include all services). Custom Group provides the any sub topology capability of service group. Service Deep Dive opens when you click any service. The honeycomb could do metrics, trace and alarm query of the selected service. Service Relationship Metrics gives the metrics of service RPC interactions and instances of these two services.  Trace Query Trace query is a typical feature as SkyWalking provided distributed agents.\n Trace Segment List is not the trace list. Every trace has several segments belonging to different services. If\nquery by all services or by trace id, different segments with same trace id could be list there. Span is clickable, the detail of each span will pop up on the left side. Trace Views provides 3 typical and different usage views to visualize the trace.  Profile Profile is an interaction feature. It provides the method level performance diagnosis.\nTo start the profile analysis, user need to create the profile task\n Select the specific service. Set the endpoint name. This endpoint name typically is the operation name of the first span. Find this on the trace segment list view. Monitor time could start right now or from any given future time. Monitor duration defines the observation time window to find the suitable request to do performance analysis. Even the profile add a very limited performance impact to the target system, but it is still an additional load. This duration make the impact controllable. Min duration threshold provides a filter mechanism, if a request of the given endpoint response quickly, it wouldn\u0026rsquo;t be profiled. This could make sure, the profiled data is the expected one. Max sampling count gives the max dataset of agent will collect. It helps to reduce the memory and network load. One implicit condition, in any moment, SkyWalking only accept one profile task for each service. Agent could have different settings to control or limit this feature, read document setup for more details. Not all SkyWalking ecosystem agent supports this feature, java agent from 7.0.0 supports this in default.  Once the profile done, the profiled trace segments would show up. And you could request for analysis for any span. Typically, we analysis spans having long self duration, if the span and its children both have long duration, you could choose include children or exclude childrend to set the analysis boundaries.\nAfter choose the right span, and click the analysis button, you will see the stack based analysis result. The slowest methods have been highlighted.\nAdvanced features  Since 7.1.0, the profiled trace collects the HTTP request parameters for Tomcat and SpringMVC Controller automatically.  Log Since 8.3.0, SkyWalking provides log query for the browser monitoring. Use Apache SkyWalking Client JS agent would collect metrics and error logs.\nAlarm Alarm page lists all triggered alarm. Read the backend setup documentation to know how to set up the alarm rule or integrate with 3rd party system.\n","excerpt":"UI Introduction SkyWalking official UI provides the default and powerful visualization capabilities …","ref":"/docs/main/v8.6.0/en/ui/readme/","title":"UI Introduction"},{"body":"Uninstrumented Gateways/Proxies The uninstrumented gateways are not instrumented by SkyWalking agent plugin when they are started, but they can be configured in gateways.yml file or via Dynamic Configuration. The reason why they can\u0026rsquo;t register to backend automatically is that there\u0026rsquo;re no suitable agent plugins, for example, there is no agent plugins for Nginx, haproxy, etc. So in order to visualize the real topology, we provide a way to configure the gateways/proxies manually.\nConfiguration Format The configuration content includes the gateways' names and their instances:\ngateways: - name: proxy0 # the name is not used for now instances: - host: 127.0.0.1 # the host/ip of this gateway instance port: 9099 # the port of this gateway instance, defaults to 80 Note that the host of the instance must be the one that is actually used in client side, for example, if the instance proxyA has 2 IPs, say 192.168.1.110 and 192.168.1.111, both of which delegates the target service, and the client connects to 192.168.1.110, then configuring 192.168.1.111 as the host won\u0026rsquo;t work properly.\n","excerpt":"Uninstrumented Gateways/Proxies The uninstrumented gateways are not instrumented by SkyWalking agent …","ref":"/docs/main/v8.6.0/en/setup/backend/uninstrumented-gateways/","title":"Uninstrumented Gateways/Proxies"},{"body":"Using E2E local remote debugging The E2E remote debugging port of service containers is 5005. If the developer wants to use remote debugging, he needs to add remote debugging parameters to the start service command, and then expose the port 5005.\nFor example, this is the configuration of a container in skywalking/test/e2e/e2e-test/docker/base-compose.yml. JAVA_OPTS is a preset variable for passing additional parameters in the AOP service startup command, so we only need to add the JAVA remote debugging parameters agentlib:jdwp=transport=dt_socket,server=y,suspend=n,address=5005 to the configuration and expose the port 5005.\noap: image: skywalking/oap:latest expose: ... - 5005 ... environment: ... JAVA_OPTS: \u0026gt;-... -agentlib:jdwp=transport=dt_socket,server=y,suspend=n,address=5005 ... At last, if the E2E test fails and is retrying, the developer may get the ports mapping in the file skywalking/test/e2e/e2e-test/remote_real_port and select the host port of the corresponding service for remote debugging. For example,\n#remote_real_port #The remote debugging port on the host is 32783 oap-localhost:32783 #The remote debugging port on the host is 32782 provider-localhost:32782 ","excerpt":"Using E2E local remote debugging The E2E remote debugging port of service containers is 5005. If the …","ref":"/docs/main/v8.6.0/en/guides/e2e-local-remote-debug/","title":"Using E2E local remote debugging"},{"body":"V6 upgrade SkyWalking v6 is widely used in many production environments. Follow the steps in the guide below to learn how to upgrade to a new release.\nNOTE: The ways to upgrade are not limited to the steps below.\nUse Canary Release Like all applications, you may upgrade SkyWalking using the canary release method through the following steps.\n Deploy a new cluster by using the latest version of SkyWalking OAP cluster with the new database cluster. Once the target service (i.e. the service being monitored) has upgraded the agent.jar (or simply by rebooting), have collector.backend_service pointing to the new OAP backend, and use/add a new namespace(agent.namespace in Table of Agent Configuration Properties). The namespace will prevent conflicts from arising between different versions. When all target services have been rebooted, the old OAP clusters could be discarded.  The Canary Release method works for any version upgrades.\nOnline Hot Reboot Upgrade The reason we require Canary Release is that the SkyWalking agent has cache mechanisms, and switching to a new cluster causes the cache to become unavailable for new OAP clusters. In version 6.5.0+ (especially for agent versions), we have Agent hot reboot trigger mechanism. This streamlines the upgrade process as we deploy a new cluster by using the latest version of SkyWalking OAP cluster with the new database cluster, and shift the traffic to the new cluster once and for all. Based on the mechanism, all agents will enter the cool_down mode, and come back online. For more details, see the backend setup documentation.\nNOTE: A known bug in 6.4.0 is that its agent may have re-connection issues; therefore, even though this bot reboot mechanism has been included in 6.4.0, it may not work under some network scenarios, especially in Kubernetes.\nAgent Compatibility All versions of SkyWalking 6.x (and even 7.x) are compatible with each other, so users could simply upgrade the OAP servers. As the agent has also been enhanced in the latest versions, according to the SkyWalking team\u0026rsquo;s recommendation, upgrade the agent as soon as practicable.\n","excerpt":"V6 upgrade SkyWalking v6 is widely used in many production environments. Follow the steps in the …","ref":"/docs/main/v8.6.0/en/faq/v6-version-upgrade/","title":"V6 upgrade"},{"body":"V8 upgrade Starting from SkyWalking v8, the v3 protocol has been used. This makes it incompatible with previous releases. Users who intend to upgrade in v8 series releases could follow the steps below.\nRegisters in v6 and v7 have been removed in v8 for better scaling out performance. Please upgrade following the instructions below.\n Use a different storage or a new namespace. You may also consider erasing the whole storage indexes or tables related to SkyWalking. Deploy the whole SkyWalking cluster, and expose it in a new network address. If you are using language agents, upgrade the new agents too; meanwhile, make sure the agents are supported in a different language. Then, set up the backend address to the new SkyWalking OAP cluster.  ","excerpt":"V8 upgrade Starting from SkyWalking v8, the v3 protocol has been used. This makes it incompatible …","ref":"/docs/main/v8.6.0/en/faq/v8-version-upgrade/","title":"V8 upgrade"},{"body":"Version 3.x -\u0026gt; 5.0.0-alpha Upgrade FAQs Collector Problem There is no information showing in the UI.\nCause In the upgrade from version 3.2.6 to 5.0.0, the existing Elasticsearch indexes are kept, but aren\u0026rsquo;t compatible with 5.0.0-alpha. When service name is registered, ElasticSearch will create this column by default type string, which will lead to an error.\nSolution Clean the data folder in ElasticSearch and restart ElasticSearch, collector and your application under monitoring.\n","excerpt":"Version 3.x -\u0026gt; 5.0.0-alpha Upgrade FAQs Collector Problem There is no information showing in the …","ref":"/docs/main/v8.6.0/en/faq/v3-version-upgrade/","title":"Version 3.x -\u003e 5.0.0-alpha Upgrade FAQs"},{"body":"Visualization The SkyWalking native UI provides a default solution for visualization. It provides observability related graphs on overview, service, service instance, endpoint, trace, and alarm, such as topology maps, dependency graphs, heatmaps, etc.\nWe know that many of our users have integrated SkyWalking into their own products. If you would like to do that too, please refer to the SkyWalking query protocol.\n","excerpt":"Visualization The SkyWalking native UI provides a default solution for visualization. It provides …","ref":"/docs/main/v8.6.0/en/concepts-and-designs/ui-overview/","title":"Visualization"},{"body":"VMs monitoring SkyWalking leverages Prometheus node-exporter for collecting metrics data from the VMs, and leverages OpenTelemetry Collector to transfer the metrics to OpenTelemetry receiver and into the Meter System.\nWe define the VM entity as a Service in OAP, and use vm:: as a prefix to identify it.\nData flow  The Prometheus node-exporter collects metrics data from the VMs. The OpenTelemetry Collector fetches metrics from the node-exporter via Prometheus Receiver and pushes metrics to SkyWalking OAP Server via the OpenCensus gRPC Exporter. The SkyWalking OAP Server parses the expression with MAL to filter/calculate/aggregate and store the results.  Setup  Setup Prometheus node-exporter. Setup OpenTelemetry Collector . This is an example for OpenTelemetry Collector configuration otel-collector-config.yaml. Config SkyWalking OpenTelemetry receiver.  Supported Metrics    Monitoring Panel Unit Metric Name Description Data Source     CPU Usage % cpu_total_percentage The total percentage usage of the CPU core. If there are 2 cores, the maximum usage is 200%. Prometheus node-exporter   Memory RAM Usage MB meter_vm_memory_used The total RAM usage Prometheus node-exporter   Memory Swap Usage % meter_vm_memory_swap_percentage The percentage usage of swap memory Prometheus node-exporter   CPU Average Used % meter_vm_cpu_average_used The percentage usage of the CPU core in each mode Prometheus node-exporter   CPU Load  meter_vm_cpu_load1\nmeter_vm_cpu_load5\nmeter_vm_cpu_load15 The CPU 1m / 5m / 15m average load Prometheus node-exporter   Memory RAM MB meter_vm_memory_total\nmeter_vm_memory_available\nmeter_vm_memory_used The RAM statistics, including Total / Available / Used Prometheus node-exporter   Memory Swap MB meter_vm_memory_swap_free\nmeter_vm_memory_swap_total The swap memory statistics, including Free / Total Prometheus node-exporter   File System Mountpoint Usage % meter_vm_filesystem_percentage The percentage usage of the file system at each mount point Prometheus node-exporter   Disk R/W KB/s meter_vm_disk_read,meter_vm_disk_written The disk read and written Prometheus node-exporter   Network Bandwidth Usage KB/s meter_vm_network_receive\nmeter_vm_network_transmit The network receive and transmit Prometheus node-exporter   Network Status  meter_vm_tcp_curr_estab\nmeter_vm_tcp_tw\nmeter_vm_tcp_alloc\nmeter_vm_sockets_used\nmeter_vm_udp_inuse The number of TCPs established / TCP time wait / TCPs allocated / sockets in use / UDPs in use Prometheus node-exporter   Filefd Allocated  meter_vm_filefd_allocated The number of file descriptors allocated Prometheus node-exporter    Customizing You can customize your own metrics/expression/dashboard panel.\nThe metrics definition and expression rules are found in /config/otel-oc-rules/vm.yaml.\nThe dashboard panel confirmations are found in /config/ui-initialized-templates/vm.yml.\nBlog For more details, see blog article SkyWalking 8.4 provides infrastructure monitoring.\nK8s monitoring SkyWalking leverages K8s kube-state-metrics and cAdvisor for collecting metrics data from K8s, and leverages OpenTelemetry Collector to transfer the metrics to OpenTelemetry receiver and into the Meter System. This feature requires authorizing the OAP Server to access K8s\u0026rsquo;s API Server.\nWe define the k8s-cluster as a Service in the OAP, and use k8s-cluster:: as a prefix to identify it.\nWe define the k8s-node as an Instance in the OAP, and set its name as the K8s node name.\nWe define the k8s-service as an Endpoint in the OAP, and set its name as $serviceName.$namespace.\nData flow  K8s kube-state-metrics and cAdvisor collect metrics data from K8s. OpenTelemetry Collector fetches metrics from kube-state-metrics and cAdvisor via Prometheus Receiver and pushes metrics to SkyWalking OAP Server via the OpenCensus GRPC Exporter. The SkyWalking OAP Server access to K8s\u0026rsquo;s API Server gets meta info and parses the expression with MAL to filter/calculate/aggregate and store the results.  Setup  Setup kube-state-metric. cAdvisor is integrated into kubelet by default. Set up OpenTelemetry Collector . For details on Prometheus Receiver in OpenTelemetry Collector for K8s, refer to here. For a quick start, we have provided a full example for OpenTelemetry Collector configuration otel-collector-config.yaml. Config SkyWalking OpenTelemetry receiver.  Supported Metrics From the different points of view to monitor K8s, there are 3 kinds of metrics: Cluster / Node / Service\nCluster These metrics are related to the selected cluster (Current Service in the dashboard).\n   Monitoring Panel Unit Metric Name Description Data Source     Node Total  k8s_cluster_node_total The number of nodes K8s kube-state-metrics   Namespace Total  k8s_cluster_namespace_total The number of namespaces K8s kube-state-metrics   Deployment Total  k8s_cluster_deployment_total The number of deployments K8s kube-state-metrics   Service Total  k8s_cluster_service_total The number of services K8s kube-state-metrics   Pod Total  k8s_cluster_pod_total The number of pods K8s kube-state-metrics   Container Total  k8s_cluster_container_total The number of containers K8s kube-state-metrics   CPU Resources m k8s_cluster_cpu_cores\nk8s_cluster_cpu_cores_requests\nk8s_cluster_cpu_cores_limits\nk8s_cluster_cpu_cores_allocatable The capacity and the Requests / Limits / Allocatable of the CPU K8s kube-state-metrics   Memory Resources GB k8s_cluster_memory_total\nk8s_cluster_memory_requests\nk8s_cluster_memory_limits\nk8s_cluster_memory_allocatable The capacity and the Requests / Limits / Allocatable of the memory K8s kube-state-metrics   Storage Resources GB k8s_cluster_storage_total\nk8s_cluster_storage_allocatable The capacity and allocatable of the storage K8s kube-state-metrics   Node Status  k8s_cluster_node_status The current status of the nodes K8s kube-state-metrics   Deployment Status  k8s_cluster_deployment_status The current status of the deployment K8s kube-state-metrics   Deployment Spec Replicas  k8s_cluster_deployment_spec_replicas The number of desired pods for a deployment K8s kube-state-metrics   Service Status  k8s_cluster_service_pod_status The services current status, depending on the related pods' status K8s kube-state-metrics   Pod Status Not Running  k8s_cluster_pod_status_not_running The pods which are not running in the current phase K8s kube-state-metrics   Pod Status Waiting  k8s_cluster_pod_status_waiting The pods and containers which are currently in the waiting status, with reasons shown K8s kube-state-metrics   Pod Status Terminated  k8s_cluster_container_status_terminated The pods and containers which are currently in the terminated status, with reasons shown K8s kube-state-metrics    Node These metrics are related to the selected node (Current Instance in the dashboard).\n   Monitoring Panel Unit Metric Name Description Data Source     Pod Total  k8s_node_pod_total The number of pods in this node K8s kube-state-metrics   Node Status  k8s_node_node_status The current status of this node K8s kube-state-metrics   CPU Resources m k8s_node_cpu_cores\nk8s_node_cpu_cores_allocatable\nk8s_node_cpu_cores_requests\nk8s_node_cpu_cores_limits The capacity and the requests / Limits / Allocatable of the CPU K8s kube-state-metrics   Memory Resources GB k8s_node_memory_total\nk8s_node_memory_allocatable\nk8s_node_memory_requests\nk8s_node_memory_limits The capacity and the requests / Limits / Allocatable of the memory K8s kube-state-metrics   Storage Resources GB k8s_node_storage_total\nk8s_node_storage_allocatable The capacity and allocatable of the storage K8s kube-state-metrics   CPU Usage m k8s_node_cpu_usage The total usage of the CPU core, if there are 2 cores the maximum usage is 2000m cAdvisor   Memory Usage GB k8s_node_memory_usage The totaly memory usage cAdvisor   Network I/O KB/s k8s_node_network_receive\nk8s_node_network_transmit The network receive and transmit cAdvisor    Service In these metrics, the pods are related to the selected service (Current Endpoint in the dashboard).\n   Monitoring Panel Unit Metric Name Description Data Source     Service Pod Total  k8s_service_pod_total The number of pods K8s kube-state-metrics   Service Pod Status  k8s_service_pod_status The current status of pods K8s kube-state-metrics   Service CPU Resources m k8s_service_cpu_cores_requests\nk8s_service_cpu_cores_limits The CPU resources requests / Limits of this service K8s kube-state-metrics   Service Memory Resources MB k8s_service_memory_requests\nk8s_service_memory_limits The memory resources requests / Limits of this service K8s kube-state-metrics   Pod CPU Usage m k8s_service_pod_cpu_usage The CPU resources total usage of pods cAdvisor   Pod Memory Usage MB k8s_service_pod_memory_usage The memory resources total usage of pods cAdvisor   Pod Waiting  k8s_service_pod_status_waiting The pods and containers which are currently in the waiting status, with reasons shown K8s kube-state-metrics   Pod Terminated  k8s_service_pod_status_terminated The pods and containers which are currently in the terminated status, with reasons shown K8s kube-state-metrics   Pod Restarts  k8s_service_pod_status_restarts_total The number of per container restarts related to the pods K8s kube-state-metrics   Pod Network Receive KB/s k8s_service_pod_network_receive The network receive of the pods cAdvisor   Pod Network Transmit KB/s k8s_service_pod_network_transmit The network transmit of the pods cAdvisor   Pod Storage Usage MB k8s_service_pod_fs_usage The storage resources total usage of pods related to this service cAdvisor    Customizing You can customize your own metrics/expression/dashboard panel.\nThe metrics definition and expression rules are found in /config/otel-oc-rules/k8s-cluster.yaml，/config/otel-oc-rules/k8s-node.yaml, /config/otel-oc-rules/k8s-service.yaml.\nThe dashboard panel configurations are found in /config/ui-initialized-templates/k8s.yml.\n","excerpt":"VMs monitoring SkyWalking leverages Prometheus node-exporter for collecting metrics data from the …","ref":"/docs/main/v8.6.0/en/setup/backend/backend-infrastructure-monitoring/","title":"VMs monitoring"},{"body":"Welcome This is the official documentation of SkyWalking 8. Welcome to the SkyWalking community!\nHere you can learn all you need to know about SkyWalking’s architecture, understand how to deploy and use SkyWalking, and contribute to the project based on SkyWalking\u0026rsquo;s contributing guidelines.\nNOTE: SkyWalking 8 uses brand new tracing APIs which are incompatible with all previous releases.\n  Concepts and Designs. You\u0026rsquo;ll find the core logic behind SkyWalking. You may start from here if you want to understand what is going on under our cool features and visualization.\n  Setup. A guide to installing SkyWalking for different use cases. It is an observability platform that supports multiple observability modes.\n  UI Introduction. An introduction to the UI components and their features.\n  Contributing Guides. If you are a PMC member, a committer, or a new contributor, learn how to start contributing with these guides!\n  Protocols. The protocols show how agents/probes and the backend communicate with one another. Anyone interested in uplink telemetry data should definitely read this.\n  FAQs. A manifest of known issues with setup and secondary developments processes. Should you encounter any problems, check here first.\n  You might also find these links interesting:\n  The latest and old releases are all available at Apache SkyWalking release page. The change logs can be found here.\n  SkyWalking WIKI hosts the context of some changes and events.\n  You can find the conference schedules, video recordings, and articles about SkyWalking in the community resource catalog.\n  We\u0026rsquo;re always looking for help to improve our documentation and codes, so please don’t hesitate to file an issue if you see any problems. Or better yet, directly contribute by submitting a pull request to help us get better!\n","excerpt":"Welcome This is the official documentation of SkyWalking 8. Welcome to the SkyWalking community! …","ref":"/docs/main/v8.6.0/readme/","title":"Welcome"},{"body":"What is VNode? On the trace page, you may sometimes find nodes with their spans named VNode, and that there are no attributes for such spans.\nVNode is created by the UI itself, rather than being reported by the agent or tracing SDK. It indicates that some spans are missed in the trace data in this query.\nHow does the UI detect the missing span(s)? The UI checks the parent spans and reference segments of all spans in real time. If no parent id(segment id + span id) could be found, then it creates a VNode automatically.\nHow did this happen? The VNode appears when the trace data is incomplete.\n The agent fail-safe mechanism has been activated. The SkyWalking agent could abandon the trace data if there are any network issues between the agent and the OAP (e.g. failure to connect, slow network speeds, etc.), or if the OAP cluster is not capable of processing all traces. Some plug-ins may have bugs, and some segments in the trace do not stop correctly and are held in the memory.  In such case, the trace would not exist in the query, thus the VNode shows up.\n","excerpt":"What is VNode? On the trace page, you may sometimes find nodes with their spans named VNode, and …","ref":"/docs/main/v8.6.0/en/faq/vnode/","title":"What is VNode?"},{"body":"Why can\u0026rsquo;t I see any data in the UI? There are three main reasons no data can be shown by the UI:\n No traces have been sent to the collector. Traces have been sent, but the timezone of your containers is incorrect. Traces are in the collector, but you\u0026rsquo;re not watching the correct timeframe in the UI.  No traces Be sure to check the logs of your agents to see if they are connected to the collector and traces are being sent.\nIncorrect timezone in containers Be sure to check the time in your containers.\nThe UI isn\u0026rsquo;t showing any data Be sure to configure the timeframe shown by the UI.\n","excerpt":"Why can\u0026rsquo;t I see any data in the UI? There are three main reasons no data can be shown by the …","ref":"/docs/main/v8.6.0/en/faq/time-and-timezone/","title":"Why can't I see any data in the UI?"},{"body":"Why do metrics indexes with Hour and Day precisions stop updating after upgrade to 7.x? This issue is to be expected with an upgrade from 6.x to 7.x. See the Downsampling Data Packing feature of the ElasticSearch storage.\nYou may simply delete all expired *-day_xxxxx and *-hour_xxxxx(xxxxx is a timestamp) indexes. Currently, SkyWalking uses the metrics name-xxxxx and metrics name-month_xxxxx indexes only.\n","excerpt":"Why do metrics indexes with Hour and Day precisions stop updating after upgrade to 7.x? This issue …","ref":"/docs/main/v8.6.0/en/faq/hour-day-metrics-stopping/","title":"Why do metrics indexes with Hour and Day precisions stop updating after upgrade to 7.x?"},{"body":"Why doesn\u0026rsquo;t SkyWalking involve MQ in its architecture? This is often asked by those who are first introduced to SkyWalking. Many believe that MQ should have better performance and should be able to support higher throughput, like the following:\nHere\u0026rsquo;s what we think.\nIs MQ appropriate for communicating with the OAP backend? This question arises when users consider the circumstances where the OAP cluster may not be powerful enough or becomes offline. But the following issues must first be addressed:\n Why do you think that the OAP is not powerful enough? Were it not powerful, the speed of data analysis wouldn\u0026rsquo;t have caught up with the producers (or agents). Then what is the point of adding new deployment requirements? Some may argue that the payload is sometimes higher than usual during peak times. But we must consider how much higher the payload really is. If it is higher by less than 40%, how many resources would you use for the new MQ cluster? How about moving them to new OAP and ES nodes? Say it is higher by 40% or more, such as by 70% to 200%. Then, it is likely that your MQ would use up more resources than it saves. Your MQ would support 2 to 3 times the payload using 10%-20% of the cost during usual times. Furthermore, in this case, if the payload/throughput are so high, how long would it take for the OAP cluster to catch up? The challenge here is that well before it catches up, the next peak times would have come.  With the analysis above in mind, why would you still want the traces to be 100%, given the resources they would cost? The preferred way to do this would be adding a better dynamic trace sampling mechanism at the backend. When throughput exceeds the threshold, gradually modify the active sampling rate from 100% to 10%, which means you could get the OAP and ES 3 times more powerful than usual, while ignoring the traces at peak times.\nIs MQ transport recommended despite its side effects? Even though MQ transport is not recommended from the production perspective, SkyWalking still provides optional plugins named kafka-reporter and kafka-fetcher for this feature since 8.1.0.\nHow about MQ metrics data exporter? The answer is that the MQ metrics data exporter is already readily available. The exporter module with gRPC default mechanism is there, and you can easily provide a new implementor of this module.\n","excerpt":"Why doesn\u0026rsquo;t SkyWalking involve MQ in its architecture? This is often asked by those who are …","ref":"/docs/main/v8.6.0/en/faq/why_mq_not_involved/","title":"Why doesn't SkyWalking involve MQ in its architecture?"},{"body":"Work with Istio Instructions for transport Istio\u0026rsquo;s metrics to the SkyWalking OAP server.\nPrerequisites Istio should be installed in the Kubernetes cluster. Follow Istio getting start to finish it.\nDeploy SkyWalking backend Follow the deploying backend in Kubernetes to install the OAP server in the kubernetes cluster. Refer to OpenTelemetry receiver to ingest metrics. otel-receiver defaults to be inactive. Set env var SW_OTEL_RECEIVER to default to enable it.\nDeploy OpenTelemetry collector OpenTelemetry collector is the location Istio telemetry sends metrics, then processing and sending them to SkyWalking backend.\nFollowing the Getting Started to deploy this collector. There are several components available in the collector, and they could be combined for different scenarios. For the sake of brevity, we use the Prometheus receiver to retrieve metrics from Istio control and data plane, then send them to SkyWalking by OpenCensus exporter.\nPrometheus receiver Refer to Prometheus Receiver to set up this receiver. you could find more configuration details in Prometheus Integration of Istio to figure out how to direct Prometheus receiver to query Istio metrics.\nSkyWalking supports receiving multi-cluster metrics in a single OAP cluster. A cluster label should be appended to every metric fetched by this receiver even there\u0026rsquo;s only a single cluster needed to be collected. You could leverage relabel to add it like below:\nrelabel_configs: - source_labels: [] target_label: cluster replacement: \u0026lt;cluster name\u0026gt; or opt to Resource Processor:\nprocessors: resource: attributes: - key: cluster value: \u0026quot;\u0026lt;cluster name\u0026gt;\u0026quot; action: upsert Notice, if you try the sample of istio Prometheus Kubernetes configuration, the issues described here might block you. Try to use the solution indicated in this issue if it\u0026rsquo;s not fixed.\nOpenCensus exporter Follow OpenCensus exporter configuration to set up a connection between OpenTelemetry collector and OAP cluster. endpoint is the address of OAP gRPC service.\nObserve Istio Open Istio Dashboard in SkyWaling UI by clicking Dashboard -\u0026gt; Istio, then you\u0026rsquo;re able to view charts and diagrams generated by Istio metrics. You also could view them by swctl and set up alarm rules based on them.\nNOTICE, if you want metrics of Istio managed services, including topology among them, we recommend you to consider our ALS solution\n","excerpt":"Work with Istio Instructions for transport Istio\u0026rsquo;s metrics to the SkyWalking OAP server. …","ref":"/docs/main/v8.6.0/en/setup/istio/readme/","title":"Work with Istio"},{"body":"Zabbix Receiver Zabbix receiver is accepting the metrics of Zabbix Agent Active Checks protocol format into the Meter System. Zabbix Agent is base on GPL-2.0 License.\nModule define receiver-zabbix: selector: ${SW_RECEIVER_ZABBIX:default} default: # Export tcp port, Zabbix agent could connected and transport data port: 10051 # Bind to host host: 0.0.0.0 # Enable config when receive agent request activeFiles: agent Configuration file Zabbix receiver is configured via a configuration file. The configuration file defines everything related to receiving from agents, as well as which rule files to load.\nOAP can load the configuration at bootstrap. If the new configuration is not well-formed, OAP fails to start up. The files are located at $CLASSPATH/zabbix-rules.\nThe file is written in YAML format, defined by the scheme described below. Square brackets indicate that a parameter is optional.\nAn example for zabbix agent configuration could be found here. You could find the Zabbix agent detail items from Zabbix Agent documentation.\nConfiguration file # insert metricPrefix into metric name: \u0026lt;metricPrefix\u0026gt;_\u0026lt;raw_metric_name\u0026gt; metricPrefix: \u0026lt;string\u0026gt; # expSuffix is appended to all expression in this file. expSuffix: \u0026lt;string\u0026gt; # Datasource from Zabbix Item keys. requiredZabbixItemKeys: - \u0026lt;zabbix item keys\u0026gt; # Support agent entities information. entities: # Allow hostname patterns to build metrics. hostPatterns: - \u0026lt;regex string\u0026gt; # Customized metrics label before parse to meter system. labels: [- \u0026lt;labels\u0026gt; ] # Metrics rule allow you to recompute queries. metrics: [ - \u0026lt;metrics_rules\u0026gt; ]  # Define the label name. The label value must query from `value` or `fromItem` attribute. name: \u0026lt;string\u0026gt; # Appoint value to label. [value: \u0026lt;string\u0026gt;] # Query label value from Zabbix Agent Item key. [fromItem: \u0026lt;string\u0026gt;] \u0026lt;metric_rules\u0026gt; # The name of rule, which combinates with a prefix \u0026#39;meter_\u0026#39; as the index/table name in storage. name: \u0026lt;string\u0026gt; # MAL expression. exp: \u0026lt;string\u0026gt; More about MAL, please refer to mal.md.\n","excerpt":"Zabbix Receiver Zabbix receiver is accepting the metrics of Zabbix Agent Active Checks protocol …","ref":"/docs/main/v8.6.0/en/setup/backend/backend-zabbix/","title":"Zabbix Receiver"},{"body":"Advanced deployment OAP servers inter communicate with each other in a cluster environment. In the cluster mode, you could run in different roles.\n Mixed(default) Receiver Aggregator  In some time, users want to deploy cluster nodes with explicit role. Then could use this.\nMixed Default role, the OAP should take responsibilities of\n Receive agent traces or metrics. Do L1 aggregation Internal communication(send/receive) Do L2 aggregation Persistence Alarm  Receiver The OAP should take responsibilities of\n Receive agent traces or metrics. Do L1 aggregation Internal communication(send)  Aggregator The OAP should take responsibilities of\n Internal communication(receive) Do L2 aggregation Persistence Alarm   These roles are designed for complex deployment requirements based on security and network policy.\nKubernetes If you are using our native Kubernetes coordinator, the labelSelector setting is used for Aggregator choose rules. Choose the right OAP deployment based on your requirements.\n","excerpt":"Advanced deployment OAP servers inter communicate with each other in a cluster environment. In the …","ref":"/docs/main/v8.5.0/en/setup/backend/advanced-deployment/","title":"Advanced deployment"},{"body":"Alarm Alarm core is driven by a collection of rules, which are defined in config/alarm-settings.yml. There are three parts in alarm rule definition.\n Alarm rules. They define how metrics alarm should be triggered, what conditions should be considered. Webhooks. The list of web service endpoint, which should be called after the alarm is triggered. gRPCHook. The host and port of remote gRPC method, which should be called after the alarm is triggered.  Entity name Define the relation between scope and entity name.\n Service: Service name Instance: {Instance name} of {Service name} Endpoint: {Endpoint name} in {Service name} Database: Database service name Service Relation: {Source service name} to {Dest service name} Instance Relation: {Source instance name} of {Source service name} to {Dest instance name} of {Dest service name} Endpoint Relation: {Source endpoint name} in {Source Service name} to {Dest endpoint name} in {Dest service name}  Rules There are two types of rules, individual rule and composite rule, composite rule is the combination of individual rules\nIndividual rules Alarm rule is constituted by following keys\n Rule name. Unique name, show in alarm message. Must end with _rule. Metrics name. A.K.A. metrics name in oal script. Only long, double, int types are supported. See List of all potential metrics name. Include names. The following entity names are included in this rule. Please follow Entity name define. Exclude names. The following entity names are excluded in this rule. Please follow Entity name define. Include names regex. Provide a regex to include the entity names. If both setting the include name list and include name regex, both rules will take effect. Exclude names regex. Provide a regex to exclude the entity names. If both setting the exclude name list and exclude name regex, both rules will take effect. Include labels. The following labels of the metric are included in this rule. Exclude labels. The following labels of the metric are excluded in this rule. Include labels regex. Provide a regex to include labels. If both setting the include label list and include label regex, both rules will take effect. Exclude labels regex. Provide a regex to exclude labels. If both setting the exclude label list and exclude label regex, both rules will take effect.  The settings of labels is required by meter-system which intends to store metrics from label-system platform, just like Prometheus, Micrometer, etc. The function supports the above four settings should implement LabeledValueHolder.\n Threshold. The target value. For multiple values metrics, such as percentile, the threshold is an array. Described like value1, value2, value3, value4, value5. Each value could the threshold for each value of the metrics. Set the value to - if don\u0026rsquo;t want to trigger alarm by this or some of the values.\nSuch as in percentile, value1 is threshold of P50, and -, -, value3, value4, value5 means, there is no threshold for P50 and P75 in percentile alarm rule. OP. Operator, support \u0026gt;, \u0026gt;=, \u0026lt;, \u0026lt;=, =. Welcome to contribute all OPs. Period. How long should the alarm rule should be checked. This is a time window, which goes with the backend deployment env time. Count. In the period window, if the number of values over threshold(by OP), reaches count, alarm should send. Only as condition. Specify if the rule can send notification or just as an condition of composite rule. Silence period. After alarm is triggered in Time-N, then keep silence in the TN -\u0026gt; TN + period. By default, it is as same as Period, which means in a period, same alarm(same ID in same metrics name) will be trigger once.  Composite rules NOTE. Composite rules only work for alarm rules targeting the same entity level, such as alarm rules of the service level. For example, service_percent_rule \u0026amp;\u0026amp; service_resp_time_percentile_rule. You shouldn\u0026rsquo;t compose alarm rules of different entity levels. such as one alarm rule of the service metrics with another rule of the endpoint metrics.\nComposite rule is constituted by the following keys\n Rule name. Unique name, show in alarm message. Must end with _rule. Expression. Specify how to compose rules, support \u0026amp;\u0026amp;, ||, (). Message. Specify the notification message when rule triggered.  rules: # Rule unique name, must be ended with `_rule`. endpoint_percent_rule: # Metrics value need to be long, double or int metrics-name: endpoint_percent threshold: 75 op: \u0026lt; # The length of time to evaluate the metrics period: 10 # How many times after the metrics match the condition, will trigger alarm count: 3 # How many times of checks, the alarm keeps silence after alarm triggered, default as same as period. silence-period: 10 # Specify if the rule can send notification or just as an condition of composite rule only-as-condition: false service_percent_rule: metrics-name: service_percent # [Optional] Default, match all services in this metrics include-names: - service_a - service_b exclude-names: - service_c # Single value metrics threshold. threshold: 85 op: \u0026lt; period: 10 count: 4 only-as-condition: false service_resp_time_percentile_rule: # Metrics value need to be long, double or int metrics-name: service_percentile op: \u0026#34;\u0026gt;\u0026#34; # Multiple value metrics threshold. Thresholds for P50, P75, P90, P95, P99. threshold: 1000,1000,1000,1000,1000 period: 10 count: 3 silence-period: 5 message: Percentile response time of service {name} alarm in 3 minutes of last 10 minutes, due to more than one condition of p50 \u0026gt; 1000, p75 \u0026gt; 1000, p90 \u0026gt; 1000, p95 \u0026gt; 1000, p99 \u0026gt; 1000 only-as-condition: false meter_service_status_code_rule: metrics-name: meter_status_code exclude-labels: - \u0026#34;200\u0026#34; op: \u0026#34;\u0026gt;\u0026#34; threshold: 10 period: 10 count: 3 silence-period: 5 message: The request number of entity {name} non-200 status is more than expected. only-as-condition: false composite-rules: comp_rule: # Must satisfied percent rule and resp time rule  expression: service_percent_rule \u0026amp;\u0026amp; service_resp_time_percentile_rule message: Service {name} successful rate is less than 80% and P50 of response time is over 1000ms Default alarm rules We provided a default alarm-setting.yml in our distribution only for convenience, which including following rules\n Service average response time over 1s in last 3 minutes. Service success rate lower than 80% in last 2 minutes. Percentile of service response time is over 1s in last 3 minutes Service Instance average response time over 1s in last 2 minutes, and the instance name matches the regex. Endpoint average response time over 1s in last 2 minutes. Database access average response time over 1s in last 2 minutes. Endpoint relation average response time over 1s in last 2 minutes.  List of all potential metrics name The metrics names are defined in official OAL scripts, right now metrics from Service, Service Instance, Endpoint, Service Relation, Service Instance Relation, Endpoint Relation scopes could be used in Alarm, and the Database access same with Service scope.\nSubmit issue or pull request if you want to support any other scope in alarm.\nWebhook Webhook requires the peer is a web container. The alarm message will send through HTTP post by application/json content type. The JSON format is based on List\u0026lt;org.apache.skywalking.oap.server.core.alarm.AlarmMessage\u0026gt; with following key information.\n scopeId, scope. All scopes are defined in org.apache.skywalking.oap.server.core.source.DefaultScopeDefine. name. Target scope entity name. Please follow Entity name define. id0. The ID of the scope entity matched the name. When using relation scope, it is the source entity ID. id1. When using relation scope, it will be the dest entity ID. Otherwise, it is empty. ruleName. The rule name you configured in alarm-settings.yml. alarmMessage. Alarm text message. startTime. Alarm time measured in milliseconds, between the current time and midnight, January 1, 1970 UTC.  Example as following\n[{ \u0026#34;scopeId\u0026#34;: 1, \u0026#34;scope\u0026#34;: \u0026#34;SERVICE\u0026#34;, \u0026#34;name\u0026#34;: \u0026#34;serviceA\u0026#34;, \u0026#34;id0\u0026#34;: \u0026#34;12\u0026#34;, \u0026#34;id1\u0026#34;: \u0026#34;\u0026#34;, \u0026#34;ruleName\u0026#34;: \u0026#34;service_resp_time_rule\u0026#34;, \u0026#34;alarmMessage\u0026#34;: \u0026#34;alarmMessage xxxx\u0026#34;, \u0026#34;startTime\u0026#34;: 1560524171000 }, { \u0026#34;scopeId\u0026#34;: 1, \u0026#34;scope\u0026#34;: \u0026#34;SERVICE\u0026#34;, \u0026#34;name\u0026#34;: \u0026#34;serviceB\u0026#34;, \u0026#34;id0\u0026#34;: \u0026#34;23\u0026#34;, \u0026#34;id1\u0026#34;: \u0026#34;\u0026#34;, \u0026#34;ruleName\u0026#34;: \u0026#34;service_resp_time_rule\u0026#34;, \u0026#34;alarmMessage\u0026#34;: \u0026#34;alarmMessage yyy\u0026#34;, \u0026#34;startTime\u0026#34;: 1560524171000 }] gRPCHook The alarm message will send through remote gRPC method by Protobuf content type. The message format with following key information which are defined in oap-server/server-alarm-plugin/src/main/proto/alarm-hook.proto.\nPart of protocol looks as following:\nmessage AlarmMessage { int64 scopeId = 1; string scope = 2; string name = 3; string id0 = 4; string id1 = 5; string ruleName = 6; string alarmMessage = 7; int64 startTime = 8;}Slack Chat Hook To do this you need to follow the Getting Started with Incoming Webhooks guide and create new Webhooks.\nThe alarm message will send through HTTP post by application/json content type if you configured Slack Incoming Webhooks as following:\nslackHooks: textTemplate: |-{ \u0026#34;type\u0026#34;: \u0026#34;section\u0026#34;, \u0026#34;text\u0026#34;: { \u0026#34;type\u0026#34;: \u0026#34;mrkdwn\u0026#34;, \u0026#34;text\u0026#34;: \u0026#34;:alarm_clock: *Apache Skywalking Alarm* \\n **%s**.\u0026#34; } } webhooks: - https://hooks.slack.com/services/x/y/z WeChat Hook Note, only WeCom(WeChat Company Edition) supports webhook. To use the WeChat webhook you need to follow the Wechat Webhooks guide. The alarm message would send through HTTP post by application/json content type after you set up Wechat Webhooks as following:\nwechatHooks: textTemplate: |-{ \u0026#34;msgtype\u0026#34;: \u0026#34;text\u0026#34;, \u0026#34;text\u0026#34;: { \u0026#34;content\u0026#34;: \u0026#34;Apache SkyWalking Alarm: \\n %s.\u0026#34; } } webhooks: - https://qyapi.weixin.qq.com/cgi-bin/webhook/send?key=dummy_key Dingtalk Hook To do this you need to follow the Dingtalk Webhooks guide and create new Webhooks. For security issue, you can config optional secret for individual webhook url. The alarm message will send through HTTP post by application/json content type if you configured Dingtalk Webhooks as following:\ndingtalkHooks: textTemplate: |-{ \u0026#34;msgtype\u0026#34;: \u0026#34;text\u0026#34;, \u0026#34;text\u0026#34;: { \u0026#34;content\u0026#34;: \u0026#34;Apache SkyWalking Alarm: \\n %s.\u0026#34; } } webhooks: - url: https://oapi.dingtalk.com/robot/send?access_token=dummy_token secret: dummysecret Feishu Hook To do this you need to follow the Feishu Webhooks guide and create new Webhooks. For security issue, you can config optional secret for individual webhook url. if you want to at someone, you can config ats which is the feishu\u0026rsquo;s user_id and separated by \u0026ldquo;,\u0026rdquo; . The alarm message will send through HTTP post by application/json content type if you configured Feishu Webhooks as following:\nfeishuHooks: textTemplate: |-{ \u0026#34;msg_type\u0026#34;: \u0026#34;text\u0026#34;, \u0026#34;content\u0026#34;: { \u0026#34;text\u0026#34;: \u0026#34;Apache SkyWalking Alarm: \\n %s.\u0026#34; }, \u0026#34;ats\u0026#34;:\u0026#34;feishu_user_id_1,feishu_user_id_2\u0026#34; } webhooks: - url: https://open.feishu.cn/open-apis/bot/v2/hook/dummy_token secret: dummysecret Update the settings dynamically Since 6.5.0, the alarm settings can be updated dynamically at runtime by Dynamic Configuration, which will override the settings in alarm-settings.yml.\nIn order to determine that whether an alarm rule is triggered or not, SkyWalking needs to cache the metrics of a time window for each alarm rule, if any attribute (metrics-name, op, threshold, period, count, etc.) of a rule is changed, the sliding window will be destroyed and re-created, causing the alarm of this specific rule to restart again.\n","excerpt":"Alarm Alarm core is driven by a collection of rules, which are defined in config/alarm-settings.yml. …","ref":"/docs/main/v8.5.0/en/setup/backend/backend-alarm/","title":"Alarm"},{"body":"Apache SkyWalking committer SkyWalking Project Management Committee(PMC) takes the responsibilities to assess the contributions of candidates.\nIn the SkyWalking, like many Apache projects, we treat contributions including, but not limited to, code contributions. Such as writing blog, guiding new users, give public speak, prompting project in various ways, are all treated as significant contributions.\nCommitter New committer nomination In the SkyWalking, new committer nomination could only be started by existing PMC members officially. The new contributor could contact any existing PMC member if he/she feels he/she is qualified. Talk with the PMC member, if some members agree, they could start the process.\nThe following steps are recommended, and could only be started by existing PMC member.\n Send the [DISCUSS] Promote xxx as new committer mail to private@skywalking.a.o. List the important contributions of the candidates, in order to help the PMC members supporting your proposal. Keep discussion open in more than 3 days, but not more than 1 week, unless there is any explicit objection or concern. Send the [VOTE] Promote xxx as new committer mail to private@skywalking.a.o, when the PMC seems to agree the proposal. Keep vote more than 3 days, but not more than 1 week. Consider the result as Consensus Approval if there 3 +1 votes and +1 votes \u0026gt; -1 votes Send the [RESULT][VOTE] Promote xxx as new committer mail to private@skywalking.a.o, and list the vote detail including the voters.  Invite new committer The PMC member, who start the promotion, takes the responsibilities to send the invitation to new committer and guide him/her to set up the ASF env.\nYou should send the mail like the following template to new committer\nTo: JoeBloggs@foo.net Cc: private@skywalking.apache.org Subject: Invitation to become SkyWalking committer: Joe Bloggs Hello [invitee name], The SkyWalking Project Management Committee] (PMC) hereby offers you committer privileges to the project . These privileges are offered on the understanding that you'll use them reasonably and with common sense. We like to work on trust rather than unnecessary constraints. Being a committer enables you to more easily make changes without needing to go through the patch submission process. Being a committer does not require you to participate any more than you already do. It does tend to make one even more committed. You will probably find that you spend more time here. Of course, you can decline and instead remain as a contributor, participating as you do now. A. This personal invitation is a chance for you to accept or decline in private. Either way, please let us know in reply to the [private@skywalking.apache.org] address only. B. If you accept, the next step is to register an iCLA: 1. Details of the iCLA and the forms are found through this link: http://www.apache.org/licenses/#clas 2. Instructions for its completion and return to the Secretary of the ASF are found at http://www.apache.org/licenses/#submitting 3. When you transmit the completed iCLA, request to notify the Apache SkyWalking and choose a unique Apache id. Look to see if your preferred id is already taken at http://people.apache.org/committer-index.html This will allow the Secretary to notify the PMC when your iCLA has been recorded. When recording of your iCLA is noticed, you will receive a follow-up message with the next steps for establishing you as a committer. Invitation acceptance process And the new committer should reply the mail to private@skywalking.apache.org(Choose reply all), and express the will to accept the invitation explicitly. Then this invitation will be treated as accepted by project PMC. Of course, the new committer could just say NO, and reject the invitation.\nIf they accepted, then they need to do the following things.\n Make sure they have subscribed the dev@skywalking.apache.org. Usually they already have. Choose a Apache ID that is not in the apache committers list page. Download the ICLA (If they are going to contribute to the project as day job, CCLA is expected). After filling the icla.pdf (or ccla.pdf) with information correctly, print, sign it manually (by hand), scan it as an pdf, and send it in mail as an attachment to the secretary@apache.org. (If they prefer to sign electronically, please follow the steps of this page) Then the PMC will wait the Apache secretary confirmed the ICLA (or CCLA) filed. The new committer and PMC will receive the mail like following  Dear XXX, This message acknowledges receipt of your ICLA, which has been filed in the Apache Software Foundation records. Your account has been requested for you and you should receive email with next steps within the next few days (can take up to a week). Please refer to https://www.apache.org/foundation/how-it-works.html#developers for more information about roles at Apache. If in some case, the account has not be requested(rarely to see), the PMC member should contact the project V.P.. The V.P. could request through the Apache Account Submission Helper Form.\nAfter several days, the new committer will receive the account created mail, as this title, Welcome to the Apache Software Foundation (ASF)!. At this point, congratulate! You have the official Apache ID.\nThe PMC member should add the new committer to official committer list through roster.\nSet up the Apache ID and dev env  Go to Apache Account Utility Platform, initial your password, set up your personal mailbox(Forwarding email address) and GitHub account(Your GitHub Username). An organisational invite will be sent to you via email shortly thereafter (within 2 hours). If you want to use xxx@apache.org to send mail, please refer to here. Gmail is recommended, because in other mailbox service settings, this forwarding mode is not easy to find. Following the authorized GitHub 2FA wiki to enable two-factors authorization (2FA) on github. When you set 2FA to \u0026ldquo;off\u0026rdquo;, it will be delisted by the corresponding Apache committer write permission group until you set it up again. (NOTE: Treat your recovery codes with the same level of attention as you would your password !) Use GitBox Account Linking Utility to obtain write permission of the SkyWalking project. Follow this doc to update the website.  If you want others could see you are in the Apache GitHub org, you need to go to Apache GitHub org people page, search for yourself, and choose Organization visibility to Public.\nCommitter rights, duties and responsibilities SkyWalking project doesn\u0026rsquo;t require the continue contributions after you become a committer, but we hope and truly want you could.\nBeing a committer, you could\n Review and merge the pull request to the master branch in the Apache repo. A pull request often contains multiple commits. Those commits must be squashed and merged into a single commit with explanatory comments. For new committer, we hope you could request some senior committer to recheck the pull request. Create and push codes to new branch in the Apache repo. Follow the Release process to process new release. Of course, you need to ask committer team to confirm it is the right time to release.  The PMC hope the new committer to take part in the release and release vote, even still be consider +1 no binding. But be familiar with the release is one of the key to be promoted as a PMC member.\nProject Management Committee Project Management Committee(PMC) member has no special rights in code contributions. They just cover and make sure the project following the Apache requirement, including\n Release binding vote and license check New committer and PMC member recognition Identify branding issue and do branding protection. Response the ASF board question, take necessary actions.  V.P. and chair of the PMC is the secretary, take responsibility of initializing the board report.\nIn the normal case, the new PMC member should be nominated from committer team. But becoming a PMC member directly is not forbidden, if the PMC could agree and be confidence that the candidate is ready, such as he/she has been a PMC member of another project, Apache member or Apache officer.\nThe process of new PMC vote should also follow the same [DISCUSS], [VOTE] and [RESULT][VOTE] in private mail list as new committer vote. One more step before sending the invitation, the PMC need to send NOTICE mail to Apache board.\nTo: board@apache.org Cc: private@skywalking.apache.org Subject: [NOTICE] Jane Doe for SkyWalking PMC SkyWalking proposes to invite Jane Doe (janedoe) to join the PMC. (include if a vote was held) The vote result is available here: https://lists.apache.org/... After 72 hours, if the board doesn\u0026rsquo;t object(usually it wouldn\u0026rsquo;t be), send the invitation.\nAfter the committer accepted the invitation, The PMC member should add the new committer to official PMC list through roster.\n","excerpt":"Apache SkyWalking committer SkyWalking Project Management Committee(PMC) takes the responsibilities …","ref":"/docs/main/v8.5.0/en/guides/asf/committer/","title":"Apache SkyWalking committer"},{"body":"Apdex threshold Apdex is a measure of response time based against a set threshold. It measures the ratio of satisfactory response times to unsatisfactory response times. The response time is measured from an asset request to completed delivery back to the requestor.\nA user defines a response time threshold T. All responses handled in T or less time satisfy the user.\nFor example, if T is 1.2 seconds and a response completes in 0.5 seconds, then the user is satisfied. All responses greater than 1.2 seconds dissatisfy the user. Responses greater than 4.8 seconds frustrate the user.\nThe apdex threshold T can be configured in service-apdex-threshold.yml file or via Dynamic Configuration. The default item will be apply to a service isn\u0026rsquo;t defined in this configuration as the default threshold.\nConfiguration Format The configuration content includes the service' names and their threshold:\n# default threshold is 500ms default: 500 # example: # the threshold of service \u0026#34;tomcat\u0026#34; is 1s # tomcat: 1000 # the threshold of service \u0026#34;springboot1\u0026#34; is 50ms # springboot1: 50 ","excerpt":"Apdex threshold Apdex is a measure of response time based against a set threshold. It measures the …","ref":"/docs/main/v8.5.0/en/setup/backend/apdex-threshold/","title":"Apdex threshold"},{"body":"Backend setup SkyWalking backend distribution package includes the following parts:\n  bin/cmd scripts, in /bin folder. Includes startup linux shell and Windows cmd scripts for Backend server and UI startup.\n  Backend config, in /config folder. Includes settings files of the backend, which are:\n application.yml log4j.xml alarm-settings.yml    Libraries of backend, in /oap-libs folder. All the dependencies of the backend are in it.\n  Webapp env, in webapp folder. UI frontend jar file is here, with its webapp.yml setting file.\n  Requirements and default settings Requirement: JDK8 to JDK12 are tested, other versions are not tested and may or may not work.\nBefore you start, you should know that the quickstart aims to get you a basic configuration mostly for previews/demo, performance and long-term running are not our goals.\nFor production/QA/tests environments, you should head to Backend and UI deployment documents.\nYou can use bin/startup.sh (or cmd) to startup the backend and UI with their default settings, which are:\n Backend storage uses H2 by default (for an easier start) Backend listens 0.0.0.0/11800 for gRPC APIs and 0.0.0.0/12800 for http rest APIs.  In Java, DotNetCore, Node.js, Istio agents/probe, you should set the gRPC service address to ip/host:11800, with ip/host where your backend is.\n UI listens on 8080 port and request 127.0.0.1/12800 to do GraphQL query.  Interaction Before deploying Skywalking in your distributed environment, you should know how agents/probes, backend, UI communicates with each other:\n All native agents and probes, either language based or mesh probe, are using gRPC service (core/default/gRPC* in application.yml) to report data to the backend. Also, jetty service supported in JSON format. UI uses GraphQL (HTTP) query to access the backend also in Jetty service (core/default/rest* in application.yml).  Startup script The default startup scripts are /bin/oapService.sh(.bat). Read start up mode document to know other options of starting backend.\napplication.yml SkyWalking backend startup behaviours are driven by config/application.yml. Understood the setting file will help you to read this document. The core concept behind this setting file is, SkyWalking collector is based on pure modularization design. End user can switch or assemble the collector features by their own requirements.\nSo, in application.yml, there are three levels.\n Level 1, module name. Meaning this module is active in running mode. Level 2, provider option list and provider selector. Available providers are listed here with a selector to indicate which one will actually take effect, if there is only one provider listed, the selector is optional and can be omitted. Level 3. settings of the provider.  Example:\nstorage: selector: mysql # the mysql storage will actually be activated, while the h2 storage takes no effect h2: driver: ${SW_STORAGE_H2_DRIVER:org.h2.jdbcx.JdbcDataSource} url: ${SW_STORAGE_H2_URL:jdbc:h2:mem:skywalking-oap-db} user: ${SW_STORAGE_H2_USER:sa} metadataQueryMaxSize: ${SW_STORAGE_H2_QUERY_MAX_SIZE:5000} mysql: properties: jdbcUrl: ${SW_JDBC_URL:\u0026#34;jdbc:mysql://localhost:3306/swtest\u0026#34;} dataSource.user: ${SW_DATA_SOURCE_USER:root} dataSource.password: ${SW_DATA_SOURCE_PASSWORD:root@1234} dataSource.cachePrepStmts: ${SW_DATA_SOURCE_CACHE_PREP_STMTS:true} dataSource.prepStmtCacheSize: ${SW_DATA_SOURCE_PREP_STMT_CACHE_SQL_SIZE:250} dataSource.prepStmtCacheSqlLimit: ${SW_DATA_SOURCE_PREP_STMT_CACHE_SQL_LIMIT:2048} dataSource.useServerPrepStmts: ${SW_DATA_SOURCE_USE_SERVER_PREP_STMTS:true} metadataQueryMaxSize: ${SW_STORAGE_MYSQL_QUERY_MAX_SIZE:5000} # other configurations  storage is the module. selector selects one out of the all providers listed below, the unselected ones take no effect as if they were deleted. default is the default implementor of core module. driver, url, \u0026hellip; metadataQueryMaxSize are all setting items of the implementor.  At the same time, modules includes required and optional, the required modules provide the skeleton of backend, even modularization supported pluggable, removing those modules are meaningless, for optional modules, some of them have a provider implementation called none, meaning it only provides a shell with no actual logic, typically such as telemetry. Setting - to the selector means this whole module will be excluded at runtime. We highly recommend you don\u0026rsquo;t try to change APIs of those modules, unless you understand SkyWalking project and its codes very well.\nList the required modules here\n Core. Do basic and major skeleton of all data analysis and stream dispatch. Cluster. Manage multiple backend instances in a cluster, which could provide high throughputs process capabilities. Storage. Make the analysis result persistence. Query. Provide query interfaces to UI.  For Cluster and Storage have provided multiple implementors(providers), see Cluster management and Choose storage documents in the link list.\nAlso, several receiver modules are provided. Receiver is the module in charge of accepting incoming data requests to backend. Most(all) provide service by some network(RPC) protocol, such as gRPC, HTTPRestful.\nThe receivers have many different module names, you could read Set receivers document in the link list.\nConfiguration Vocabulary All available configurations in application.yml could be found in Configuration Vocabulary.\nAdvanced feature document link list After understand the setting file structure, you could choose your interesting feature document. We recommend you to read the feature documents in our following order.\n Overriding settings in application.yml is supported IP and port setting. Introduce how IP and port set and be used. Backend init mode startup. How to init the environment and exit graciously. Read this before you try to initial a new cluster. Cluster management. Guide you to set backend server in cluster mode. Deploy in kubernetes. Guide you to build and use SkyWalking image, and deploy in k8s. Choose storage. As we know, in default quick start, backend is running with H2 DB. But clearly, it doesn\u0026rsquo;t fit the product env. In here, you could find what other choices do you have. Choose the ones you like, we are also welcome anyone to contribute new storage implementor. Set receivers. You could choose receivers by your requirements, most receivers are harmless, at least our default receivers are. You would set and active all receivers provided. Open fetchers. You could open different fetchers to read metrics from the target applications. These ones work like receivers, but in pulling mode, typically like Prometheus. Token authentication. You could add token authentication mechanisms to avoid OAP receiving untrusted data. Do trace sampling at backend. This sample keep the metrics accurate, only don\u0026rsquo;t save some of traces in storage based on rate. Follow slow DB statement threshold config document to understand that, how to detect the Slow database statements(including SQL statements) in your system. Official OAL scripts. As you have known from our OAL introduction, most of backend analysis capabilities based on the scripts. Here is the description of official scripts, which helps you to understand which metrics data are in process, also could be used in alarm. Alarm. Alarm provides a time-series based check mechanism. You could set alarm rules targeting the analysis oal metrics objects. Advanced deployment options. If you want to deploy backend in very large scale and support high payload, you may need this. Metrics exporter. Use metrics data exporter to forward metrics data to 3rd party system. Time To Live (TTL). Metrics and trace are time series data, TTL settings affect the expired time of them. Dynamic Configuration. Make configuration of OAP changed dynamic, from remote service or 3rd party configuration management system. Uninstrumented Gateways. Configure gateways/proxies that are not supported by SkyWalking agent plugins, to reflect the delegation in topology graph. Apdex threshold. Configure the thresholds for different services if Apdex calculation is activated in the OAL. Service Grouping. An automatic grouping mechanism for all services based on name. Group Parameterized Endpoints. Configure the grouping rules for parameterized endpoints, to improve the meaning of the metrics. OpenTelemetry Metrics Analysis. Activate built-in configurations to convert the metrics forwarded from OpenTelemetry collector. And learn how to write your own conversion rules. Meter Analysis. Set up the backend analysis rules, when use SkyWalking Meter System Toolkit or meter plugins. Spring Sleuth Metrics Analysis. Configure the agent and backend to receiver metrics from micrometer. Log Analyzer  Telemetry for backend OAP backend cluster itself underlying is a distributed streaming process system. For helping the Ops team, we provide the telemetry for OAP backend itself. Follow document to use it.\nAt the same time, we provide Health Check to get a score for the health status.\n 0 means healthy, more than 0 means unhealthy and less than 0 means oap doesn\u0026rsquo;t startup.\n FAQs When and why do we need to set Timezone? SkyWalking provides downsampling time series metrics features. Query and storage at each time dimension(minute, hour, day, month metrics indexes) related to timezone when doing time format.\nFor example, metrics time will be formatted like YYYYMMDDHHmm in minute dimension metrics, which format process is timezone related.\nIn default, SkyWalking OAP backend choose the OS default timezone. If you want to override it, please follow Java and OS documents to do so.\nHow to query the storage directly from 3rd party tool? SkyWalking provides browser UI, CLI and GraphQL ways to support extensions. But some users may have the idea to query data directly from the storage. Such as in ElasticSearch case, Kibana is a great tool to do this.\nIn default, due to reduce memory, network and storage space usages, SkyWalking saves based64-encoded id(s) only in the metrics entities. But these tools usually don\u0026rsquo;t support nested query, or don\u0026rsquo;t work conveniently. In this special case, SkyWalking provide a config to add all necessary name column(s) into the final metrics entities with ID as a trade-off.\nTake a look at core/default/activeExtraModelColumns config in the application.yaml, and set it as true to open this feature.\nThis feature wouldn\u0026rsquo;t provide any new feature to the native SkyWalking scenarios, just for the 3rd party integration.\n","excerpt":"Backend setup SkyWalking backend distribution package includes the following parts:\n  bin/cmd …","ref":"/docs/main/v8.5.0/en/setup/backend/backend-setup/","title":"Backend setup"},{"body":"Backend storage SkyWalking storage is pluggable, we have provided the following storage solutions, you could easily use one of them by specifying it as the selector in the application.yml：\nstorage: selector: ${SW_STORAGE:elasticsearch7} Native supported storage\n H2 ElasticSearch 6, 7 MySQL TiDB InfluxDB PostgreSQL  H2 Active H2 as storage, set storage provider to H2 In-Memory Databases. Default in distribution package. Please read Database URL Overview in H2 official document, you could set the target to H2 in Embedded, Server and Mixed modes.\nSetting fragment example\nstorage: selector: ${SW_STORAGE:h2} h2: driver: org.h2.jdbcx.JdbcDataSource url: jdbc:h2:mem:skywalking-oap-db user: sa ElasticSearch  In order to activate ElasticSearch 6 as storage, set storage provider to elasticsearch In order to activate ElasticSearch 7 as storage, set storage provider to elasticsearch7  Required ElasticSearch 6.3.2 or higher. HTTP RestHighLevelClient is used to connect server.\n For ElasticSearch 6.3.2 ~ 7.0.0 (excluded), please download the apache-skywalking-bin.tar.gz or apache-skywalking-bin.zip, For ElasticSearch 7.0.0 ~ 8.0.0 (excluded), please download the apache-skywalking-bin-es7.tar.gz or apache-skywalking-bin-es7.zip.  For now, ElasticSearch 6 and ElasticSearch 7 share the same configurations, as follows:\nstorage: selector: ${SW_STORAGE:elasticsearch} elasticsearch: nameSpace: ${SW_NAMESPACE:\u0026#34;\u0026#34;} clusterNodes: ${SW_STORAGE_ES_CLUSTER_NODES:localhost:9200} protocol: ${SW_STORAGE_ES_HTTP_PROTOCOL:\u0026#34;http\u0026#34;} trustStorePath: ${SW_STORAGE_ES_SSL_JKS_PATH:\u0026#34;\u0026#34;} trustStorePass: ${SW_STORAGE_ES_SSL_JKS_PASS:\u0026#34;\u0026#34;} user: ${SW_ES_USER:\u0026#34;\u0026#34;} password: ${SW_ES_PASSWORD:\u0026#34;\u0026#34;} secretsManagementFile: ${SW_ES_SECRETS_MANAGEMENT_FILE:\u0026#34;\u0026#34;} # Secrets management file in the properties format includes the username, password, which are managed by 3rd party tool. dayStep: ${SW_STORAGE_DAY_STEP:1} # Represent the number of days in the one minute/hour/day index. indexShardsNumber: ${SW_STORAGE_ES_INDEX_SHARDS_NUMBER:1} # Shard number of new indexes indexReplicasNumber: ${SW_STORAGE_ES_INDEX_REPLICAS_NUMBER:1} # Replicas number of new indexes # Super data set has been defined in the codes, such as trace segments.The following 3 config would be improve es performance when storage super size data in es. superDatasetDayStep: ${SW_SUPERDATASET_STORAGE_DAY_STEP:-1} # Represent the number of days in the super size dataset record index, the default value is the same as dayStep when the value is less than 0 superDatasetIndexShardsFactor: ${SW_STORAGE_ES_SUPER_DATASET_INDEX_SHARDS_FACTOR:5} # This factor provides more shards for the super data set, shards number = indexShardsNumber * superDatasetIndexShardsFactor. Also, this factor effects Zipkin and Jaeger traces. superDatasetIndexReplicasNumber: ${SW_STORAGE_ES_SUPER_DATASET_INDEX_REPLICAS_NUMBER:0} # Represent the replicas number in the super size dataset record index, the default value is 0. bulkActions: ${SW_STORAGE_ES_BULK_ACTIONS:1000} # Execute the async bulk record data every ${SW_STORAGE_ES_BULK_ACTIONS} requests flushInterval: ${SW_STORAGE_ES_FLUSH_INTERVAL:10} # flush the bulk every 10 seconds whatever the number of requests concurrentRequests: ${SW_STORAGE_ES_CONCURRENT_REQUESTS:2} # the number of concurrent requests resultWindowMaxSize: ${SW_STORAGE_ES_QUERY_MAX_WINDOW_SIZE:10000} metadataQueryMaxSize: ${SW_STORAGE_ES_QUERY_MAX_SIZE:5000} segmentQueryMaxSize: ${SW_STORAGE_ES_QUERY_SEGMENT_SIZE:200} profileTaskQueryMaxSize: ${SW_STORAGE_ES_QUERY_PROFILE_TASK_SIZE:200} oapAnalyzer: ${SW_STORAGE_ES_OAP_ANALYZER:\u0026#34;{\\\u0026#34;analyzer\\\u0026#34;:{\\\u0026#34;oap_analyzer\\\u0026#34;:{\\\u0026#34;type\\\u0026#34;:\\\u0026#34;stop\\\u0026#34;}}}\u0026#34;} # the oap analyzer. oapLogAnalyzer: ${SW_STORAGE_ES_OAP_LOG_ANALYZER:\u0026#34;{\\\u0026#34;analyzer\\\u0026#34;:{\\\u0026#34;oap_log_analyzer\\\u0026#34;:{\\\u0026#34;type\\\u0026#34;:\\\u0026#34;standard\\\u0026#34;}}}\u0026#34;} # the oap log analyzer. It could be customized by the ES analyzer configuration to support more language log formats, such as Chinese log, Japanese log and etc. advanced: ${SW_STORAGE_ES_ADVANCED:\u0026#34;\u0026#34;} ElasticSearch 6 With Https SSL Encrypting communications. example:\nstorage: selector: ${SW_STORAGE:elasticsearch} elasticsearch: # nameSpace: ${SW_NAMESPACE:\u0026#34;\u0026#34;} user: ${SW_ES_USER:\u0026#34;\u0026#34;} # User needs to be set when Http Basic authentication is enabled password: ${SW_ES_PASSWORD:\u0026#34;\u0026#34;} # Password to be set when Http Basic authentication is enabled clusterNodes: ${SW_STORAGE_ES_CLUSTER_NODES:localhost:443} trustStorePath: ${SW_SW_STORAGE_ES_SSL_JKS_PATH:\u0026#34;../es_keystore.jks\u0026#34;} trustStorePass: ${SW_SW_STORAGE_ES_SSL_JKS_PASS:\u0026#34;\u0026#34;} protocol: ${SW_STORAGE_ES_HTTP_PROTOCOL:\u0026#34;https\u0026#34;} ...  File at trustStorePath is being monitored, once it is changed, the ElasticSearch client will do reconnecting. trustStorePass could be changed on the runtime through Secrets Management File Of ElasticSearch Authentication.  Daily Index Step Daily index step(storage/elasticsearch/dayStep, default 1) represents the index creation period. In this period, several days(dayStep value)' metrics are saved.\nMostly, users don\u0026rsquo;t need to change the value manually. As SkyWalking is designed to observe large scale distributed system. But in some specific cases, users want to set a long TTL value, such as more than 60 days, but their ElasticSearch cluster isn\u0026rsquo;t powerful due to the low traffic in the production environment. This value could be increased to 5(or more), if users could make sure single one index could support these days(5 in this case) metrics and traces.\nSuch as, if dayStep == 11,\n data in [2000-01-01, 2000-01-11] will be merged into the index-20000101. data in [2000-01-12, 2000-01-22] will be merged into the index-20000112.  storage/elasticsearch/superDatasetDayStep override the storage/elasticsearch/dayStep if the value is positive. This would affect the record related entities, such as the trace segment. In some cases, the size of metrics is much less than the record(trace), this would help the shards balance in the ElasticSearch cluster.\nNOTICE, TTL deletion would be affected by these. You should set an extra more dayStep in your TTL. Such as you want to TTL == 30 days and dayStep == 10, you actually need to set TTL = 40;\nSecrets Management File Of ElasticSearch Authentication The value of secretsManagementFile should point to the secrets management file absolute path. The file includes username, password and JKS password of ElasticSearch server in the properties format.\nuser=xxx password=yyy trustStorePass=zzz The major difference between using user, password, trustStorePass configs in the application.yaml file is, the Secrets Management File is being watched by the OAP server. Once it is changed manually or through 3rd party tool, such as Vault, the storage provider will use the new username, password and JKS password to establish the connection and close the old one. If the information exist in the file, the user/password will be overrided.\nAdvanced Configurations For Elasticsearch Index You can add advanced configurations in JSON format to set ElasticSearch index settings by following ElasticSearch doc\nFor example, set translog settings:\nstorage: elasticsearch: # ...... advanced: ${SW_STORAGE_ES_ADVANCED:\u0026#34;{\\\u0026#34;index.translog.durability\\\u0026#34;:\\\u0026#34;request\\\u0026#34;,\\\u0026#34;index.translog.sync_interval\\\u0026#34;:\\\u0026#34;5s\\\u0026#34;}\u0026#34;} Recommended ElasticSearch server-side configurations You could add following config to elasticsearch.yml, set the value based on your env.\n# In tracing scenario, consider to set more than this at least. thread_pool.index.queue_size: 1000 # Only suitable for ElasticSearch 6 thread_pool.write.queue_size: 1000 # Suitable for ElasticSearch 6 and 7 # When you face query error at trace page, remember to check this. index.max_result_window: 1000000 We strongly advice you to read more about these configurations from ElasticSearch official document. This effects the performance of ElasticSearch very much.\nElasticSearch 7 with Zipkin trace extension This implementation shares most of elasticsearch7, just extends to support zipkin span storage. It has all same configs.\nstorage: selector: ${SW_STORAGE:zipkin-elasticsearch7} zipkin-elasticsearch7: nameSpace: ${SW_NAMESPACE:\u0026#34;\u0026#34;} clusterNodes: ${SW_STORAGE_ES_CLUSTER_NODES:localhost:9200} protocol: ${SW_STORAGE_ES_HTTP_PROTOCOL:\u0026#34;http\u0026#34;} user: ${SW_ES_USER:\u0026#34;\u0026#34;} password: ${SW_ES_PASSWORD:\u0026#34;\u0026#34;} indexShardsNumber: ${SW_STORAGE_ES_INDEX_SHARDS_NUMBER:2} indexReplicasNumber: ${SW_STORAGE_ES_INDEX_REPLICAS_NUMBER:0} # Batch process setting, refer to https://www.elastic.co/guide/en/elasticsearch/client/java-api/5.5/java-docs-bulk-processor.html bulkActions: ${SW_STORAGE_ES_BULK_ACTIONS:2000} # Execute the bulk every 2000 requests bulkSize: ${SW_STORAGE_ES_BULK_SIZE:20} # flush the bulk every 20mb flushInterval: ${SW_STORAGE_ES_FLUSH_INTERVAL:10} # flush the bulk every 10 seconds whatever the number of requests concurrentRequests: ${SW_STORAGE_ES_CONCURRENT_REQUESTS:2} # the number of concurrent requests About Namespace When namespace is set, names of all indexes in ElasticSearch will use it as prefix.\nMySQL Active MySQL as storage, set storage provider to mysql.\nNOTICE: MySQL driver is NOT allowed in Apache official distribution and source codes. Please download MySQL driver by yourself. Copy the connection driver jar to oap-libs.\nstorage: selector: ${SW_STORAGE:mysql} mysql: properties: jdbcUrl: ${SW_JDBC_URL:\u0026#34;jdbc:mysql://localhost:3306/swtest\u0026#34;} dataSource.user: ${SW_DATA_SOURCE_USER:root} dataSource.password: ${SW_DATA_SOURCE_PASSWORD:root@1234} dataSource.cachePrepStmts: ${SW_DATA_SOURCE_CACHE_PREP_STMTS:true} dataSource.prepStmtCacheSize: ${SW_DATA_SOURCE_PREP_STMT_CACHE_SQL_SIZE:250} dataSource.prepStmtCacheSqlLimit: ${SW_DATA_SOURCE_PREP_STMT_CACHE_SQL_LIMIT:2048} dataSource.useServerPrepStmts: ${SW_DATA_SOURCE_USE_SERVER_PREP_STMTS:true} metadataQueryMaxSize: ${SW_STORAGE_MYSQL_QUERY_MAX_SIZE:5000} All connection related settings including link url, username and password are in application.yml. Here are some of the settings, please follow HikariCP connection pool document for all the settings.\nTiDB Tested TiDB Server 4.0.8 version and Mysql Client driver 8.0.13 version currently. Active TiDB as storage, set storage provider to tidb.\nstorage: selector: ${SW_STORAGE:tidb} tidb: properties: jdbcUrl: ${SW_JDBC_URL:\u0026#34;jdbc:mysql://localhost:4000/swtest\u0026#34;} dataSource.user: ${SW_DATA_SOURCE_USER:root} dataSource.password: ${SW_DATA_SOURCE_PASSWORD:\u0026#34;\u0026#34;} dataSource.cachePrepStmts: ${SW_DATA_SOURCE_CACHE_PREP_STMTS:true} dataSource.prepStmtCacheSize: ${SW_DATA_SOURCE_PREP_STMT_CACHE_SQL_SIZE:250} dataSource.prepStmtCacheSqlLimit: ${SW_DATA_SOURCE_PREP_STMT_CACHE_SQL_LIMIT:2048} dataSource.useServerPrepStmts: ${SW_DATA_SOURCE_USE_SERVER_PREP_STMTS:true} dataSource.useAffectedRows: ${SW_DATA_SOURCE_USE_AFFECTED_ROWS:true} metadataQueryMaxSize: ${SW_STORAGE_MYSQL_QUERY_MAX_SIZE:5000} maxSizeOfArrayColumn: ${SW_STORAGE_MAX_SIZE_OF_ARRAY_COLUMN:20} numOfSearchableValuesPerTag: ${SW_STORAGE_NUM_OF_SEARCHABLE_VALUES_PER_TAG:2} All connection related settings including link url, username and password are in application.yml. These settings can refer to the configuration of MySQL above.\nInfluxDB InfluxDB storage provides a time-series database as a new storage option.\nstorage: selector: ${SW_STORAGE:influxdb} influxdb: url: ${SW_STORAGE_INFLUXDB_URL:http://localhost:8086} user: ${SW_STORAGE_INFLUXDB_USER:root} password: ${SW_STORAGE_INFLUXDB_PASSWORD:} database: ${SW_STORAGE_INFLUXDB_DATABASE:skywalking} actions: ${SW_STORAGE_INFLUXDB_ACTIONS:1000} # the number of actions to collect duration: ${SW_STORAGE_INFLUXDB_DURATION:1000} # the time to wait at most (milliseconds) fetchTaskLogMaxSize: ${SW_STORAGE_INFLUXDB_FETCH_TASK_LOG_MAX_SIZE:5000} # the max number of fetch task log in a request All connection related settings including link url, username and password are in application.yml. The Metadata storage provider settings can refer to the configuration of H2/MySQL above.\nPostgreSQL PostgreSQL jdbc driver uses version 42.2.18, it supports PostgreSQL 8.2 or newer. Active PostgreSQL as storage, set storage provider to postgresql.\nstorage: selector: ${SW_STORAGE:postgresql} postgresql: properties: jdbcUrl: ${SW_JDBC_URL:\u0026#34;jdbc:postgresql://localhost:5432/skywalking\u0026#34;} dataSource.user: ${SW_DATA_SOURCE_USER:postgres} dataSource.password: ${SW_DATA_SOURCE_PASSWORD:123456} dataSource.cachePrepStmts: ${SW_DATA_SOURCE_CACHE_PREP_STMTS:true} dataSource.prepStmtCacheSize: ${SW_DATA_SOURCE_PREP_STMT_CACHE_SQL_SIZE:250} dataSource.prepStmtCacheSqlLimit: ${SW_DATA_SOURCE_PREP_STMT_CACHE_SQL_LIMIT:2048} dataSource.useServerPrepStmts: ${SW_DATA_SOURCE_USE_SERVER_PREP_STMTS:true} metadataQueryMaxSize: ${SW_STORAGE_MYSQL_QUERY_MAX_SIZE:5000} maxSizeOfArrayColumn: ${SW_STORAGE_MAX_SIZE_OF_ARRAY_COLUMN:20} numOfSearchableValuesPerTag: ${SW_STORAGE_NUM_OF_SEARCHABLE_VALUES_PER_TAG:2} All connection related settings including link url, username and password are in application.yml. Here are some of the settings, please follow HikariCP connection pool document for all the settings.\nMore storage solution extension Follow Storage extension development guide in Project Extensions document in development guide.\n","excerpt":"Backend storage SkyWalking storage is pluggable, we have provided the following storage solutions, …","ref":"/docs/main/v8.5.0/en/setup/backend/backend-storage/","title":"Backend storage"},{"body":"Browser Monitoring Apache SkyWalking Client JS is client-side JavaScript exception and tracing library.\n Provide metrics and error collection to SkyWalking backend. Lightweight, no browser plugin, just a simple JavaScript library. Make browser as a start of whole distributed tracing.  Go to the Client JS official doc to learn more.\nNote, make sure the receiver-browser has been opened, default is ON since 8.2.0.\n","excerpt":"Browser Monitoring Apache SkyWalking Client JS is client-side JavaScript exception and tracing …","ref":"/docs/main/v8.5.0/en/setup/service-agent/browser-agent/","title":"Browser Monitoring"},{"body":"Browser Protocol Browser protocol describes the data format between skywalking-client-js and backend.\nOverview Browser protocol is defined and provided in gRPC format, also implemented in HTTP 1.1\nSend performance data and error log You can send performance data and error logs via the following services:\n BrowserPerfService#collectPerfData for performance data format. BrowserPerfService#collectErrorLogs for error log format.  For error log format, there are some notices\n BrowserErrorLog#uniqueId should be unique in the whole distributed environments.  ","excerpt":"Browser Protocol Browser protocol describes the data format between skywalking-client-js and …","ref":"/docs/main/v8.5.0/en/protocols/browser-protocol/","title":"Browser Protocol"},{"body":"CDS - Configuration Discovery Service CDS - Configuration Discovery Service provides the dynamic configuration for the agent, defined in gRPC.\nConfiguration Format The configuration content includes the service name and their configs. The\nconfigurations: //service name serviceA: // Configurations of service A // Key and Value are determined by the agent side. // Check the agent setup doc for all available configurations. key1: value1 key2: value2 ... serviceB: ... Available key(s) and value(s) in Java Agent. Java agent supports the following dynamic configurations.\n   Config Key Value Description Value Format Example Required Plugin(s)     agent.sample_n_per_3_secs The number of sampled traces per 3 seconds -1 -   agent.ignore_suffix If the operation name of the first span is included in this set, this segment should be ignored. Multiple values should be separated by , .txt,.log -   agent.trace.ignore_path The value is the path that you need to ignore, multiple paths should be separated by , more details /your/path/1/**,/your/path/2/** apm-trace-ignore-plugin   agent.span_limit_per_segment The max number of spans per segment. 300 -     Required plugin(s), the configuration affects only when the required plugins activated.  ","excerpt":"CDS - Configuration Discovery Service CDS - Configuration Discovery Service provides the dynamic …","ref":"/docs/main/v8.5.0/en/setup/service-agent/java-agent/configuration-discovery/","title":"CDS - Configuration Discovery Service"},{"body":"Choose receiver Receiver is a concept in SkyWalking backend. All modules, which are responsible for receiving telemetry or tracing data from other being monitored system, are all being called Receiver. If you are looking for the pull mode, Take a look at fetcher document.\nWe have following receivers, and default implementors are provided in our Apache distribution.\n receiver-trace. gRPC and HTTPRestful services to accept SkyWalking format traces. receiver-register. gRPC and HTTPRestful services to provide service, service instance and endpoint register. service-mesh. gRPC services accept data from inbound mesh probes. receiver-jvm. gRPC services accept JVM metrics data. envoy-metric. Envoy metrics_service and ALS(access log service) supported by this receiver. OAL script support all GAUGE type metrics. receiver-profile. gRPC services accept profile task status and snapshot reporter. receiver-otel. See details. Receiver for analyzing metrics data from OpenTelemetry receiver-meter. See details. Receiver for analyzing metrics in SkyWalking native meter format. receiver-browser. gRPC services to accept browser performance data and error log. receiver-log. Receiver for native log format. Read Log Analyzer for advanced features. configuration-discovery. gRPC services handle configurationDiscovery. receiver-event. gRPC services to handle events data. receiver-zabbix. See details. Experimental receivers.  receiver_zipkin. See details.    The sample settings of these receivers should be already in default application.yml, and also list here\nreceiver-register: selector: ${SW_RECEIVER_REGISTER:default} default: receiver-trace: selector: ${SW_RECEIVER_TRACE:default} default: receiver-jvm: selector: ${SW_RECEIVER_JVM:default} default: service-mesh: selector: ${SW_SERVICE_MESH:default} default: envoy-metric: selector: ${SW_ENVOY_METRIC:default} default: acceptMetricsService: ${SW_ENVOY_METRIC_SERVICE:true} alsHTTPAnalysis: ${SW_ENVOY_METRIC_ALS_HTTP_ANALYSIS:\u0026#34;\u0026#34;} receiver_zipkin: selector: ${SW_RECEIVER_ZIPKIN:-} default: host: ${SW_RECEIVER_ZIPKIN_HOST:0.0.0.0} port: ${SW_RECEIVER_ZIPKIN_PORT:9411} contextPath: ${SW_RECEIVER_ZIPKIN_CONTEXT_PATH:/} jettyMinThreads: ${SW_RECEIVER_ZIPKIN_JETTY_MIN_THREADS:1} jettyMaxThreads: ${SW_RECEIVER_ZIPKIN_JETTY_MAX_THREADS:200} jettyIdleTimeOut: ${SW_RECEIVER_ZIPKIN_JETTY_IDLE_TIMEOUT:30000} jettyAcceptorPriorityDelta: ${SW_RECEIVER_ZIPKIN_JETTY_DELTA:0} jettyAcceptQueueSize: ${SW_RECEIVER_ZIPKIN_QUEUE_SIZE:0} receiver-profile: selector: ${SW_RECEIVER_PROFILE:default} default: receiver-browser: selector: ${SW_RECEIVER_BROWSER:default} default: sampleRate: ${SW_RECEIVER_BROWSER_SAMPLE_RATE:10000} log-analyzer: selector: ${SW_LOG_ANALYZER:default} default: lalFiles: ${SW_LOG_LAL_FILES:default} malFiles: ${SW_LOG_MAL_FILES:\u0026#34;\u0026#34;} configuration-discovery: selector: ${SW_CONFIGURATION_DISCOVERY:default} default: receiver-event: selector: ${SW_RECEIVER_EVENT:default} default: gRPC/HTTP server for receiver In default, all gRPC/HTTP services should be served at core/gRPC and core/rest. But the receiver-sharing-server module provide a way to make all receivers serving at different ip:port, if you set them explicitly.\nreceiver-sharing-server: selector: ${SW_RECEIVER_SHARING_SERVER:default} default: host: ${SW_RECEIVER_JETTY_HOST:0.0.0.0} contextPath: ${SW_RECEIVER_JETTY_CONTEXT_PATH:/} authentication: ${SW_AUTHENTICATION:\u0026#34;\u0026#34;} jettyMinThreads: ${SW_RECEIVER_SHARING_JETTY_MIN_THREADS:1} jettyMaxThreads: ${SW_RECEIVER_SHARING_JETTY_MAX_THREADS:200} jettyIdleTimeOut: ${SW_RECEIVER_SHARING_JETTY_IDLE_TIMEOUT:30000} jettyAcceptorPriorityDelta: ${SW_RECEIVER_SHARING_JETTY_DELTA:0} jettyAcceptQueueSize: ${SW_RECEIVER_SHARING_JETTY_QUEUE_SIZE:0} Notice, if you add these settings, make sure they are not as same as core module, because gRPC/HTTP servers of core are still used for UI and OAP internal communications.\nOpenTelemetry receiver OpenTelemetry receiver supports to ingest agent metrics by meter-system. OAP can load the configuration at bootstrap. If the new configuration is not well-formed, OAP fails to start up. The files are located at $CLASSPATH/otel-\u0026lt;handler\u0026gt;-rules. Eg, the oc handler loads fules from $CLASSPATH/otel-oc-rules,\nSupported handlers: * oc: OpenCensus gRPC service handler.\nThe rule file should be in YAML format, defined by the scheme described in prometheus-fetcher. Notice, receiver-otel only support group, defaultMetricLevel and metricsRules nodes of scheme due to the push mode it opts to.\nTo active the oc handler and istio relevant rules:\nreceiver-otel: selector: ${SW_OTEL_RECEIVER:default} default: enabledHandlers: ${SW_OTEL_RECEIVER_ENABLED_HANDLERS:\u0026#34;oc\u0026#34;} enabledOcRules: ${SW_OTEL_RECEIVER_ENABLED_OC_RULES:\u0026#34;istio-controlplane\u0026#34;} The receiver adds labels with key = node_identifier_host_name and key = node_identifier_pid to the collected data samples， and values from Node.identifier.host_name and Node.identifier.pid defined in opencensus agent proto, to be the identification of the metric data.\n   Rule Name Description Configuration File Data Source     istio-controlplane Metrics of Istio control panel otel-oc-rules/istio-controlplane.yaml Istio Control Panel -\u0026gt; OpenTelemetry Collector \u0026ndash;OC format\u0026ndash;\u0026gt; SkyWalking OAP Server   oap Metrics of SkyWalking OAP server itself otel-oc-rules/oap.yaml SkyWalking OAP Server(SelfObservability) -\u0026gt; OpenTelemetry Collector \u0026ndash;OC format\u0026ndash;\u0026gt; SkyWalking OAP Server   vm Metrics of VMs otel-oc-rules/vm.yaml Prometheus node-exporter(VMs) -\u0026gt; OpenTelemetry Collector \u0026ndash;OC format\u0026ndash;\u0026gt; SkyWalking OAP Server   k8s-cluster Metrics of K8s cluster otel-oc-rules/k8s-cluster.yaml K8s kube-state-metrics -\u0026gt; OpenTelemetry Collector \u0026ndash;OC format\u0026ndash;\u0026gt; SkyWalking OAP Server   k8s-node Metrics of K8s cluster otel-oc-rules/k8s-node.yaml cAdvisor \u0026amp; K8s kube-state-metrics -\u0026gt; OpenTelemetry Collector \u0026ndash;OC format\u0026ndash;\u0026gt; SkyWalking OAP Server   k8s-service Metrics of K8s cluster otel-oc-rules/k8s-service.yaml cAdvisor \u0026amp; K8s kube-state-metrics -\u0026gt; OpenTelemetry Collector \u0026ndash;OC format\u0026ndash;\u0026gt; SkyWalking OAP Server    Meter receiver Meter receiver supports accept the metrics into the meter-system. OAP can load the configuration at bootstrap.\nThe file is written in YAML format, defined by the scheme described in backend-meter.\nTo active the default implementation:\nreceiver-meter: selector: ${SW_RECEIVER_METER:default} default: Zipkin receiver Zipkin receiver makes the OAP server as an alternative Zipkin server implementation. It supports Zipkin v1/v2 formats through HTTP service. Make sure you use this with SW_STORAGE=zipkin-elasticsearch7 option to activate Zipkin storage implementation. Once this receiver and storage activated, SkyWalking native traces would be ignored, and SkyWalking wouldn\u0026rsquo;t analysis topology, metrics, endpoint dependency from Zipkin\u0026rsquo;s trace.\nUse following config to active.\nreceiver_zipkin: selector: ${SW_RECEIVER_ZIPKIN:-} default: host: ${SW_RECEIVER_ZIPKIN_HOST:0.0.0.0} port: ${SW_RECEIVER_ZIPKIN_PORT:9411} contextPath: ${SW_RECEIVER_ZIPKIN_CONTEXT_PATH:/} jettyMinThreads: ${SW_RECEIVER_ZIPKIN_JETTY_MIN_THREADS:1} jettyMaxThreads: ${SW_RECEIVER_ZIPKIN_JETTY_MAX_THREADS:200} jettyIdleTimeOut: ${SW_RECEIVER_ZIPKIN_JETTY_IDLE_TIMEOUT:30000} jettyAcceptorPriorityDelta: ${SW_RECEIVER_ZIPKIN_JETTY_DELTA:0} jettyAcceptQueueSize: ${SW_RECEIVER_ZIPKIN_QUEUE_SIZE:0} NOTICE, Zipkin receiver is only provided in apache-skywalking-apm-es7-x.y.z.tar.gz tar. And this requires zipkin-elasticsearch7 storage implementation active. Read this doc to know Zipkin as storage option.\n","excerpt":"Choose receiver Receiver is a concept in SkyWalking backend. All modules, which are responsible for …","ref":"/docs/main/v8.5.0/en/setup/backend/backend-receivers/","title":"Choose receiver"},{"body":"Cluster Management In many product environments, backend needs to support high throughput and provides HA to keep robustness, so you should need cluster management always in product env.\nBackend provides several ways to do cluster management. Choose the one you need/want.\n Zookeeper coordinator. Use Zookeeper to let backend instance detects and communicates with each other. Kubernetes. When backend cluster are deployed inside kubernetes, you could choose this by using k8s native APIs to manage cluster. Consul. Use Consul as backend cluster management implementor, to coordinate backend instances. Etcd. Use Etcd to coordinate backend instances. Nacos. Use Nacos to coordinate backend instances. In the application.yml, there\u0026rsquo;re default configurations for the aforementioned coordinators under the section cluster, you can specify one of them in the selector property to enable it.  Zookeeper coordinator Zookeeper is a very common and wide used cluster coordinator. Set the cluster/selector to zookeeper in the yml to enable.\nRequired Zookeeper version, 3.4+\ncluster: selector: ${SW_CLUSTER:zookeeper} # other configurations  hostPort is the list of zookeeper servers. Format is IP1:PORT1,IP2:PORT2,...,IPn:PORTn enableACL enable Zookeeper ACL to control access to its znode. schema is Zookeeper ACL schemas. expression is a expression of ACL. The format of the expression is specific to the schema. hostPort, baseSleepTimeMs and maxRetries are settings of Zookeeper curator client.  Note:\n If Zookeeper ACL is enabled and /skywalking existed, must be sure SkyWalking has CREATE, READ and WRITE permissions. If /skywalking is not exists, it will be created by SkyWalking and grant all permissions to the specified user. Simultaneously, znode is granted READ to anyone. If set schema as digest, the password of expression is set in clear text.  In some cases, oap default gRPC host and port in core are not suitable for internal communication among the oap nodes. The following setting are provided to set the host and port manually, based on your own LAN env.\n internalComHost, the host registered and other oap node use this to communicate with current node. internalComPort, the port registered and other oap node use this to communicate with current node.  zookeeper: nameSpace: ${SW_NAMESPACE:\u0026#34;\u0026#34;} hostPort: ${SW_CLUSTER_ZK_HOST_PORT:localhost:2181} #Retry Policy baseSleepTimeMs: ${SW_CLUSTER_ZK_SLEEP_TIME:1000} # initial amount of time to wait between retries maxRetries: ${SW_CLUSTER_ZK_MAX_RETRIES:3} # max number of times to retry internalComHost: 172.10.4.10 internalComPort: 11800 # Enable ACL enableACL: ${SW_ZK_ENABLE_ACL:false} # disable ACL in default schema: ${SW_ZK_SCHEMA:digest} # only support digest schema expression: ${SW_ZK_EXPRESSION:skywalking:skywalking} Kubernetes Require backend cluster are deployed inside kubernetes, guides are in Deploy in kubernetes. Set the selector to kubernetes.\ncluster: selector: ${SW_CLUSTER:kubernetes} # other configurations Consul Now, consul is becoming a famous system, many of companies and developers using consul to be their service discovery solution. Set the cluster/selector to consul in the yml to enable.\ncluster: selector: ${SW_CLUSTER:consul} # other configurations Same as Zookeeper coordinator, in some cases, oap default gRPC host and port in core are not suitable for internal communication among the oap nodes. The following setting are provided to set the host and port manually, based on your own LAN env.\n internalComHost, the host registered and other oap node use this to communicate with current node. internalComPort, the port registered and other oap node use this to communicate with current node.  Etcd Set the cluster/selector to etcd in the yml to enable.\ncluster: selector: ${SW_CLUSTER:etcd} # other configurations Same as Zookeeper coordinator, in some cases, oap default gRPC host and port in core are not suitable for internal communication among the oap nodes. The following setting are provided to set the host and port manually, based on your own LAN env.\n internalComHost, the host registered and other oap node use this to communicate with current node. internalComPort, the port registered and other oap node use this to communicate with current node.  Nacos Set the cluster/selector to nacos in the yml to enable.\ncluster: selector: ${SW_CLUSTER:nacos} # other configurations Nacos support authenticate by username or accessKey, empty means no need auth. extra config is bellow:\nnacos: username: password: accessKey: secretKey: Same as Zookeeper coordinator, in some cases, oap default gRPC host and port in core are not suitable for internal communication among the oap nodes. The following setting are provided to set the host and port manually, based on your own LAN env.\n internalComHost, the host registered and other oap node use this to communicate with current node. internalComPort, the port registered and other oap node use this to communicate with current node.  ","excerpt":"Cluster Management In many product environments, backend needs to support high throughput and …","ref":"/docs/main/v8.5.0/en/setup/backend/backend-cluster/","title":"Cluster Management"},{"body":"Compatibility with other Java agent bytecode processes Problem   When using the SkyWalking agent, some other agents, such as Arthas, can\u0026rsquo;t work properly. https://github.com/apache/skywalking/pull/4858\n  The retransform classes in the Java agent conflict with the SkyWalking agent, as illustrated in this demo\n  Cause The SkyWalking agent uses ByteBuddy to transform classes when the Java application starts. ByteBuddy generates auxiliary classes with different random names every time.\nWhen another Java agent retransforms the same class, it triggers the SkyWalking agent to enhance the class again. Since the bytecode has been regenerated by ByteBuddy, the fields and imported class names have been modified, and the JVM verifications on class bytecode have failed, the retransform classes would therefore be unsuccessful.\nResolution 1. Enable the class cache feature\nAdd JVM parameters:\n-Dskywalking.agent.is_cache_enhanced_class=true -Dskywalking.agent.class_cache_mode=MEMORY\nOr uncomment the following options in agent.conf:\n# If true, the SkyWalking agent will cache all instrumented classes files to memory or disk files (as determined by the class cache mode), # Allow other Java agents to enhance those classes that are enhanced by the SkyWalking agent. agent.is_cache_enhanced_class = ${SW_AGENT_CACHE_CLASS:false} # The instrumented classes cache mode: MEMORY or FILE # MEMORY: cache class bytes to memory; if there are too many instrumented classes or if their sizes are too large, it may take up more memory # FILE: cache class bytes to user temp folder starts with 'class-cache', and automatically clean up cached class files when the application exits agent.class_cache_mode = ${SW_AGENT_CLASS_CACHE_MODE:MEMORY} If the class cache feature is enabled, save the instrumented class bytecode to memory or a temporary file. When other Java agents retransform the same class, the SkyWalking agent first attempts to load from the cache.\nIf the cached class is found, it will be used directly without regenerating an auxiliary class with a new random name. Then, the process of the subsequent Java agent will not be affected.\n2. Class cache save mode\nWe recommend saving cache classes to memory, if it takes up more memory space. Alternatively, you can use the local file system. Set the class cache mode in one of the folliwng ways:\n-Dskywalking.agent.class_cache_mode=MEMORY : save cache classes to Java memory. -Dskywalking.agent.class_cache_mode=FILE : save cache classes to SkyWalking agent path \u0026lsquo;/class-cache\u0026rsquo;.\nOr modify these options in agent.conf:\nagent.class_cache_mode = ${SW_AGENT_CLASS_CACHE_MODE:MEMORY} agent.class_cache_mode = ${SW_AGENT_CLASS_CACHE_MODE:FILE}\n","excerpt":"Compatibility with other Java agent bytecode processes Problem   When using the SkyWalking agent, …","ref":"/docs/main/v8.5.0/en/faq/compatible-with-other-javaagent-bytecode-processing/","title":"Compatibility with other Java agent bytecode processes"},{"body":"Compiling issues on Mac\u0026rsquo;s M1 chip Problem  When compiling according to How-to-build, The following problems may occur, causing the build to fail.  [ERROR] Failed to execute goal org.xolstice.maven.plugins:protobuf-maven-plugin:0.6.1:compile (grpc-build) on project apm-network: Unable to resolve artifact: Missing: [ERROR] ---------- [ERROR] 1) com.google.protobuf:protoc:exe:osx-aarch_64:3.12.0 [ERROR] [ERROR] Try downloading the file manually from the project website. [ERROR] [ERROR] Then, install it using the command: [ERROR] mvn install:install-file -DgroupId=com.google.protobuf -DartifactId=protoc -Dversion=3.12.0 -Dclassifier=osx-aarch_64 -Dpackaging=exe -Dfile=/path/to/file [ERROR] [ERROR] Alternatively, if you host your own repository you can deploy the file there: [ERROR] mvn deploy:deploy-file -DgroupId=com.google.protobuf -DartifactId=protoc -Dversion=3.12.0 -Dclassifier=osx-aarch_64 -Dpackaging=exe -Dfile=/path/to/file -Durl=[url] -DrepositoryId=[id] [ERROR] [ERROR] Path to dependency: [ERROR] 1) org.apache.skywalking:apm-network:jar:8.4.0-SNAPSHOT [ERROR] 2) com.google.protobuf:protoc:exe:osx-aarch_64:3.12.0 [ERROR] [ERROR] ---------- [ERROR] 1 required artifact is missing. Reason The dependent Protocol Buffers v3.14.0 does not come with an osx-aarch_64 version. You may find the osx-aarch_64 version at the Protocol Buffers Releases link here: https://github.com/protocolbuffers/protobuf/releases. Since Mac\u0026rsquo;s M1 is compatible with the osx-x86_64 version, before this version is available for downloading, you need to manually specify the osx-x86_64 version.\nResolution You may add -Dos.detected.classifier=osx-x86_64 after the original compilation parameters, such as: ./mvnw clean package -DskipTests -Dos.detected.classifier=osx-x86_64. After specifying the version, compile and run normally.\n","excerpt":"Compiling issues on Mac\u0026rsquo;s M1 chip Problem  When compiling according to How-to-build, The …","ref":"/docs/main/v8.5.0/en/faq/how-to-build-with-mac-m1/","title":"Compiling issues on Mac's M1 chip"},{"body":"Component library settings Component library settings are about your own or 3rd part libraries used in monitored application.\nIn agent or SDK, no matter library name collected as ID or String(literally, e.g. SpringMVC), collector formats data in ID for better performance and less storage requirements.\nAlso, collector conjectures the remote service based on the component library, such as: the component library is MySQL Driver library, then the remote service should be MySQL Server.\nFor those two reasons, collector require two parts of settings in this file:\n Component Library id, name and languages. Remote server mapping, based on local library.  All component names and IDs must be defined in this file.\nComponent Library id Define all component libraries' names and IDs, used in monitored application. This is a both-way mapping, agent or SDK could use the value(ID) to represent the component name in uplink data.\n Name: the component name used in agent and UI id: Unique ID. All IDs are reserved, once it is released. languages: Program languages may use this component. Multi languages should be separated by ,  ID rules  Java and multi languages shared: (0, 3000) .NET Platform reserved: [3000, 4000) Node.js Platform reserved: [4000, 5000) Go reserved: [5000, 6000) Lua reserved: [6000, 7000) Python reserved: [7000, 8000) PHP reserved: [8000, 9000) C++ reserved: [9000, 10000)  Example\nTomcat: id: 1 languages: Java HttpClient: id: 2 languages: Java,C#,Node.js Dubbo: id: 3 languages: Java H2: id: 4 languages: Java Remote server mapping Remote server will be conjectured by the local component. The mappings are based on Component library names.\n Key: client component library name Value: server component name  Component-Server-Mappings: Jedis: Redis StackExchange.Redis: Redis Redisson: Redis Lettuce: Redis Zookeeper: Zookeeper SqlClient: SqlServer Npgsql: PostgreSQL MySqlConnector: Mysql EntityFrameworkCore.InMemory: InMemoryDatabase ","excerpt":"Component library settings Component library settings are about your own or 3rd part libraries used …","ref":"/docs/main/v8.5.0/en/guides/component-library-settings/","title":"Component library settings"},{"body":"Configuration Vocabulary Configuration Vocabulary lists all available configurations provided by application.yml.\n   Module Provider Settings Value(s) and Explanation System Environment Variable¹ Default     core default role Option values, Mixed/Receiver/Aggregator. Receiver mode OAP open the service to the agents, analysis and aggregate the results and forward the results for distributed aggregation. Aggregator mode OAP receives data from Mixer and Receiver role OAP nodes, and do 2nd level aggregation. Mixer means being Receiver and Aggregator both. SW_CORE_ROLE Mixed   - - restHost Binding IP of restful service. Services include GraphQL query and HTTP data report SW_CORE_REST_HOST 0.0.0.0   - - restPort Binding port of restful service SW_CORE_REST_PORT 12800   - - restContextPath Web context path of restful service SW_CORE_REST_CONTEXT_PATH /   - - restMinThreads Min threads number of restful service SW_CORE_REST_JETTY_MIN_THREADS 1   - - restMaxThreads Max threads number of restful service SW_CORE_REST_JETTY_MAX_THREADS 200   - - restIdleTimeOut Connector idle timeout in milliseconds of restful service SW_CORE_REST_JETTY_IDLE_TIMEOUT 30000   - - restAcceptorPriorityDelta Thread priority delta to give to acceptor threads of restful service SW_CORE_REST_JETTY_DELTA 0   - - restAcceptQueueSize ServerSocketChannel backlog of restful service SW_CORE_REST_JETTY_QUEUE_SIZE 0   - - gRPCHost Binding IP of gRPC service. Services include gRPC data report and internal communication among OAP nodes SW_CORE_GRPC_HOST 0.0.0.0   - - gRPCPort Binding port of gRPC service SW_CORE_GRPC_PORT 11800   - - gRPCSslEnabled Activate SSL for gRPC service SW_CORE_GRPC_SSL_ENABLED false   - - gRPCSslKeyPath The file path of gRPC SSL key SW_CORE_GRPC_SSL_KEY_PATH -   - - gRPCSslCertChainPath The file path of gRPC SSL cert chain SW_CORE_GRPC_SSL_CERT_CHAIN_PATH -   - - gRPCSslTrustedCAPath The file path of gRPC trusted CA SW_CORE_GRPC_SSL_TRUSTED_CA_PATH -   - - downsampling The activated level of down sampling aggregation  Hour,Day   - - enableDataKeeperExecutor Controller of TTL scheduler. Once disabled, TTL wouldn\u0026rsquo;t work. SW_CORE_ENABLE_DATA_KEEPER_EXECUTOR true   - - dataKeeperExecutePeriod The execution period of TTL scheduler, unit is minute. Execution doesn\u0026rsquo;t mean deleting data. The storage provider could override this, such as ElasticSearch storage. SW_CORE_DATA_KEEPER_EXECUTE_PERIOD 5   - - recordDataTTL The lifecycle of record data. Record data includes traces, top n sampled records, and logs. Unit is day. Minimal value is 2. SW_CORE_RECORD_DATA_TTL 3   - - metricsDataTTL The lifecycle of metrics data, including the metadata. Unit is day. Recommend metricsDataTTL \u0026gt;= recordDataTTL. Minimal value is 2. SW_CORE_METRICS_DATA_TTL 7   - - enableDatabaseSession Cache metrics data for 1 minute to reduce database queries, and if the OAP cluster changes within that minute. SW_CORE_ENABLE_DATABASE_SESSION true   - - topNReportPeriod The execution period of top N sampler, which saves sampled data into the storage. Unit is minute SW_CORE_TOPN_REPORT_PERIOD 10   - - activeExtraModelColumns Append the names of entity, such as service name, into the metrics storage entities. SW_CORE_ACTIVE_EXTRA_MODEL_COLUMNS false   - - serviceNameMaxLength Max length limitation of service name. SW_SERVICE_NAME_MAX_LENGTH 70   - - instanceNameMaxLength Max length limitation of service instance name. The max length of service + instance names should be less than 200. SW_INSTANCE_NAME_MAX_LENGTH 70   - - endpointNameMaxLength Max length limitation of endpoint name. The max length of service + endpoint names should be less than 240. SW_ENDPOINT_NAME_MAX_LENGTH 150   - - searchableTracesTags Define the set of span tag keys, which should be searchable through the GraphQL. Multiple values should be separated through the comma. SW_SEARCHABLE_TAG_KEYS http.method,status_code,db.type,db.instance,mq.queue,mq.topic,mq.broker   - - searchableLogsTags Define the set of log tag keys, which should be searchable through the GraphQL. Multiple values should be separated through the comma. SW_SEARCHABLE_LOGS_TAG_KEYS level   - - gRPCThreadPoolSize Pool size of gRPC server SW_CORE_GRPC_THREAD_POOL_SIZE CPU core * 4   - - gRPCThreadPoolQueueSize The queue size of gRPC server SW_CORE_GRPC_POOL_QUEUE_SIZE 10000   - - maxConcurrentCallsPerConnection The maximum number of concurrent calls permitted for each incoming connection. Defaults to no limit. SW_CORE_GRPC_MAX_CONCURRENT_CALL -   - - maxMessageSize Sets the maximum message size allowed to be received on the server. Empty means 4 MiB SW_CORE_GRPC_MAX_MESSAGE_SIZE 4M(based on Netty)   - - remoteTimeout Timeout for cluster internal communication, in seconds. - 20   - - maxSizeOfNetworkAddressAlias Max size of network address detected in the be monitored system. - 1_000_000   - - maxPageSizeOfQueryProfileSnapshot The max size in every OAP query for snapshot analysis - 500   - - maxSizeOfAnalyzeProfileSnapshot The max number of snapshots analyzed by OAP - 12000   - - syncThreads The number of threads used to synchronously refresh the metrics data to the storage. SW_CORE_SYNC_THREADS 2   - - maxSyncOperationNum The maximum number of processes supported for each synchronous storage operation. When the number of the flush data is greater than this value, it will be assigned to multiple cores for execution. SW_CORE_MAX_SYNC_OPERATION_NUM 50000   cluster standalone - standalone is not suitable for one node running, no available configuration. - -   - zookeeper nameSpace The namespace, represented by root path, isolates the configurations in the zookeeper. SW_NAMESPACE /, root path   - - hostPort hosts and ports of Zookeeper Cluster SW_CLUSTER_ZK_HOST_PORT localhost:2181   - - baseSleepTimeMs The period of Zookeeper client between two retries. Unit is ms. SW_CLUSTER_ZK_SLEEP_TIME 1000   - - maxRetries The max retry time of re-trying. SW_CLUSTER_ZK_MAX_RETRIES 3   - - enableACL Open ACL by using schema and expression SW_ZK_ENABLE_ACL false   - - schema schema for the authorization SW_ZK_SCHEMA digest   - - expression expression for the authorization SW_ZK_EXPRESSION skywalking:skywalking   - - internalComHost The hostname registered in the Zookeeper for the internal communication of OAP cluster. - -   - - internalComPort The port registered in the Zookeeper for the internal communication of OAP cluster. - -1   - kubernetes namespace Namespace SkyWalking deployed in the k8s SW_CLUSTER_K8S_NAMESPACE default   - - labelSelector Labels used for filtering the OAP deployment in the k8s SW_CLUSTER_K8S_LABEL app=collector,release=skywalking   - - uidEnvName Environment variable name for reading uid. SW_CLUSTER_K8S_UID SKYWALKING_COLLECTOR_UID   - consul serviceName Service name used for SkyWalking cluster. SW_SERVICE_NAME SkyWalking_OAP_Cluster   - - hostPort hosts and ports used of Consul cluster. SW_CLUSTER_CONSUL_HOST_PORT localhost:8500   - - aclToken ALC Token of Consul. Empty string means without ALC token. SW_CLUSTER_CONSUL_ACLTOKEN -   - - internalComHost The hostname registered in the Consul for the internal communication of OAP cluster. - -   - - internalComPort The port registered in the Consul for the internal communication of OAP cluster. - -1   - etcd serviceName Service name used for SkyWalking cluster. SW_SERVICE_NAME SkyWalking_OAP_Cluster   - - hostPort hosts and ports used of etcd cluster. SW_CLUSTER_ETCD_HOST_PORT localhost:2379   - - isSSL Open SSL for the connection between SkyWalking and etcd cluster. - -   - - internalComHost The hostname registered in the etcd for the internal communication of OAP cluster. - -   - - internalComPort The port registered in the etcd for the internal communication of OAP cluster. - -1   - Nacos serviceName Service name used for SkyWalking cluster. SW_SERVICE_NAME SkyWalking_OAP_Cluster   - - hostPort hosts and ports used of Nacos cluster. SW_CLUSTER_NACOS_HOST_PORT localhost:8848   - - namespace Namespace used by SkyWalking node coordination. SW_CLUSTER_NACOS_NAMESPACE public   - - internalComHost The hostname registered in the Nacos for the internal communication of OAP cluster. - -   - - internalComPort The port registered in the Nacos for the internal communication of OAP cluster. - -1   - - username Nacos Auth username SW_CLUSTER_NACOS_USERNAME -   - - password Nacos Auth password SW_CLUSTER_NACOS_PASSWORD -   - - accessKey Nacos Auth accessKey SW_CLUSTER_NACOS_ACCESSKEY -   - - secretKey Nacos Auth secretKey SW_CLUSTER_NACOS_SECRETKEY -   storage elasticsearch - ElasticSearch 6 storage implementation - -   - - nameSpace Prefix of indexes created and used by SkyWalking. SW_NAMESPACE -   - - clusterNodes ElasticSearch cluster nodes for client connection. SW_STORAGE_ES_CLUSTER_NODES localhost   - - protocol HTTP or HTTPs. SW_STORAGE_ES_HTTP_PROTOCOL HTTP   - - user User name of ElasticSearch cluster SW_ES_USER -   - - password Password of ElasticSearch cluster SW_ES_PASSWORD -   - - trustStorePath Trust JKS file path. Only work when user name and password opened SW_STORAGE_ES_SSL_JKS_PATH -   - - trustStorePass Trust JKS file password. Only work when user name and password opened SW_STORAGE_ES_SSL_JKS_PASS -   - - secretsManagementFile Secrets management file in the properties format includes the username, password, which are managed by 3rd party tool. Provide the capability to update them in the runtime. SW_ES_SECRETS_MANAGEMENT_FILE -   - - dayStep Represent the number of days in the one minute/hour/day index. SW_STORAGE_DAY_STEP 1   - - indexShardsNumber Shard number of new indexes SW_STORAGE_ES_INDEX_SHARDS_NUMBER 1   - - indexReplicasNumber Replicas number of new indexes SW_STORAGE_ES_INDEX_REPLICAS_NUMBER 0   - - superDatasetDayStep Represent the number of days in the super size dataset record index, the default value is the same as dayStep when the value is less than 0. SW_SUPERDATASET_STORAGE_DAY_STEP -1   - - superDatasetIndexShardsFactor Super data set has been defined in the codes, such as trace segments. This factor provides more shards for the super data set, shards number = indexShardsNumber * superDatasetIndexShardsFactor. Also, this factor effects Zipkin and Jaeger traces. SW_STORAGE_ES_SUPER_DATASET_INDEX_SHARDS_FACTOR 5   - - superDatasetIndexReplicasNumber Represent the replicas number in the super size dataset record index. SW_STORAGE_ES_SUPER_DATASET_INDEX_REPLICAS_NUMBER 0   - - bulkActions Async bulk size of the record data batch execution. SW_STORAGE_ES_BULK_ACTIONS 1000   - - flushInterval Period of flush, no matter bulkActions reached or not. Unit is second. SW_STORAGE_ES_FLUSH_INTERVAL 10   - - concurrentRequests The number of concurrent requests allowed to be executed. SW_STORAGE_ES_CONCURRENT_REQUESTS 2   - - resultWindowMaxSize The max size of dataset when OAP loading cache, such as network alias. SW_STORAGE_ES_QUERY_MAX_WINDOW_SIZE 10000   - - metadataQueryMaxSize The max size of metadata per query. SW_STORAGE_ES_QUERY_MAX_SIZE 5000   - - segmentQueryMaxSize The max size of trace segments per query. SW_STORAGE_ES_QUERY_SEGMENT_SIZE 200   - - profileTaskQueryMaxSize The max size of profile task per query. SW_STORAGE_ES_QUERY_PROFILE_TASK_SIZE 200   - - advanced All settings of ElasticSearch index creation. The value should be in JSON format SW_STORAGE_ES_ADVANCED -   - elasticsearch7 - ElasticSearch 7 storage implementation - -   - - nameSpace Prefix of indexes created and used by SkyWalking. SW_NAMESPACE -   - - clusterNodes ElasticSearch cluster nodes for client connection. SW_STORAGE_ES_CLUSTER_NODES localhost   - - protocol HTTP or HTTPs. SW_STORAGE_ES_HTTP_PROTOCOL HTTP   - - user User name of ElasticSearch cluster SW_ES_USER -   - - password Password of ElasticSearch cluster SW_ES_PASSWORD -   - - trustStorePath Trust JKS file path. Only work when user name and password opened SW_STORAGE_ES_SSL_JKS_PATH -   - - trustStorePass Trust JKS file password. Only work when user name and password opened SW_STORAGE_ES_SSL_JKS_PASS -   - - secretsManagementFile Secrets management file in the properties format includes the username, password, which are managed by 3rd party tool. Provide the capability to update them in the runtime. SW_ES_SECRETS_MANAGEMENT_FILE -   - - dayStep Represent the number of days in the one minute/hour/day index. SW_STORAGE_DAY_STEP 1   - - indexShardsNumber Shard number of new indexes SW_STORAGE_ES_INDEX_SHARDS_NUMBER 1   - - indexReplicasNumber Replicas number of new indexes SW_STORAGE_ES_INDEX_REPLICAS_NUMBER 0   - - superDatasetDayStep Represent the number of days in the super size dataset record index, the default value is the same as dayStep when the value is less than 0. SW_SUPERDATASET_STORAGE_DAY_STEP -1   - - superDatasetIndexShardsFactor Super data set has been defined in the codes, such as trace segments. This factor provides more shards for the super data set, shards number = indexShardsNumber * superDatasetIndexShardsFactor. Also, this factor effects Zipkin and Jaeger traces. SW_STORAGE_ES_SUPER_DATASET_INDEX_SHARDS_FACTOR 5   - - superDatasetIndexReplicasNumber Represent the replicas number in the super size dataset record index. SW_STORAGE_ES_SUPER_DATASET_INDEX_REPLICAS_NUMBER 0   - - bulkActions Async bulk size of the record data batch execution. SW_STORAGE_ES_BULK_ACTIONS 1000   - - syncBulkActions Sync bulk size of the metrics data batch execution. SW_STORAGE_ES_SYNC_BULK_ACTIONS 50000   - - flushInterval Period of flush, no matter bulkActions reached or not. Unit is second. SW_STORAGE_ES_FLUSH_INTERVAL 10   - - concurrentRequests The number of concurrent requests allowed to be executed. SW_STORAGE_ES_CONCURRENT_REQUESTS 2   - - resultWindowMaxSize The max size of dataset when OAP loading cache, such as network alias. SW_STORAGE_ES_QUERY_MAX_WINDOW_SIZE 10000   - - metadataQueryMaxSize The max size of metadata per query. SW_STORAGE_ES_QUERY_MAX_SIZE 5000   - - segmentQueryMaxSize The max size of trace segments per query. SW_STORAGE_ES_QUERY_SEGMENT_SIZE 200   - - profileTaskQueryMaxSize The max size of profile task per query. SW_STORAGE_ES_QUERY_PROFILE_TASK_SIZE 200   - - advanced All settings of ElasticSearch index creation. The value should be in JSON format SW_STORAGE_ES_ADVANCED -   - h2 - H2 storage is designed for demonstration and running in short term(1-2 hours) only - -   - - driver H2 JDBC driver. SW_STORAGE_H2_DRIVER org.h2.jdbcx.JdbcDataSource   - - url H2 connection URL. Default is H2 memory mode SW_STORAGE_H2_URL jdbc:h2:mem:skywalking-oap-db   - - user User name of H2 database. SW_STORAGE_H2_USER sa   - - password Password of H2 database. - -   - - metadataQueryMaxSize The max size of metadata per query. SW_STORAGE_H2_QUERY_MAX_SIZE 5000   - - maxSizeOfArrayColumn Some entities, such as trace segment, include the logic column with multiple values. In the H2, we use multiple physical columns to host the values, such as, Change column_a with values [1,2,3,4,5] to column_a_0 = 1, column_a_1 = 2, column_a_2 = 3 , column_a_3 = 4, column_a_4 = 5 SW_STORAGE_MAX_SIZE_OF_ARRAY_COLUMN 20   - - numOfSearchableValuesPerTag In a trace segment, it includes multiple spans with multiple tags. Different spans could have same tag keys, such as multiple HTTP exit spans all have their own http.method tag. This configuration set the limitation of max num of values for the same tag key. SW_STORAGE_NUM_OF_SEARCHABLE_VALUES_PER_TAG 2   - mysql - MySQL Storage. The MySQL JDBC Driver is not in the dist, please copy it into oap-lib folder manually - -   - - properties Hikari connection pool configurations - Listed in the application.yaml.   - - metadataQueryMaxSize The max size of metadata per query. SW_STORAGE_MYSQL_QUERY_MAX_SIZE 5000   - - maxSizeOfArrayColumn Some entities, such as trace segment, include the logic column with multiple values. In the MySQL, we use multiple physical columns to host the values, such as, Change column_a with values [1,2,3,4,5] to column_a_0 = 1, column_a_1 = 2, column_a_2 = 3 , column_a_3 = 4, column_a_4 = 5 SW_STORAGE_MAX_SIZE_OF_ARRAY_COLUMN 20   - - numOfSearchableValuesPerTag In a trace segment, it includes multiple spans with multiple tags. Different spans could have same tag keys, such as multiple HTTP exit spans all have their own http.method tag. This configuration set the limitation of max num of values for the same tag key. SW_STORAGE_NUM_OF_SEARCHABLE_VALUES_PER_TAG 2   - postgresql - PostgreSQL storage. - -   - - properties Hikari connection pool configurations - Listed in the application.yaml.   - - metadataQueryMaxSize The max size of metadata per query. SW_STORAGE_MYSQL_QUERY_MAX_SIZE 5000   - - maxSizeOfArrayColumn Some entities, such as trace segment, include the logic column with multiple values. In the PostgreSQL, we use multiple physical columns to host the values, such as, Change column_a with values [1,2,3,4,5] to column_a_0 = 1, column_a_1 = 2, column_a_2 = 3 , column_a_3 = 4, column_a_4 = 5 SW_STORAGE_MAX_SIZE_OF_ARRAY_COLUMN 20   - - numOfSearchableValuesPerTag In a trace segment, it includes multiple spans with multiple tags. Different spans could have same tag keys, such as multiple HTTP exit spans all have their own http.method tag. This configuration set the limitation of max num of values for the same tag key. SW_STORAGE_NUM_OF_SEARCHABLE_VALUES_PER_TAG 2   - influxdb - InfluxDB storage. - -   - - url InfluxDB connection URL. SW_STORAGE_INFLUXDB_URL http://localhost:8086   - - user User name of InfluxDB. SW_STORAGE_INFLUXDB_USER root   - - password Password of InfluxDB. SW_STORAGE_INFLUXDB_PASSWORD -   - - database Database of InfluxDB. SW_STORAGE_INFLUXDB_DATABASE skywalking   - - actions The number of actions to collect. SW_STORAGE_INFLUXDB_ACTIONS 1000   - - duration The time to wait at most (milliseconds). SW_STORAGE_INFLUXDB_DURATION 1000   - - batchEnabled If true, write points with batch api. SW_STORAGE_INFLUXDB_BATCH_ENABLED true   - - fetchTaskLogMaxSize The max number of fetch task log in a request. SW_STORAGE_INFLUXDB_FETCH_TASK_LOG_MAX_SIZE 5000   - - connectionResponseFormat The response format of connection to influxDB, cannot be anything but MSGPACK or JSON. SW_STORAGE_INFLUXDB_CONNECTION_RESPONSE_FORMAT MSGPACK   agent-analyzer default Agent Analyzer. SW_AGENT_ANALYZER default    - - sampleRate Sampling rate for receiving trace. The precision is 1/10000. 10000 means 100% sample in default. SW_TRACE_SAMPLE_RATE 10000   - - slowDBAccessThreshold The slow database access thresholds. Unit ms. SW_SLOW_DB_THRESHOLD default:200,mongodb:100   - - forceSampleErrorSegment When sampling mechanism activated, this config would make the error status segment sampled, ignoring the sampling rate. SW_FORCE_SAMPLE_ERROR_SEGMENT true   - - segmentStatusAnalysisStrategy Determine the final segment status from the status of spans. Available values are FROM_SPAN_STATUS , FROM_ENTRY_SPAN and FROM_FIRST_SPAN. FROM_SPAN_STATUS represents the segment status would be error if any span is in error status. FROM_ENTRY_SPAN means the segment status would be determined by the status of entry spans only. FROM_FIRST_SPAN means the segment status would be determined by the status of the first span only. SW_SEGMENT_STATUS_ANALYSIS_STRATEGY FROM_SPAN_STATUS   - - noUpstreamRealAddressAgents Exit spans with the component in the list would not generate the client-side instance relation metrics. As some tracing plugins can\u0026rsquo;t collect the real peer ip address, such as Nginx-LUA and Envoy. SW_NO_UPSTREAM_REAL_ADDRESS 6000,9000   - - slowTraceSegmentThreshold Setting this threshold about the latency would make the slow trace segments sampled if they cost more time, even the sampling mechanism activated. The default value is -1, which means would not sample slow traces. Unit, millisecond. SW_SLOW_TRACE_SEGMENT_THRESHOLD -1   - - meterAnalyzerActiveFiles Which files could be meter analyzed, files split by \u0026ldquo;,\u0026rdquo; SW_METER_ANALYZER_ACTIVE_FILES    receiver-sharing-server default Sharing server provides new gRPC and restful servers for data collection. Ana make the servers in the core module working for internal communication only. - -    - - restHost Binding IP of restful service. Services include GraphQL query and HTTP data report SW_RECEIVER_SHARING_REST_HOST -   - - restPort Binding port of restful service SW_RECEIVER_SHARING_REST_PORT -   - - restContextPath Web context path of restful service SW_RECEIVER_SHARING_REST_CONTEXT_PATH -   - - restMinThreads Min threads number of restful service SW_RECEIVER_SHARING_JETTY_MIN_THREADS 1   - - restMaxThreads Max threads number of restful service SW_RECEIVER_SHARING_JETTY_MAX_THREADS 200   - - restIdleTimeOut Connector idle timeout in milliseconds of restful service SW_RECEIVER_SHARING_JETTY_IDLE_TIMEOUT 30000   - - restAcceptorPriorityDelta Thread priority delta to give to acceptor threads of restful service SW_RECEIVER_SHARING_JETTY_DELTA 0   - - restAcceptQueueSize ServerSocketChannel backlog of restful service SW_RECEIVER_SHARING_JETTY_QUEUE_SIZE 0   - - gRPCHost Binding IP of gRPC service. Services include gRPC data report and internal communication among OAP nodes SW_RECEIVER_GRPC_HOST 0.0.0.0. Not Activated   - - gRPCPort Binding port of gRPC service SW_RECEIVER_GRPC_PORT Not Activated   - - gRPCThreadPoolSize Pool size of gRPC server SW_RECEIVER_GRPC_THREAD_POOL_SIZE CPU core * 4   - - gRPCThreadPoolQueueSize The queue size of gRPC server SW_RECEIVER_GRPC_POOL_QUEUE_SIZE 10000   - - gRPCSslEnabled Activate SSL for gRPC service SW_RECEIVER_GRPC_SSL_ENABLED false   - - gRPCSslKeyPath The file path of gRPC SSL key SW_RECEIVER_GRPC_SSL_KEY_PATH -   - - gRPCSslCertChainPath The file path of gRPC SSL cert chain SW_RECEIVER_GRPC_SSL_CERT_CHAIN_PATH -   - - maxConcurrentCallsPerConnection The maximum number of concurrent calls permitted for each incoming connection. Defaults to no limit. SW_RECEIVER_GRPC_MAX_CONCURRENT_CALL -   - - authentication The token text for the authentication. Work for gRPC connection only. Once this is set, the client is required to use the same token. SW_AUTHENTICATION -   log-analyzer default Log Analyzer. SW_LOG_ANALYZER default    - - lalFiles The LAL configuration file names (without file extension) to be activated. Read LAL for more details. SW_LOG_LAL_FILES default   - - malFiles The MAL configuration file names (without file extension) to be activated. Read LAL for more details. SW_LOG_MAL_FILES \u0026quot;\u0026quot;   event-analyzer default Event Analyzer. SW_EVENT_ANALYZER default    receiver-register default Read receiver doc for more details - -    receiver-trace default Read receiver doc for more details - -    receiver-jvm default Read receiver doc for more details - -    receiver-clr default Read receiver doc for more details - -    receiver-profile default Read receiver doc for more details - -    receiver-zabbix default Read receiver doc for more details - -    - - port Exported tcp port, Zabbix agent could connect and transport data SW_RECEIVER_ZABBIX_PORT 10051   - - host Bind to host SW_RECEIVER_ZABBIX_HOST 0.0.0.0   - - activeFiles Enable config when receive agent request SW_RECEIVER_ZABBIX_ACTIVE_FILES agent   service-mesh default Read receiver doc for more details - -    envoy-metric default Read receiver doc for more details - -    - - acceptMetricsService Open Envoy Metrics Service analysis SW_ENVOY_METRIC_SERVICE true   - - alsHTTPAnalysis Open Envoy Access Log Service analysis. Value = k8s-mesh means open the analysis SW_ENVOY_METRIC_ALS_HTTP_ANALYSIS -   - - k8sServiceNameRule k8sServiceNameRule allows you to customize the service name in ALS via Kubernetes metadata, the available variables are pod, service, e.g., you can use ${service.metadata.name}-${pod.metadata.labels.version} to append the version number to the service name. Be careful, when using environment variables to pass this configuration, use single quotes('') to avoid it being evaluated by the shell. -    receiver-otel default Read receiver doc for more details - -    - - enabledHandlers Enabled handlers for otel SW_OTEL_RECEIVER_ENABLED_HANDLERS -   - - enabledOcRules Enabled metric rules for OC handler SW_OTEL_RECEIVER_ENABLED_OC_RULES -   receiver_zipkin default Read receiver doc - -    - - restHost Binding IP of restful service. SW_RECEIVER_ZIPKIN_HOST 0.0.0.0   - - restPort Binding port of restful service SW_RECEIVER_ZIPKIN_PORT 9411   - - restContextPath Web context path of restful service SW_RECEIVER_ZIPKIN_CONTEXT_PATH /   receiver_jaeger default Read receiver doc - -    - - gRPCHost Binding IP of gRPC service. Services include gRPC data report and internal communication among OAP nodes SW_RECEIVER_JAEGER_HOST -   - - gRPCPort Binding port of gRPC service SW_RECEIVER_JAEGER_PORT -   - - gRPCThreadPoolSize Pool size of gRPC server - CPU core * 4   - - gRPCThreadPoolQueueSize The queue size of gRPC server - 10000   - - maxConcurrentCallsPerConnection The maximum number of concurrent calls permitted for each incoming connection. Defaults to no limit. - -   - - maxMessageSize Sets the maximum message size allowed to be received on the server. Empty means 4 MiB - 4M(based on Netty)   prometheus-fetcher default Read fetcher doc for more details - -    - - active Activate the Prometheus fetcher. SW_PROMETHEUS_FETCHER_ACTIVE false   kafka-fetcher default Read fetcher doc for more details - -    - - bootstrapServers A list of host/port pairs to use for establishing the initial connection to the Kafka cluster. SW_KAFKA_FETCHER_SERVERS localhost:9092   - - groupId A unique string that identifies the consumer group this consumer belongs to. - skywalking-consumer   - - consumePartitions Which PartitionId(s) of the topics assign to the OAP server. If more than one, is separated by commas. SW_KAFKA_FETCHER_CONSUME_PARTITIONS -   - - isSharding it was true when OAP Server in cluster. SW_KAFKA_FETCHER_IS_SHARDING false   - - createTopicIfNotExist If true, create the Kafka topic when it does not exist. - true   - - partitions The number of partitions for the topic being created. SW_KAFKA_FETCHER_PARTITIONS 3   - - enableMeterSystem To enable to fetch and handle Meter System data. SW_KAFKA_FETCHER_ENABLE_METER_SYSTEM false   - - enableLog To enable to fetch and handle log data. SW_KAFKA_FETCHER_ENABLE_LOG false   - - replicationFactor The replication factor for each partition in the topic being created. SW_KAFKA_FETCHER_PARTITIONS_FACTOR 2   - - kafkaHandlerThreadPoolSize Pool size of kafka message handler executor. SW_KAFKA_HANDLER_THREAD_POOL_SIZE CPU core * 2   - - kafkaHandlerThreadPoolQueueSize The queue size of kafka message handler executor. SW_KAFKA_HANDLER_THREAD_POOL_QUEUE_SIZE 10000   - - topicNameOfMeters Specifying Kafka topic name for Meter system data. - skywalking-meters   - - topicNameOfMetrics Specifying Kafka topic name for JVM Metrics data. - skywalking-metrics   - - topicNameOfProfiling Specifying Kafka topic name for Profiling data. - skywalking-profilings   - - topicNameOfTracingSegments Specifying Kafka topic name for Tracing data. - skywalking-segments   - - topicNameOfManagements Specifying Kafka topic name for service instance reporting and registering. - skywalking-managements   - - topicNameOfLogs Specifying Kafka topic name for log data. - skywalking-logs   receiver-browser default Read receiver doc for more details - - -   - - sampleRate Sampling rate for receiving trace. The precision is 1/10000. 10000 means 100% sample in default. SW_RECEIVER_BROWSER_SAMPLE_RATE 10000   query graphql - GraphQL query implementation -    - - path Root path of GraphQL query and mutation. SW_QUERY_GRAPHQL_PATH /graphql   alarm default - Read alarm doc for more details. -    telemetry - - Read telemetry doc for more details. -    - none - No op implementation -    - prometheus host Binding host for Prometheus server fetching data SW_TELEMETRY_PROMETHEUS_HOST 0.0.0.0   - - port Binding port for Prometheus server fetching data SW_TELEMETRY_PROMETHEUS_PORT 1234   configuration - - Read dynamic configuration doc for more details. -    - grpc host DCS server binding hostname SW_DCS_SERVER_HOST -   - - port DCS server binding port SW_DCS_SERVER_PORT 80   - - clusterName Cluster name when reading latest configuration from DSC server. SW_DCS_CLUSTER_NAME SkyWalking   - - period The period of OAP reading data from DSC server. Unit is second. SW_DCS_PERIOD 20   - apollo apolloMeta apollo.meta in Apollo SW_CONFIG_APOLLO http://106.12.25.204:8080   - - apolloCluster apollo.cluster in Apollo SW_CONFIG_APOLLO_CLUSTER default   - - apolloEnv env in Apollo SW_CONFIG_APOLLO_ENV -   - - appId app.id in Apollo SW_CONFIG_APOLLO_APP_ID skywalking   - - period The period of data sync. Unit is second. SW_CONFIG_APOLLO_PERIOD 60   - zookeeper nameSpace The namespace, represented by root path, isolates the configurations in the zookeeper. SW_CONFIG_ZK_NAMESPACE /, root path   - - hostPort hosts and ports of Zookeeper Cluster SW_CONFIG_ZK_HOST_PORT localhost:2181   - - baseSleepTimeMs The period of Zookeeper client between two retries. Unit is ms. SW_CONFIG_ZK_BASE_SLEEP_TIME_MS 1000   - - maxRetries The max retry time of re-trying. SW_CONFIG_ZK_MAX_RETRIES 3   - - period The period of data sync. Unit is second. SW_CONFIG_ZK_PERIOD 60   - etcd clusterName Service name used for SkyWalking cluster. SW_CONFIG_ETCD_CLUSTER_NAME default   - - serverAddr hosts and ports used of etcd cluster. SW_CONFIG_ETCD_SERVER_ADDR localhost:2379   - - group Additional prefix of the configuration key SW_CONFIG_ETCD_GROUP skywalking   - - period The period of data sync. Unit is second. SW_CONFIG_ZK_PERIOD 60   - consul hostPort hosts and ports used of Consul cluster. SW_CONFIG_CONSUL_HOST_AND_PORTS localhost:8500   - - aclToken ALC Token of Consul. Empty string means without ALC token. SW_CONFIG_CONSUL_ACL_TOKEN -   - - period The period of data sync. Unit is second. SW_CONFIG_CONSUL_PERIOD 60   - k8s-configmap namespace Deployment namespace of the config map. SW_CLUSTER_K8S_NAMESPACE default   - - labelSelector Labels used for locating configmap. SW_CLUSTER_K8S_LABEL app=collector,release=skywalking   - - period The period of data sync. Unit is second. SW_CONFIG_ZK_PERIOD 60   - nacos serverAddr Nacos Server Host SW_CONFIG_NACOS_SERVER_ADDR 127.0.0.1   - - port Nacos Server Port SW_CONFIG_NACOS_SERVER_PORT 8848   - - group Nacos Configuration namespace SW_CONFIG_NACOS_SERVER_NAMESPACE -   - - period The period of data sync. Unit is second. SW_CONFIG_CONFIG_NACOS_PERIOD 60   - - username Nacos Auth username SW_CONFIG_NACOS_USERNAME -   - - password Nacos Auth password SW_CONFIG_NACOS_PASSWORD -   - - accessKey Nacos Auth accessKey SW_CONFIG_NACOS_ACCESSKEY -   - - secretKey Nacos Auth secretKey SW_CONFIG_NACOS_SECRETKEY -   exporter grpc targetHost The host of target grpc server for receiving export data. SW_EXPORTER_GRPC_HOST 127.0.0.1   - - targetPort The port of target grpc server for receiving export data. SW_EXPORTER_GRPC_PORT 9870   health-checker default checkIntervalSeconds The period of check OAP internal health status. Unit is second. SW_HEALTH_CHECKER_INTERVAL_SECONDS 5   configuration-discovery default disableMessageDigest If true, agent receives the latest configuration every time even without change. In default, OAP uses SHA512 message digest mechanism to detect changes of configuration. SW_DISABLE_MESSAGE_DIGEST false   receiver-event default Read receiver doc for more details - -     Notice ¹ System Environment Variable name could be declared and changed in the application.yml. The names listed here, are just provided in the default application.yml file.\n","excerpt":"Configuration Vocabulary Configuration Vocabulary lists all available configurations provided by …","ref":"/docs/main/v8.5.0/en/setup/backend/configuration-vocabulary/","title":"Configuration Vocabulary"},{"body":" Dependency the toolkit, such as using maven or gradle  \u0026lt;dependency\u0026gt; \u0026lt;groupId\u0026gt;org.apache.skywalking\u0026lt;/groupId\u0026gt; \u0026lt;artifactId\u0026gt;apm-toolkit-log4j-1.x\u0026lt;/artifactId\u0026gt; \u0026lt;version\u0026gt;{project.release.version}\u0026lt;/version\u0026gt; \u0026lt;/dependency\u0026gt; Print trace ID in your logs  Config a layout  log4j.appender.CONSOLE.layout=TraceIdPatternLayout  set %T in layout.ConversionPattern ( In 2.0-2016, you should use %x, Why change? )  log4j.appender.CONSOLE.layout.ConversionPattern=%d [%T] %-5p %c{1}:%L - %m%n  When you use -javaagent to active the sky-walking tracer, log4j will output traceId, if it existed. If the tracer is inactive, the output will be TID: N/A.  gRPC reporter The gRPC report could forward the collected logs to SkyWalking OAP server, or SkyWalking Satellite sidecar. Trace id, segment id, and span id will attach to logs automatically. You don\u0026rsquo;t need to change the layout.\n Add GRPCLogClientAppender in log4j.properties  log4j.rootLogger=INFO,CustomAppender log4j.appender.CustomAppender=org.apache.skywalking.apm.toolkit.log.log4j.v1.x.log.GRPCLogClientAppender log4j.appender.CustomAppender.layout=org.apache.log4j.PatternLayout log4j.appender.CustomAppender.layout.ConversionPattern=[%t] %-5p %c %x - %m%n  Add config of the plugin or use default  plugin.toolkit.log.grpc.reporter.server_host=${SW_GRPC_LOG_SERVER_HOST:127.0.0.1} plugin.toolkit.log.grpc.reporter.server_port=${SW_GRPC_LOG_SERVER_PORT:11800} plugin.toolkit.log.grpc.reporter.max_message_size=${SW_GRPC_LOG_MAX_MESSAGE_SIZE:10485760} plugin.toolkit.log.grpc.reporter.upstream_timeout=${SW_GRPC_LOG_GRPC_UPSTREAM_TIMEOUT:30} ","excerpt":"Dependency the toolkit, such as using maven or gradle  \u0026lt;dependency\u0026gt; …","ref":"/docs/main/v8.5.0/en/setup/service-agent/java-agent/application-toolkit-log4j-1.x/","title":"Dependency the toolkit, such as using maven or gradle"},{"body":" Dependency the toolkit, such as using maven or gradle  \u0026lt;dependency\u0026gt; \u0026lt;groupId\u0026gt;org.apache.skywalking\u0026lt;/groupId\u0026gt; \u0026lt;artifactId\u0026gt;apm-toolkit-log4j-2.x\u0026lt;/artifactId\u0026gt; \u0026lt;version\u0026gt;{project.release.version}\u0026lt;/version\u0026gt; \u0026lt;/dependency\u0026gt; Print trace ID in your logs  Config the [%traceId] pattern in your log4j2.xml  \u0026lt;Appenders\u0026gt; \u0026lt;Console name=\u0026#34;Console\u0026#34; target=\u0026#34;SYSTEM_OUT\u0026#34;\u0026gt; \u0026lt;PatternLayout pattern=\u0026#34;%d [%traceId] %-5p %c{1}:%L - %m%n\u0026#34;/\u0026gt; \u0026lt;/Console\u0026gt; \u0026lt;/Appenders\u0026gt;  Support log4j2 AsyncRoot , No additional configuration is required. Refer to the demo of log4j2.xml below. For details: Log4j2 Async Loggers  \u0026lt;Configuration\u0026gt; \u0026lt;Appenders\u0026gt; \u0026lt;Console name=\u0026#34;Console\u0026#34; target=\u0026#34;SYSTEM_OUT\u0026#34;\u0026gt; \u0026lt;PatternLayout pattern=\u0026#34;%d [%traceId] %-5p %c{1}:%L - %m%n\u0026#34;/\u0026gt; \u0026lt;/Console\u0026gt; \u0026lt;/Appenders\u0026gt; \u0026lt;Loggers\u0026gt; \u0026lt;AsyncRoot level=\u0026#34;INFO\u0026#34;\u0026gt; \u0026lt;AppenderRef ref=\u0026#34;Console\u0026#34;/\u0026gt; \u0026lt;/AsyncRoot\u0026gt; \u0026lt;/Loggers\u0026gt; \u0026lt;/Configuration\u0026gt;   Support log4j2 AsyncAppender , No additional configuration is required. Refer to the demo of log4j2.xml below.\nFor details: All Loggers Async\nLog4j-2.9 and higher require disruptor-3.3.4.jar or higher on the classpath. Prior to Log4j-2.9, disruptor-3.0.0.jar or higher was required. This is simplest to configure and gives the best performance. To make all loggers asynchronous, add the disruptor jar to the classpath and set the system property log4j2.contextSelector to org.apache.logging.log4j.core.async.AsyncLoggerContextSelector.\n\u0026lt;Configuration status=\u0026#34;WARN\u0026#34;\u0026gt; \u0026lt;Appenders\u0026gt; \u0026lt;!-- Async Loggers will auto-flush in batches, so switch off immediateFlush. --\u0026gt; \u0026lt;RandomAccessFile name=\u0026#34;RandomAccessFile\u0026#34; fileName=\u0026#34;async.log\u0026#34; immediateFlush=\u0026#34;false\u0026#34; append=\u0026#34;false\u0026#34;\u0026gt; \u0026lt;PatternLayout\u0026gt; \u0026lt;Pattern\u0026gt;%d %p %c{1.} [%t] [%traceId] %m %ex%n\u0026lt;/Pattern\u0026gt; \u0026lt;/PatternLayout\u0026gt; \u0026lt;/RandomAccessFile\u0026gt; \u0026lt;/Appenders\u0026gt; \u0026lt;Loggers\u0026gt; \u0026lt;Root level=\u0026#34;info\u0026#34; includeLocation=\u0026#34;false\u0026#34;\u0026gt; \u0026lt;AppenderRef ref=\u0026#34;RandomAccessFile\u0026#34;/\u0026gt; \u0026lt;/Root\u0026gt; \u0026lt;/Loggers\u0026gt; \u0026lt;/Configuration\u0026gt; For details: Mixed Sync \u0026amp; Async\nLog4j-2.9 and higher require disruptor-3.3.4.jar or higher on the classpath. Prior to Log4j-2.9, disruptor-3.0.0.jar or higher was required. There is no need to set system property Log4jContextSelector to any value.\n\u0026lt;Configuration status=\u0026#34;WARN\u0026#34;\u0026gt; \u0026lt;Appenders\u0026gt; \u0026lt;!-- Async Loggers will auto-flush in batches, so switch off immediateFlush. --\u0026gt; \u0026lt;RandomAccessFile name=\u0026#34;RandomAccessFile\u0026#34; fileName=\u0026#34;asyncWithLocation.log\u0026#34; immediateFlush=\u0026#34;false\u0026#34; append=\u0026#34;false\u0026#34;\u0026gt; \u0026lt;PatternLayout\u0026gt; \u0026lt;Pattern\u0026gt;%d %p %class{1.} [%t] [%traceId] %location %m %ex%n\u0026lt;/Pattern\u0026gt; \u0026lt;/PatternLayout\u0026gt; \u0026lt;/RandomAccessFile\u0026gt; \u0026lt;/Appenders\u0026gt; \u0026lt;Loggers\u0026gt; \u0026lt;!-- pattern layout actually uses location, so we need to include it --\u0026gt; \u0026lt;AsyncLogger name=\u0026#34;com.foo.Bar\u0026#34; level=\u0026#34;trace\u0026#34; includeLocation=\u0026#34;true\u0026#34;\u0026gt; \u0026lt;AppenderRef ref=\u0026#34;RandomAccessFile\u0026#34;/\u0026gt; \u0026lt;/AsyncLogger\u0026gt; \u0026lt;Root level=\u0026#34;info\u0026#34; includeLocation=\u0026#34;true\u0026#34;\u0026gt; \u0026lt;AppenderRef ref=\u0026#34;RandomAccessFile\u0026#34;/\u0026gt; \u0026lt;/Root\u0026gt; \u0026lt;/Loggers\u0026gt; \u0026lt;/Configuration\u0026gt;   Support log4j2 AsyncAppender, For details: Log4j2 AsyncAppender\n  \u0026lt;Configuration\u0026gt; \u0026lt;Appenders\u0026gt; \u0026lt;Console name=\u0026#34;Console\u0026#34; target=\u0026#34;SYSTEM_OUT\u0026#34;\u0026gt; \u0026lt;PatternLayout pattern=\u0026#34;%d [%traceId] %-5p %c{1}:%L - %m%n\u0026#34;/\u0026gt; \u0026lt;/Console\u0026gt; \u0026lt;Async name=\u0026#34;Async\u0026#34;\u0026gt; \u0026lt;AppenderRef ref=\u0026#34;Console\u0026#34;/\u0026gt; \u0026lt;/Async\u0026gt; \u0026lt;/Appenders\u0026gt; \u0026lt;Loggers\u0026gt; \u0026lt;Root level=\u0026#34;INFO\u0026#34;\u0026gt; \u0026lt;AppenderRef ref=\u0026#34;Async\u0026#34;/\u0026gt; \u0026lt;/Root\u0026gt; \u0026lt;/Loggers\u0026gt; \u0026lt;/Configuration\u0026gt;  When you use -javaagent to active the sky-walking tracer, log4j2 will output traceId, if it existed. If the tracer is inactive, the output will be TID: N/A.  gRPC reporter The gRPC report could forward the collected logs to SkyWalking OAP server, or SkyWalking Satellite sidecar. Trace id, segment id, and span id will attach to logs automatically. You don\u0026rsquo;t need to change the layout.\n Add GRPCLogClientAppender in log4j2.xml  \u0026lt;GRPCLogClientAppender name=\u0026#34;grpc-log\u0026#34;\u0026gt; \u0026lt;PatternLayout pattern=\u0026#34;%d{HH:mm:ss.SSS} [%t] %-5level %logger{36} - %msg%n\u0026#34;/\u0026gt; \u0026lt;/GRPCLogClientAppender\u0026gt;  Add config of the plugin or use default  plugin.toolkit.log.grpc.reporter.server_host=${SW_GRPC_LOG_SERVER_HOST:127.0.0.1} plugin.toolkit.log.grpc.reporter.server_port=${SW_GRPC_LOG_SERVER_PORT:11800} plugin.toolkit.log.grpc.reporter.max_message_size=${SW_GRPC_LOG_MAX_MESSAGE_SIZE:10485760} plugin.toolkit.log.grpc.reporter.upstream_timeout=${SW_GRPC_LOG_GRPC_UPSTREAM_TIMEOUT:30} Transmitting un-formatted messages The log4j 2.x gRPC reporter supports transmitting logs as formatted or un-formatted. Transmitting formatted data is the default but can be disabled by adding the following to the agent config:\nplugin.toolkit.log.transmit_formatted=false The above will result in the content field being used for the log pattern with additional log tags of argument.0, argument.1, and so on representing each logged argument as well as an additional exception tag which is only present if a throwable is also logged.\nFor example, the following code:\nlog.info(\u0026#34;{} {} {}\u0026#34;, 1, 2, 3); Will result in:\n{ \u0026#34;content\u0026#34;: \u0026#34;{} {} {}\u0026#34;, \u0026#34;tags\u0026#34;: [ { \u0026#34;key\u0026#34;: \u0026#34;argument.0\u0026#34;, \u0026#34;value\u0026#34;: \u0026#34;1\u0026#34; }, { \u0026#34;key\u0026#34;: \u0026#34;argument.1\u0026#34;, \u0026#34;value\u0026#34;: \u0026#34;2\u0026#34; }, { \u0026#34;key\u0026#34;: \u0026#34;argument.2\u0026#34;, \u0026#34;value\u0026#34;: \u0026#34;3\u0026#34; } ] } ","excerpt":"Dependency the toolkit, such as using maven or gradle  \u0026lt;dependency\u0026gt; …","ref":"/docs/main/v8.5.0/en/setup/service-agent/java-agent/application-toolkit-log4j-2.x/","title":"Dependency the toolkit, such as using maven or gradle"},{"body":" Dependency the toolkit, such as using maven or gradle  \u0026lt;dependency\u0026gt; \u0026lt;groupId\u0026gt;org.apache.skywalking\u0026lt;/groupId\u0026gt; \u0026lt;artifactId\u0026gt;apm-toolkit-meter\u0026lt;/artifactId\u0026gt; \u0026lt;version\u0026gt;${skywalking.version}\u0026lt;/version\u0026gt; \u0026lt;/dependency\u0026gt; If you\u0026rsquo;re using Spring sleuth, you could use Spring Sleuth Setup.\n Counter API represents a single monotonically increasing counter, automatic collect data and report to backend.  import org.apache.skywalking.apm.toolkit.meter.MeterFactory; Counter counter = MeterFactory.counter(meterName).tag(\u0026#34;tagKey\u0026#34;, \u0026#34;tagValue\u0026#34;).mode(Counter.Mode.INCREMENT).build(); counter.increment(1d);  MeterFactory.counter Create a new counter builder with the meter name. Counter.Builder.tag(String key, String value) Mark a tag key/value pair. Counter.Builder.mode(Counter.Mode mode) Change the counter mode, RATE mode means reporting rate to the backend. Counter.Builder.build() Build a new Counter which is collected and reported to the backend. Counter.increment(double count) Increment count to the Counter, It could be a positive value.   Gauge API represents a single numerical value.  import org.apache.skywalking.apm.toolkit.meter.MeterFactory; ThreadPoolExecutor threadPool = ...; Gauge gauge = MeterFactory.gauge(meterName, () -\u0026gt; threadPool.getActiveCount()).tag(\u0026#34;tagKey\u0026#34;, \u0026#34;tagValue\u0026#34;).build();  MeterFactory.gauge(String name, Supplier\u0026lt;Double\u0026gt; getter) Create a new gauge builder with the meter name and supplier function, this function need to return a double value. Gauge.Builder.tag(String key, String value) Mark a tag key/value pair. Gauge.Builder.build() Build a new Gauge which is collected and reported to the backend.   Histogram API represents a summary sample observations with customize buckets.  import org.apache.skywalking.apm.toolkit.meter.MeterFactory; Histogram histogram = MeterFactory.histogram(\u0026#34;test\u0026#34;).tag(\u0026#34;tagKey\u0026#34;, \u0026#34;tagValue\u0026#34;).steps(Arrays.asList(1, 5, 10)).minValue(0).build(); histogram.addValue(3);  MeterFactory.histogram(String name) Create a new histogram builder with the meter name. Histogram.Builder.tag(String key, String value) Mark a tag key/value pair. Histogram.Builder.steps(List\u0026lt;Double\u0026gt; steps) Set up the max values of every histogram buckets. Histogram.Builder.minValue(double value) Set up the minimal value of this histogram, default is 0. Histogram.Builder.build() Build a new Histogram which is collected and reported to the backend. Histogram.addValue(double value) Add value into the histogram, automatically analyze what bucket count needs to be increment. rule: count into [step1, step2).  ","excerpt":"Dependency the toolkit, such as using maven or gradle  \u0026lt;dependency\u0026gt; …","ref":"/docs/main/v8.5.0/en/setup/service-agent/java-agent/application-toolkit-meter/","title":"Dependency the toolkit, such as using maven or gradle"},{"body":" Dependency the toolkit, such as using maven or gradle  \u0026lt;dependency\u0026gt; \u0026lt;groupId\u0026gt;org.apache.skywalking\u0026lt;/groupId\u0026gt; \u0026lt;artifactId\u0026gt;apm-toolkit-micrometer-registry\u0026lt;/artifactId\u0026gt; \u0026lt;version\u0026gt;${skywalking.version}\u0026lt;/version\u0026gt; \u0026lt;/dependency\u0026gt;  Using org.apache.skywalking.apm.meter.micrometer.SkywalkingMeterRegistry as the registry, it could forward the MicroMeter collected metrics to OAP server.  import org.apache.skywalking.apm.meter.micrometer.SkywalkingMeterRegistry; SkywalkingMeterRegistry registry = new SkywalkingMeterRegistry(); // If you has some counter want to rate by agent side SkywalkingConfig config = new SkywalkingConfig(Arrays.asList(\u0026#34;test_rate_counter\u0026#34;)); new SkywalkingMeterRegistry(config); // Also you could using composite registry to combine multiple meter registry, such as collect to Skywalking and prometheus CompositeMeterRegistry compositeRegistry = new CompositeMeterRegistry(); compositeRegistry.add(new PrometheusMeterRegistry(PrometheusConfig.DEFAULT)); compositeRegistry.add(new SkywalkingMeterRegistry());   Using snake case as the naming convention. Such as test.meter will be send to test_meter.\n  Using Millisecond as the time unit.\n  Adapt micrometer data convention.\n     Micrometer data type Transform to meter name Skywalking data type Description     Counter Counter name Counter Same with counter   Gauges Gauges name Gauges Same with gauges   Timer Timer name + \u0026ldquo;_count\u0026rdquo; Counter Execute finished count    Timer name + \u0026ldquo;_sum\u0026rdquo; Counter Total execute finished duration    Timer name + \u0026ldquo;_max\u0026rdquo; Gauges Max duration of execute finished time    Timer name + \u0026ldquo;_histogram\u0026rdquo; Histogram Histogram of execute finished duration   LongTaskTimer Timer name + \u0026ldquo;_active_count\u0026rdquo; Gauges Executing task count    Timer name + \u0026ldquo;_duration_sum\u0026rdquo; Counter All of executing task sum duration    Timer name + \u0026ldquo;_max\u0026rdquo; Counter Current longest running task execute duration   Function Timer Timer name + \u0026ldquo;_count\u0026rdquo; Gauges Execute finished timer count    Timer name + \u0026ldquo;_sum\u0026rdquo; Gauges Execute finished timer total duration   Function Counter Counter name Counter Custom counter value   Distribution summary Summary name + \u0026ldquo;_count\u0026rdquo; Counter Total record count    Summary name + \u0026ldquo;_sum\u0026rdquo; Counter Total record amount sum    Summary name + \u0026ldquo;_max\u0026rdquo; Gauges Max record amount    Summary name + \u0026ldquo;_histogram\u0026rdquo; Gauges Histogram of the amount     Not Adapt data convention.     Micrometer data type Data type     LongTaskTimer Histogram    ","excerpt":"Dependency the toolkit, such as using maven or gradle  \u0026lt;dependency\u0026gt; …","ref":"/docs/main/v8.5.0/en/setup/service-agent/java-agent/application-toolkit-micrometer/","title":"Dependency the toolkit, such as using maven or gradle"},{"body":" Dependency the toolkit, such as using maven or gradle  \u0026lt;dependency\u0026gt; \u0026lt;groupId\u0026gt;org.apache.skywalking\u0026lt;/groupId\u0026gt; \u0026lt;artifactId\u0026gt;apm-toolkit-trace\u0026lt;/artifactId\u0026gt; \u0026lt;version\u0026gt;${skywalking.version}\u0026lt;/version\u0026gt; \u0026lt;/dependency\u0026gt;  Use TraceContext.traceId() API to obtain traceId.  import TraceContext; ... modelAndView.addObject(\u0026#34;traceId\u0026#34;, TraceContext.traceId());  Use TraceContext.segmentId() API to obtain segmentId.  import TraceContext; ... modelAndView.addObject(\u0026#34;segmentId\u0026#34;, TraceContext.segmentId());  Use TraceContext.spanId() API to obtain spanId.  import TraceContext; ... modelAndView.addObject(\u0026#34;spanId\u0026#34;, TraceContext.spanId()); Sample codes only\n  Add @Trace to any method you want to trace. After that, you can see the span in the Stack.\n  Methods annotated with @Tag will try to tag the current active span with the given key (Tag#key()) and (Tag#value()), if there is no active span at all, this annotation takes no effect. @Tag can be repeated, and can be used in companion with @Trace, see examples below. The value of Tag is the same as what are supported in Customize Enhance Trace.\n  Add custom tag in the context of traced method, ActiveSpan.tag(\u0026quot;key\u0026quot;, \u0026quot;val\u0026quot;).\n  ActiveSpan.error() Mark the current span as error status.\n  ActiveSpan.error(String errorMsg) Mark the current span as error status with a message.\n  ActiveSpan.error(Throwable throwable) Mark the current span as error status with a Throwable.\n  ActiveSpan.debug(String debugMsg) Add a debug level log message in the current span.\n  ActiveSpan.info(String infoMsg) Add an info level log message in the current span.\n  ActiveSpan.setOperationName(String operationName) Customize an operation name.\n  ActiveSpan.tag(\u0026#34;my_tag\u0026#34;, \u0026#34;my_value\u0026#34;); ActiveSpan.error(); ActiveSpan.error(\u0026#34;Test-Error-Reason\u0026#34;); ActiveSpan.error(new RuntimeException(\u0026#34;Test-Error-Throwable\u0026#34;)); ActiveSpan.info(\u0026#34;Test-Info-Msg\u0026#34;); ActiveSpan.debug(\u0026#34;Test-debug-Msg\u0026#34;); /** * The codes below will generate a span, * and two types of tags, one type tag: keys are `tag1` and `tag2`, values are the passed-in parameters, respectively, the other type tag: keys are `username` and `age`, values are the return value in User, respectively */ @Trace @Tag(key = \u0026#34;tag1\u0026#34;, value = \u0026#34;arg[0]\u0026#34;) @Tag(key = \u0026#34;tag2\u0026#34;, value = \u0026#34;arg[1]\u0026#34;) @Tag(key = \u0026#34;username\u0026#34;, value = \u0026#34;returnedObj.username\u0026#34;) @Tag(key = \u0026#34;age\u0026#34;, value = \u0026#34;returnedObj.age\u0026#34;) public User methodYouWantToTrace(String param1, String param2) { // ActiveSpan.setOperationName(\u0026#34;Customize your own operation name, if this is an entry span, this would be an endpoint name\u0026#34;);  // ... }  Use TraceContext.putCorrelation() API to put custom data in tracing context.  Optional\u0026lt;String\u0026gt; previous = TraceContext.putCorrelation(\u0026#34;customKey\u0026#34;, \u0026#34;customValue\u0026#34;); CorrelationContext will remove the item when the value is null or empty.\n Use TraceContext.getCorrelation() API to get custom data.  Optional\u0026lt;String\u0026gt; value = TraceContext.getCorrelation(\u0026#34;customKey\u0026#34;); CorrelationContext configuration descriptions could be found in the agent configuration documentation, with correlation. as the prefix.\n","excerpt":"Dependency the toolkit, such as using maven or gradle  \u0026lt;dependency\u0026gt; …","ref":"/docs/main/v8.5.0/en/setup/service-agent/java-agent/application-toolkit-trace/","title":"Dependency the toolkit, such as using maven or gradle"},{"body":" Dependency the toolkit, such as using maven or gradle  \u0026lt;dependency\u0026gt; \u0026lt;groupId\u0026gt;org.apache.skywalking\u0026lt;/groupId\u0026gt; \u0026lt;artifactId\u0026gt;apm-toolkit-opentracing\u0026lt;/artifactId\u0026gt; \u0026lt;version\u0026gt;{project.release.version}\u0026lt;/version\u0026gt; \u0026lt;/dependency\u0026gt;  Use our OpenTracing tracer implementation  Tracer tracer = new SkywalkingTracer(); Tracer.SpanBuilder spanBuilder = tracer.buildSpan(\u0026#34;/yourApplication/yourService\u0026#34;); ","excerpt":" Dependency the toolkit, such as using maven or gradle  \u0026lt;dependency\u0026gt; …","ref":"/docs/main/v8.5.0/en/setup/service-agent/java-agent/opentracing/","title":"Dependency the toolkit, such as using maven or gradle"},{"body":"Deploy SkyWalking backend and UI in kubernetes Follow instructions in the deploying SkyWalking backend to Kubernetes cluster to deploy oap and ui to a kubernetes cluster.\nPlease read the Readme file.\n","excerpt":"Deploy SkyWalking backend and UI in kubernetes Follow instructions in the deploying SkyWalking …","ref":"/docs/main/v8.5.0/en/setup/backend/backend-k8s/","title":"Deploy SkyWalking backend and UI in kubernetes"},{"body":"Design Goals This document outlines the core design goals for the SkyWalking project.\n  Maintaining Observability. Regardless of the deployment method of the target system, SkyWalking provides an integration solution for it to maintain observability. Based on this, SkyWalking provides multiple runtime forms and probes.\n  Topology, Metrics and Trace Together. The first step to understanding a distributed system is the topology map. It visualizes the entire complex system in an easy-to-read layout. Under the topology, the OSS personnel have higher requirements in terms of the metrics for service, instance, endpoint and calls. Traces are in the form of detailed logs to make sense of those metrics. For example, when the endpoint latency becomes long, you want to see the slowest the trace to find out why. So you can see, they are from big picture to details, they are all needed. SkyWalking integrates and provides a lot of features to make this possible and easy understand.\n  Light Weight. There two parts of light weight are needed. (1) In probe, we just depend on network communication framework, prefer gRPC. By that, the probe should be as small as possible, to avoid the library conflicts and the payload of VM, such as permsize requirement in JVM. (2) As an observability platform, it is secondary and third level system in your project environment. So we are using our own light weight framework to build the backend core. Then you don\u0026rsquo;t need to deploy big data tech platform and maintain them. SkyWalking should be simple in tech stack.\n  Pluggable. SkyWalking core team provides many default implementations, but definitely it is not enough, and also don\u0026rsquo;t fit every scenario. So, we provide a lot of features for being pluggable.\n  Portability. SkyWalking can run in multiple environments, including: (1) Use traditional register center like eureka. (2) Use RPC framework including service discovery, like Spring Cloud, Apache Dubbo. (3) Use Service Mesh in modern infrastructure. (4) Use cloud services. (5) Across cloud deployment. SkyWalking should run well in all of these cases.\n  Interoperability. The observability landscape is so vast that it is virtually impossible for SkyWalking to support all systems, even with the support of its community. Currently, it supports interoperability with other OSS systems, especially probes, such as Zipkin, Jaeger, OpenTracing, and OpenCensus. It is very important to end users that SkyWalking has the ability to accept and read these data formats, since the users are not required to switch their libraries.\n  What is next?  See probe Introduction to learn about SkyWalking\u0026rsquo;s probe groups. From backend overview, you can understand what the backend does after it receives probe data. If you want to customize the UI, start with the UI overview document.  ","excerpt":"Design Goals This document outlines the core design goals for the SkyWalking project.\n  Maintaining …","ref":"/docs/main/v8.5.0/en/concepts-and-designs/project-goals/","title":"Design Goals"},{"body":"Disable plugins Delete or remove the specific libraries / jars in skywalking-agent/plugins/*.jar\n+-- skywalking-agent +-- activations apm-toolkit-log4j-1.x-activation.jar apm-toolkit-log4j-2.x-activation.jar apm-toolkit-logback-1.x-activation.jar ... +-- config agent.config +-- plugins apm-dubbo-plugin.jar apm-feign-default-http-9.x.jar apm-httpClient-4.x-plugin.jar ..... skywalking-agent.jar ","excerpt":"Disable plugins Delete or remove the specific libraries / jars in skywalking-agent/plugins/*.jar\n+-- …","ref":"/docs/main/v8.5.0/en/setup/service-agent/java-agent/how-to-disable-plugin/","title":"Disable plugins"},{"body":"Dynamic Configuration SkyWalking Configurations mostly are set through application.yml and OS system environment variables. At the same time, some of them are supporting dynamic settings from upstream management system.\nRight now, SkyWalking supports following dynamic configurations.\n   Config Key Value Description Value Format Example     agent-analyzer.default.slowDBAccessThreshold Thresholds of slow Database statement, override receiver-trace/default/slowDBAccessThreshold of application.yml. default:200,mongodb:50   agent-analyzer.default.uninstrumentedGateways The uninstrumented gateways, override gateways.yml. same as gateways.yml   alarm.default.alarm-settings The alarm settings, will override alarm-settings.yml. same as alarm-settings.yml   core.default.apdexThreshold The apdex threshold settings, will override service-apdex-threshold.yml. same as service-apdex-threshold.yml   core.default.endpoint-name-grouping The endpoint name grouping setting, will override endpoint-name-grouping.yml. same as endpoint-name-grouping.yml   agent-analyzer.default.sampleRate Trace sampling , override receiver-trace/default/sampleRate of application.yml. 10000   agent-analyzer.default.slowTraceSegmentThreshold Setting this threshold about the latency would make the slow trace segments sampled if they cost more time, even the sampling mechanism activated. The default value is -1, which means would not sample slow traces. Unit, millisecond. override receiver-trace/default/slowTraceSegmentThreshold of application.yml. -1   configuration-discovery.default.agentConfigurations The ConfigurationDiscovery settings look at configuration-discovery.md    This feature depends on upstream service, so it is DISABLED by default.\nconfiguration: selector: ${SW_CONFIGURATION:none} none: grpc: host: ${SW_DCS_SERVER_HOST:\u0026#34;\u0026#34;} port: ${SW_DCS_SERVER_PORT:80} clusterName: ${SW_DCS_CLUSTER_NAME:SkyWalking} period: ${SW_DCS_PERIOD:20} # ... other implementations Dynamic Configuration Service, DCS Dynamic Configuration Service is a gRPC service, which requires the upstream system implemented. The SkyWalking OAP fetches the configuration from the implementation(any system), after you open this implementation like this.\nconfiguration: selector: ${SW_CONFIGURATION:grpc} grpc: host: ${SW_DCS_SERVER_HOST:\u0026#34;\u0026#34;} port: ${SW_DCS_SERVER_PORT:80} clusterName: ${SW_DCS_CLUSTER_NAME:SkyWalking} period: ${SW_DCS_PERIOD:20} Dynamic Configuration Zookeeper Implementation Zookeeper is also supported as DCC(Dynamic Configuration Center), to use it, please configure as follows:\nconfiguration: selector: ${SW_CONFIGURATION:zookeeper} zookeeper: period: ${SW_CONFIG_ZK_PERIOD:60} # Unit seconds, sync period. Default fetch every 60 seconds. nameSpace: ${SW_CONFIG_ZK_NAMESPACE:/default} hostPort: ${SW_CONFIG_ZK_HOST_PORT:localhost:2181} # Retry Policy baseSleepTimeMs: ${SW_CONFIG_ZK_BASE_SLEEP_TIME_MS:1000} # initial amount of time to wait between retries maxRetries: ${SW_CONFIG_ZK_MAX_RETRIES:3} # max number of times to retry The nameSpace is the ZooKeeper path. The config key and value are the properties of the namespace folder.\nDynamic Configuration Etcd Implementation Etcd is also supported as DCC(Dynamic Configuration Center), to use it, please configure as follows:\nconfiguration: selector: ${SW_CONFIGURATION:etcd} etcd: period: ${SW_CONFIG_ETCD_PERIOD:60} # Unit seconds, sync period. Default fetch every 60 seconds. group: ${SW_CONFIG_ETCD_GROUP:skywalking} serverAddr: ${SW_CONFIG_ETCD_SERVER_ADDR:localhost:2379} clusterName: ${SW_CONFIG_ETCD_CLUSTER_NAME:default} Dynamic Configuration Consul Implementation Consul is also supported as DCC(Dynamic Configuration Center), to use it, please configure as follows:\nconfiguration: selector: ${SW_CONFIGURATION:consul} consul: # Consul host and ports, separated by comma, e.g. 1.2.3.4:8500,2.3.4.5:8500 hostAndPorts: ${SW_CONFIG_CONSUL_HOST_AND_PORTS:1.2.3.4:8500} # Sync period in seconds. Defaults to 60 seconds. period: ${SW_CONFIG_CONSUL_PERIOD:1} # Consul aclToken aclToken: ${SW_CONFIG_CONSUL_ACL_TOKEN:\u0026#34;\u0026#34;} Dynamic Configuration Apollo Implementation Apollo is also supported as DCC(Dynamic Configuration Center), to use it, just configured as follows:\nconfiguration: selector: ${SW_CONFIGURATION:apollo} apollo: apolloMeta: ${SW_CONFIG_APOLLO:http://106.12.25.204:8080} apolloCluster: ${SW_CONFIG_APOLLO_CLUSTER:default} apolloEnv: ${SW_CONFIG_APOLLO_ENV:\u0026#34;\u0026#34;} appId: ${SW_CONFIG_APOLLO_APP_ID:skywalking} period: ${SW_CONFIG_APOLLO_PERIOD:5} Dynamic Configuration Kuberbetes Configmap Implementation configmap is also supported as DCC(Dynamic Configuration Center), to use it, just configured as follows:\nconfiguration: selector: ${SW_CONFIGURATION:k8s-configmap} # [example] (../../../../oap-server/server-configuration/configuration-k8s-configmap/src/test/resources/skywalking-dynamic-configmap.example.yaml) k8s-configmap: # Sync period in seconds. Defaults to 60 seconds. period: ${SW_CONFIG_CONFIGMAP_PERIOD:60} # Which namespace is confiigmap deployed in. namespace: ${SW_CLUSTER_K8S_NAMESPACE:default} # Labelselector is used to locate specific configmap labelSelector: ${SW_CLUSTER_K8S_LABEL:app=collector,release=skywalking} Dynamic Configuration Nacos Implementation Nacos is also supported as DCC(Dynamic Configuration Center), to use it, please configure as follows:\nconfiguration: selector: ${SW_CONFIGURATION:nacos} nacos: # Nacos Server Host serverAddr: ${SW_CONFIG_NACOS_SERVER_ADDR:127.0.0.1} # Nacos Server Port port: ${SW_CONFIG_NACOS_SERVER_PORT:8848} # Nacos Configuration Group group: ${SW_CONFIG_NACOS_SERVER_GROUP:skywalking} # Nacos Configuration namespace namespace: ${SW_CONFIG_NACOS_SERVER_NAMESPACE:} # Unit seconds, sync period. Default fetch every 60 seconds. period: ${SW_CONFIG_NACOS_PERIOD:60} # the name of current cluster, set the name if you want to upstream system known. clusterName: ${SW_CONFIG_NACOS_CLUSTER_NAME:default} ","excerpt":"Dynamic Configuration SkyWalking Configurations mostly are set through application.yml and OS system …","ref":"/docs/main/v8.5.0/en/setup/backend/dynamic-config/","title":"Dynamic Configuration"},{"body":"ElasticSearch Some new users may encounter the following issues:\n The performance of ElasticSearch is not as good as expected. For instance, the latest data cannot be accessed after some time.  Or\n ERROR CODE 429.   Suppressed: org.elasticsearch.client.ResponseException: method [POST], host [http://127.0.0.1:9200], URI [/service_instance_inventory/type/6_tcc-app-gateway-77b98ff6ff-crblx.cards_0_0/_update?refresh=true\u0026amp;timeout=1m], status line [HTTP/1.1 429 Too Many Requests] {\u0026quot;error\u0026quot;:{\u0026quot;root_cause\u0026quot;:[{\u0026quot;type\u0026quot;:\u0026quot;remote_transport_exception\u0026quot;,\u0026quot;reason\u0026quot;:\u0026quot;[elasticsearch-0][10.16.9.130:9300][indices:data/write/update[s]]\u0026quot;}],\u0026quot;type\u0026quot;:\u0026quot;es_rejected_execution_exception\u0026quot;,\u0026quot;reason\u0026quot;:\u0026quot;rejected execution of org.elasticsearch.transport.TransportService$7@19a5cf02 on EsThreadPoolExecutor[name = elasticsearch-0/write, queue capacity = 200, org.elasticsearch.common.util.concurrent.EsThreadPoolExecutor@389297ad[Running, pool size = 2, active threads = 2, queued tasks = 200, completed tasks = 147611]]\u0026quot;},\u0026quot;status\u0026quot;:429} at org.elasticsearch.client.RestClient$SyncResponseListener.get(RestClient.java:705) ~[elasticsearch-rest-client-6.3.2.jar:6.3.2] at org.elasticsearch.client.RestClient.performRequest(RestClient.java:235) ~[elasticsearch-rest-client-6.3.2.jar:6.3.2] at org.elasticsearch.client.RestClient.performRequest(RestClient.java:198) ~[elasticsearch-rest-client-6.3.2.jar:6.3.2] at org.elasticsearch.client.RestHighLevelClient.performRequest(RestHighLevelClient.java:522) ~[elasticsearch You could add the following config to elasticsearch.yml, and set the value based on your environment variable.\n# In the case of tracing, consider setting a value higher than this. thread_pool.index.queue_size: 1000 thread_pool.write.queue_size: 1000 # When you face query error at trace page, remember to check this. index.max_result_window: 1000000 For more information, see ElasticSearch\u0026rsquo;s official documentation.\n","excerpt":"ElasticSearch Some new users may encounter the following issues:\n The performance of ElasticSearch …","ref":"/docs/main/v8.5.0/en/faq/es-server-faq/","title":"ElasticSearch"},{"body":"Events SkyWalking already supports the three pillars of observability, namely logs, metrics, and traces. In reality, a production system experiences many other events that may affect the performance of the system, such as upgrading, rebooting, chaos testing, etc. Although some of these events are reflected in the logs, many others are not. Hence, SkyWalking provides a more native way to collect these events. This doc details how SkyWalking collects events and what events look like in SkyWalking.\nHow to Report Events The SkyWalking backend supports three protocols to collect events: gRPC, HTTP, and Kafka. Any agent or CLI that implements one of these protocols can report events to SkyWalking. Currently, the officially supported clients to report events are:\n Java Agent Toolkit: Using the Java agent toolkit to report events within the applications. SkyWalking CLI: Using the CLI to report events from the command line interface. Kubernetes Event Exporter: Deploying an event exporter to refine and report Kubernetes events.  Event Definitions An event contains the following fields. The definitions of event can be found at the protocol repo.\nUUID Unique ID of the event. Since an event may span a long period of time, the UUID is necessary to associate the start time with the end time of the same event.\nSource The source object on which the event occurs. In SkyWalking, the object is typically a service, service instance, etc.\nName Name of the event. For example, Start, Stop, Crash, Reboot, Upgrade, etc.\nType Type of the event. This field is friendly for UI visualization, where events of type Normal are considered normal operations, while Error is considered unexpected operations, such as Crash events. Marking them with different colors allows us to more easily identify them.\nMessage The detail of the event that describes why this event happened. This should be a one-line message that briefly describes why the event is reported. Examples of an Upgrade event may be something like Upgrade from ${from_version} to ${to_version}. It\u0026rsquo;s NOT recommended to include the detailed logs of this event, such as the exception stack trace.\nParameters The parameters in the message field. This is a simple \u0026lt;string,string\u0026gt; map.\nStart Time The start time of the event. This field is mandatory when an event occurs.\nEnd Time The end time of the event. This field may be empty if the event has not ended yet, otherwise there should be a valid timestamp after startTime.\nNOTE: When reporting an event, you typically call the report function twice, the first time for starting of the event and the second time for ending of the event, both with the same UUID. There are also cases where you would already have both the start time and end time. For example, when exporting events from a third-party system, the start time and end time are already known so you may simply call the report function once.\n","excerpt":"Events SkyWalking already supports the three pillars of observability, namely logs, metrics, and …","ref":"/docs/main/v8.5.0/en/concepts-and-designs/event/","title":"Events"},{"body":"Exporter tool of profile raw data When the visualization doesn\u0026rsquo;t work well through the official UI, users could submit the issue to report. This tool helps the users to package the original profile data for helping the community to locate the issue in the user case. NOTICE, this report includes the class name, method name, line number, etc. Before submit this, please make sure this wouldn\u0026rsquo;t become your system vulnerability.\nExport command line Usage  Set the storage in tools/profile-exporter/application.yml file by following your use case. Prepare data  Profile task id: Profile task id Trace id: Wrong profiled trace id Export dir: Directory of the data will export   Enter the Skywalking root path Execute shell command bash tools/profile-exporter/profile_exporter.sh --taskid={profileTaskId} --traceid={traceId} {exportDir}  The file {traceId}.tar.gz will be generated after execution shell.  Exported data content  basic.yml: Contains the complete information of the profiled segments in the trace. snapshot.data: All monitored thread snapshot data in the current segment.  Report profile issue  Provide exported data generated from this tool. Provide span operation name, analyze mode(include/exclude children). Issue description. (If there have the UI screenshots, it\u0026rsquo;s better)  ","excerpt":"Exporter tool of profile raw data When the visualization doesn\u0026rsquo;t work well through the …","ref":"/docs/main/v8.5.0/en/guides/backend-profile-export/","title":"Exporter tool of profile raw data"},{"body":"Extend storage SkyWalking has already provided several storage solutions. In this document, you could learn how to implement a new storage easily.\nDefine your storage provider  Define a class extends org.apache.skywalking.oap.server.library.module.ModuleProvider. Set this provider targeting to Storage module.  @Override public Class\u0026lt;? extends ModuleDefine\u0026gt; module() { return StorageModule.class; } Implement all DAOs Here is the list of all DAO interfaces in storage\n  IServiceInventoryCacheDAO\n  IServiceInstanceInventoryCacheDAO\n  IEndpointInventoryCacheDAO\n  INetworkAddressInventoryCacheDAO\n  IBatchDAO\n  StorageDAO\n  IRegisterLockDAO\n  ITopologyQueryDAO\n  IMetricsQueryDAO\n  ITraceQueryDAO\n  IMetadataQueryDAO\n  IAggregationQueryDAO\n  IAlarmQueryDAO\n  IHistoryDeleteDAO\n  IMetricsDAO\n  IRecordDAO\n  IRegisterDAO\n  ILogQueryDAO\n  ITopNRecordsQueryDAO\n  IBrowserLogQueryDAO\n  IProfileTaskQueryDAO\n  IProfileTaskLogQueryDAO\n  IProfileThreadSnapshotQueryDAO\n  UITemplateManagementDAO\n  Register all service implementations In public void prepare(), use this#registerServiceImplementation method to do register binding your implementation with the above interfaces.\nExample Take org.apache.skywalking.oap.server.storage.plugin.elasticsearch.StorageModuleElasticsearchProvider or org.apache.skywalking.oap.server.storage.plugin.jdbc.mysql.MySQLStorageProvider as a good example.\nRedistribution with new storage implementation. You don\u0026rsquo;t have to clone the main repo just for implementing the storage. You could just easy depend our Apache releases. Take a look at SkyAPM/SkyWalking-With-Es5x-Storage repo, SkyWalking v6 redistribution with ElasticSearch 5 TCP connection storage implementation.\n","excerpt":"Extend storage SkyWalking has already provided several storage solutions. In this document, you …","ref":"/docs/main/v8.5.0/en/guides/storage-extention/","title":"Extend storage"},{"body":"FAQs These are known and frequently asked questions about SkyWalking. We welcome you to contribute here.\nDesign  Why doesn\u0026rsquo;t SkyWalking involve MQ in its architecture?  Compiling  Protoc plugin fails in maven build Required items could not be found when importing project into Eclipse Maven compilation failure with error such as python2 not found Compiling issues on Mac\u0026rsquo;s M1 chip  Runtime  Version 8.x+ upgrade Why do metrics indexes with Hour and Day precisions stop updating after upgrade to 7.x? Version 6.x upgrade Why are there only traces in UI? Tracing doesn\u0026rsquo;t work on the Kafka consumer end Agent or collector version upgrade, 3.x -\u0026gt; 5.0.0-alpha EnhanceRequireObjectCache class cast exception ElasticSearch server performance issues, including ERROR CODE:429 IllegalStateException when installing Java agent on WebSphere 7 \u0026ldquo;FORBIDDEN/12/index read-only / allow delete (api)\u0026rdquo; appears in the log No data shown and backend replies with \u0026ldquo;Variable \u0026lsquo;serviceId\u0026rsquo; has coerced Null value for NonNull type \u0026lsquo;ID!'\u0026quot; Unexpected endpoint register warning after 6.6.0 Use the profile exporter tool if the profile analysis is not right Compatibility with other javaagent bytecode processes Java agent memory leak when enhancing Worker thread at Thread Pool Thrift plugin  UI  What is VNode? And why does SkyWalking have that?  ","excerpt":"FAQs These are known and frequently asked questions about SkyWalking. We welcome you to contribute …","ref":"/docs/main/v8.5.0/en/faq/readme/","title":"FAQs"},{"body":"Group Parameterized Endpoints In most cases, the endpoint should be detected automatically through the language agents, service mesh observability solution, or configuration of meter system.\nThere are some special cases, especially when people use REST style URI, the application codes put the parameter in the endpoint name, such as putting order id in the URI, like /prod/ORDER123 and /prod/ORDER123. But logically, people expect they could have an endpoint name like prod/{order-id}. This is the feature of parameterized endpoint grouping designed for.\nCurrent, user could set up grouping rules through the static YAML file, named endpoint-name-grouping.yml, or use Dynamic Configuration to initial and update the endpoint grouping rule.\nConfiguration Format No matter in static local file or dynamic configuration value, they are sharing the same YAML format.\ngrouping: # Endpoint of the service would follow the following rules - service-name: serviceA rules: # Logic name when the regex expression matched. - endpoint-name: /prod/{id} regex: \\/prod\\/.+ ","excerpt":"Group Parameterized Endpoints In most cases, the endpoint should be detected automatically through …","ref":"/docs/main/v8.5.0/en/setup/backend/endpoint-grouping-rules/","title":"Group Parameterized Endpoints"},{"body":"Guides There are many ways that you can help the SkyWalking community.\n Go through our documents, point out or fix unclear things. Translate the documents to other languages. Download our releases, try to monitor your applications, and feedback to us about what you think. Read our source codes, Ask questions for details. Find some bugs, submit issue, and try to fix it. Find help wanted issues, which are good for you to start. Submit issue or start discussion through GitHub issue. See all mail list discussion through website list review. If you are a SkyWalking committer, could login and use the mail list in browser mode. Otherwise, follow the next step to subscribe. Issue report and discussion also could take place in dev@skywalking.apache.org. Mail to dev-subscribe@skywalking.apache.org, follow the reply to subscribe the mail list.  Contact Us All the following channels are open to the community, you could choose the way you like.\n Submit an issue Mail list: dev@skywalking.apache.org. Mail to dev-subscribe@skywalking.apache.org, follow the reply to subscribe the mail list. Gitter QQ Group: 392443393  Become official Apache SkyWalking Committer The PMC will assess the contributions of every contributor, including, but not limited to, code contributions, and follow the Apache guides to promote, vote and invite new committer and PMC member. Read Become official Apache SkyWalking Committer to get details.\nFor code developer For developers, first step, read Compiling Guide. It teaches developer how to build the project in local and set up the environment.\nIntegration Tests After setting up the environment and writing your codes, in order to make it more easily accepted by SkyWalking project, you\u0026rsquo;ll need to run the tests locally to verify that your codes don\u0026rsquo;t break any existed features, and write some unit test (UT) codes to verify that the new codes work well, preventing them being broke by future contributors. If the new codes involve other components or libraries, you\u0026rsquo;re also supposed to write integration tests (IT).\nSkyWalking leverages plugin maven-surefire-plugin to run the UTs while using maven-failsafe-plugin to run the ITs, maven-surefire-plugin will exclude ITs (whose class name starts with IT) and leave them for maven-failsafe-plugin to run, which is bound to the verify goal, CI-with-IT profile. Therefore, to run the UTs, try ./mvnw clean test, this will only run the UTs, not including ITs.\nIf you want to run the ITs please activate the CI-with-IT profile as well as the the profiles of the modules whose ITs you want to run. e.g. if you want to run the ITs in oap-server, try ./mvnw -Pbackend,CI-with-IT clean verify, and if you\u0026rsquo;d like to run all the ITs, simply run ./mvnw -Pall,CI-with-IT clean verify.\nPlease be advised that if you\u0026rsquo;re writing integration tests, name it with the pattern IT* to make them only run in CI-with-IT profile.\nEnd to End Tests (E2E for short) Since version 6.3.0, we have introduced more automatic tests to perform software quality assurance, E2E is one of the most important parts.\n End-to-end testing is a methodology used to test whether the flow of an application is performing as designed from start to finish. The purpose of carrying out end-to-end tests is to identify system dependencies and to ensure that the right information is passed between various system components and systems.\n The e2e test involves some/all of the OAP server, storage, coordinator, webapp, and the instrumented services, all of which are orchestrated by docker-compose, besides, there is a test controller(JUnit test) running outside of the container that sends traffics to the instrumented service, and then verifies the corresponding results after those requests, by GraphQL API of the SkyWalking Web App.\nBefore all following steps, please set the SkyWalking version sw.version in the pom.xml so that you can build it in your local IDE, but please make sure not to check this change into the codebase. However, if you prefer to build it in command line interface with ./mvnw, you can simply use property -Dsw.version=x.y.z without modifying the pom.xml.\nWriting E2E Cases  Set up environment in IntelliJ IDEA  The e2e test is an separated project under the SkyWalking root directory and the IDEA cannot recognize it by default, right click on the file test/e2e/pom.xml and click Add as Maven Project, things should be ready now. But we recommend to open the directory skywalking/test/e2e in a separated IDE window for better experience because there may be shaded classes issues.\n Orchestrate the components  Our goal of E2E tests is to test the SkyWalking project in a whole, including the OAP server, storage, coordinator, webapp, and even the frontend UI(not now), in single node mode as well as cluster mode, therefore the first step is to determine what case we are going to verify and orchestrate the components.\nIn order to make it more easily to orchestrate, we\u0026rsquo;re using a docker-compose that provides a convenient file format (docker-compose.yml) to orchestrate the needed containers, and gives us possibilities to define the dependencies of the components.\nBasically you will need:\n Decide what (and how many) containers will be needed, e.g. for cluster testing, you\u0026rsquo;ll need \u0026gt; 2 OAP nodes, coordinators like zookeeper, storage like ElasticSearch, and instrumented services; Define the containers in docker-compose.yml, and carefully specify the dependencies, starting orders, and most importantly, link them together, e.g. set correct OAP address in the agent side, set correct coordinator address in OAP, etc. Write (or hopefully reuse) the test codes, to verify the results is correct.  As for the last step, we have a friendly framework to help you get started more quickly, which provides annotation @DockerCompose(\u0026quot;docker-compose.yml\u0026quot;) to load/parse and start up all the containers in a proper order, @ContainerHost/@ContainerPort to get the real host/port of the container, @ContainerHostAndPort to get both, @DockerContainer to get the running container.\n Write test controller  To put it simple, test controllers are basically tests that can be bound to the Maven integration-test/verify phase. They send designed requests to the instrumented service, and expect to get corresponding traces/metrics/metadata from the SkyWalking webapp GraphQL API.\nIn the test framework, we provide a TrafficController to periodically send traffic data to the instrumented services, you can simply enable it by giving a url and traffic data, refer to this.\n Troubleshooting  We expose all the logs from all containers to the stdout in non-CI (local) mode, but save/and upload them all to the GitHub server and you can download them (only when tests failed) in the right-up button \u0026ldquo;Artifacts/Download artifacts/logs\u0026rdquo; for debugging.\nNOTE: Please verify the newly-added E2E test case locally first, however, if you find it passed locally but failed in the PR check status, make sure all the updated/newly-added files (especially those in submodules) are committed and included in that PR, or reset the git HEAD to the remote and verify locally again.\nE2E local remote debugging When the E2E test is executed locally, if any test case fails, the E2E local remote debugging function can be used to quickly troubleshoot the bug.\nProject Extensions SkyWalking project supports many ways to extend existing features. If you are interesting in these ways, read the following guides.\n Java agent plugin development guide. This guide helps you to develop SkyWalking agent plugin to support more frameworks. Both open source plugin and private plugin developer should read this. If you want to build a new probe or plugin in any language, please read Component library definition and extension document. Storage extension development guide. Help potential contributors to build a new storage implementor besides the official. Customize analysis by oal script. OAL scripts locate in config/oal/*.oal. You could change it and reboot the OAP server. Read Observability Analysis Language Introduction if you need to learn about OAL script. Source and scope extension for new metrics. If you want to analysis a new metrics, which SkyWalking haven\u0026rsquo;t provide. You need to add a new receiver rather than choosing existed receiver. At that moment, you most likely need to add a new source and scope. This document will teach you how to do.  UI developer Our UI is constituted by static pages and web container.\n RocketBot UI is SkyWalking primary UI since 6.1 release. It is built with vue + typescript. You could know more at the rocketbot repository. Web container source codes are in apm-webapp module. This is a just an easy zuul proxy to host static resources and send GraphQL query requests to backend. Legacy UI repository is still there, but not included in SkyWalking release, after 6.0.0-GA.  OAP backend dependency management  This section is only applicable to the dependencies of the backend module\n Being one of the Top Level Projects of The Apache Software Foundation (ASF), SkyWalking is supposed to follow the ASF 3RD PARTY LICENSE POLICY, so if you\u0026rsquo;re adding new dependencies to the project, you\u0026rsquo;re responsible to check the newly-added dependencies won\u0026rsquo;t break the policy, and add their LICENSE\u0026rsquo;s and NOTICES\u0026rsquo;s to the project.\nWe have a simple script to help you make sure that you didn\u0026rsquo;t miss any newly-added dependency:\n Build a distribution package and unzip/untar it to folder dist. Run the script in the root directory, it will print out all newly-added dependencies. Check the LICENSE\u0026rsquo;s and NOTICE\u0026rsquo;s of those dependencies, if they can be included in an ASF project, add them in the apm-dist/release-docs/{LICENSE,NOTICE} file. Add those dependencies' names to the tools/dependencies/known-oap-backend-dependencies.txt file (alphabetical order), the next run of check-LICENSE.sh should pass.  Profile The performance profile is an enhancement feature in the APM system. We are using the thread dump to estimate the method execution time, rather than adding many local spans. In this way, the resource cost would be much less than using distributed tracing to locate slow method. This feature is suitable in the production environment. The following documents are important for developers to understand the key parts of this feature\n Profile data report protocol is provided like other trace, JVM data through gRPC. Thread dump merging mechanism introduces the merging mechanism, which helps the end users to understand the profile report. Exporter tool of profile raw data introduces when the visualization doesn\u0026rsquo;t work well through the official UI, how to package the original profile data, which helps the users report the issue.  For release Apache Release Guide introduces to the committer team about doing official Apache version release, to avoid breaking any Apache rule. Apache license allows everyone to redistribute if you keep our licenses and NOTICE in your redistribution.\n","excerpt":"Guides There are many ways that you can help the SkyWalking community.\n Go through our documents, …","ref":"/docs/main/v8.5.0/en/guides/readme/","title":"Guides"},{"body":"Health Check Health check intends to provide a unique approach to check the healthy status of OAP server. It includes the health status of modules, GraphQL and gRPC services readiness.\nHealth Checker Module. Health Checker module could solute how to observe the health status of modules. We can active it by below:\nhealth-checker: selector: ${SW_HEALTH_CHECKER:default} default: checkIntervalSeconds: ${SW_HEALTH_CHECKER_INTERVAL_SECONDS:5} Notice, we should enable telemetry module at the same time. That means the provider should not be - and none.\nAfter that, we can query OAP server health status by querying GraphQL:\nquery{ checkHealth{ score details } } If the OAP server is healthy, the response should be\n{ \u0026#34;data\u0026#34;: { \u0026#34;checkHealth\u0026#34;: { \u0026#34;score\u0026#34;: 0, \u0026#34;details\u0026#34;: \u0026#34;\u0026#34; } } } Once some modules are unhealthy, for instance, storage H2 is down. The result might be like below:\n{ \u0026#34;data\u0026#34;: { \u0026#34;checkHealth\u0026#34;: { \u0026#34;score\u0026#34;: 1, \u0026#34;details\u0026#34;: \u0026#34;storage_h2,\u0026#34; } } } You could refer to checkHealth query for more details.\nThe readiness of GraphQL and gRPC We could opt to above query to check the readiness of GraphQL.\nOAP has implemented gRPC Health Checking Protocol. We could use grpc-health-probe or any other tools to check the health of OAP gRPC services.\nCLI tool Please follow the CLI doc to get the health status score directly through the checkhealth command.\n","excerpt":"Health Check Health check intends to provide a unique approach to check the healthy status of OAP …","ref":"/docs/main/v8.5.0/en/setup/backend/backend-health-check/","title":"Health Check"},{"body":"How to build project This document helps people to compile and build the project in your maven and set your IDE.\nBuild Project Because we are using Git submodule, we recommend don\u0026rsquo;t use GitHub tag or release page to download source codes for compiling.\nMaven behind Proxy If you need to execute build behind the proxy, edit the .mvn/jvm.config and put the follow properties:\n-Dhttp.proxyHost=proxy_ip -Dhttp.proxyPort=proxy_port -Dhttps.proxyHost=proxy_ip -Dhttps.proxyPort=proxy_port -Dhttp.proxyUser=username -Dhttp.proxyPassword=password Build from GitHub   Prepare git, JDK8+ and Maven 3.6+\n  Clone project\nIf you want to build a release from source codes, provide a tag name by using git clone -b [tag_name] ... while cloning.\ngit clone --recurse-submodules https://github.com/apache/skywalking.git cd skywalking/ OR git clone https://github.com/apache/skywalking.git cd skywalking/ git submodule init git submodule update   Run ./mvnw clean package -DskipTests\n  All packages are in /dist (.tar.gz for Linux and .zip for Windows).\n  Build from Apache source code release  What is Apache source code release?  For each official Apache release, there is a complete and independent source code tar, which is including all source codes. You could download it from SkyWalking Apache download page. No git related stuff required when compiling this. Just follow these steps.\n Prepare JDK8+ and Maven 3.6+ Run ./mvnw clean package -DskipTests All packages are in /dist.(.tar.gz for Linux and .zip for Windows).  Advanced compile SkyWalking is a complex maven project, including many modules, which could cause long compiling time. If you just want to recompile part of the project, you have following options\n Compile agent and package   ./mvnw package -Pagent,dist\n or\n make build.agent\n If you intend to compile a single one plugin, such as in the dev stage, you could\n cd plugin_module_dir \u0026amp; mvn clean package\n  Compile backend and package   ./mvnw package -Pbackend,dist\n or\n make build.backend\n  Compile UI and package   ./mvnw package -Pui,dist\n or\n make build.ui\n Build docker images We can build docker images of backend and ui with Makefile located in root folder.\nRefer to Build docker image for more details.\nSetup your IntelliJ IDEA NOTICE: If you clone the codes from GitHub, please make sure that you had finished step 1 to 3 in section Build from GitHub, if you download the source codes from the official website of SkyWalking, please make sure that you had followed the steps in section Build from Apache source code release.\n Import the project as a maven project Run ./mvnw compile -Dmaven.test.skip=true to compile project and generate source codes. Because we use gRPC and protobuf. Set Generated Source Codes folders.  grpc-java and java folders in apm-protocol/apm-network/target/generated-sources/protobuf grpc-java and java folders in oap-server/server-core/target/generated-sources/protobuf grpc-java and java folders in oap-server/server-receiver-plugin/receiver-proto/target/generated-sources/fbs grpc-java and java folders in oap-server/server-receiver-plugin/receiver-proto/target/generated-sources/protobuf grpc-java and java folders in oap-server/exporter/target/generated-sources/protobuf grpc-java and java folders in oap-server/server-configuration/grpc-configuration-sync/target/generated-sources/protobuf grpc-java and java folders in oap-server/server-alarm-plugin/target/generated-sources/protobuf antlr4 folder in oap-server/oal-grammar/target/generated-sources    ","excerpt":"How to build project This document helps people to compile and build the project in your maven and …","ref":"/docs/main/v8.5.0/en/guides/how-to-build/","title":"How to build project"},{"body":"How to enable Kafka Reporter The Kafka reporter plugin support report traces, JVM metrics, Instance Properties, and profiled snapshots to Kafka cluster, which is disabled in default. Move the jar of the plugin, kafka-reporter-plugin-x.y.z.jar, from agent/optional-reporter-plugins to agent/plugins for activating.\nNotice, currently, the agent still needs to configure GRPC receiver for delivering the task of profiling. In other words, the following configure cannot be omitted.\n# Backend service addresses. collector.backend_service=${SW_AGENT_COLLECTOR_BACKEND_SERVICES:127.0.0.1:11800} # Kafka producer configuration plugin.kafka.bootstrap_servers=${SW_KAFKA_BOOTSTRAP_SERVERS:localhost:9092} plugin.kafka.producer_config[delivery.timeout.ms]=12000 plugin.kafka.get_topic_timeout=${SW_GET_TOPIC_TIMEOUT:10} Kafka reporter plugin support to customize all configurations of listed in here.\nBefore you activated the Kafka reporter, you have to make sure that Kafka fetcher has been opened in service.\n","excerpt":"How to enable Kafka Reporter The Kafka reporter plugin support report traces, JVM metrics, Instance …","ref":"/docs/main/v8.5.0/en/setup/service-agent/java-agent/how-to-enable-kafka-reporter/","title":"How to enable Kafka Reporter"},{"body":"How to tolerate custom exceptions In some codes, the exception is being used as a way of controlling business flow. Skywalking provides 2 ways to tolerate an exception which is traced in a span.\n Set the names of exception classes in the agent config Use our annotation in the codes.  Set the names of exception classes in the agent config The property named \u0026ldquo;statuscheck.ignored_exceptions\u0026rdquo; is used to set up class names in the agent configuration file. if the exception listed here are detected in the agent, the agent core would flag the related span as the error status.\nDemo   A custom exception.\n TestNamedMatchException  package org.apache.skywalking.apm.agent.core.context.status; public class TestNamedMatchException extends RuntimeException { public TestNamedMatchException() { } public TestNamedMatchException(final String message) { super(message); } ... }  TestHierarchyMatchException  package org.apache.skywalking.apm.agent.core.context.status; public class TestHierarchyMatchException extends TestNamedMatchException { public TestHierarchyMatchException() { } public TestHierarchyMatchException(final String message) { super(message); } ... }   When the above exceptions traced in some spans, the status is like the following.\n   The traced exception Final span status     org.apache.skywalking.apm.agent.core.context.status.TestNamedMatchException true   org.apache.skywalking.apm.agent.core.context.status.TestHierarchyMatchException true      After set these class names through \u0026ldquo;statuscheck.ignored_exceptions\u0026rdquo;, the status of spans would be changed.\nstatuscheck.ignored_exceptions=org.apache.skywalking.apm.agent.core.context.status.TestNamedMatchException    The traced exception Final span status     org.apache.skywalking.apm.agent.core.context.status.TestNamedMatchException false   org.apache.skywalking.apm.agent.core.context.status.TestHierarchyMatchException false      Use our annotation in the codes. If an exception has the @IgnoredException annotation, the exception wouldn\u0026rsquo;t be marked as error status when tracing. Because the annotation supports inheritance, also affects the subclasses.\nDependency  Dependency the toolkit, such as using maven or gradle. Since 8.2.0.  \u0026lt;dependency\u0026gt; \u0026lt;groupId\u0026gt;org.apache.skywalking\u0026lt;/groupId\u0026gt; \u0026lt;artifactId\u0026gt;apm-toolkit-log4j-1.x\u0026lt;/artifactId\u0026gt; \u0026lt;version\u0026gt;{project.release.version}\u0026lt;/version\u0026gt; \u0026lt;/dependency\u0026gt; Demo   A custom exception.\npackage org.apache.skywalking.apm.agent.core.context.status; public class TestAnnotatedException extends RuntimeException { public TestAnnotatedException() { } public TestAnnotatedException(final String message) { super(message); } ... }   When the above exception traced in some spans, the status is like the following.\n   The traced exception Final span status     org.apache.skywalking.apm.agent.core.context.status.TestAnnotatedException true      However, when the exception annotated with the annotation, the status would be changed.\npackage org.apache.skywalking.apm.agent.core.context.status; @IgnoredException public class TestAnnotatedException extends RuntimeException { public TestAnnotatedException() { } public TestAnnotatedException(final String message) { super(message); } ... }    The traced exception Final span status     org.apache.skywalking.apm.agent.core.context.status.TestAnnotatedException false      Recursive check Due to the wrapper nature of Java exceptions, sometimes users need recursive checking. Skywalking also supports it. Typically, we don\u0026rsquo;t recommend setting this more than 10, which could cause a performance issue. Negative value and 0 would be ignored, which means all exceptions would make the span tagged in error status.\n statuscheck.max_recursive_depth=${SW_STATUSCHECK_MAX_RECURSIVE_DEPTH:1} ","excerpt":"How to tolerate custom exceptions In some codes, the exception is being used as a way of controlling …","ref":"/docs/main/v8.5.0/en/setup/service-agent/java-agent/how-to-tolerate-exceptions/","title":"How to tolerate custom exceptions"},{"body":"HTTP API Protocol HTTP API Protocol defines the API data format, including api request and response data format. They use the HTTP1.1 wrapper of the official SkyWalking Browser Protocol. Read it for more details.\nPerformance Data Report Detail information about data format can be found in BrowserPerf.proto.\nPOST http://localhost:12800/browser/perfData Send a performance data object with JSON format.\nInput:\n{ \u0026#34;service\u0026#34;: \u0026#34;web\u0026#34;, \u0026#34;serviceVersion\u0026#34;: \u0026#34;v0.0.1\u0026#34;, \u0026#34;pagePath\u0026#34;: \u0026#34;/index.html\u0026#34;, \u0026#34;redirectTime\u0026#34;: 10, \u0026#34;dnsTime\u0026#34;: 10, \u0026#34;ttfbTime\u0026#34;: 10, \u0026#34;tcpTime\u0026#34;: 10, \u0026#34;transTime\u0026#34;: 10, \u0026#34;domAnalysisTime\u0026#34;: 10, \u0026#34;fptTime\u0026#34;: 10, \u0026#34;domReadyTime\u0026#34;: 10, \u0026#34;loadPageTime\u0026#34;: 10, \u0026#34;resTime\u0026#34;: 10, \u0026#34;sslTime\u0026#34;: 10, \u0026#34;ttlTime\u0026#34;: 10, \u0026#34;firstPackTime\u0026#34;: 10, \u0026#34;fmpTime\u0026#34;: 10 } OutPut:\nHttp Status: 204\nError Log Report Detail information about data format can be found in BrowserPerf.proto.\nPOST http://localhost:12800/browser/errorLogs Send an error log object list with JSON format.\nInput:\n[ { \u0026#34;uniqueId\u0026#34;: \u0026#34;55ec6178-3fb7-43ef-899c-a26944407b01\u0026#34;, \u0026#34;service\u0026#34;: \u0026#34;web\u0026#34;, \u0026#34;serviceVersion\u0026#34;: \u0026#34;v0.0.1\u0026#34;, \u0026#34;pagePath\u0026#34;: \u0026#34;/index.html\u0026#34;, \u0026#34;category\u0026#34;: \u0026#34;ajax\u0026#34;, \u0026#34;message\u0026#34;: \u0026#34;error\u0026#34;, \u0026#34;line\u0026#34;: 1, \u0026#34;col\u0026#34;: 1, \u0026#34;stack\u0026#34;: \u0026#34;error\u0026#34;, \u0026#34;errorUrl\u0026#34;: \u0026#34;/index.html\u0026#34; }, { \u0026#34;uniqueId\u0026#34;: \u0026#34;55ec6178-3fb7-43ef-899c-a26944407b02\u0026#34;, \u0026#34;service\u0026#34;: \u0026#34;web\u0026#34;, \u0026#34;serviceVersion\u0026#34;: \u0026#34;v0.0.1\u0026#34;, \u0026#34;pagePath\u0026#34;: \u0026#34;/index.html\u0026#34;, \u0026#34;category\u0026#34;: \u0026#34;ajax\u0026#34;, \u0026#34;message\u0026#34;: \u0026#34;error\u0026#34;, \u0026#34;line\u0026#34;: 1, \u0026#34;col\u0026#34;: 1, \u0026#34;stack\u0026#34;: \u0026#34;error\u0026#34;, \u0026#34;errorUrl\u0026#34;: \u0026#34;/index.html\u0026#34; } ] OutPut:\nHttp Status: 204\nPOST http://localhost:12800/browser/errorLog Send a single error log object with JSON format.\nInput:\n{ \u0026#34;uniqueId\u0026#34;: \u0026#34;55ec6178-3fb7-43ef-899c-a26944407b01\u0026#34;, \u0026#34;service\u0026#34;: \u0026#34;web\u0026#34;, \u0026#34;serviceVersion\u0026#34;: \u0026#34;v0.0.1\u0026#34;, \u0026#34;pagePath\u0026#34;: \u0026#34;/index.html\u0026#34;, \u0026#34;category\u0026#34;: \u0026#34;ajax\u0026#34;, \u0026#34;message\u0026#34;: \u0026#34;error\u0026#34;, \u0026#34;line\u0026#34;: 1, \u0026#34;col\u0026#34;: 1, \u0026#34;stack\u0026#34;: \u0026#34;error\u0026#34;, \u0026#34;errorUrl\u0026#34;: \u0026#34;/index.html\u0026#34; } OutPut:\nHttp Status: 204\n","excerpt":"HTTP API Protocol HTTP API Protocol defines the API data format, including api request and response …","ref":"/docs/main/v8.5.0/en/protocols/browser-http-api-protocol/","title":"HTTP API Protocol"},{"body":"HTTP API Protocol HTTP API Protocol defines the API data format, including api request and response data format. They use the HTTP1.1 wrapper of the official SkyWalking Trace Data Protocol v3. Read it for more details.\nInstance Management Detail information about data format can be found in Instance Management.\n Report service instance properties   POST http://localhost:12800/v3/management/reportProperties\n Input:\n{ \u0026#34;service\u0026#34;: \u0026#34;User Service Name\u0026#34;, \u0026#34;serviceInstance\u0026#34;: \u0026#34;User Service Instance Name\u0026#34;, \u0026#34;properties\u0026#34;: [{ \u0026#34;language\u0026#34;: \u0026#34;Lua\u0026#34; }] } Output JSON Array:\n{}  Service instance ping   POST http://localhost:12800/v3/management/keepAlive\n Input:\n{ \u0026#34;service\u0026#34;: \u0026#34;User Service Name\u0026#34;, \u0026#34;serviceInstance\u0026#34;: \u0026#34;User Service Instance Name\u0026#34; } OutPut:\n{} Trace Report Detail information about data format can be found in Instance Management. There are two ways to report segment data, one segment per request or segment array in the bulk mode.\nPOST http://localhost:12800/v3/segment Send a single segment object with JSON format.\nInput:\n{ \u0026#34;traceId\u0026#34;: \u0026#34;a12ff60b-5807-463b-a1f8-fb1c8608219e\u0026#34;, \u0026#34;serviceInstance\u0026#34;: \u0026#34;User_Service_Instance_Name\u0026#34;, \u0026#34;spans\u0026#34;: [{ \u0026#34;operationName\u0026#34;: \u0026#34;/ingress\u0026#34;, \u0026#34;startTime\u0026#34;: 1588664577013, \u0026#34;endTime\u0026#34;: 1588664577028, \u0026#34;spanType\u0026#34;: \u0026#34;Exit\u0026#34;, \u0026#34;spanId\u0026#34;: 1, \u0026#34;isError\u0026#34;: false, \u0026#34;parentSpanId\u0026#34;: 0, \u0026#34;componentId\u0026#34;: 6000, \u0026#34;peer\u0026#34;: \u0026#34;upstream service\u0026#34;, \u0026#34;spanLayer\u0026#34;: \u0026#34;Http\u0026#34; }, { \u0026#34;operationName\u0026#34;: \u0026#34;/ingress\u0026#34;, \u0026#34;startTime\u0026#34;: 1588664577013, \u0026#34;tags\u0026#34;: [{ \u0026#34;key\u0026#34;: \u0026#34;http.method\u0026#34;, \u0026#34;value\u0026#34;: \u0026#34;GET\u0026#34; }, { \u0026#34;key\u0026#34;: \u0026#34;http.params\u0026#34;, \u0026#34;value\u0026#34;: \u0026#34;http://localhost/ingress\u0026#34; }], \u0026#34;endTime\u0026#34;: 1588664577028, \u0026#34;spanType\u0026#34;: \u0026#34;Entry\u0026#34;, \u0026#34;spanId\u0026#34;: 0, \u0026#34;parentSpanId\u0026#34;: -1, \u0026#34;isError\u0026#34;: false, \u0026#34;spanLayer\u0026#34;: \u0026#34;Http\u0026#34;, \u0026#34;componentId\u0026#34;: 6000 }], \u0026#34;service\u0026#34;: \u0026#34;User_Service_Name\u0026#34;, \u0026#34;traceSegmentId\u0026#34;: \u0026#34;a12ff60b-5807-463b-a1f8-fb1c8608219e\u0026#34; } OutPut:\nPOST http://localhost:12800/v3/segments Send a segment object list with JSON format.\nInput:\n[{ \u0026#34;traceId\u0026#34;: \u0026#34;a12ff60b-5807-463b-a1f8-fb1c8608219e\u0026#34;, \u0026#34;serviceInstance\u0026#34;: \u0026#34;User_Service_Instance_Name\u0026#34;, \u0026#34;spans\u0026#34;: [{ \u0026#34;operationName\u0026#34;: \u0026#34;/ingress\u0026#34;, \u0026#34;startTime\u0026#34;: 1588664577013, \u0026#34;endTime\u0026#34;: 1588664577028, \u0026#34;spanType\u0026#34;: \u0026#34;Exit\u0026#34;, \u0026#34;spanId\u0026#34;: 1, \u0026#34;isError\u0026#34;: false, \u0026#34;parentSpanId\u0026#34;: 0, \u0026#34;componentId\u0026#34;: 6000, \u0026#34;peer\u0026#34;: \u0026#34;upstream service\u0026#34;, \u0026#34;spanLayer\u0026#34;: \u0026#34;Http\u0026#34; }, { \u0026#34;operationName\u0026#34;: \u0026#34;/ingress\u0026#34;, \u0026#34;startTime\u0026#34;: 1588664577013, \u0026#34;tags\u0026#34;: [{ \u0026#34;key\u0026#34;: \u0026#34;http.method\u0026#34;, \u0026#34;value\u0026#34;: \u0026#34;GET\u0026#34; }, { \u0026#34;key\u0026#34;: \u0026#34;http.params\u0026#34;, \u0026#34;value\u0026#34;: \u0026#34;http://localhost/ingress\u0026#34; }], \u0026#34;endTime\u0026#34;: 1588664577028, \u0026#34;spanType\u0026#34;: \u0026#34;Entry\u0026#34;, \u0026#34;spanId\u0026#34;: 0, \u0026#34;parentSpanId\u0026#34;: -1, \u0026#34;isError\u0026#34;: false, \u0026#34;spanLayer\u0026#34;: \u0026#34;Http\u0026#34;, \u0026#34;componentId\u0026#34;: 6000 }], \u0026#34;service\u0026#34;: \u0026#34;User_Service_Name\u0026#34;, \u0026#34;traceSegmentId\u0026#34;: \u0026#34;a12ff60b-5807-463b-a1f8-fb1c8608219e\u0026#34; }, { \u0026#34;traceId\u0026#34;: \u0026#34;f956699e-5106-4ea3-95e5-da748c55bac1\u0026#34;, \u0026#34;serviceInstance\u0026#34;: \u0026#34;User_Service_Instance_Name\u0026#34;, \u0026#34;spans\u0026#34;: [{ \u0026#34;operationName\u0026#34;: \u0026#34;/ingress\u0026#34;, \u0026#34;startTime\u0026#34;: 1588664577250, \u0026#34;endTime\u0026#34;: 1588664577250, \u0026#34;spanType\u0026#34;: \u0026#34;Exit\u0026#34;, \u0026#34;spanId\u0026#34;: 1, \u0026#34;isError\u0026#34;: false, \u0026#34;parentSpanId\u0026#34;: 0, \u0026#34;componentId\u0026#34;: 6000, \u0026#34;peer\u0026#34;: \u0026#34;upstream service\u0026#34;, \u0026#34;spanLayer\u0026#34;: \u0026#34;Http\u0026#34; }, { \u0026#34;operationName\u0026#34;: \u0026#34;/ingress\u0026#34;, \u0026#34;startTime\u0026#34;: 1588664577250, \u0026#34;tags\u0026#34;: [{ \u0026#34;key\u0026#34;: \u0026#34;http.method\u0026#34;, \u0026#34;value\u0026#34;: \u0026#34;GET\u0026#34; }, { \u0026#34;key\u0026#34;: \u0026#34;http.params\u0026#34;, \u0026#34;value\u0026#34;: \u0026#34;http://localhost/ingress\u0026#34; }], \u0026#34;endTime\u0026#34;: 1588664577250, \u0026#34;spanType\u0026#34;: \u0026#34;Entry\u0026#34;, \u0026#34;spanId\u0026#34;: 0, \u0026#34;parentSpanId\u0026#34;: -1, \u0026#34;isError\u0026#34;: false, \u0026#34;spanLayer\u0026#34;: \u0026#34;Http\u0026#34;, \u0026#34;componentId\u0026#34;: 6000 }], \u0026#34;service\u0026#34;: \u0026#34;User_Service_Name\u0026#34;, \u0026#34;traceSegmentId\u0026#34;: \u0026#34;f956699e-5106-4ea3-95e5-da748c55bac1\u0026#34; }] OutPut:\n","excerpt":"HTTP API Protocol HTTP API Protocol defines the API data format, including api request and response …","ref":"/docs/main/v8.5.0/en/protocols/http-api-protocol/","title":"HTTP API Protocol"},{"body":"IllegalStateException when installing Java agent on WebSphere This issue was found in our community discussion and feedback. A user installed the SkyWalking Java agent on WebSphere 7.0.0.11 and ibm jdk 1.8_20160719 and 1.7.0_20150407, and experienced the following error logs:\nWARN 2019-05-09 17:01:35:905 SkywalkingAgent-1-GRPCChannelManager-0 ProtectiveShieldMatcher : Byte-buddy occurs exception when match type. java.lang.IllegalStateException: Cannot resolve type description for java.security.PrivilegedAction at org.apache.skywalking.apm.dependencies.net.bytebuddy.pool.TypePool$Resolution$Illegal.resolve(TypePool.java:144) at org.apache.skywalking.apm.dependencies.net.bytebuddy.pool.TypePool$Default$WithLazyResolution$LazyTypeDescription.delegate(TypePool.java:1392) at org.apache.skywalking.apm.dependencies.net.bytebuddy.description.type.TypeDescription$AbstractBase$OfSimpleType$WithDelegation.getInterfaces(TypeDescription.java:8016) at org.apache.skywalking.apm.dependencies.net.bytebuddy.description.type.TypeDescription$Generic$OfNonGenericType.getInterfaces(TypeDescription.java:3621) at org.apache.skywalking.apm.dependencies.net.bytebuddy.matcher.HasSuperTypeMatcher.hasInterface(HasSuperTypeMatcher.java:53) at org.apache.skywalking.apm.dependencies.net.bytebuddy.matcher.HasSuperTypeMatcher.hasInterface(HasSuperTypeMatcher.java:54) at org.apache.skywalking.apm.dependencies.net.bytebuddy.matcher.HasSuperTypeMatcher.matches(HasSuperTypeMatcher.java:38) at org.apache.skywalking.apm.dependencies.net.bytebuddy.matcher.HasSuperTypeMatcher.matches(HasSuperTypeMatcher.java:15) at org.apache.skywalking.apm.dependencies.net.bytebuddy.matcher.ElementMatcher$Junction$Conjunction.matches(ElementMatcher.java:107) at org.apache.skywalking.apm.dependencies.net.bytebuddy.matcher.ElementMatcher$Junction$Disjunction.matches(ElementMatcher.java:147) at org.apache.skywalking.apm.dependencies.net.bytebuddy.matcher.ElementMatcher$Junction$Disjunction.matches(ElementMatcher.java:147) at org.apache.skywalking.apm.dependencies.net.bytebuddy.matcher.ElementMatcher$Junction$Disjunction.matches(ElementMatcher.java:147) at org.apache.skywalking.apm.dependencies.net.bytebuddy.matcher.ElementMatcher$Junction$Disjunction.matches(ElementMatcher.java:147) at org.apache.skywalking.apm.dependencies.net.bytebuddy.matcher.ElementMatcher$Junction$Disjunction.matches(ElementMatcher.java:147) ... The exception occured because access grant was required in WebSphere. Simply follow these steps:\n Set the agent\u0026rsquo;s owner to the owner of WebSphere. Add \u0026ldquo;grant codeBase \u0026ldquo;file:${agent_dir}/-\u0026rdquo; { permission java.security.AllPermission; };\u0026rdquo; in the file of \u0026ldquo;server.policy\u0026rdquo;.  ","excerpt":"IllegalStateException when installing Java agent on WebSphere This issue was found in our community …","ref":"/docs/main/v8.5.0/en/faq/install_agent_on_websphere/","title":"IllegalStateException when installing Java agent on WebSphere"},{"body":"Init mode SkyWalking backend supports multiple storage implementors. Most of them could initialize the storage, such as Elastic Search, Database automatically when the backend startup in first place.\nBut there are some unexpected happens based on the storage, such as When create Elastic Search indexes concurrently, because of several backend instances startup at the same time., there is a change, the APIs of Elastic Search would be blocked without any exception. And this has more chances happen in container management platform, like k8s.\nThat is where you need Init mode startup.\nSolution Only one single instance should run in Init mode before other instances start up. And this instance will exit graciously after all initialization steps are done.\nUse oapServiceInit.sh/oapServiceInit.bat to start up backend. You should see the following logs\n 2018-11-09 23:04:39,465 - org.apache.skywalking.oap.server.starter.OAPServerStartUp -2214 [main] INFO [] - OAP starts up in init mode successfully, exit now\u0026hellip;\n Kubernetes Initialization in this mode would be included in our Kubernetes scripts and Helm.\n","excerpt":"Init mode SkyWalking backend supports multiple storage implementors. Most of them could initialize …","ref":"/docs/main/v8.5.0/en/setup/backend/backend-init-mode/","title":"Init mode"},{"body":"IP and port setting Backend is using IP and port binding, in order to support the OS having multiple IPs. The binding/listening IP and port are specified by core module\ncore: default: restHost: 0.0.0.0 restPort: 12800 restContextPath: / gRPCHost: 0.0.0.0 gRPCPort: 11800 There are two IP/port pair for gRPC and HTTP rest services.\n Most agents and probes use gRPC service for better performance and code readability. Few agent use rest service, because gRPC may be not supported in that language. UI uses rest service, but data in GraphQL format, always.  Notice IP binding In case some users are not familiar with IP binding, you should know, after you did that, the client could only use this IP to access the service. For example, bind 172.09.13.28, even you are in this machine, must use 172.09.13.28 rather than 127.0.0.1 or localhost to access the service.\nModule provider specified IP and port The IP and port in core are only default provided by core. But some module provider may provide other IP and port settings, this is common. Such as many receiver modules provide this.\n","excerpt":"IP and port setting Backend is using IP and port binding, in order to support the OS having multiple …","ref":"/docs/main/v8.5.0/en/setup/backend/backend-ip-port/","title":"IP and port setting"},{"body":"JVM Metrics Service Abstract Uplink the JVM metrics, including PermSize, HeapSize, CPU, Memory, etc., every second.\ngRPC service define\n","excerpt":"JVM Metrics Service Abstract Uplink the JVM metrics, including PermSize, HeapSize, CPU, Memory, …","ref":"/docs/main/v8.5.0/en/protocols/jvm-protocol/","title":"JVM Metrics Service"},{"body":"Language agents in Service   Java agent. Introduces how to install java agent to your service, without any impact in your code.\n  LUA agent. Introduce how to install the lua agent in Nginx + LUA module or OpenResty.\n  Python Agent. Introduce how to install the Python Agent in a Python service.\n  Node.js agent. Introduce how to install the NodeJS Agent in a NodeJS service.\n  The following agents and SDKs are compatible with the SkyWalking\u0026rsquo;s data formats and network protocols, but are maintained by 3rd-parties. You can go to their project repositories for additional info about guides and releases.\n  SkyAPM .NET Core agent. See .NET Core agent project document for more details.\n  SkyAPM Node.js agent. See Node.js server side agent project document for more details.\n  SkyAPM PHP agent. See PHP agent project document for more details.\n  SkyAPM Go SDK. See go2sky project document for more details.\n  SkyAPM C++ SDK. See cpp2sky project document for more details.\n  ","excerpt":"Language agents in Service   Java agent. Introduces how to install java agent to your service, …","ref":"/docs/main/v8.5.0/en/setup/service-agent/server-agents/","title":"Language agents in Service"},{"body":"Locate agent config file by system property Supported version 5.0.0-RC+\nWhat is Locate agent config file by system property ？ In Default. The agent will try to locate agent.config, which should be in the /config dictionary of agent package. If User sets the specified agent config file through system properties, The agent will try to load file from there. By the way, This function has no conflict with Setting Override\nOverride priority The specified agent config \u0026gt; The default agent config\nHow to use The content formats of the specified config must be same as the default config.\nUsing System.Properties(-D) to set the specified config path\n-Dskywalking_config=/path/to/agent.config /path/to/agent.config is the absolute path of the specified config file\n","excerpt":"Locate agent config file by system property Supported version 5.0.0-RC+\nWhat is Locate agent config …","ref":"/docs/main/v8.5.0/en/setup/service-agent/java-agent/specified-agent-config/","title":"Locate agent config file by system property"},{"body":"Log Analysis Language Log Analysis Language (LAL) in SkyWalking is essentially a Domain-Specific Language (DSL) to analyze logs. You can use LAL to parse, extract, and save the logs, as well as collaborate the logs with traces (by extracting the trace ID, segment ID and span ID) and metrics (by generating metrics from the logs and sending them to the meter system).\nThe LAL config files are in YAML format, and are located under directory lal. You can set log-analyzer/default/lalFiles in the application.yml file or set environment variable SW_LOG_LAL_FILES to activate specific LAL config files.\nFilter A filter is a group of parser, extractor and sink. Users can use one or more filters to organize their processing logic. Every piece of log will be sent to all filters in an LAL rule. A piece of log sent to the filter is available as property log in the LAL, therefore you can access the log service name via log.service. For all available fields of log, please refer to the protocol definition.\nAll components are executed sequentially in the orders they are declared.\nGlobal Functions Globally available functions may be used them in all components (i.e. parsers, extractors, and sinks) where necessary.\n abort  By default, all components declared are executed no matter what flags (dropped, saved, etc.) have been set. There are cases where you may want the filter chain to stop earlier when specified conditions are met. abort function aborts the remaining filter chain from where it\u0026rsquo;s declared, and all the remaining components won\u0026rsquo;t be executed at all. abort function serves as a fast-fail mechanism in LAL.\nfilter { if (log.service == \u0026#34;TestingService\u0026#34;) { // Don\u0026#39;t waste resources on TestingServices  abort {} // all remaining components won\u0026#39;t be executed at all  } // ... parsers, extractors, sinks } Note that when you put regexp in an if statement, you need to surround the expression with () like regexp(\u0026lt;the expression\u0026gt;), instead of regexp \u0026lt;the expression\u0026gt;.\nParser Parsers are responsible for parsing the raw logs into structured data in SkyWalking for further processing. There are 3 types of parsers at the moment, namely json, yaml, and text.\nWhen a piece of log is parsed, there is a corresponding property available, called parsed, injected by LAL. Property parsed is typically a map, containing all the fields parsed from the raw logs. For example, if the parser is json / yaml, parsed is a map containing all the key-values in the json / yaml; if the parser is text , parsed is a map containing all the captured groups and their values (for regexp and grok).\nAll parsers share the following options:\n   Option Type Description Default Value     abortOnFailure boolean Whether the filter chain should abort if the parser failed to parse / match the logs true    See examples below.\njson filter { json { abortOnFailure true // this is optional because it\u0026#39;s default behaviour  } } yaml filter { yaml { abortOnFailure true // this is optional because it\u0026#39;s default behaviour  } } text For unstructured logs, there are some text parsers for use.\n regexp  regexp parser uses a regular expression (regexp) to parse the logs. It leverages the captured groups of the regexp, all the captured groups can be used later in the extractors or sinks. regexp returns a boolean indicating whether the log matches the pattern or not.\nfilter { text { abortOnFailure true // this is optional because it\u0026#39;s default behaviour  // this is just a demo pattern  regexp \u0026#34;(?\u0026lt;timestamp\u0026gt;\\\\d{8}) (?\u0026lt;thread\u0026gt;\\\\w+) (?\u0026lt;level\u0026gt;\\\\w+) (?\u0026lt;traceId\u0026gt;\\\\w+) (?\u0026lt;msg\u0026gt;.+)\u0026#34; } extractor { tag level: parsed.level // we add a tag called `level` and its value is parsed.level, captured from the regexp above  traceId parsed.traceId // we also extract the trace id from the parsed result, which will be used to associate the log with the trace  } // ... }  grok (TODO)  We\u0026rsquo;re aware of certains performance issues in the grok Java library, and so we\u0026rsquo;re currently conducting investigations and benchmarking. Contributions are welcome.\nExtractor Extractors aim to extract metadata from the logs. The metadata can be a service name, a service instance name, an endpoint name, or even a trace ID, all of which can be associated with the existing traces and metrics.\n service  service extracts the service name from the parsed result, and set it into the LogData, which will be persisted (if not dropped) and is used to associate with traces / metrics.\n instance  instance extracts the service instance name from the parsed result, and set it into the LogData, which will be persisted (if not dropped) and is used to associate with traces / metrics.\n endpoint  endpoint extracts the service instance name from the parsed result, and set it into the LogData, which will be persisted (if not dropped) and is used to associate with traces / metrics.\n traceId  traceId extracts the trace ID from the parsed result, and set it into the LogData, which will be persisted (if not dropped) and is used to associate with traces / metrics.\n segmentId  segmentId extracts the segment ID from the parsed result, and set it into the LogData, which will be persisted (if not dropped) and is used to associate with traces / metrics.\n spanId  spanId extracts the span ID from the parsed result, and set it into the LogData, which will be persisted (if not dropped) and is used to associate with traces / metrics.\n timestamp  timestamp extracts the timestamp from the parsed result, and set it into the LogData, which will be persisted (if not dropped) and is used to associate with traces / metrics.\nThe unit of timestamp is millisecond.\n tag  tag extracts the tags from the parsed result, and set them into the LogData. The form of this extractor should look something like this: tag key1: value, key2: value2. You may use the properties of parsed as both keys and values.\nfilter { // ... parser  extractor { tag level: parsed.level, (parsed.statusCode): parsed.statusMsg tag anotherKey: \u0026#34;anotherConstantValue\u0026#34; } }  metrics  metrics extracts / generates metrics from the logs, and sends the generated metrics to the meter system. You may configure MAL for further analysis of these metrics. The dedicated MAL config files are under directory log-mal-rules, and you can set log-analyzer/default/malFiles to enable configured files.\n# application.yml # ... log-analyzer: selector: ${SW_LOG_ANALYZER:default} default: lalFiles: ${SW_LOG_LAL_FILES:my-lal-config} # files are under \u0026#34;lal\u0026#34; directory malFiles: ${SW_LOG_MAL_FILES:my-lal-mal-config,another-lal-mal-config} # files are under \u0026#34;log-mal-rules\u0026#34; directory Examples are as follows:\nfilter { // ...  extractor { service parsed.serviceName metrics { name \u0026#34;log_count\u0026#34; timestamp parsed.timestamp labels level: parsed.level, service: parsed.service, instance: parsed.instance value 1 } metrics { name \u0026#34;http_response_time\u0026#34; timestamp parsed.timestamp labels status_code: parsed.statusCode, service: parsed.service, instance: parsed.instance value parsed.duration } } // ... } The extractor above generates a metrics named log_count, with tag key level and value 1. After that, you can configure MAL rules to calculate the log count grouping by logging level like this:\n# ... other configurations of MAL metrics: - name: log_count_debug exp: log_count.tagEqual(\u0026#39;level\u0026#39;, \u0026#39;DEBUG\u0026#39;).sum([\u0026#39;service\u0026#39;, \u0026#39;instance\u0026#39;]).increase(\u0026#39;PT1M\u0026#39;) - name: log_count_error exp: log_count.tagEqual(\u0026#39;level\u0026#39;, \u0026#39;ERROR\u0026#39;).sum([\u0026#39;service\u0026#39;, \u0026#39;instance\u0026#39;]).increase(\u0026#39;PT1M\u0026#39;) The other metrics generated is http_response_time, so you can configure MAL rules to generate more useful metrics like percentiles.\n# ... other configurations of MAL metrics: - name: response_time_percentile exp: http_response_time.sum([\u0026#39;le\u0026#39;, \u0026#39;service\u0026#39;, \u0026#39;instance\u0026#39;]).increase(\u0026#39;PT5M\u0026#39;).histogram().histogram_percentile([50,70,90,99]) Sink Sinks are the persistent layer of the LAL. By default, all the logs of each filter are persisted into the storage. However, some mechanisms allow you to selectively save some logs, or even drop all the logs after you\u0026rsquo;ve extracted useful information, such as metrics.\nSampler Sampler allows you to save the logs in a sampling manner. Currently, the sampling strategy rateLimit is supported. We welcome contributions on more sampling strategies. If multiple samplers are specified, the last one determines the final sampling result. See examples in Enforcer.\nrateLimit samples n logs at a maximum rate of 1 second. rateLimit(\u0026quot;SamplerID\u0026quot;) requires an ID for the sampler. Sampler declarations with the same ID share the same sampler instance, thus sharing the same qps and resetting logic.\nExamples:\nfilter { // ... parser  sink { sampler { if (parsed.service == \u0026#34;ImportantApp\u0026#34;) { rateLimit(\u0026#34;ImportantAppSampler\u0026#34;) { qps 30 // samples 30 pieces of logs every second for service \u0026#34;ImportantApp\u0026#34;  } } else { rateLimit(\u0026#34;OtherSampler\u0026#34;) { qps 3 // samples 3 pieces of logs every second for other services than \u0026#34;ImportantApp\u0026#34;  } } } } } Dropper Dropper is a special sink, meaning that all logs are dropped without any exception. This is useful when you want to drop debugging logs.\nfilter { // ... parser  sink { if (parsed.level == \u0026#34;DEBUG\u0026#34;) { dropper {} } else { sampler { // ... configs  } } } } Or if you have multiple filters, some of which are for extracting metrics, only one of them has to be persisted.\nfilter { // filter A: this is for persistence  // ... parser  sink { sampler { // .. sampler configs  } } } filter { // filter B:  // ... extractors to generate many metrics  extractors { metrics { // ... metrics  } } sink { dropper {} // drop all logs because they have been saved in \u0026#34;filter A\u0026#34; above.  } } Enforcer Enforcer is another special sink that forcibly samples the log. A typical use case of enforcer is when you have configured a sampler and want to save some logs forcibly, such as to save error logs even if the sampling mechanism has been configured.\nfilter { // ... parser  sink { sampler { // ... sampler configs  } if (parserd.level == \u0026#34;ERROR\u0026#34; || parsed.userId == \u0026#34;TestingUserId\u0026#34;) { // sample error logs or testing users\u0026#39; logs (userId == \u0026#34;TestingUserId\u0026#34;) even if the sampling strategy is configured  enforcer { } } } } You can use enforcer and dropper to simulate a probabilistic sampler like this.\nfilter { // ... parser  sink { sampler { // simulate a probabilistic sampler with sampler rate 30% (not accurate though)  if (Math.abs(Math.random()) \u0026gt; 0.3) { enforcer {} } else { dropper {} } } } } ","excerpt":"Log Analysis Language Log Analysis Language (LAL) in SkyWalking is essentially a Domain-Specific …","ref":"/docs/main/v8.5.0/en/concepts-and-designs/lal/","title":"Log Analysis Language"},{"body":"Log Analyzer Log analyzer supports native log data. OAP could use Log Analysis Language to structurize log content through parse, extract, and save logs. Also the analyzer leverages Meter Analysis Language Engine for further metrics calculation.\nlog-analyzer: selector: ${SW_LOG_ANALYZER:default} default: lalFiles: ${SW_LOG_LAL_FILES:default} malFiles: ${SW_LOG_MAL_FILES:\u0026#34;\u0026#34;} Read Log Analysis Language documentation to learn log structurize and metrics analysis.\n","excerpt":"Log Analyzer Log analyzer supports native log data. OAP could use Log Analysis Language to …","ref":"/docs/main/v8.5.0/en/setup/backend/log-analyzer/","title":"Log Analyzer"},{"body":"Log Data Protocol Report log data via protocol.\ngRPC service define\n","excerpt":"Log Data Protocol Report log data via protocol.\ngRPC service define","ref":"/docs/main/v8.5.0/en/protocols/log-data-protocol/","title":"Log Data Protocol"},{"body":"logback plugin  Dependency the toolkit, such as using maven or gradle  \u0026lt;dependency\u0026gt; \u0026lt;groupId\u0026gt;org.apache.skywalking\u0026lt;/groupId\u0026gt; \u0026lt;artifactId\u0026gt;apm-toolkit-logback-1.x\u0026lt;/artifactId\u0026gt; \u0026lt;version\u0026gt;{project.release.version}\u0026lt;/version\u0026gt; \u0026lt;/dependency\u0026gt; Print trace ID in your logs  set %tid in Pattern section of logback.xml  \u0026lt;appender name=\u0026#34;STDOUT\u0026#34; class=\u0026#34;ch.qos.logback.core.ConsoleAppender\u0026#34;\u0026gt; \u0026lt;encoder class=\u0026#34;ch.qos.logback.core.encoder.LayoutWrappingEncoder\u0026#34;\u0026gt; \u0026lt;layout class=\u0026#34;org.apache.skywalking.apm.toolkit.log.logback.v1.x.TraceIdPatternLogbackLayout\u0026#34;\u0026gt; \u0026lt;Pattern\u0026gt;%d{yyyy-MM-dd HH:mm:ss.SSS} [%tid] [%thread] %-5level %logger{36} -%msg%n\u0026lt;/Pattern\u0026gt; \u0026lt;/layout\u0026gt; \u0026lt;/encoder\u0026gt; \u0026lt;/appender\u0026gt;  with the MDC, set %X{tid} in Pattern section of logback.xml  \u0026lt;appender name=\u0026#34;STDOUT\u0026#34; class=\u0026#34;ch.qos.logback.core.ConsoleAppender\u0026#34;\u0026gt; \u0026lt;encoder class=\u0026#34;ch.qos.logback.core.encoder.LayoutWrappingEncoder\u0026#34;\u0026gt; \u0026lt;layout class=\u0026#34;org.apache.skywalking.apm.toolkit.log.logback.v1.x.mdc.TraceIdMDCPatternLogbackLayout\u0026#34;\u0026gt; \u0026lt;Pattern\u0026gt;%d{yyyy-MM-dd HH:mm:ss.SSS} [%X{tid}] [%thread] %-5level %logger{36} -%msg%n\u0026lt;/Pattern\u0026gt; \u0026lt;/layout\u0026gt; \u0026lt;/encoder\u0026gt; \u0026lt;/appender\u0026gt;  Support logback AsyncAppender(MDC also support), No additional configuration is required. Refer to the demo of logback.xml below. For details: Logback AsyncAppender  \u0026lt;configuration scan=\u0026#34;true\u0026#34; scanPeriod=\u0026#34; 5 seconds\u0026#34;\u0026gt; \u0026lt;appender name=\u0026#34;STDOUT\u0026#34; class=\u0026#34;ch.qos.logback.core.ConsoleAppender\u0026#34;\u0026gt; \u0026lt;encoder class=\u0026#34;ch.qos.logback.core.encoder.LayoutWrappingEncoder\u0026#34;\u0026gt; \u0026lt;layout class=\u0026#34;org.apache.skywalking.apm.toolkit.log.logback.v1.x.mdc.TraceIdMDCPatternLogbackLayout\u0026#34;\u0026gt; \u0026lt;Pattern\u0026gt;%d{yyyy-MM-dd HH:mm:ss.SSS} [%X{tid}] [%thread] %-5level %logger{36} -%msg%n\u0026lt;/Pattern\u0026gt; \u0026lt;/layout\u0026gt; \u0026lt;/encoder\u0026gt; \u0026lt;/appender\u0026gt; \u0026lt;appender name=\u0026#34;ASYNC\u0026#34; class=\u0026#34;ch.qos.logback.classic.AsyncAppender\u0026#34;\u0026gt; \u0026lt;discardingThreshold\u0026gt;0\u0026lt;/discardingThreshold\u0026gt; \u0026lt;queueSize\u0026gt;1024\u0026lt;/queueSize\u0026gt; \u0026lt;neverBlock\u0026gt;true\u0026lt;/neverBlock\u0026gt; \u0026lt;appender-ref ref=\u0026#34;STDOUT\u0026#34;/\u0026gt; \u0026lt;/appender\u0026gt; \u0026lt;root level=\u0026#34;INFO\u0026#34;\u0026gt; \u0026lt;appender-ref ref=\u0026#34;ASYNC\u0026#34;/\u0026gt; \u0026lt;/root\u0026gt; \u0026lt;/configuration\u0026gt;  When you use -javaagent to active the sky-walking tracer, logback will output traceId, if it existed. If the tracer is inactive, the output will be TID: N/A.  logstash logback plugin  Dependency the toolkit, such as using maven or gradle  \u0026lt;dependency\u0026gt; \u0026lt;groupId\u0026gt;org.apache.skywalking\u0026lt;/groupId\u0026gt; \u0026lt;artifactId\u0026gt;apm-toolkit-logback-1.x\u0026lt;/artifactId\u0026gt; \u0026lt;version\u0026gt;${skywalking.version}\u0026lt;/version\u0026gt; \u0026lt;/dependency\u0026gt;  set LogstashEncoder of logback.xml  \u0026lt;encoder charset=\u0026#34;UTF-8\u0026#34; class=\u0026#34;net.logstash.logback.encoder.LogstashEncoder\u0026#34;\u0026gt; \u0026lt;provider class=\u0026#34;org.apache.skywalking.apm.toolkit.log.logback.v1.x.logstash.TraceIdJsonProvider\u0026#34;\u0026gt; \u0026lt;/provider\u0026gt; \u0026lt;/encoder\u0026gt;  set LoggingEventCompositeJsonEncoder of logstash in logback-spring.xml for custom json format  1.add converter for %tid as child of  node\n\u0026lt;!--add converter for %tid --\u0026gt; \u0026lt;conversionRule conversionWord=\u0026#34;tid\u0026#34; converterClass=\u0026#34;org.apache.skywalking.apm.toolkit.log.logback.v1.x.LogbackPatternConverter\u0026#34;/\u0026gt; 2.add json encoder for custom json format\n\u0026lt;encoder class=\u0026#34;net.logstash.logback.encoder.LoggingEventCompositeJsonEncoder\u0026#34;\u0026gt; \u0026lt;providers\u0026gt; \u0026lt;timestamp\u0026gt; \u0026lt;timeZone\u0026gt;UTC\u0026lt;/timeZone\u0026gt; \u0026lt;/timestamp\u0026gt; \u0026lt;pattern\u0026gt; \u0026lt;pattern\u0026gt; { \u0026#34;level\u0026#34;: \u0026#34;%level\u0026#34;, \u0026#34;tid\u0026#34;: \u0026#34;%tid\u0026#34;, \u0026#34;thread\u0026#34;: \u0026#34;%thread\u0026#34;, \u0026#34;class\u0026#34;: \u0026#34;%logger{1.}:%L\u0026#34;, \u0026#34;message\u0026#34;: \u0026#34;%message\u0026#34;, \u0026#34;stackTrace\u0026#34;: \u0026#34;%exception{10}\u0026#34; } \u0026lt;/pattern\u0026gt; \u0026lt;/pattern\u0026gt; \u0026lt;/providers\u0026gt; \u0026lt;/encoder\u0026gt; gRPC reporter The gRPC reporter could forward the collected logs to SkyWalking OAP server, or SkyWalking Satellite sidecar. Trace id, segment id, and span id will attach to logs automatically. There is no need to modify existing layouts.\n Add GRPCLogClientAppender in logback.xml  \u0026lt;appender name=\u0026#34;grpc-log\u0026#34; class=\u0026#34;org.apache.skywalking.apm.toolkit.log.logback.v1.x.log.GRPCLogClientAppender\u0026#34;\u0026gt; \u0026lt;encoder class=\u0026#34;ch.qos.logback.core.encoder.LayoutWrappingEncoder\u0026#34;\u0026gt; \u0026lt;layout class=\u0026#34;org.apache.skywalking.apm.toolkit.log.logback.v1.x.mdc.TraceIdMDCPatternLogbackLayout\u0026#34;\u0026gt; \u0026lt;Pattern\u0026gt;%d{yyyy-MM-dd HH:mm:ss.SSS} [%X{tid}] [%thread] %-5level %logger{36} -%msg%n\u0026lt;/Pattern\u0026gt; \u0026lt;/layout\u0026gt; \u0026lt;/encoder\u0026gt; \u0026lt;/appender\u0026gt;  Add config of the plugin or use default  plugin.toolkit.log.grpc.reporter.server_host=${SW_GRPC_LOG_SERVER_HOST:127.0.0.1} plugin.toolkit.log.grpc.reporter.server_port=${SW_GRPC_LOG_SERVER_PORT:11800} plugin.toolkit.log.grpc.reporter.max_message_size=${SW_GRPC_LOG_MAX_MESSAGE_SIZE:10485760} plugin.toolkit.log.grpc.reporter.upstream_timeout=${SW_GRPC_LOG_GRPC_UPSTREAM_TIMEOUT:30} Transmitting un-formatted messages The logback 1.x gRPC reporter supports transmitting logs as formatted or un-formatted. Transmitting formatted data is the default but can be disabled by adding the following to the agent config:\nplugin.toolkit.log.transmit_formatted=false The above will result in the content field being used for the log pattern with additional log tags of argument.0, argument.1, and so on representing each logged argument as well as an additional exception tag which is only present if a throwable is also logged.\nFor example, the following code:\nlog.info(\u0026#34;{} {} {}\u0026#34;, 1, 2, 3); Will result in:\n{ \u0026#34;content\u0026#34;: \u0026#34;{} {} {}\u0026#34;, \u0026#34;tags\u0026#34;: [ { \u0026#34;key\u0026#34;: \u0026#34;argument.0\u0026#34;, \u0026#34;value\u0026#34;: \u0026#34;1\u0026#34; }, { \u0026#34;key\u0026#34;: \u0026#34;argument.1\u0026#34;, \u0026#34;value\u0026#34;: \u0026#34;2\u0026#34; }, { \u0026#34;key\u0026#34;: \u0026#34;argument.2\u0026#34;, \u0026#34;value\u0026#34;: \u0026#34;3\u0026#34; } ] } ","excerpt":"logback plugin  Dependency the toolkit, such as using maven or gradle  \u0026lt;dependency\u0026gt; …","ref":"/docs/main/v8.5.0/en/setup/service-agent/java-agent/application-toolkit-logback-1.x/","title":"logback plugin"},{"body":"Manual instrument SDK Our incredible community has contributed to the manual instrument SDK.\n Go2Sky. Go SDK follows the SkyWalking format. C++. C++ SDK follows the SkyWalking format.  What are the SkyWalking format and the propagation protocols? See these protocols in protocols document.\nEnvoy tracer Envoy has its internal tracer implementation for SkyWalking. Read SkyWalking Tracer doc and SkyWalking tracing sandbox for more details.\n","excerpt":"Manual instrument SDK Our incredible community has contributed to the manual instrument SDK. …","ref":"/docs/main/v8.5.0/en/concepts-and-designs/manual-sdk/","title":"Manual instrument SDK"},{"body":"Meter Analysis Language The meter system provides a functional analysis language called MAL (Meter Analysis Language) that lets users analyze and aggregate meter data in the OAP streaming system. The result of an expression can either be ingested by the agent analyzer, or the OC/Prometheus analyzer.\nLanguage data type In MAL, an expression or sub-expression can evaluate to one of the following two types:\n Sample family: A set of samples (metrics) containing a range of metrics whose names are identical. Scalar: A simple numeric value that supports integer/long and floating/double.  Sample family A set of samples, which acts as the basic unit in MAL. For example:\ninstance_trace_count The sample family above may contain the following samples which are provided by external modules, such as the agent analyzer:\ninstance_trace_count{region=\u0026quot;us-west\u0026quot;,az=\u0026quot;az-1\u0026quot;} 100 instance_trace_count{region=\u0026quot;us-east\u0026quot;,az=\u0026quot;az-3\u0026quot;} 20 instance_trace_count{region=\u0026quot;asia-north\u0026quot;,az=\u0026quot;az-1\u0026quot;} 33 Tag filter MAL supports four type operations to filter samples in a sample family:\n tagEqual: Filter tags exactly equal to the string provided. tagNotEqual: Filter tags not equal to the string provided. tagMatch: Filter tags that regex-match the string provided. tagNotMatch: Filter labels that do not regex-match the string provided.  For example, this filters all instance_trace_count samples for us-west and asia-north region and az-1 az:\ninstance_trace_count.tagMatch(\u0026quot;region\u0026quot;, \u0026quot;us-west|asia-north\u0026quot;).tagEqual(\u0026quot;az\u0026quot;, \u0026quot;az-1\u0026quot;) Value filter MAL supports six type operations to filter samples in a sample family by value:\n valueEqual: Filter values exactly equal to the value provided. valueNotEqual: Filter values equal to the value provided. valueGreater: Filter values greater than the value provided. valueGreaterEqual: Filter values greater than or equal to the value provided. valueLess: Filter values less than the value provided. valueLessEqual: Filter values less than or equal to the value provided.  For example, this filters all instance_trace_count samples for values \u0026gt;= 33:\ninstance_trace_count.valueGreaterEqual(33) Tag manipulator MAL allows tag manipulators to change (i.e. add/delete/update) tags and their values.\nK8s MAL supports using the metadata of K8s to manipulate the tags and their values. This feature requires authorizing the OAP Server to access K8s\u0026rsquo;s API Server.\nretagByK8sMeta retagByK8sMeta(newLabelName, K8sRetagType, existingLabelName, namespaceLabelName). Add a new tag to the sample family based on the value of an existing label. Provide several internal converting types, including\n K8sRetagType.Pod2Service  Add a tag to the sample using service as the key, $serviceName.$namespace as the value, and according to the given value of the tag key, which represents the name of a pod.\nFor example:\ncontainer_cpu_usage_seconds_total{namespace=default, container=my-nginx, cpu=total, pod=my-nginx-5dc4865748-mbczh} 2 Expression:\ncontainer_cpu_usage_seconds_total.retagByK8sMeta('service' , K8sRetagType.Pod2Service , 'pod' , 'namespace') Output:\ncontainer_cpu_usage_seconds_total{namespace=default, container=my-nginx, cpu=total, pod=my-nginx-5dc4865748-mbczh, service='nginx-service.default'} 2 Binary operators The following binary arithmetic operators are available in MAL:\n + (addition) - (subtraction) * (multiplication) / (division)  Binary operators are defined between scalar/scalar, sampleFamily/scalar and sampleFamily/sampleFamily value pairs.\nBetween two scalars: they evaluate to another scalar that is the result of the operator being applied to both scalar operands:\n1 + 2 Between a sample family and a scalar, the operator is applied to the value of every sample in the sample family. For example:\ninstance_trace_count + 2 or\n2 + instance_trace_count results in\ninstance_trace_count{region=\u0026quot;us-west\u0026quot;,az=\u0026quot;az-1\u0026quot;} 102 // 100 + 2 instance_trace_count{region=\u0026quot;us-east\u0026quot;,az=\u0026quot;az-3\u0026quot;} 22 // 20 + 2 instance_trace_count{region=\u0026quot;asia-north\u0026quot;,az=\u0026quot;az-1\u0026quot;} 35 // 33 + 2 Between two sample families, a binary operator is applied to each sample in the sample family on the left and its matching sample in the sample family on the right. A new sample family with empty name will be generated. Only the matched tags will be reserved. Samples with no matching samples in the sample family on the right will not be found in the result.\nAnother sample family instance_trace_analysis_error_count is\ninstance_trace_analysis_error_count{region=\u0026quot;us-west\u0026quot;,az=\u0026quot;az-1\u0026quot;} 20 instance_trace_analysis_error_count{region=\u0026quot;asia-north\u0026quot;,az=\u0026quot;az-1\u0026quot;} 11 Example expression:\ninstance_trace_analysis_error_count / instance_trace_count This returns a resulting sample family containing the error rate of trace analysis. Samples with region us-west and az az-3 have no match and will not show up in the result:\n{region=\u0026quot;us-west\u0026quot;,az=\u0026quot;az-1\u0026quot;} 0.8 // 20 / 100 {region=\u0026quot;asia-north\u0026quot;,az=\u0026quot;az-1\u0026quot;} 0.3333 // 11 / 33 Aggregation Operation Sample family supports the following aggregation operations that can be used to aggregate the samples of a single sample family, resulting in a new sample family having fewer samples (sometimes having just a single sample) with aggregated values:\n sum (calculate sum over dimensions) min (select minimum over dimensions) max (select maximum over dimensions) avg (calculate the average over dimensions)  These operations can be used to aggregate overall label dimensions or preserve distinct dimensions by inputting by parameter.\n\u0026lt;aggr-op\u0026gt;(by: \u0026lt;tag1, tag2, ...\u0026gt;) Example expression:\ninstance_trace_count.sum(by: ['az']) will output the following result:\ninstance_trace_count{az=\u0026quot;az-1\u0026quot;} 133 // 100 + 33 instance_trace_count{az=\u0026quot;az-3\u0026quot;} 20 Function Duraton is a textual representation of a time range. The formats accepted are based on the ISO-8601 duration format {@code PnDTnHnMn.nS} where a day is regarded as exactly 24 hours.\nExamples:\n \u0026ldquo;PT20.345S\u0026rdquo; \u0026ndash; parses as \u0026ldquo;20.345 seconds\u0026rdquo; \u0026ldquo;PT15M\u0026rdquo; \u0026ndash; parses as \u0026ldquo;15 minutes\u0026rdquo; (where a minute is 60 seconds) \u0026ldquo;PT10H\u0026rdquo; \u0026ndash; parses as \u0026ldquo;10 hours\u0026rdquo; (where an hour is 3600 seconds) \u0026ldquo;P2D\u0026rdquo; \u0026ndash; parses as \u0026ldquo;2 days\u0026rdquo; (where a day is 24 hours or 86400 seconds) \u0026ldquo;P2DT3H4M\u0026rdquo; \u0026ndash; parses as \u0026ldquo;2 days, 3 hours and 4 minutes\u0026rdquo; \u0026ldquo;P-6H3M\u0026rdquo; \u0026ndash; parses as \u0026ldquo;-6 hours and +3 minutes\u0026rdquo; \u0026ldquo;-P6H3M\u0026rdquo; \u0026ndash; parses as \u0026ldquo;-6 hours and -3 minutes\u0026rdquo; \u0026ldquo;-P-6H+3M\u0026rdquo; \u0026ndash; parses as \u0026ldquo;+6 hours and -3 minutes\u0026rdquo;  increase increase(Duration): Calculates the increase in the time range.\nrate rate(Duration): Calculates the per-second average rate of increase in the time range.\nirate irate(): Calculates the per-second instant rate of increase in the time range.\ntag tag({allTags -\u0026gt; }): Updates tags of samples. User can add, drop, rename and update tags.\nhistogram histogram(le: '\u0026lt;the tag name of le\u0026gt;'): Transforms less-based histogram buckets to meter system histogram buckets. le parameter represents the tag name of the bucket.\nhistogram_percentile histogram_percentile([\u0026lt;p scalar\u0026gt;]). Represents the meter-system to calculate the p-percentile (0 ≤ p ≤ 100) from the buckets.\ntime time(): Returns the number of seconds since January 1, 1970 UTC.\nDown Sampling Operation MAL should instruct meter-system on how to downsample for metrics. It doesn\u0026rsquo;t only refer to aggregate raw samples to minute level, but also expresses data from minute in higher levels, such as hour and day.\nDown sampling function is called downsampling in MAL, and it accepts the following types:\n AVG SUM LATEST MIN (TODO) MAX (TODO) MEAN (TODO) COUNT (TODO)  The default type is AVG.\nIf users want to get the latest time from last_server_state_sync_time_in_seconds:\nlast_server_state_sync_time_in_seconds.tagEqual('production', 'catalog').downsampling(LATEST) Metric level function There are three levels in metric: service, instance and endpoint. They extract level relevant labels from metric labels, then informs the meter-system the level to which this metric belongs.\n servcie([svc_label1, svc_label2...]) extracts service level labels from the array argument. instance([svc_label1, svc_label2...], [ins_label1, ins_label2...]) extracts service level labels from the first array argument, extracts instance level labels from the second array argument. endpoint([svc_label1, svc_label2...], [ep_label1, ep_label2...]) extracts service level labels from the first array argument, extracts endpoint level labels from the second array argument.  More Examples Please refer to OAP Self-Observability\n","excerpt":"Meter Analysis Language The meter system provides a functional analysis language called MAL (Meter …","ref":"/docs/main/v8.5.0/en/concepts-and-designs/mal/","title":"Meter Analysis Language"},{"body":"Meter Receiver Meter receiver is accepting the metrics of meter protocol format into the Meter System.\nModule define receiver-meter: selector: ${SW_RECEIVER_METER:default} default: In Kafka Fetcher, we need to follow the configuration to enable it.\nkafka-fetcher: selector: ${SW_KAFKA_FETCHER:default} default: bootstrapServers: ${SW_KAFKA_FETCHER_SERVERS:localhost:9092} enableMeterSystem: ${SW_KAFKA_FETCHER_ENABLE_METER_SYSTEM:true} Configuration file Meter receiver is configured via a configuration file. The configuration file defines everything related to receiving from agents, as well as which rule files to load.\nOAP can load the configuration at bootstrap. If the new configuration is not well-formed, OAP fails to start up. The files are located at $CLASSPATH/meter-analyzer-config.\nThe file is written in YAML format, defined by the scheme described below. Brackets indicate that a parameter is optional.\nAn example can be found here. If you\u0026rsquo;re using Spring sleuth, you could use Spring Sleuth Setup.\nMeters configure # expSuffix is appended to all expression in this file. expSuffix: \u0026lt;string\u0026gt; # insert metricPrefix into metric name: \u0026lt;metricPrefix\u0026gt;_\u0026lt;raw_metric_name\u0026gt; metricPrefix: \u0026lt;string\u0026gt; # Metrics rule allow you to recompute queries. metricsRules: # The name of rule, which combinates with a prefix \u0026#39;meter_\u0026#39; as the index/table name in storage. name: \u0026lt;string\u0026gt; # MAL expression. exp: \u0026lt;string\u0026gt; More about MAL, please refer to mal.md\nAbout rate, irate, increase Even we supported rate, irate, increase function in the backend, but we still recommend user to consider using client-side APIs to do these. Because\n The OAP has to set up caches to calculate the value. Once the agent reconnected to another OAP instance, the time windows of rate calculation will break. Then, the result would not be accurate.  ","excerpt":"Meter Receiver Meter receiver is accepting the metrics of meter protocol format into the Meter …","ref":"/docs/main/v8.5.0/en/setup/backend/backend-meter/","title":"Meter Receiver"},{"body":"Meter System Meter system is another streaming calculation mode designed for metrics data. In the OAL, there are clear Scope Definitions, including definitions for native objects. Meter system is focused on the data type itself, and provides a more flexible approach to the end user in defining the scope entity.\nThe meter system is open to different receivers and fetchers in the backend, see the backend setup document for more details.\nEvery metric is declared in the meter system to include the following attributes:\n Metrics Name. A globally unique name to avoid overlapping between the OAL variable names. Function Name. The function used for this metric, namely distributed aggregation, value calculation or down sampling calculation based on the function implementation. Further, the data structure is determined by the function as well, such as function Avg is for Long. Scope Type. Unlike within the OAL, there are plenty of logic scope definitions. In the meter system, only type is required. Type values include service, instance, and endpoint, just as we have described in the Overview section. The values of scope entity name, such as service name, are required when metrics data are generated with the metrics data values.  NOTE: The metrics must be declared in the bootstrap stage, and there must be no change to runtime.\nThe Meter System supports the following binding functions:\n avg. Calculates the avg value for every entity under the same metrics name. histogram. Aggregates the counts in the configurable buckets. Buckets are configurable but must be assigned in the declaration stage. percentile. See percentile in WIKI. Unlike the OAL, we provide 50/75/90/95/99 by default. In the meter system function, the percentile function accepts several ranks, which should be in the (0, 100) range.  ","excerpt":"Meter System Meter system is another streaming calculation mode designed for metrics data. In the …","ref":"/docs/main/v8.5.0/en/concepts-and-designs/meter/","title":"Meter System"},{"body":"Metrics Exporter SkyWalking provides basic and most important metrics aggregation, alarm and analysis. In real world, people may want to forward the data to their 3rd party system, for deeper analysis or anything else. Metrics Exporter makes that possible.\nMetrics exporter is an independent module, you need manually active it.\nRight now, we provide the following exporters\n gRPC exporter  gRPC exporter gRPC exporter uses SkyWalking native exporter service definition. Here is proto definition.\nservice MetricExportService { rpc export (stream ExportMetricValue) returns (ExportResponse) { } rpc subscription (SubscriptionReq) returns (SubscriptionsResp) { }}message ExportMetricValue { string metricName = 1; string entityName = 2; string entityId = 3; ValueType type = 4; int64 timeBucket = 5; int64 longValue = 6; double doubleValue = 7; repeated int64 longValues = 8;}message SubscriptionsResp { repeated SubscriptionMetric metrics = 1;}message SubscriptionMetric { string metricName = 1; EventType eventType = 2;}enum ValueType { LONG = 0; DOUBLE = 1; MULTI_LONG = 2;}enum EventType { // The metrics aggregated in this bulk, not include the existing persistent data.  INCREMENT = 0; // Final result of the metrics at this moment.  TOTAL = 1;}message SubscriptionReq {}message ExportResponse {}To active the exporter, you should add this into your application.yml\nexporter: grpc: targetHost: 127.0.0.1 targetPort: 9870  targetHost:targetPort is the expected target service address. You could set any gRPC server to receive the data. Target gRPC service needs to be standby, otherwise, the OAP starts up failure.  For target exporter service subscription implementation Return the expected metrics name list with event type(increment or total), all the names must match the OAL/MAL script definition. Return empty list, if you want to export all metrics in increment event type.\nexport implementation Stream service, all subscribed metrics will be sent to here, based on OAP core schedule. Also, if the OAP deployed as cluster, then this method will be called concurrently. For metrics value, you need follow #type to choose #longValue or #doubleValue.\n","excerpt":"Metrics Exporter SkyWalking provides basic and most important metrics aggregation, alarm and …","ref":"/docs/main/v8.5.0/en/setup/backend/metrics-exporter/","title":"Metrics Exporter"},{"body":"Namespace Background SkyWalking is a monitoring tool, which collects metrics from a distributed system. In the real world, a very large distributed system includes hundreds of services, thousands of service instances. In that case, most likely, more than one group, even more than one company are maintaining and monitoring the distributed system. Each one of them takes charge of different parts, don\u0026rsquo;t want or shouldn\u0026rsquo;t share there metrics.\nNamespace is the proposal from this.It is used for tracing and monitoring isolation.\nSet the namespace Set agent.namespace in agent config # The agent namespace # agent.namespace=default-namespace The default value of agent.namespace is empty.\nInfluence The default header key of SkyWalking is sw8, more in this document. After agent.namespace is set, the key changes to namespace-sw8.\nThe across process propagation chain breaks, when the two sides are using different namespace.\n","excerpt":"Namespace Background SkyWalking is a monitoring tool, which collects metrics from a distributed …","ref":"/docs/main/v8.5.0/en/setup/service-agent/java-agent/namespace/","title":"Namespace"},{"body":"Observability Analysis Language OAL(Observability Analysis Language) serves to analyze incoming data in streaming mode.\nOAL focuses on metrics in Service, Service Instance and Endpoint. Therefore, the language is easy to learn and use.\nSince 6.3, the OAL engine is embedded in OAP server runtime as oal-rt(OAL Runtime). OAL scripts are now found in the /config folder, and users could simply change and reboot the server to run them. However, the OAL script is a compiled language, and the OAL Runtime generates java codes dynamically.\nYou can open set SW_OAL_ENGINE_DEBUG=Y at system env to see which classes are generated.\nGrammar Scripts should be named *.oal\n// Declare the metrics. METRICS_NAME = from(SCOPE.(* | [FIELD][,FIELD ...])) [.filter(FIELD OP [INT | STRING])] .FUNCTION([PARAM][, PARAM ...]) // Disable hard code disable(METRICS_NAME); Scope Primary SCOPEs are All, Service, ServiceInstance, Endpoint, ServiceRelation, ServiceInstanceRelation, and EndpointRelation. There are also some secondary scopes which belong to a primary scope.\nSee Scope Definitions, where you can find all existing Scopes and Fields.\nFilter Use filter to build conditions for the value of fields by using field name and expression.\nThe expressions support linking by and, or and (...). The OPs support ==, !=, \u0026gt;, \u0026lt;, \u0026gt;=, \u0026lt;=, in [...] ,like %..., like ...% , like %...% , contain and not contain, with type detection based on field type. In the event of incompatibility, compile or code generation errors may be triggered.\nAggregation Function The default functions are provided by the SkyWalking OAP core, and it is possible to implement additional functions.\nFunctions provided\n longAvg. The avg of all input per scope entity. The input field must be a long.   instance_jvm_memory_max = from(ServiceInstanceJVMMemory.max).longAvg();\n In this case, the input represents the request of each ServiceInstanceJVMMemory scope, and avg is based on field max.\n doubleAvg. The avg of all input per scope entity. The input field must be a double.   instance_jvm_cpu = from(ServiceInstanceJVMCPU.usePercent).doubleAvg();\n In this case, the input represents the request of each ServiceInstanceJVMCPU scope, and avg is based on field usePercent.\n percent. The number or ratio is expressed as a fraction of 100, where the input matches with the condition.   endpoint_percent = from(Endpoint.*).percent(status == true);\n In this case, all input represents requests of each endpoint, and the condition is endpoint.status == true.\n rate. The rate expressed is as a fraction of 100, where the input matches with the condition.   browser_app_error_rate = from(BrowserAppTraffic.*).rate(trafficCategory == BrowserAppTrafficCategory.FIRST_ERROR, trafficCategory == BrowserAppTrafficCategory.NORMAL);\n In this case, all input represents requests of each browser app traffic, the numerator condition is trafficCategory == BrowserAppTrafficCategory.FIRST_ERROR and denominator condition is trafficCategory == BrowserAppTrafficCategory.NORMAL. Parameter (1) is the numerator condition. Parameter (2) is the denominator condition.\n count. The sum of calls per scope entity.   service_calls_sum = from(Service.*).count();\n In this case, the number of calls of each service.\n histogram. See Heatmap in WIKI.   all_heatmap = from(All.latency).histogram(100, 20);\n In this case, the thermodynamic heatmap of all incoming requests. Parameter (1) is the precision of latency calculation, such as in the above case, where 113ms and 193ms are considered the same in the 101-200ms group. Parameter (2) is the group amount. In the above case, 21(param value + 1) groups are 0-100ms, 101-200ms, \u0026hellip; 1901-2000ms, 2000+ms\n apdex. See Apdex in WIKI.   service_apdex = from(Service.latency).apdex(name, status);\n In this case, the apdex score of each service. Parameter (1) is the service name, which reflects the Apdex threshold value loaded from service-apdex-threshold.yml in the config folder. Parameter (2) is the status of this request. The status(success/failure) reflects the Apdex calculation.\n p99, p95, p90, p75, p50. See percentile in WIKI.   all_percentile = from(All.latency).percentile(10);\n percentile is the first multiple-value metric, which has been introduced since 7.0.0. As a metric with multiple values, it could be queried through the getMultipleLinearIntValues GraphQL query. In this case, see p99, p95, p90, p75, and p50 of all incoming requests. The parameter is precise to a latency at p99, such as in the above case, and 120ms and 124ms are considered to produce the same response time. Before 7.0.0, p99, p95, p90, p75, p50 func(s) are used to calculate metrics separately. They are still supported in 7.x, but they are no longer recommended and are not included in the current official OAL script.\n all_p99 = from(All.latency).p99(10);\n In this case, the p99 value of all incoming requests. The parameter is precise to a latency at p99, such as in the above case, and 120ms and 124ms are considered to produce the same response time.\nMetrics name The metrics name for storage implementor, alarm and query modules. The type inference is supported by core.\nGroup All metrics data will be grouped by Scope.ID and min-level TimeBucket.\n In the Endpoint scope, the Scope.ID is same as the Endpoint ID (i.e. the unique ID based on service and its endpoint).  Disable Disable is an advanced statement in OAL, which is only used in certain cases. Some of the aggregation and metrics are defined through core hard codes. Examples include segment and top_n_database_statement. This disable statement is designed to render them inactive. By default, none of them are disabled.\nNOTICE, all disable statements should be in oal/disable.oal script file.\nExamples // Calculate p99 of both Endpoint1 and Endpoint2 endpoint_p99 = from(Endpoint.latency).filter(name in (\u0026quot;Endpoint1\u0026quot;, \u0026quot;Endpoint2\u0026quot;)).summary(0.99) // Calculate p99 of Endpoint name started with `serv` serv_Endpoint_p99 = from(Endpoint.latency).filter(name like \u0026quot;serv%\u0026quot;).summary(0.99) // Calculate the avg response time of each Endpoint endpoint_avg = from(Endpoint.latency).avg() // Calculate the p50, p75, p90, p95 and p99 of each Endpoint by 50 ms steps. endpoint_percentile = from(Endpoint.latency).percentile(10) // Calculate the percent of response status is true, for each service. endpoint_success = from(Endpoint.*).filter(status == true).percent() // Calculate the sum of response code in [404, 500, 503], for each service. endpoint_abnormal = from(Endpoint.*).filter(responseCode in [404, 500, 503]).count() // Calculate the sum of request type in [RequestType.RPC, RequestType.gRPC], for each service. endpoint_rpc_calls_sum = from(Endpoint.*).filter(type in [RequestType.RPC, RequestType.gRPC]).count() // Calculate the sum of endpoint name in [\u0026quot;/v1\u0026quot;, \u0026quot;/v2\u0026quot;], for each service. endpoint_url_sum = from(Endpoint.*).filter(name in [\u0026quot;/v1\u0026quot;, \u0026quot;/v2\u0026quot;]).count() // Calculate the sum of calls for each service. endpoint_calls = from(Endpoint.*).count() // Calculate the CPM with the GET method for each service.The value is made up with `tagKey:tagValue`. service_cpm_http_get = from(Service.*).filter(tags contain \u0026quot;http.method:GET\u0026quot;).cpm() // Calculate the CPM with the HTTP method except for the GET method for each service.The value is made up with `tagKey:tagValue`. service_cpm_http_other = from(Service.*).filter(tags not contain \u0026quot;http.method:GET\u0026quot;).cpm() disable(segment); disable(endpoint_relation_server_side); disable(top_n_database_statement); ","excerpt":"Observability Analysis Language OAL(Observability Analysis Language) serves to analyze incoming data …","ref":"/docs/main/v8.5.0/en/concepts-and-designs/oal/","title":"Observability Analysis Language"},{"body":"Observability Analysis Platform SkyWalking is an Observability Analysis Platform that provides full observability to services running in both brown and green zones, as well as services using a hybrid model.\nCapabilities SkyWalking covers all 3 areas of observability, including, Tracing, Metrics and Logging.\n Tracing. SkyWalking native data formats, including Zipkin v1 and v2, as well as Jaeger. Metrics. SkyWalking integrates with Service Mesh platforms, such as Istio, Envoy, and Linkerd, to build observability into the data panel or control panel. Also, SkyWalking native agents can run in the metrics mode, which greatly improves performances. Logging. Includes logs collected from disk or through network. Native agents could bind the tracing context with logs automatically, or use SkyWalking to bind the trace and log through the text content.  There are 3 powerful and native language engines designed to analyze observability data from the above areas.\n Observability Analysis Language processes native traces and service mesh data. Meter Analysis Language is responsible for metrics calculation for native meter data, and adopts a stable and widely used metrics system, such as Prometheus and OpenTelemetry. Log Analysis Language focuses on log contents and collaborate with Meter Analysis Language.  ","excerpt":"Observability Analysis Platform SkyWalking is an Observability Analysis Platform that provides full …","ref":"/docs/main/v8.5.0/en/concepts-and-designs/backend-overview/","title":"Observability Analysis Platform"},{"body":"Observe Service Mesh through ALS Envoy Access Log Service (ALS) provides full logs about RPC routed, including HTTP and TCP.\nBackground The solution was initialized and firstly implemented by Sheng Wu, Hongtao Gao, Lizan Zhou, and Dhi Aurrahman at 17 May. 2019, and was presented on KubeCon China 2019. Here is the recorded video.\nSkyWalking is the first open source project introducing this ALS based solution to the world. This provides a new way with very low payload to service mesh, but the same observability.\nEnable ALS and SkyWalking Receiver You need the following steps to set up ALS.\n  Enable envoyAccessLogService in ProxyConfig and set the ALS address to where SkyWalking OAP listens. On Istio version 1.6.0+, if Istio is installed with demo profile, you can enable ALS with command:\nistioctl manifest apply \\  --set profile=demo \\  --set meshConfig.enableEnvoyAccessLogService=true \\  --set meshConfig.defaultConfig.envoyAccessLogService.address=\u0026lt;skywalking-oap.skywalking.svc:11800\u0026gt; Note: Replace \u0026lt;skywalking-oap.skywalking.svc:11800\u0026gt; with the real address where SkyWalking OAP is deployed.\n  Activate SkyWalking Envoy Receiver. This is activated by default.\n  Choose an ALS analyzer. There are two available analyzers, k8s-mesh and mx-mesh. Set the system environment variable SW_ENVOY_METRIC_ALS_HTTP_ANALYSIS such as SW_ENVOY_METRIC_ALS_HTTP_ANALYSIS=k8s-mesh or in the application.yaml to activate the analyzer. For more about the analyzers, see SkyWalking ALS Analyzers\nenvoy-metric: selector: ${SW_ENVOY_METRIC:default} default: acceptMetricsService: ${SW_ENVOY_METRIC_SERVICE:true} alsHTTPAnalysis: ${SW_ENVOY_METRIC_ALS_HTTP_ANALYSIS:\u0026#34;\u0026#34;} # Setting the system env variable would override this.  To use multiple analyzers as a fallback，please use , to concatenate.\n  Example Here\u0026rsquo;s an example to install Istio and deploy SkyWalking by Helm chart.\nistioctl install \\  --set profile=demo \\  --set meshConfig.enableEnvoyAccessLogService=true \\  --set meshConfig.defaultConfig.envoyAccessLogService.address=skywalking-oap.istio-system:11800 git clone https://github.com/apache/skywalking-kubernetes.git cd skywalking-kubernetes/chart helm repo add elastic https://helm.elastic.co helm dep up skywalking helm install 8.1.0 skywalking -n istio-system \\  --set oap.env.SW_ENVOY_METRIC_ALS_HTTP_ANALYSIS=k8s-mesh \\  --set fullnameOverride=skywalking \\  --set oap.envoy.als.enabled=true You can use kubectl -n istio-system logs -l app=skywalking | grep \u0026quot;K8sALSServiceMeshHTTPAnalysis\u0026quot; to ensure OAP ALS k8s-mesh analyzer has been activated.\nSkyWalking ALS Analyzers There are several available analyzers, k8s-mesh, mx-mesh and persistence, you can specify one or more analyzers to analyze the access logs. When multiple analyzers are specified, it acts as a fast-success mechanism: SkyWalking loops over the analyzers and use it to analyze the logs, once there is an analyzer that is able to produce a result, it stops the loop.\nk8s-mesh k8s-mesh uses the metadata from Kubernetes cluster, hence in this analyzer OAP needs access roles to Pod, Service, and Endpoints.\nThe blog illustrates the detail of how it works, and a step-by-step tutorial to apply it into the bookinfo application.\nmx-mesh mx-mesh uses the Envoy metadata exchange mechanism to get the service name, etc., this analyzer requires Istio to enable the metadata exchange plugin (you can enable it by --set values.telemetry.v2.enabled=true, or if you\u0026rsquo;re using Istio 1.7+ and installing it with profile demo/preview, it should be enabled then).\nThe blog illustrates the detail of how it works, and a step-by-step tutorial to apply it into the Online Boutique system.\npersistence persistence analyzer adapts the Envoy access log format to SkyWalking\u0026rsquo;s native log format , and forwards the formatted logs to LAL, where you can configure persistent conditions, such as sampler, only persist error logs, etc. SkyWalking provides a default configuration file envoy-als.yaml that you can adjust as per your needs. Please make sure to activate this rule via adding the rule name envoy-als into config item log-analyzer/default/lalFiles (or environment variable SW_LOG_LAL_FILES, e.g. SW_LOG_LAL_FILES=envoy-als).\nAttention: because persistence analyzer also needs a mechanism to map the logs into responding services, hence, you need to configure at least one of k8s-mesh or mx-mesh as its antecedent so that persistence analyzer knows which service the logs belong to. For example, you should set envoy-metric/default/alsHTTPAnalysis (or environment variable SW_ENVOY_METRIC_ALS_HTTP_ANALYSIS) to something like k8s-mesh,persistence, mx-mesh,persistence or mx-mesh,k8s-mesh,persistence.\n","excerpt":"Observe Service Mesh through ALS Envoy Access Log Service (ALS) provides full logs about RPC routed, …","ref":"/docs/main/v8.5.0/en/setup/envoy/als_setting/","title":"Observe Service Mesh through ALS"},{"body":"Official OAL script First, read OAL introduction.\nFind OAL script at the /config/oal/*.oal of SkyWalking dist, since 8.0.0. You could change it(such as adding filter condition, or add new metrics) and reboot the OAP server, then it will affect.\nAll metrics named in this script could be used in alarm and UI query.\nNotice,\nIf you try to add or remove some metrics, UI may break, we only recommend you to do this when you plan to build your own UI based on the customization analysis core.\n","excerpt":"Official OAL script First, read OAL introduction.\nFind OAL script at the /config/oal/*.oal of …","ref":"/docs/main/v8.5.0/en/guides/backend-oal-scripts/","title":"Official OAL script"},{"body":"Open Fetcher Fetcher is a concept in SkyWalking backend. It uses pulling mode rather than receiver, which read the data from the target systems. This mode is typically in some metrics SDKs, such as Prometheus.\nPrometheus Fetcher Suppose you want to enable some metric-custom.yaml files stored at fetcher-prom-rules, append its name to enabledRules of prometheus-fetcher as below:\nprometheus-fetcher: selector: ${SW_PROMETHEUS_FETCHER:default} default: enabledRules: ${SW_PROMETHEUS_FETCHER_ENABLED_RULES:\u0026#34;self,metric-custom\u0026#34;} Configuration file Prometheus fetcher is configured via a configuration file. The configuration file defines everything related to fetching services and their instances, as well as which rule files to load.\nOAP can load the configuration at bootstrap. If the new configuration is not well-formed, OAP fails to start up. The files are located at $CLASSPATH/fetcher-prom-rules.\nThe file is written in YAML format, defined by the scheme described below. Brackets indicate that a parameter is optional.\nA full example can be found here\nGeneric placeholders are defined as follows:\n \u0026lt;duration\u0026gt;: a duration This will parse a textual representation of a duration. The formats accepted are based on the ISO-8601 duration format PnDTnHnMn.nS with days considered to be exactly 24 hours. \u0026lt;labelname\u0026gt;: a string matching the regular expression [a-zA-Z_][a-zA-Z0-9_]* \u0026lt;labelvalue\u0026gt;: a string of unicode characters \u0026lt;host\u0026gt;: a valid string consisting of a hostname or IP followed by an optional port number \u0026lt;path\u0026gt;: a valid URL path \u0026lt;string\u0026gt;: a regular string  # How frequently to fetch targets. fetcherInterval: \u0026lt;duration\u0026gt; # Per-fetch timeout when fetching this target. fetcherTimeout: \u0026lt;duration\u0026gt; # The HTTP resource path on which to fetch metrics from targets. metricsPath: \u0026lt;path\u0026gt; #Statically configured targets. staticConfig: # The targets specified by the static config. targets: [ - \u0026lt;target\u0026gt; ] # Labels assigned to all metrics fetched from the targets. labels: [ \u0026lt;labelname\u0026gt;: \u0026lt;labelvalue\u0026gt; ... ] # expSuffix is appended to all expression in this file. expSuffix: \u0026lt;string\u0026gt; # insert metricPrefix into metric name: \u0026lt;metricPrefix\u0026gt;_\u0026lt;raw_metric_name\u0026gt; metricPrefix: \u0026lt;string\u0026gt; # Metrics rule allow you to recompute queries. metricsRules: [ - \u0026lt;metric_rules\u0026gt; ]  # The url of target exporter. the format should be complied with \u0026#34;java.net.URI\u0026#34; url: \u0026lt;string\u0026gt; # The path of root CA file. sslCaFilePath: \u0026lt;string\u0026gt; \u0026lt;metric_rules\u0026gt; # The name of rule, which combinates with a prefix \u0026#39;meter_\u0026#39; as the index/table name in storage. name: \u0026lt;string\u0026gt; # MAL expression. exp: \u0026lt;string\u0026gt; More about MAL, please refer to mal.md\nKafka Fetcher Kafka Fetcher pulls messages from Kafka Broker(s) what is the Agent delivered. Check the agent documentation about the details. Typically Tracing Segments, Service/Instance properties, JVM Metrics, and Meter system data are supported. Kafka Fetcher can work with gRPC/HTTP Receivers at the same time for adopting different transport protocols.\nKafka Fetcher is disabled in default, and we configure as following to enable.\nkafka-fetcher: selector: ${SW_KAFKA_FETCHER:default} default: bootstrapServers: ${SW_KAFKA_FETCHER_SERVERS:localhost:9092} skywalking-segments, skywalking-metrics, skywalking-profile, skywalking-managements and skywalking-meters topics are required by kafka-fetcher. If they do not exist, Kafka Fetcher will create them in default. Also, you can create them by yourself before the OAP server started.\nWhen using the OAP server automatical creation mechanism, you could modify the number of partitions and replications of the topics through the following configurations:\nkafka-fetcher: selector: ${SW_KAFKA_FETCHER:default} default: bootstrapServers: ${SW_KAFKA_FETCHER_SERVERS:localhost:9092} partitions: ${SW_KAFKA_FETCHER_PARTITIONS:3} replicationFactor: ${SW_KAFKA_FETCHER_PARTITIONS_FACTOR:2} enableMeterSystem: ${SW_KAFKA_FETCHER_ENABLE_METER_SYSTEM:false} isSharding: ${SW_KAFKA_FETCHER_IS_SHARDING:false} consumePartitions: ${SW_KAFKA_FETCHER_CONSUME_PARTITIONS:\u0026#34;\u0026#34;} In cluster mode, all topics have the same number of partitions. Then we have to set \u0026quot;isSharding\u0026quot; to \u0026quot;true\u0026quot; and assign the partitions to consume for OAP server. The OAP server can use commas to separate multiple partitions.\nKafka Fetcher allows to configure all the Kafka producers listed here in property kafkaConsumerConfig. Such as:\nkafka-fetcher: selector: ${SW_KAFKA_FETCHER:default} default: bootstrapServers: ${SW_KAFKA_FETCHER_SERVERS:localhost:9092} partitions: ${SW_KAFKA_FETCHER_PARTITIONS:3} replicationFactor: ${SW_KAFKA_FETCHER_PARTITIONS_FACTOR:2} enableMeterSystem: ${SW_KAFKA_FETCHER_ENABLE_METER_SYSTEM:false} isSharding: ${SW_KAFKA_FETCHER_IS_SHARDING:true} consumePartitions: ${SW_KAFKA_FETCHER_CONSUME_PARTITIONS:1,3,5} kafkaConsumerConfig: enable.auto.commit: true ... When use Kafka MirrorMaker 2.0 to replicate topics between Kafka clusters, you can set the source Kafka Cluster alias(mm2SourceAlias) and separator(mm2SourceSeparator) according to your Kafka MirrorMaker config.\nkafka-fetcher: selector: ${SW_KAFKA_FETCHER:default} default: bootstrapServers: ${SW_KAFKA_FETCHER_SERVERS:localhost:9092} partitions: ${SW_KAFKA_FETCHER_PARTITIONS:3} replicationFactor: ${SW_KAFKA_FETCHER_PARTITIONS_FACTOR:2} enableMeterSystem: ${SW_KAFKA_FETCHER_ENABLE_METER_SYSTEM:false} isSharding: ${SW_KAFKA_FETCHER_IS_SHARDING:true} consumePartitions: ${SW_KAFKA_FETCHER_CONSUME_PARTITIONS:1,3,5} mm2SourceAlias: ${SW_KAFKA_MM2_SOURCE_ALIAS:\u0026#34;\u0026#34;} mm2SourceSeparator: ${SW_KAFKA_MM2_SOURCE_SEPARATOR:\u0026#34;\u0026#34;} kafkaConsumerConfig: enable.auto.commit: true ... ","excerpt":"Open Fetcher Fetcher is a concept in SkyWalking backend. It uses pulling mode rather than receiver, …","ref":"/docs/main/v8.5.0/en/setup/backend/backend-fetcher/","title":"Open Fetcher"},{"body":"Oracle and Resin plugins These plugins can\u0026rsquo;t be provided in Apache release because of Oracle and Resin Licenses. If you want to know details, please read Apache license legal document\nDue to license incompatibilities/restrictions these plugins are hosted and released in 3rd part repository, go to OpenSkywalking java plugin extension repository to get these.\n","excerpt":"Oracle and Resin plugins These plugins can\u0026rsquo;t be provided in Apache release because of Oracle …","ref":"/docs/main/v8.5.0/en/setup/service-agent/java-agent/agent-optional-plugins/oracle-resin-plugins/","title":"Oracle and Resin plugins"},{"body":"Overview SkyWalking is an open source observability platform used to collect, analyze, aggregate and visualize data from services and cloud native infrastructures. SkyWalking provides an easy way to maintain a clear view of your distributed systems, even across Clouds. It is a modern APM, specially designed for cloud native, container based distributed systems.\nWhy use SkyWalking? SkyWalking provides solutions for observing and monitoring distributed systems, in many different scenarios. First of all, like traditional approaches, SkyWalking provides auto instrument agents for services, such as Java, C#, Node.js, Go, PHP and Nginx LUA. (with calls out for Python and C++ SDK contributions). In multi-language, continuously deployed environments, cloud native infrastructures grow more powerful but also more complex. SkyWalking\u0026rsquo;s service mesh receiver allows SkyWalking to receive telemetry data from service mesh frameworks such as Istio/Envoy and Linkerd, allowing users to understand the entire distributed system.\nSkyWalking provides observability capabilities for service(s), service instance(s), endpoint(s). The terms Service, Instance and Endpoint are used everywhere today, so it is worth defining their specific meanings in the context of SkyWalking:\n Service. Represents a set/group of workloads which provide the same behaviours for incoming requests. You can define the service name when you are using instrument agents or SDKs. SkyWalking can also use the name you define in platforms such as Istio. Service Instance. Each individual workload in the Service group is known as an instance. Like pods in Kubernetes, it doesn\u0026rsquo;t need to be a single OS process, however, if you are using instrument agents, an instance is actually a real OS process. Endpoint. A path in a service for incoming requests, such as an HTTP URI path or a gRPC service class + method signature.  SkyWalking allows users to understand the topology relationship between Services and Endpoints, to view the metrics of every Service/Service Instance/Endpoint and to set alarm rules.\nIn addition, you can integrate\n Other distributed tracing using SkyWalking native agents and SDKs with Zipkin, Jaeger and OpenCensus. Other metrics systems, such as Prometheus, Sleuth(Micrometer), OpenTelemetry.  Architecture SkyWalking is logically split into four parts: Probes, Platform backend, Storage and UI.\n Probes collect data and reformat them for SkyWalking requirements (different probes support different sources). Platform backend supports data aggregation, analysis and streaming process covers traces, metrics, and logs. Storage houses SkyWalking data through an open/plugable interface. You can choose an existing implementation, such as ElasticSearch, H2, MySQL, TiDB, InfluxDB, or implement your own. Patches for new storage implementors welcome! UI is a highly customizable web based interface allowing SkyWalking end users to visualize and manage SkyWalking data.  What is next?  Learn SkyWalking\u0026rsquo;s Project Goals FAQ, Why doesn\u0026rsquo;t SkyWalking involve MQ in the architecture in default?  ","excerpt":"Overview SkyWalking is an open source observability platform used to collect, analyze, aggregate and …","ref":"/docs/main/v8.5.0/en/concepts-and-designs/overview/","title":"Overview"},{"body":"Plugin automatic test framework Plugin test framework is designed for verifying the plugins' function and compatible status. As there are dozens of plugins and hundreds of versions need to be verified, it is impossible to do manually. The test framework uses container based tech stack, requires a set of real services with agent installed, then the test mock OAP backend is running to check the segments data sent from agents.\nEvery plugin maintained in the main repo requires corresponding test cases, also matching the versions in the supported list doc.\nEnvironment Requirements  MacOS/Linux JDK 8+ Docker Docker Compose  Case Base Image Introduction The test framework provides JVM-container and Tomcat-container base images including JDK8, JDK14. You could choose the suitable one for your test case, if both are suitable, JVM-container is preferred.\nJVM-container Image Introduction JVM-container uses openjdk:8 as the base image. JVM-container has supported JDK14, which inherits openjdk:14. The test case project is required to be packaged as project-name.zip, including startup.sh and uber jar, by using mvn clean package.\nTake the following test projects as good examples\n sofarpc-scenario as a single project case. webflux-scenario as a case including multiple projects. jdk14-with-gson-scenario as a single project case with JDK14.  Tomcat-container Image Introduction Tomcat-container uses tomcat:8.5.57-jdk8-openjdk or tomcat:8.5.57-jdk14-openjdk as the base image. The test case project is required to be packaged as project-name.war by using mvn package.\nTake the following test project as a good example\n spring-4.3.x-scenario  Test project hierarchical structure The test case is an independent maven project, and it is required to be packaged as a war tar ball or zip file, depends on the chosen base image. Also, two external accessible endpoints, mostly two URLs, are required.\nAll test case codes should be in org.apache.skywalking.apm.testcase.* package, unless there are some codes expected being instrumented, then the classes could be in test.org.apache.skywalking.apm.testcase.* package.\nJVM-container test project hierarchical structure\n[plugin-scenario] |- [bin] |- startup.sh |- [config] |- expectedData.yaml |- [src] |- [main] |- ... |- [resource] |- log4j2.xml |- pom.xml |- configuration.yaml |- support-version.list [] = directory Tomcat-container test project hierarchical structure\n[plugin-scenario] |- [config] |- expectedData.yaml |- [src] |- [main] |- ... |- [resource] |- log4j2.xml |- [webapp] |- [WEB-INF] |- web.xml |- pom.xml |- configuration.yaml |- support-version.list [] = directory Test case configuration files The following files are required in every test case.\n   File Name Descriptions     configuration.yml Declare the basic case inform, including, case name, entrance endpoints, mode, dependencies.   expectedData.yaml Describe the expected segmentItems.   support-version.list List the target versions for this case   startup.sh JVM-container only, don\u0026rsquo;t need this when useTomcat-container    * support-version.list format requires every line for a single version(Contains only the last version number of each minor version). Could use # to comment out this version.\nconfiguration.yml    Field description     type Image type, options, jvm or tomcat. Required.   entryService The entrance endpoint(URL) for test case access. Required. (HTTP Method: GET)   healthCheck The health check endpoint(URL) for test case access. Required. (HTTP Method: HEAD)   startScript Path of start up script. Required in type: jvm only.   runningMode Running mode whether with the optional plugin, options, default(default), with_optional, with_bootstrap   withPlugins Plugin selector rule. eg:apm-spring-annotation-plugin-*.jar. Required when runningMode=with_optional or runningMode=with_bootstrap.   environment Same as docker-compose#environment.   depends_on Same as docker-compose#depends_on.   dependencies Same as docker-compose#services, image、links、hostname、environment、depends_on are supported.    Notice:, docker-compose active only when dependencies is only blank.\nrunningMode option description.\n   Option description     default Active all plugins in plugin folder like the official distribution agent.   with_optional Active default and plugins in optional-plugin by the give selector.   with_bootstrap Active default and plugins in bootstrap-plugin by the give selector.    with_optional/with_bootstrap supports multiple selectors, separated by ;.\nFile Format\ntype: entryService: healthCheck: startScript: runningMode: withPlugins: environment: ... depends_on: ... dependencies: service1: image: hostname: expose: ... environment: ... depends_on: ... links: ... entrypoint: ... healthcheck: ...  dependencies supports docker compose healthcheck. But the format is a little difference. We need - as the start of every config item, and describe it as a string line.  Such as in official doc, the health check is\nhealthcheck: test: [\u0026#34;CMD\u0026#34;, \u0026#34;curl\u0026#34;, \u0026#34;-f\u0026#34;, \u0026#34;http://localhost\u0026#34;] interval: 1m30s timeout: 10s retries: 3 start_period: 40s The here, you should write as\nhealthcheck: - \u0026#39;test: [\u0026#34;CMD\u0026#34;, \u0026#34;curl\u0026#34;, \u0026#34;-f\u0026#34;, \u0026#34;http://localhost\u0026#34;]\u0026#39; - \u0026#34;interval: 1m30s\u0026#34; - \u0026#34;timeout: 10s\u0026#34; - \u0026#34;retries: 3\u0026#34; - \u0026#34;start_period: 40s\u0026#34; In some cases, the dependency service, mostly 3rd party server like SolrJ server, is required to keep the same version as client lib version, which defined as ${test.framework.version} in pom. Could use ${CASE_SERVER_IMAGE_VERSION} as the version number, it will be changed in the test for every version.\n Don\u0026rsquo;t support resource related configurations, such as volumes, ports and ulimits. Because in test scenarios, don\u0026rsquo;t need mapping any port to the host VM, or mount any folder.\n Take following test cases as examples\n dubbo-2.7.x with JVM-container jetty with JVM-container gateway with runningMode canal with docker-compose  expectedData.yaml Operator for number\n   Operator Description     nq Not equal   eq Equal(default)   ge Greater than or equal   gt Greater than    Operator for String\n   Operator Description     not null Not null   null Null or empty String   eq Equal(default)    Expected Data Format Of The Segment\nsegmentItems: - serviceName: SERVICE_NAME(string) segmentSize: SEGMENT_SIZE(int) segments: - segmentId: SEGMENT_ID(string) spans: ...    Field Description     serviceName Service Name.   segmentSize The number of segments is expected.   segmentId trace ID.   spans segment span list. Follow the next section to see how to describe every span.    Expected Data Format Of The Span\nNotice: The order of span list should follow the order of the span finish time.\noperationName: OPERATION_NAME(string) parentSpanId: PARENT_SPAN_ID(int) spanId: SPAN_ID(int) startTime: START_TIME(int) endTime: END_TIME(int) isError: IS_ERROR(string: true, false) spanLayer: SPAN_LAYER(string: DB, RPC_FRAMEWORK, HTTP, MQ, CACHE) spanType: SPAN_TYPE(string: Exit, Entry, Local) componentId: COMPONENT_ID(int) tags: - {key: TAG_KEY(string), value: TAG_VALUE(string)} ... logs: - {key: LOG_KEY(string), value: LOG_VALUE(string)} ... peer: PEER(string) refs: - { traceId: TRACE_ID(string), parentTraceSegmentId: PARENT_TRACE_SEGMENT_ID(string), parentSpanId: PARENT_SPAN_ID(int), parentService: PARENT_SERVICE(string), parentServiceInstance: PARENT_SERVICE_INSTANCE(string), parentEndpoint: PARENT_ENDPOINT_NAME(string), networkAddress: NETWORK_ADDRESS(string), refType: REF_TYPE(string: CrossProcess, CrossThread) } ...    Field Description     operationName Span Operation Name.   parentSpanId Parent span id. Notice: The parent span id of the first span should be -1.   spanId Span Id. Notice, start from 0.   startTime Span start time. It is impossible to get the accurate time, not 0 should be enough.   endTime Span finish time. It is impossible to get the accurate time, not 0 should be enough.   isError Span status, true or false.   componentId Component id for your plugin.   tags Span tag list. Notice, Keep in the same order as the plugin coded.   logs Span log list. Notice, Keep in the same order as the plugin coded.   SpanLayer Options, DB, RPC_FRAMEWORK, HTTP, MQ, CACHE.   SpanType Span type, options, Exit, Entry or Local.   peer Remote network address, IP + port mostly. For exit span, this should be required.    The verify description for SegmentRef\n   Field Description     traceId    parentTraceSegmentId Parent SegmentId, pointing to the segment id in the parent segment.   parentSpanId Parent SpanID, pointing to the span id in the parent segment.   parentService The service of parent/downstream service name.   parentServiceInstance The instance of parent/downstream service instance name.   parentEndpoint The endpoint of parent/downstream service.   networkAddress The peer value of parent exit span.   refType Ref type, options, CrossProcess or CrossThread.    Expected Data Format Of The Meter Items\nmeterItems: - serviceName: SERVICE_NAME(string) meterSize: METER_SIZE(int) meters: - ...    Field Description     serviceName Service Name.   meterSize The number of meters is expected.   meters meter list. Follow the next section to see how to describe every meter.    Expected Data Format Of The Meter\nmeterId: name: NAME(string) tags: - {name: TAG_NAME(string), value: TAG_VALUE(string)} singleValue: SINGLE_VALUE(double) histogramBuckets: - HISTOGRAM_BUCKET(double) ... The verify description for MeterId\n   Field Description     name meter name.   tags meter tags.   tags.name tag name.   tags.value tag value.   singleValue counter or gauge value. Using condition operate of the number to validate, such as gt, ge. If current meter is histogram, don\u0026rsquo;t need to write this field.   histogramBuckets histogram bucket. The bucket list must be ordered. The tool assert at least one bucket of the histogram having nonzero count. If current meter is counter or gauge, don\u0026rsquo;t need to write this field.    startup.sh This script provide a start point to JVM based service, most of them starts by a java -jar, with some variables. The following system environment variables are available in the shell.\n   Variable Description     agent_opts Agent plugin opts, check the detail in plugin doc or the same opt added in this PR.   SCENARIO_NAME Service name. Default same as the case folder name   SCENARIO_VERSION Version   SCENARIO_ENTRY_SERVICE Entrance URL to access this service   SCENARIO_HEALTH_CHECK_URL Health check URL     ${agent_opts} is required to add into your java -jar command, which including the parameter injected by test framework, and make agent installed. All other parameters should be added after ${agent_opts}.\n The test framework will set the service name as the test case folder name by default, but in some cases, there are more than one test projects are required to run in different service codes, could set it explicitly like the following example.\nExample\nhome=\u0026#34;$(cd \u0026#34;$(dirname $0)\u0026#34;; pwd)\u0026#34; java -jar ${agent_opts} \u0026#34;-Dskywalking.agent.service_name=jettyserver-scenario\u0026#34; ${home}/../libs/jettyserver-scenario.jar \u0026amp; sleep 1 java -jar ${agent_opts} \u0026#34;-Dskywalking.agent.service_name=jettyclient-scenario\u0026#34; ${home}/../libs/jettyclient-scenario.jar \u0026amp;  Only set this or use other skywalking options when it is really necessary.\n Take the following test cases as examples\n undertow webflux  Best Practices How To Use The Archetype To Create A Test Case Project We provided archetypes and a script to make creating a project easier. It creates a completed project of a test case. So that we only need to focus on cases. First, we can use followed command to get usage about the script.\nbash ${SKYWALKING_HOME}/test/plugin/generator.sh\nThen, runs and generates a project, named by scenario_name, in ./scenarios.\nRecommendations for pom \u0026lt;properties\u0026gt; \u0026lt;!-- Provide and use this property in the pom. --\u0026gt; \u0026lt;!-- This version should match the library version, --\u0026gt; \u0026lt;!-- in this case, http components lib version 4.3. --\u0026gt; \u0026lt;test.framework.version\u0026gt;4.3\u0026lt;/test.framework.version\u0026gt; \u0026lt;/properties\u0026gt; \u0026lt;dependencies\u0026gt; \u0026lt;dependency\u0026gt; \u0026lt;groupId\u0026gt;org.apache.httpcomponents\u0026lt;/groupId\u0026gt; \u0026lt;artifactId\u0026gt;httpclient\u0026lt;/artifactId\u0026gt; \u0026lt;version\u0026gt;${test.framework.version}\u0026lt;/version\u0026gt; \u0026lt;/dependency\u0026gt; ... \u0026lt;/dependencies\u0026gt; \u0026lt;build\u0026gt; \u0026lt;!-- Set the package final name as same as the test case folder case. --\u0026gt; \u0026lt;finalName\u0026gt;httpclient-4.3.x-scenario\u0026lt;/finalName\u0026gt; .... \u0026lt;/build\u0026gt; How To Implement Heartbeat Service Heartbeat service is designed for checking the service available status. This service is a simple HTTP service, returning 200 means the target service is ready. Then the traffic generator will access the entry service and verify the expected data. User should consider to use this service to detect such as whether the dependent services are ready, especially when dependent services are database or cluster.\nNotice, because heartbeat service could be traced fully or partially, so, segmentSize in expectedData.yaml should use ge as the operator, and don\u0026rsquo;t include the segments of heartbeat service in the expected segment data.\nThe example Process of Writing Tracing Expected Data Expected data file, expectedData.yaml, include SegmentItems part.\nWe are using the HttpClient plugin to show how to write the expected data.\nThere are two key points of testing\n Whether is HttpClient span created. Whether the ContextCarrier created correctly, and propagates across processes.  +-------------+ +------------------+ +-------------------------+ | Browser | | Case Servlet | | ContextPropagateServlet | | | | | | | +-----|-------+ +---------|--------+ +------------|------------+ | | | | | | | WebHttp +-+ | +------------------------\u0026gt; |-| HttpClient +-+ | |--------------------------------\u0026gt; |-| | |-| |-| | |-| |-| | |-| \u0026lt;--------------------------------| | |-| +-+ | \u0026lt;--------------------------| | | +-+ | | | | | | | | | | | | | + + + segmentItems By following the flow of HttpClient case, there should be two segments created.\n Segment represents the CaseServlet access. Let\u0026rsquo;s name it as SegmentA. Segment represents the ContextPropagateServlet access. Let\u0026rsquo;s name it as SegmentB.  segmentItems: - serviceName: httpclient-case segmentSize: ge 2 # Could have more than one health check segments, because, the dependency is not standby. Because Tomcat plugin is a default plugin of SkyWalking, so, in SegmentA, there are two spans\n Tomcat entry span HttpClient exit span  SegmentA span list should like following\n- segmentId: not null spans: - operationName: /httpclient-case/case/context-propagate parentSpanId: 0 spanId: 1 startTime: nq 0 endTime: nq 0 isError: false spanLayer: Http spanType: Exit componentId: eq 2 tags: - {key: url, value: \u0026#39;http://127.0.0.1:8080/httpclient-case/case/context-propagate\u0026#39;} - {key: http.method, value: GET} logs: [] peer: 127.0.0.1:8080 - operationName: /httpclient-case/case/httpclient parentSpanId: -1 spanId: 0 startTime: nq 0 endTime: nq 0 spanLayer: Http isError: false spanType: Entry componentId: 1 tags: - {key: url, value: \u0026#39;http://localhost:{SERVER_OUTPUT_PORT}/httpclient-case/case/httpclient\u0026#39;} - {key: http.method, value: GET} logs: [] peer: null SegmentB should only have one Tomcat entry span, but includes the Ref pointing to SegmentA.\nSegmentB span list should like following\n- segmentId: not null spans: - operationName: /httpclient-case/case/context-propagate parentSpanId: -1 spanId: 0 tags: - {key: url, value: \u0026#39;http://127.0.0.1:8080/httpclient-case/case/context-propagate\u0026#39;} - {key: http.method, value: GET} logs: [] startTime: nq 0 endTime: nq 0 spanLayer: Http isError: false spanType: Entry componentId: 1 peer: null refs: - {parentEndpoint: /httpclient-case/case/httpclient, networkAddress: \u0026#39;localhost:8080\u0026#39;, refType: CrossProcess, parentSpanId: 1, parentTraceSegmentId: not null, parentServiceInstance: not null, parentService: not null, traceId: not null} The example Process of Writing Meter Expected Data Expected data file, expectedData.yaml, include MeterItems part.\nWe are using the toolkit plugin to demonstrate how to write the expected data. When write the meter plugin, the expected data file keeps the same.\nThere is one key point of testing\n Build a meter and operate it.  Such as Counter:\nMeterFactory.counter(\u0026#34;test_counter\u0026#34;).tag(\u0026#34;ck1\u0026#34;, \u0026#34;cv1\u0026#34;).build().increment(1d); MeterFactory.histogram(\u0026#34;test_histogram\u0026#34;).tag(\u0026#34;hk1\u0026#34;, \u0026#34;hv1\u0026#34;).steps(1d, 5d, 10d).build().addValue(2d); +-------------+ +------------------+ | Plugin | | Agent core | | | | | +-----|-------+ +---------|--------+ | | | | | Build or operate +-+ +------------------------\u0026gt; |-| | |-] | |-| | |-| | |-| | |-| | \u0026lt;--------------------------| | +-+ | | | | | | | | + + meterItems By following the flow of the toolkit case, there should be two meters created.\n Meter test_counter created from MeterFactory#counter. Let\u0026rsquo;s name it as MeterA. Meter test_histogram created from MeterFactory#histogram. Let\u0026rsquo;s name it as MeterB.  meterItems: - serviceName: toolkit-case meterSize: 2 They\u0026rsquo;re showing two kinds of meter, MeterA has a single value, MeterB has a histogram value.\nMeterA should like following, counter and gauge use the same data format.\n- meterId: name: test_counter tags: - {name: ck1, value: cv1} singleValue: gt 0 MeterB should like following.\n- meterId: name: test_histogram tags: - {name: hk1, value: hv1} histogramBuckets: - 0.0 - 1.0 - 5.0 - 10.0 Local Test and Pull Request To The Upstream First of all, the test case project could be compiled successfully, with right project structure and be able to deploy. The developer should test the start script could run in Linux/MacOS, and entryService/health services are able to provide the response.\nYou could run test by using following commands\ncd ${SKYWALKING_HOME} bash ./test/plugin/run.sh -f ${scenario_name} Notice，if codes in ./apm-sniffer have been changed, no matter because your change or git update， please recompile the skywalking-agent. Because the test framework will use the existing skywalking-agent folder, rather than recompiling it every time.\nUse ${SKYWALKING_HOME}/test/plugin/run.sh -h to know more command options.\nIf the local test passed, then you could add it to .github/workflows/plugins-test.\u0026lt;n\u0026gt;.yaml file, which will drive the tests running on the GitHub Actions of official SkyWalking repository. Based on your plugin\u0026rsquo;s name, please add the test case into file .github/workflows/plugins-test.\u0026lt;n\u0026gt;.yaml, by alphabetical orders.\nEvery test case is a GitHub Actions Job. Please use the scenario directory name as the case name, mostly you\u0026rsquo;ll just need to decide which file (plugins-test.\u0026lt;n\u0026gt;.yaml) to add your test case, and simply put one line (as follows) in it, take the existed cases as examples. You can run python3 tools/select-group.py to see which file contains the least cases and add your cases into it, in order to balance the running time of each group.\nIf a test case required to run in JDK 14 environment, please add you test case into file plugins-jdk14-test.\u0026lt;n\u0026gt;.yaml.\njobs: PluginsTest: name: Plugin runs-on: ubuntu-18.04 timeout-minutes: 90 strategy: fail-fast: true matrix: case: # ... - \u0026lt;your scenario test directory name\u0026gt; # ... ","excerpt":"Plugin automatic test framework Plugin test framework is designed for verifying the plugins' …","ref":"/docs/main/v8.5.0/en/guides/plugin-test/","title":"Plugin automatic test framework"},{"body":"Plugin Development Guide This document describe how to understand, develop and contribute plugin.\nThere are 2 kinds of plugin\n Tracing plugin. Follow the distributed tracing concept to collect spans with tags and logs. Meter plugin. Collect numeric metrics in Counter, Gauge, and Histogram formats.  We also provide the plugin test tool to verify the data collected and reported by the plugin. If you plan to contribute any plugin to our main repo, the data would be verified by this tool too.\nTracing plugin Concepts Span Span is an important and common concept in distributed tracing system. Learn Span from Google Dapper Paper and OpenTracing\nSkyWalking supports OpenTracing and OpenTracing-Java API from 2017. Our Span concepts are similar with the paper and OpenTracing. Also we extend the Span.\nThere are three types of Span\n1.1 EntrySpan EntrySpan represents a service provider, also the endpoint of server side. As an APM system, we are targeting the application servers. So almost all the services and MQ-consumer are EntrySpan(s).\n1.2 LocalSpan LocalSpan represents a normal Java method, which does not relate to remote service, neither a MQ producer/consumer nor a service(e.g. HTTP service) provider/consumer.\n1.3 ExitSpan ExitSpan represents a client of service or MQ-producer, as named as LeafSpan at early age of SkyWalking. e.g. accessing DB by JDBC, reading Redis/Memcached are cataloged an ExitSpan.\nContextCarrier In order to implement distributed tracing, the trace across process need to be bind, and the context should propagate across the process. That is ContextCarrier\u0026rsquo;s duty.\nHere are the steps about how to use ContextCarrier in a A-\u0026gt;B distributed call.\n Create a new and empty ContextCarrier at client side. Create an ExitSpan by ContextManager#createExitSpan or use ContextManager#inject to init the ContextCarrier. Put all items of ContextCarrier into heads(e.g. HTTP HEAD), attachments(e.g. Dubbo RPC framework) or messages(e.g. Kafka) The ContextCarrier propagates to server side by the service call. At server side, get all items from heads, attachments or messages. Create an EntrySpan by ContextManager#createEntrySpan or use ContextManager#extract to bind the client and server.  Let\u0026rsquo;s demonstrate the steps by Apache HTTPComponent client plugin and Tomcat 7 server plugin\n Client side steps by Apache HTTPComponent client plugin  span = ContextManager.createExitSpan(\u0026#34;/span/operation/name\u0026#34;, contextCarrier, \u0026#34;ip:port\u0026#34;); CarrierItem next = contextCarrier.items(); while (next.hasNext()) { next = next.next(); httpRequest.setHeader(next.getHeadKey(), next.getHeadValue()); } Server side steps by Tomcat 7 server plugin  ContextCarrier contextCarrier = new ContextCarrier(); CarrierItem next = contextCarrier.items(); while (next.hasNext()) { next = next.next(); next.setHeadValue(request.getHeader(next.getHeadKey())); } span = ContextManager.createEntrySpan(“/span/operation/name”, contextCarrier); ContextSnapshot Besides across process, across thread but in a process need to be supported, because async process(In-memory MQ) and batch process are common in Java. Across process and across thread are similar, because they are both about propagating context. The only difference is that, don\u0026rsquo;t need to serialize for across thread.\nHere are the three steps about across thread propagation:\n Use ContextManager#capture to get the ContextSnapshot object. Let the sub-thread access the ContextSnapshot by any way, through method arguments or carried by an existed arguments Use ContextManager#continued in sub-thread.  Core APIs ContextManager ContextManager provides all major and primary APIs.\n Create EntrySpan  public static AbstractSpan createEntrySpan(String endpointName, ContextCarrier carrier) Create EntrySpan by operation name(e.g. service name, uri) and ContextCarrier.\nCreate LocalSpan  public static AbstractSpan createLocalSpan(String endpointName) Create LocalSpan by operation name(e.g. full method signature)\nCreate ExitSpan  public static AbstractSpan createExitSpan(String endpointName, ContextCarrier carrier, String remotePeer) Create ExitSpan by operation name(e.g. service name, uri) and new ContextCarrier and peer address (e.g. ip+port, hostname+port)\nAbstractSpan /** * Set the component id, which defines in {@link ComponentsDefine} * * @param component * @return the span for chaining. */ AbstractSpan setComponent(Component component); AbstractSpan setLayer(SpanLayer layer); /** * Set a key:value tag on the Span. * * @return this Span instance, for chaining */ AbstractSpan tag(String key, String value); /** * Record an exception event of the current walltime timestamp. * * @param t any subclass of {@link Throwable}, which occurs in this span. * @return the Span, for chaining */ AbstractSpan log(Throwable t); AbstractSpan errorOccurred(); /** * Record an event at a specific timestamp. * * @param timestamp The explicit timestamp for the log record. * @param event the events * @return the Span, for chaining */ AbstractSpan log(long timestamp, Map\u0026lt;String, ?\u0026gt; event); /** * Sets the string name for the logical operation this span represents. * * @return this Span instance, for chaining */ AbstractSpan setOperationName(String endpointName); Besides setting operation name, tags and logs, two attributes should be set, which are component and layer, especially for EntrySpan and ExitSpan\nSpanLayer is the catalog of span. Here are 5 values:\n UNKNOWN (default) DB RPC_FRAMEWORK, for a RPC framework, not an ordinary HTTP HTTP MQ  Component IDs are defined and reserved by SkyWalking project. For component name/ID extension, please follow Component library definition and extension document.\nSpecial Span Tags All tags are available in the trace view, meanwhile, in the OAP backend analysis, some special tag or tag combination could provide other advanced features.\nTag key status_code The value should be an integer. The response code of OAL entities is according to this.\nTag key db.statement and db.type. The value of db.statement should be a String, representing the Database statement, such as SQL, or [No statement]/+span#operationName if value is empty. When exit span has this tag, OAP samples the slow statements based on agent-analyzer/default/maxSlowSQLLength. The threshold of slow statement is defined by following agent-analyzer/default/slowDBAccessThreshold\nExtension logic endpoint. Tag key x-le Logic endpoint is a concept, which doesn\u0026rsquo;t represent a real RPC call, but requires the statistic. The value of x-le should be JSON format, with two options.\n Define a separated logic endpoint. Provide its own endpoint name, latency and status. Suitable for entry and local span.  { \u0026#34;name\u0026#34;: \u0026#34;GraphQL-service\u0026#34;, \u0026#34;latency\u0026#34;: 100, \u0026#34;status\u0026#34;: true } Declare the current local span representing a logic endpoint.  { \u0026#34;logic-span\u0026#34;: true } Advanced APIs Async Span APIs There is a set of advanced APIs in Span, which work specific for async scenario. When tags, logs, attributes(including end time) of the span needs to set in another thread, you should use these APIs.\n/** * The span finish at current tracing context, but the current span is still alive, until {@link #asyncFinish} * called. * * This method must be called\u0026lt;br/\u0026gt; * 1. In original thread(tracing context). * 2. Current span is active span. * * During alive, tags, logs and attributes of the span could be changed, in any thread. * * The execution times of {@link #prepareForAsync} and {@link #asyncFinish()} must match. * * @return the current span */ AbstractSpan prepareForAsync(); /** * Notify the span, it could be finished. * * The execution times of {@link #prepareForAsync} and {@link #asyncFinish()} must match. * * @return the current span */ AbstractSpan asyncFinish();  Call #prepareForAsync in original context. Do ContextManager#stopSpan in original context when your job in current thread is done. Propagate the span to any other thread. After all set, call #asyncFinish in any thread. Tracing context will be finished and report to backend when all spans\u0026rsquo;s #prepareForAsync finished(Judged by count of API execution).  Develop a plugin Abstract The basic method to trace is intercepting a Java method, by using byte code manipulation tech and AOP concept. SkyWalking boxed the byte code manipulation tech and tracing context propagation, so you just need to define the intercept point(a.k.a. aspect pointcut in Spring)\nIntercept SkyWalking provide two common defines to intercept constructor, instance method and class method.\n Extend ClassInstanceMethodsEnhancePluginDefine defines constructor intercept points and instance method intercept points. Extend ClassStaticMethodsEnhancePluginDefine defines class method intercept points.  Of course, you can extend ClassEnhancePluginDefine to set all intercept points. But it is unusual.\nImplement plugin I will demonstrate about how to implement a plugin by extending ClassInstanceMethodsEnhancePluginDefine\n Define the target class name  protected abstract ClassMatch enhanceClass(); ClassMatch represents how to match the target classes, there are 4 ways:\n byName, through the full class name(package name + . + class name) byClassAnnotationMatch, through the class existed certain annotations. byMethodAnnotationMatch, through the class\u0026rsquo;s method existed certain annotations. byHierarchyMatch, through the class\u0026rsquo;s parent classes or interfaces  Attentions:\n Never use ThirdPartyClass.class in the instrumentation definitions, such as takesArguments(ThirdPartyClass.class), or byName(ThirdPartyClass.class.getName()), because of the fact that ThirdPartyClass dose not necessarily exist in the target application and this will break the agent; we have import checks to help on checking this in CI, but it doesn\u0026rsquo;t cover all scenarios of this limitation, so never try to work around this limitation by something like using full-qualified-class-name (FQCN), i.e. takesArguments(full.qualified.ThirdPartyClass.class) and byName(full.qualified.ThirdPartyClass.class.getName()) will pass the CI check, but are still invalid in the agent codes, Use Full Qualified Class Name String Literature Instead. Even you are perfectly sure that the class to be intercepted exists in the target application (such as JDK classes), still, don\u0026rsquo;t use *.class.getName() to get the class String name. Recommend you to use literal String. This is for avoiding ClassLoader issues. by*AnnotationMatch doesn\u0026rsquo;t support the inherited annotations. Don\u0026rsquo;t recommend to use byHierarchyMatch, unless it is really necessary. Because using it may trigger intercepting many unexcepted methods, which causes performance issues and concerns.  Example：\n@Override protected ClassMatch enhanceClassName() { return byName(\u0026#34;org.apache.catalina.core.StandardEngineValve\u0026#34;);\t}\tDefine an instance method intercept point  public InstanceMethodsInterceptPoint[] getInstanceMethodsInterceptPoints(); public interface InstanceMethodsInterceptPoint { /** * class instance methods matcher. * * @return methods matcher */ ElementMatcher\u0026lt;MethodDescription\u0026gt; getMethodsMatcher(); /** * @return represents a class name, the class instance must instanceof InstanceMethodsAroundInterceptor. */ String getMethodsInterceptor(); boolean isOverrideArgs(); } Also use Matcher to set the target methods. Return true in isOverrideArgs, if you want to change the argument ref in interceptor.\nThe following sections will tell you how to implement the interceptor.\nAdd plugin define into skywalking-plugin.def file  tomcat-7.x/8.x=TomcatInstrumentation  Set up witnessClasses and/or witnessMethods if the instrumentation should be activated in specific versions.\nExample:\n// The plugin is activated only when the foo.Bar class exists. @Override protected String[] witnessClasses() { return new String[] { \u0026#34;foo.Bar\u0026#34; }; } // The plugin is activated only when the foo.Bar#hello method exists. @Override protected List\u0026lt;WitnessMethod\u0026gt; witnessMethods() { List\u0026lt;WitnessMethod\u0026gt; witnessMethodList = new ArrayList\u0026lt;\u0026gt;(); WitnessMethod witnessMethod = new WitnessMethod(\u0026#34;foo.Bar\u0026#34;, ElementMatchers.named(\u0026#34;hello\u0026#34;)); witnessMethodList.add(witnessMethod); return witnessMethodList; } For more example, see WitnessTest.java\n  Implement an interceptor As an interceptor for an instance method, the interceptor implements org.apache.skywalking.apm.agent.core.plugin.interceptor.enhance.InstanceMethodsAroundInterceptor\n/** * A interceptor, which intercept method\u0026#39;s invocation. The target methods will be defined in {@link * ClassEnhancePluginDefine}\u0026#39;s subclass, most likely in {@link ClassInstanceMethodsEnhancePluginDefine} */ public interface InstanceMethodsAroundInterceptor { /** * called before target method invocation. * * @param result change this result, if you want to truncate the method. * @throws Throwable */ void beforeMethod(EnhancedInstance objInst, Method method, Object[] allArguments, Class\u0026lt;?\u0026gt;[] argumentsTypes, MethodInterceptResult result) throws Throwable; /** * called after target method invocation. Even method\u0026#39;s invocation triggers an exception. * * @param ret the method\u0026#39;s original return value. * @return the method\u0026#39;s actual return value. * @throws Throwable */ Object afterMethod(EnhancedInstance objInst, Method method, Object[] allArguments, Class\u0026lt;?\u0026gt;[] argumentsTypes, Object ret) throws Throwable; /** * called when occur exception. * * @param t the exception occur. */ void handleMethodException(EnhancedInstance objInst, Method method, Object[] allArguments, Class\u0026lt;?\u0026gt;[] argumentsTypes, Throwable t); } Use the core APIs in before, after and exception handle stages.\nDo bootstrap class instrumentation. SkyWalking has packaged the bootstrap instrumentation in the agent core. It is easy to open by declaring it in the Instrumentation definition.\nOverride the public boolean isBootstrapInstrumentation() and return true. Such as\npublic class URLInstrumentation extends ClassEnhancePluginDefine { private static String CLASS_NAME = \u0026#34;java.net.URL\u0026#34;; @Override protected ClassMatch enhanceClass() { return byName(CLASS_NAME); } @Override public ConstructorInterceptPoint[] getConstructorsInterceptPoints() { return new ConstructorInterceptPoint[] { new ConstructorInterceptPoint() { @Override public ElementMatcher\u0026lt;MethodDescription\u0026gt; getConstructorMatcher() { return any(); } @Override public String getConstructorInterceptor() { return \u0026#34;org.apache.skywalking.apm.plugin.jre.httpurlconnection.Interceptor2\u0026#34;; } } }; } @Override public InstanceMethodsInterceptPoint[] getInstanceMethodsInterceptPoints() { return new InstanceMethodsInterceptPoint[0]; } @Override public StaticMethodsInterceptPoint[] getStaticMethodsInterceptPoints() { return new StaticMethodsInterceptPoint[0]; } @Override public boolean isBootstrapInstrumentation() { return true; } } NOTICE, doing bootstrap instrumentation should only happen in necessary, but mostly it effect the JRE core(rt.jar), and could make very unexpected result or side effect.\nProvide Customization Config for the Plugin The config could provide different behaviours based on the configurations. SkyWalking plugin mechanism provides the configuration injection and initialization system in the agent core.\nEvery plugin could declare one or more classes to represent the config by using @PluginConfig annotation. The agent core could initialize this class' static field though System environments, System properties, and agent.config static file.\nThe #root() method in the @PluginConfig annotation requires to declare the root class for the initialization process. Typically, SkyWalking prefers to use nested inner static classes for the hierarchy of the configuration. Recommend using Plugin/plugin-name/config-key as the nested classes structure of the Config class.\nNOTE, because of the Java ClassLoader mechanism, the @PluginConfig annotation should be added on the real class used in the interceptor codes.\nSuch as, in the following example, @PluginConfig(root = SpringMVCPluginConfig.class) represents the initialization should start with using SpringMVCPluginConfig as the root. Then the config key of the attribute USE_QUALIFIED_NAME_AS_ENDPOINT_NAME, should be plugin.springmvc.use_qualified_name_as_endpoint_name.\npublic class SpringMVCPluginConfig { public static class Plugin { // NOTE, if move this annotation on the `Plugin` or `SpringMVCPluginConfig` class, it no longer has any effect.  @PluginConfig(root = SpringMVCPluginConfig.class) public static class SpringMVC { /** * If true, the fully qualified method name will be used as the endpoint name instead of the request URL, * default is false. */ public static boolean USE_QUALIFIED_NAME_AS_ENDPOINT_NAME = false; /** * This config item controls that whether the SpringMVC plugin should collect the parameters of the * request. */ public static boolean COLLECT_HTTP_PARAMS = false; } @PluginConfig(root = SpringMVCPluginConfig.class) public static class Http { /** * When either {@link Plugin.SpringMVC#COLLECT_HTTP_PARAMS} is enabled, how many characters to keep and send * to the OAP backend, use negative values to keep and send the complete parameters, NB. this config item is * added for the sake of performance */ public static int HTTP_PARAMS_LENGTH_THRESHOLD = 1024; } } } Meter Plugin Java agent plugin could use meter APIs to collect the metrics for backend analysis.\n Counter API represents a single monotonically increasing counter, automatic collect data and report to backend.  import org.apache.skywalking.apm.agent.core.meter.MeterFactory; Counter counter = MeterFactory.counter(meterName).tag(\u0026#34;tagKey\u0026#34;, \u0026#34;tagValue\u0026#34;).mode(Counter.Mode.INCREMENT).build(); counter.increment(1d);  MeterFactory.counter Create a new counter builder with the meter name. Counter.Builder.tag(String key, String value) Mark a tag key/value pair. Counter.Builder.mode(Counter.Mode mode) Change the counter mode, RATE mode means reporting rate to the backend. Counter.Builder.build() Build a new Counter which is collected and reported to the backend. Counter.increment(double count) Increment count to the Counter, It could be a positive value.   Gauge API represents a single numerical value.  import org.apache.skywalking.apm.agent.core.meter.MeterFactory; ThreadPoolExecutor threadPool = ...; Gauge gauge = MeterFactory.gauge(meterName, () -\u0026gt; threadPool.getActiveCount()).tag(\u0026#34;tagKey\u0026#34;, \u0026#34;tagValue\u0026#34;).build();  MeterFactory.gauge(String name, Supplier\u0026lt;Double\u0026gt; getter) Create a new gauge builder with the meter name and supplier function, this function need to return a double value. Gauge.Builder.tag(String key, String value) Mark a tag key/value pair. Gauge.Builder.build() Build a new Gauge which is collected and reported to the backend.   Histogram API represents a summary sample observations with customize buckets.  import org.apache.skywalking.apm.agent.core.meter.MeterFactory; Histogram histogram = MeterFactory.histogram(\u0026#34;test\u0026#34;).tag(\u0026#34;tagKey\u0026#34;, \u0026#34;tagValue\u0026#34;).steps(Arrays.asList(1, 5, 10)).minValue(0).build(); histogram.addValue(3);  MeterFactory.histogram(String name) Create a new histogram builder with the meter name. Histogram.Builder.tag(String key, String value) Mark a tag key/value pair. Histogram.Builder.steps(List\u0026lt;Double\u0026gt; steps) Set up the max values of every histogram buckets. Histogram.Builder.minValue(double value) Set up the minimal value of this histogram, default is 0. Histogram.Builder.build() Build a new Histogram which is collected and reported to the backend. Histogram.addValue(double value) Add value into the histogram, automatically analyze what bucket count needs to be increment. rule: count into [step1, step2).  Plugin Test Tool Apache SkyWalking Agent Test Tool Suite a tremendously useful test tools suite in a wide variety of languages of Agent. Includes mock collector and validator. The mock collector is a SkyWalking receiver, like OAP server.\nYou could learn how to use this tool to test the plugin in this doc. If you want to contribute plugins to SkyWalking official repo, this is required.\nContribute plugins into Apache SkyWalking repository We are welcome everyone to contribute plugins.\nPlease follow there steps:\n Submit an issue about which plugins you are going to contribute, including supported version. Create sub modules under apm-sniffer/apm-sdk-plugin or apm-sniffer/optional-plugins, and the name should include supported library name and versions Follow this guide to develop. Make sure comments and test cases are provided. Develop and test. Provide the automatic test cases. Learn how to write the plugin test case from this doc Send the pull request and ask for review. The plugin committers approve your plugins, plugin CI-with-IT, e2e and plugin tests passed. The plugin accepted by SkyWalking.  ","excerpt":"Plugin Development Guide This document describe how to understand, develop and contribute plugin. …","ref":"/docs/main/v8.5.0/en/guides/java-plugin-development-guide/","title":"Plugin Development Guide"},{"body":"Probe Introduction In SkyWalking, probe means an agent or SDK library integrated into a target system that takes charge of collecting telemetry data, including tracing and metrics. Depending on the target system tech stack, there are very different ways how the probe performs such tasks. But ultimately, they all work towards the same goal — to collect and reformat data, and then to send them to the backend.\nOn a high level, there are three typical categories in all SkyWalking probes.\n  Language based native agent. These agents run in target service user spaces, such as a part of user codes. For example, the SkyWalking Java agent uses the -javaagent command line argument to manipulate codes in runtime, where manipulate means to change and inject user\u0026rsquo;s codes. Another kind of agents uses certain hook or intercept mechanism provided by target libraries. As you can see, these agents are based on languages and libraries.\n  Service Mesh probes. Service Mesh probes collect data from sidecar, control panel in service mesh or proxy. In the old days, proxy is only used as an ingress of the whole cluster, but with the Service Mesh and sidecar, we can now perform observability functions.\n  3rd-party instrument library. SkyWalking accepts many widely used instrument libraries data formats. It analyzes the data, transfers it to SkyWalking\u0026rsquo;s formats of trace, metrics or both. This feature starts with accepting Zipkin span data. See Receiver for other tracers for more information.\n  You don\u0026rsquo;t need to use Language based native agent and Service Mesh probe at the same time, since they both serve to collect metrics data. Otherwise, your system will suffer twice the payload, and the analytic numbers will be doubled.\nThere are several recommended ways on how to use these probes:\n Use Language based native agent only. Use 3rd-party instrument library only, like the Zipkin instrument ecosystem. Use Service Mesh probe only. Use Service Mesh probe with Language based native agent or 3rd-party instrument library in tracing status. (Advanced usage)  What is the meaning of in tracing status?\nBy default, Language based native agent and 3rd-party instrument library both send distributed traces to the backend, where analyses and aggregation on those traces are performed. In tracing status means that the backend considers these traces as something like logs. In other words, the backend saves them, and builds the links between traces and metrics, like which endpoint and service does the trace belong?.\nWhat is next?  Learn more about the probes supported by SkyWalking in Service auto instrument agent, Manual instrument SDK, Service Mesh probe and Zipkin receiver. After understanding how the probe works, see the backend overview for more on analysis and persistence.  ","excerpt":"Probe Introduction In SkyWalking, probe means an agent or SDK library integrated into a target …","ref":"/docs/main/v8.5.0/en/concepts-and-designs/probe-introduction/","title":"Probe Introduction"},{"body":"Problem When you start your application with the skywalking agent, you may find this exception in your agent log which means that EnhanceRequireObjectCache cannot be casted to EnhanceRequireObjectCache. For example:\nERROR 2018-05-07 21:31:24 InstMethodsInter : class[class org.springframework.web.method.HandlerMethod] after method[getBean] intercept failure java.lang.ClassCastException: org.apache.skywalking.apm.plugin.spring.mvc.commons.EnhanceRequireObjectCache cannot be cast to org.apache.skywalking.apm.plugin.spring.mvc.commons.EnhanceRequireObjectCache at org.apache.skywalking.apm.plugin.spring.mvc.commons.interceptor.GetBeanInterceptor.afterMethod(GetBeanInterceptor.java:45) at org.apache.skywalking.apm.agent.core.plugin.interceptor.enhance.InstMethodsInter.intercept(InstMethodsInter.java:105) at org.springframework.web.method.HandlerMethod.getBean(HandlerMethod.java) at org.springframework.web.servlet.handler.AbstractHandlerMethodExceptionResolver.shouldApplyTo(AbstractHandlerMethodExceptionResolver.java:47) at org.springframework.web.servlet.handler.AbstractHandlerExceptionResolver.resolveException(AbstractHandlerExceptionResolver.java:131) at org.springframework.web.servlet.handler.HandlerExceptionResolverComposite.resolveException(HandlerExceptionResolverComposite.java:76) ... Reason This exception may be caused by hot deployment tools (spring-boot-devtool) or otherwise, which changes the classloader in runtime.\nResolution  This error does not occur under the production environment, since developer tools are automatically disabled: See spring-boot-devtools. If you would like to debug in your development environment as usual, you should temporarily remove such hot deployment package in your lib path.  ","excerpt":"Problem When you start your application with the skywalking agent, you may find this exception in …","ref":"/docs/main/v8.5.0/en/faq/enhancerequireobjectcache-cast-exception/","title":"Problem"},{"body":"Problem  When importing the SkyWalking project to Eclipse, the following errors may occur:   Software being installed: Checkstyle configuration plugin for M2Eclipse 1.0.0.201705301746 (com.basistech.m2e.code.quality.checkstyle.feature.feature.group 1.0.0.201705301746) Missing requirement: Checkstyle configuration plugin for M2Eclipse 1.0.0.201705301746 (com.basistech.m2e.code.quality.checkstyle.feature.feature.group 1.0.0.201705301746) requires \u0026lsquo;net.sf.eclipsecs.core 5.2.0\u0026rsquo; but it could not be found\n Reason The Eclipse Checkstyle Plug-in has not been installed.\nResolution Download the plug-in at the link here: https://sourceforge.net/projects/eclipse-cs/?source=typ_redirect Eclipse Checkstyle Plug-in version 8.7.0.201801131309 is required. Plug-in notification: The Eclipse Checkstyle plug-in integrates the Checkstyle Java code auditor into the Eclipse IDE. The plug-in provides real-time feedback to the user on rule violations, including checking against coding style and error-prone code constructs.\n","excerpt":"Problem  When importing the SkyWalking project to Eclipse, the following errors may occur: …","ref":"/docs/main/v8.5.0/en/faq/import-project-eclipse-requireitems-exception/","title":"Problem"},{"body":"Problem Tracing doesn\u0026rsquo;t work on the Kafka consumer end.\nReason The kafka client is responsible for pulling messages from the brokers, after which the data will be processed by user-defined codes. However, only the poll action can be traced by the plug-in and the subsequent data processing work inevitably goes beyond the scope of the trace context. Thus, in order to complete tracing on the client end, manual instrumentation is required, i.e. the poll action and the processing action should be wrapped manually.\nResolve For a native Kafka client, please use the Application Toolkit libraries to do the manual instrumentation, with the help of the @KafkaPollAndInvoke annotation in apm-toolkit-kafka or with OpenTracing API. If you\u0026rsquo;re using spring-kafka 1.3.x, 2.2.x or above, you can easily trace the consumer end without further configuration.\n","excerpt":"Problem Tracing doesn\u0026rsquo;t work on the Kafka consumer end.\nReason The kafka client is responsible …","ref":"/docs/main/v8.5.0/en/faq/kafka-plugin/","title":"Problem"},{"body":"Problem When using a thread pool, TraceSegment data in a thread cannot be reported and there are memory data that cannot be recycled (memory leaks).\nExample ThreadPoolTaskExecutor executor = new ThreadPoolTaskExecutor(); executor.setThreadFactory(r -\u0026gt; new Thread(RunnableWrapper.of(r))); Reason  Worker threads are enhanced when using the thread pool. Based on the design of the SkyWalking Java Agent, when tracing a cross thread, you must enhance the task thread.  Resolution   When using Thread Schedule Framework: See SkyWalking Thread Schedule Framework at SkyWalking Java agent supported list, such as Spring FrameWork @Async, which can implement tracing without any modification.\n  When using Custom Thread Pool: Enhance the task thread with the following code.\n  ExecutorService executorService = Executors.newFixedThreadPool(1); executorService.execute(RunnableWrapper.of(new Runnable() { @Override public void run() { //your code  } })); See across thread solution APIs for more use cases.\n","excerpt":"Problem When using a thread pool, TraceSegment data in a thread cannot be reported and there are …","ref":"/docs/main/v8.5.0/en/faq/memory-leak-enhance-worker-thread/","title":"Problem"},{"body":"Problem  In maven build, the following error may occur with the protoc-plugin:  [ERROR] Failed to execute goal org.xolstice.maven.plugins:protobuf-maven-plugin:0.5.0:compile-custom (default) on project apm-network: Unable to copy the file to \\skywalking\\apm-network\\target\\protoc-plugins: \\skywalking\\apm-network\\target\\protoc-plugins\\protoc-3.3.0-linux-x86_64.exe (The process cannot access the file because it is being used by another process) -\u0026gt; [Help 1] Reason  The Protobuf compiler is dependent on the glibc. However, glibc has not been installed, or there is an old version already installed in the system.  Resolution  Install or upgrade to the latest version of the glibc library. Under the container environment, the latest glibc version of the alpine system is recommended. Please refer to http://www.gnu.org/software/libc/documentation.html.  ","excerpt":"Problem  In maven build, the following error may occur with the protoc-plugin:  [ERROR] Failed to …","ref":"/docs/main/v8.5.0/en/faq/protoc-plugin-fails-when-build/","title":"Problem"},{"body":"Problem The message with Field ID, 8888, must be reserved.\nReason Because Thrift cannot carry metadata to transport Trace Header in the original API, we transport them by wrapping TProtocolFactory.\nThrift allows us to append any additional fields in the message even if the receiver doesn\u0026rsquo;t deal with them. Those data will be skipped and left unread. Based on this, the 8888th field of the message is used to store Trace Header (or metadata) and to transport them. That means the message with Field ID, 8888, must be reserved.\nResolution Avoid using the Field(ID is 8888) in your application.\n","excerpt":"Problem The message with Field ID, 8888, must be reserved.\nReason Because Thrift cannot carry …","ref":"/docs/main/v8.5.0/en/faq/thrift-plugin/","title":"Problem"},{"body":"Problem  There is no abnormal log in Agent log and Collector log. The traces can be seen, but no other information is available in UI.  Reason The operating system where the monitored system is located is not set as the current time zone, causing statistics collection time points to deviate.\nResolution Make sure the time is synchronized between collector servers and monitored application servers.\n","excerpt":"Problem  There is no abnormal log in Agent log and Collector log. The traces can be seen, but no …","ref":"/docs/main/v8.5.0/en/faq/why-have-traces-no-others/","title":"Problem"},{"body":"Problem： Maven compilation failure with error such as Error: not found: python2 When you compile the project via Maven, it fails at module apm-webapp and the following error occurs.\nPay attention to keywords such as node-sass and Error: not found: python2.\n[INFO] \u0026gt; node-sass@4.11.0 postinstall C:\\XXX\\skywalking\\skywalking-ui\\node_modules\\node-sass [INFO] \u0026gt; node scripts/build.js [ERROR] gyp verb check python checking for Python executable \u0026quot;python2\u0026quot; in the PATH [ERROR] gyp verb `which` failed Error: not found: python2 [ERROR] gyp verb `which` failed at getNotFoundError (C:\\XXX\\skywalking\\skywalking-ui\\node_modules\\which\\which.js:13:12) [ERROR] gyp verb `which` failed at F (C:\\XXX\\skywalking\\skywalking-ui\\node_modules\\which\\which.js:68:19) [ERROR] gyp verb `which` failed at E (C:\\XXX\\skywalking\\skywalking-ui\\node_modules\\which\\which.js:80:29) [ERROR] gyp verb `which` failed at C:\\XXX\\skywalking\\skywalking-ui\\node_modules\\which\\which.js:89:16 [ERROR] gyp verb `which` failed at C:\\XXX\\skywalking\\skywalking-ui\\node_modules\\isexe\\index.js:42:5 [ERROR] gyp verb `which` failed at C:\\XXX\\skywalking\\skywalking-ui\\node_modules\\isexe\\windows.js:36:5 [ERROR] gyp verb `which` failed at FSReqWrap.oncomplete (fs.js:152:21) [ERROR] gyp verb `which` failed code: 'ENOENT' } [ERROR] gyp verb check python checking for Python executable \u0026quot;python\u0026quot; in the PATH [ERROR] gyp verb `which` succeeded python C:\\Users\\XXX\\AppData\\Local\\Programs\\Python\\Python37\\python.EXE [ERROR] gyp ERR! configure error [ERROR] gyp ERR! stack Error: Command failed: C:\\Users\\XXX\\AppData\\Local\\Programs\\Python\\Python37\\python.EXE -c import sys; print \u0026quot;%s.%s.%s\u0026quot; % sys.version_info[:3]; [ERROR] gyp ERR! stack File \u0026quot;\u0026lt;string\u0026gt;\u0026quot;, line 1 [ERROR] gyp ERR! stack import sys; print \u0026quot;%s.%s.%s\u0026quot; % sys.version_info[:3]; [ERROR] gyp ERR! stack ^ [ERROR] gyp ERR! stack SyntaxError: invalid syntax [ERROR] gyp ERR! stack [ERROR] gyp ERR! stack at ChildProcess.exithandler (child_process.js:275:12) [ERROR] gyp ERR! stack at emitTwo (events.js:126:13) [ERROR] gyp ERR! stack at ChildProcess.emit (events.js:214:7) [ERROR] gyp ERR! stack at maybeClose (internal/child_process.js:925:16) [ERROR] gyp ERR! stack at Process.ChildProcess._handle.onexit (internal/child_process.js:209:5) [ERROR] gyp ERR! System Windows_NT 10.0.17134 ...... [INFO] server-starter-es7 ................................. SUCCESS [ 11.657 s] [INFO] apm-webapp ......................................... FAILURE [ 25.857 s] [INFO] apache-skywalking-apm .............................. SKIPPED [INFO] apache-skywalking-apm-es7 .......................... SKIPPED Reason The error has nothing to do with SkyWalking.\nAccording to the issue here (https://github.com/sass/node-sass/issues/1176), if you live in countries where requesting resources from GitHub and npmjs.org runs slow, some precompiled binaries for dependency node-sass would fail to be downloaded during npm install, and npm would try to compile them itself. That\u0026rsquo;s why python2 is needed.\nResolution 1. Use mirror. For instance, if you\u0026rsquo;re in China, please edit skywalking\\apm-webapp\\pom.xml as follows. Find\n\u0026lt;configuration\u0026gt; \u0026lt;arguments\u0026gt;install --registry=https://registry.npmjs.org/\u0026lt;/arguments\u0026gt; \u0026lt;/configuration\u0026gt; Replace it with\n\u0026lt;configuration\u0026gt; \u0026lt;arguments\u0026gt;install --registry=https://registry.npm.taobao.org/ --sass_binary_site=https://npm.taobao.org/mirrors/node-sass/\u0026lt;/arguments\u0026gt; \u0026lt;/configuration\u0026gt; 2. Get a sufficiently powerful VPN. ","excerpt":"Problem： Maven compilation failure with error such as Error: not found: python2 When you compile the …","ref":"/docs/main/v8.5.0/en/faq/maven-compile-npm-failure/","title":"Problem： Maven compilation failure with error such as `Error： not found： python2`"},{"body":"Protocols There are two types of protocols list here.\n  Probe Protocol. Include the descriptions and definitions about how agent send collected metrics data and traces, also the formats of each entities.\n  Query Protocol. The backend provide query capability to SkyWalking own UI and others. These queries are based on GraphQL.\n  Probe Protocols They also related to the probe group, for understand that, look Concepts and Designs document. These groups are Language based native agent protocol, Service Mesh protocol and 3rd-party instrument protocol.\nLanguage based native agent protocol There are two types of protocols to make language agents work in distributed environments.\n Cross Process Propagation Headers Protocol and Cross Process Correlation Headers Protocol are in wire data format, agent/SDK usually uses HTTP/MQ/HTTP2 headers to carry the data with rpc request. The remote agent will receive this in the request handler, and bind the context with this specific request. Trace Data Protocol is out of wire data, agent/SDK uses this to send traces and metrics to skywalking or other compatible backend.  Cross Process Propagation Headers Protocol v3 is the new protocol for in-wire context propagation, started in 8.0.0 release.\nCross Process Correlation Headers Protocol v1 is a new in-wire context propagation additional and optional protocols. Please read SkyWalking language agents documentations to see whether it is supported. This protocol defines the data format of transporting custom data with Cross Process Propagation Headers Protocol. SkyWalking javaagent begins to support this since 8.0.0.\nSkyWalking Trace Data Protocol v3 defines the communication way and format between agent and backend.\nSkyWalking Log Data Protocol defines the communication way and format between agent and backend.\nBrowser probe protocol The browser probe, such as skywalking-client-js could use this protocol to send to backend. This service provided by gRPC.\nSkyWalking Browser Protocol define the communication way and format between skywalking-client-js and backend.\nService Mesh probe protocol The probe in sidecar or proxy could use this protocol to send data to backendEnd. This service provided by gRPC, requires the following key info:\n Service Name or ID at both sides. Service Instance Name or ID at both sides. Endpoint. URI in HTTP, service method full signature in gRPC. Latency. In milliseconds. Response code in HTTP Status. Success or fail. Protocol. HTTP, gRPC DetectPoint. In Service Mesh sidecar, client or server. In normal L7 proxy, value is proxy.  Events Report Protocol The protocol is used to report events to the backend. The doc introduces the definition of an event, and the protocol repository defines gRPC services and messages formats of events.\n3rd-party instrument protocol 3rd-party instrument protocols are not defined by SkyWalking. They are just protocols/formats, which SkyWalking is compatible and could receive from their existed libraries. SkyWalking starts with supporting Zipkin v1, v2 data formats.\nBackend is based on modularization principle, so very easy to extend a new receiver to support new protocol/format.\nQuery Protocol Query protocol follows GraphQL grammar, provides data query capabilities, which depends on your analysis metrics. Read query protocol doc for more details.\n","excerpt":"Protocols There are two types of protocols list here.\n  Probe Protocol. Include the descriptions and …","ref":"/docs/main/v8.5.0/en/protocols/readme/","title":"Protocols"},{"body":"Query Protocol Query Protocol defines a set of APIs in GraphQL grammar to provide data query and interactive capabilities with SkyWalking native visualization tool or 3rd party system, including Web UI, CLI or private system.\nQuery protocol official repository, https://github.com/apache/skywalking-query-protocol.\nMetadata Metadata includes the brief info of the whole under monitoring services and their instances, endpoints, etc. Use multiple ways to query this meta data.\nextend type Query { getGlobalBrief(duration: Duration!): ClusterBrief # Normal service related metainfo  getAllServices(duration: Duration!): [Service!]! searchServices(duration: Duration!, keyword: String!): [Service!]! searchService(serviceCode: String!): Service # Fetch all services of Browser type getAllBrowserServices(duration: Duration!): [Service!]! # Service intance query getServiceInstances(duration: Duration!, serviceId: ID!): [ServiceInstance!]! # Endpoint query # Consider there are huge numbers of endpoint, # must use endpoint owner\u0026#39;s service id, keyword and limit filter to do query. searchEndpoint(keyword: String!, serviceId: ID!, limit: Int!): [Endpoint!]! getEndpointInfo(endpointId: ID!): EndpointInfo # Database related meta info. getAllDatabases(duration: Duration!): [Database!]! getTimeInfo: TimeInfo } Topology Show the topology and dependency graph of services or endpoints. Including direct relationship or global map.\nextend type Query { # Query the global topology getGlobalTopology(duration: Duration!): Topology # Query the topology, based on the given service getServiceTopology(serviceId: ID!, duration: Duration!): Topology # Query the instance topology, based on the given clientServiceId and serverServiceId getServiceInstanceTopology(clientServiceId: ID!, serverServiceId: ID!, duration: Duration!): ServiceInstanceTopology # Query the topology, based on the given endpoint getEndpointTopology(endpointId: ID!, duration: Duration!): Topology } Metrics Metrics query targets all the objects defined in OAL script. You could get the metrics data in linear or thermodynamic matrix formats based on the aggregation functions in script.\n3 types of metrics could be query\n Single value. The type of most default metrics is single value, consider this as default. getValues and getLinearIntValues are suitable for this. Multiple value. One metrics defined in OAL include multiple value calculations. Use getMultipleLinearIntValues to get all values. percentile is a typical multiple value func in OAL. Heatmap value. Read Heatmap in WIKI for detail. thermodynamic is the only OAL func. Use getThermodynamic to get the values.  extend type Query { getValues(metric: BatchMetricConditions!, duration: Duration!): IntValues getLinearIntValues(metric: MetricCondition!, duration: Duration!): IntValues # Query the type of metrics including multiple values, and format them as multiple linears. # The seq of these multiple lines base on the calculation func in OAL # Such as, should us this to query the result of func percentile(50,75,90,95,99) in OAL, # then five lines will be responsed, p50 is the first element of return value. getMultipleLinearIntValues(metric: MetricCondition!, numOfLinear: Int!, duration: Duration!): [IntValues!]! getThermodynamic(metric: MetricCondition!, duration: Duration!): Thermodynamic } Metrics are defined in the config/oal/*.oal files.\nAggregation Aggregation query means the metrics data need a secondary aggregation in query stage, which makes the query interfaces have some different arguments. Such as, TopN list of services is a very typical aggregation query, metrics stream aggregation just calculates the metrics values of each service, but the expected list needs ordering metrics data by the values.\nAggregation query is for single value metrics only.\n# The aggregation query is different with the metric query. # All aggregation queries require backend or/and storage do aggregation in query time. extend type Query { # TopN is an aggregation query. getServiceTopN(name: String!, topN: Int!, duration: Duration!, order: Order!): [TopNEntity!]! getAllServiceInstanceTopN(name: String!, topN: Int!, duration: Duration!, order: Order!): [TopNEntity!]! getServiceInstanceTopN(serviceId: ID!, name: String!, topN: Int!, duration: Duration!, order: Order!): [TopNEntity!]! getAllEndpointTopN(name: String!, topN: Int!, duration: Duration!, order: Order!): [TopNEntity!]! getEndpointTopN(serviceId: ID!, name: String!, topN: Int!, duration: Duration!, order: Order!): [TopNEntity!]! } Others The following query(s) are for specific features, including trace, alarm or profile.\n Trace. Query distributed traces by this. Alarm. Through alarm query, you can have alarm trend and details.  The actual query GraphQL scrips could be found inside query-protocol folder in here.\nCondition Duration Duration is a widely used parameter type as the APM data is time related. The explanations are as following. Step is related the precision.\n# The Duration defines the start and end time for each query operation. # Fields: `start` and `end` # represents the time span. And each of them matches the step. # ref https://www.ietf.org/rfc/rfc3339.txt # The time formats are # `SECOND` step: yyyy-MM-dd HHmmss # `MINUTE` step: yyyy-MM-dd HHmm # `HOUR` step: yyyy-MM-dd HH # `DAY` step: yyyy-MM-dd # `MONTH` step: yyyy-MM # Field: `step` # represents the accurate time point. # e.g. # if step==HOUR , start=2017-11-08 09, end=2017-11-08 19 # then # metrics from the following time points expected # 2017-11-08 9:00 -\u0026gt; 2017-11-08 19:00 # there are 11 time points (hours) in the time span. input Duration { start: String! end: String! step: Step! } enum Step { MONTH DAY HOUR MINUTE SECOND } ","excerpt":"Query Protocol Query Protocol defines a set of APIs in GraphQL grammar to provide data query and …","ref":"/docs/main/v8.5.0/en/protocols/query-protocol/","title":"Query Protocol"},{"body":"Register mechanism is no longer required for local / exit span Since version 6.6.0, SkyWalking has removed the local and exit span registers. If an old java agent (before 6.6.0) is still running, which registers to the 6.6.0+ backend, you will face the following warning message.\nclass=RegisterServiceHandler, message = Unexpected endpoint register, endpoint isn't detected from server side. This will not harm the backend or cause any issues, but serves as a reminder that your agent or other clients should follow the new protocol requirements.\nYou could simply use log4j2.xml to filter this warning message out.\n","excerpt":"Register mechanism is no longer required for local / exit span Since version 6.6.0, SkyWalking has …","ref":"/docs/main/v8.5.0/en/faq/unexpected-endpoint-register/","title":"Register mechanism is no longer required for local / exit span"},{"body":"Scopes and Fields Using the Aggregation Function, the requests will be grouped by time and Group Key(s) in each scope.\nSCOPE All    Name Remarks Group Key Type     name The service name of each request.  string   serviceInstanceName The name of the service instance ID.  string   endpoint The endpoint path of each request.  string   latency The time taken by each request.  int(in ms)   status The success or failure of the request.  bool(true for success)   responseCode The response code of the HTTP response, and if this request is the HTTP call. E.g. 200, 404, 302  int   type The type of each request, such as Database, HTTP, RPC, or gRPC.  enum   tags The labels of each request. Each value is made up by TagKey:TagValue in the segment.  List\u0026lt;String\u0026gt;    SCOPE Service This calculates the metrics data from each request of the service.\n   Name Remarks Group Key Type     name The name of the service.  string   nodeType The kind of node to which the Service or Network address belongs, such as Normal, Database, MQ, or Cache.  enum   serviceInstanceName The name of the service instance ID.  string   endpointName The name of the endpoint, such as a full path of HTTP URI.  string   latency The time taken by each request.  int   status Indicates the success or failure of the request.  bool(true for success)   responseCode The response code of the HTTP response, if this request is an HTTP call.  int   type The type of each request. Such as: Database, HTTP, RPC, gRPC.  enum   tags The labels of each request. Each value is made up by TagKey:TagValue in the segment.  List\u0026lt;String\u0026gt;   sideCar.internalErrorCode The sidecar/gateway proxy internal error code. The value is based on the implementation.  string    SCOPE ServiceInstance This calculates the metrics data from each request of the service instance.\n   Name Remarks Group Key Type     name The name of the service instance, such as ip:port@Service Name. Note: Currently, the native agent uses uuid@ipv4 as the instance name, which does not assist in setting up a filter in aggregation.  string   serviceName The name of the service.  string   nodeType The kind of node to which the Service or Network address belongs, such as Normal, Database, MQ, or Cache.  enum   endpointName The name of the endpoint, such as a full path of the HTTP URI.  string   latency The time taken by each request.  int   status Indicates the success or failure of the request.  bool(true for success)   responseCode The response code of HTTP response, if this request is an HTTP call.  int   type The type of each request, such as Database, HTTP, RPC, or gRPC.  enum   tags The labels of each request. Each value is made up by TagKey:TagValue in the segment.  List\u0026lt;String\u0026gt;   sideCar.internalErrorCode The sidecar/gateway proxy internal error code. The value is based on the implementation.  string    Secondary scopes of ServiceInstance This calculates the metrics data if the service instance is a JVM and collects through javaagent.\n SCOPE ServiceInstanceJVMCPU     Name Remarks Group Key Type     name The name of the service instance, such as ip:port@Service Name. Note: Currently, the native agent uses uuid@ipv4 as the instance name, which does not assist in setting up a filter in aggregation.  string   serviceName The name of the service.  string   usePercent The percentage of CPU time spent.  double    SCOPE ServiceInstanceJVMMemory     Name Remarks Group Key Type     name The name of the service instance, such as ip:port@Service Name. Note: Currently, the native agent uses uuid@ipv4 as the instance name, which does not assist in setting up a filter in aggregation.  string   serviceName The name of the service.  string   heapStatus Indicates whether the metric has a heap property or not.  bool   init See the JVM documentation.  long   max See the JVM documentation.  long   used See the JVM documentation.  long   committed See the JVM documentation.  long    SCOPE ServiceInstanceJVMMemoryPool     Name Remarks Group Key Type     name The name of the service instance, such as ip:port@Service Name. Note: Currently, the native agent uses uuid@ipv4 as the instance name, which does not assist in setting up a filter in aggregation.  string   serviceName The name of the service.  string   poolType The type may be CODE_CACHE_USAGE, NEWGEN_USAGE, OLDGEN_USAGE, SURVIVOR_USAGE, PERMGEN_USAGE, or METASPACE_USAGE based on different versions of JVM.  enum   init See the JVM documentation.  long   max See the JVM documentation.  long   used See the JVM documentation.  long   committed See the JVM documentation.  long    SCOPE ServiceInstanceJVMGC     Name Remarks Group Key Type     name The name of the service instance, such as ip:port@Service Name. Note: Currently, the native agent uses uuid@ipv4 as the instance name, which does not assist in setting up a filter in aggregation.  string   serviceName The name of the service.  string   phrase Includes both NEW and OLD.  Enum   time The time spent in GC.  long   count The count in GC operations.  long    SCOPE ServiceInstanceJVMThread     Name Remarks Group Key Type     name The name of the service instance, such as ip:port@Service Name. Note: Currently, the native agent uses uuid@ipv4 as the instance name, which does not assist in setting up a filter in aggregation.  string   serviceName The name of the service.  string   liveCount The current number of live threads.  int   daemonCount The current number of daemon threads.  int   peakCount The current number of peak threads.  int    SCOPE Endpoint This calculates the metrics data from each request of the endpoint in the service.\n   Name Remarks Group Key Type     name The name of the endpoint, such as a full path of the HTTP URI.  string   serviceName The name of the service.  string   serviceNodeType The type of node to which the Service or Network address belongs, such as Normal, Database, MQ, or Cache.  enum   serviceInstanceName The name of the service instance ID.  string   latency The time taken by each request.  int   status Indicates the success or failure of the request.  bool(true for success)   responseCode The response code of HTTP response, if this request is an HTTP call.  int   type The type of each request, such as Database, HTTP, RPC, or gRPC.  enum   tags The labels of each request. Each value is made up by TagKey:TagValue in the segment.  List\u0026lt;String\u0026gt;   sideCar.internalErrorCode The sidecar/gateway proxy internal error code. The value is based on the implementation.  string    SCOPE ServiceRelation This calculates the metrics data from each request between services.\n   Name Remarks Group Key Type     sourceServiceName The name of the source service.  string   sourceServiceNodeType The type of node to which the Service or Network address belongs, such as Normal, Database, MQ, or Cache.  enum   sourceServiceInstanceName The name of the source service instance.  string   destServiceName The name of the destination service.  string   destServiceNodeType The type of node of to which the Service or Network address belongs.  enum   destServiceInstanceName The name of the destination service instance.  string   endpoint The endpoint used in this call.  string   componentId The ID of component used in this call. yes string   latency The time taken by each request.  int   status Indicates the success or failure of the request.  bool(true for success)   responseCode The response code of HTTP response, if this request is an HTTP call.  int   type The type of each request, such as Database, HTTP, RPC, or gRPC.  enum   detectPoint Where the relation is detected. The value may be client, server, or proxy. yes enum   tlsMode The TLS mode between source and destination services, such as service_relation_mtls_cpm = from(ServiceRelation.*).filter(tlsMode == \u0026quot;mTLS\u0026quot;).cpm()  string   sideCar.internalErrorCode The sidecar/gateway proxy internal error code. The value is based on the implementation.  string    SCOPE ServiceInstanceRelation This calculates the metrics data from each request between service instances.\n   Name Remarks Group Key Type     sourceServiceName The name of the source service.  string   sourceServiceNodeType The type of node to which the Service or Network address belongs, such as Normal, Database, MQ, or Cache.  enum   sourceServiceInstanceName The name of the source service instance.  string   destServiceName The name of the destination service.     destServiceNodeType The type of node to which the Service or Network address belongs, such as Normal, Database, MQ, or Cache.  string   destServiceInstanceName The name of the destination service instance.  string   endpoint The endpoint used in this call.  string   componentId The ID of the component used in this call. yes string   latency The time taken by each request.  int   status Indicates the success or failure of the request.  bool(true for success)   responseCode The response code of the HTTP response, if this request is an HTTP call.  int   type The type of each request, such as Database, HTTP, RPC, or gRPC.  enum   detectPoint Where the relation is detected. The value may be client, server, or proxy. yes enum   tlsMode The TLS mode between source and destination service instances, such as service_instance_relation_mtls_cpm = from(ServiceInstanceRelation.*).filter(tlsMode == \u0026quot;mTLS\u0026quot;).cpm()  string   sideCar.internalErrorCode The sidecar/gateway proxy internal error code. The value is based on the implementation.  string    SCOPE EndpointRelation This calculates the metrics data of the dependency between endpoints. This relation is hard to detect, and it depends on the tracing library to propagate the previous endpoint. Therefore, the EndpointRelation scope aggregation comes into effect only in services under tracing by SkyWalking native agents, including auto instrument agents (like Java and .NET), OpenCensus SkyWalking exporter implementation, or other tracing context propagation in SkyWalking specification.\n   Name Remarks Group Key Type     endpoint The parent endpoint in the dependency.  string   serviceName The name of the service.  string   serviceNodeType The type of node to which the Service or Network address belongs, such as Normal, Database, MQ, or Cache.  enum   childEndpoint The endpoint used by the parent endpoint in row(1).  string   childServiceName The endpoint used by the parent service in row(1).  string   childServiceNodeType The type of node to which the Service or Network address belongs, such as Normal, Database, MQ, or Cache.  string   childServiceInstanceName The endpoint used by the parent service instance in row(1).  string   rpcLatency The latency of the RPC between the parent endpoint and childEndpoint, excluding the latency caused by the parent endpoint itself.     componentId The ID of the component used in this call. yes string   status Indicates the success or failure of the request.  bool(true for success)   responseCode The response code of the HTTP response, if this request is an HTTP call.  int   type The type of each request, such as Database, HTTP, RPC, or gRPC.  enum   detectPoint Indicates where the relation is detected. The value may be client, server, or proxy. yes enum    SCOPE BrowserAppTraffic This calculates the metrics data from each request of the browser application (browser only).\n   Name Remarks Group Key Type     name The browser application name of each request.  string   count The number of request, which is fixed at 1.  int   trafficCategory The traffic category. The value may be NORMAL, FIRST_ERROR, or ERROR.  enum   errorCategory The error category. The value may be AJAX, RESOURCE, VUE, PROMISE, or UNKNOWN.  enum    SCOPE BrowserAppSingleVersionTraffic This calculates the metrics data from each request of a single version in the browser application (browser only).\n   Name Remarks Group Key Type     name The single version name of each request.  string   serviceName The name of the browser application.  string   count The number of request, which is fixed at 1.  int   trafficCategory The traffic category. The value may be NORMAL, FIRST_ERROR, or ERROR.  enum   errorCategory The error category. The value may be AJAX, RESOURCE, VUE, PROMISE, or UNKNOWN.  enum    SCOPE BrowserAppPageTraffic This calculates the metrics data from each request of the page in the browser application (browser only).\n   Name Remarks Group Key Type     name The page name of each request.  string   serviceName The name of the browser application.  string   count The number of request, which is fixed at 1.  int   trafficCategory The traffic category. The value may be NORMAL, FIRST_ERROR, or ERROR.  enum   errorCategory The error category. The value may be AJAX, RESOURCE, VUE, PROMISE, or UNKNOWN.  enum    SCOPE BrowserAppPagePerf This calculates the metrics data form each request of the page in the browser application (browser only).\n   Name Remarks Group Key Type     name The page name of each request.  string   serviceName The name of the browser application.  string   redirectTime The time taken to redirect.  int(in ms)   dnsTime The DNS query time.  int(in ms)   ttfbTime Time to first byte.  int(in ms)   tcpTime TCP connection time.  int(in ms)   transTime Content transfer time.  int(in ms)   domAnalysisTime Dom parsing time.  int(in ms)   fptTime First paint time or blank screen time.  int(in ms)   domReadyTime Dom ready time.  int(in ms)   loadPageTime Page full load time.  int(in ms)   resTime Synchronous load resources in the page.  int(in ms)   sslTime Only valid for HTTPS.  int(in ms)   ttlTime Time to interact.  int(in ms)   firstPackTime First pack time.  int(in ms)   fmpTime First Meaningful Paint.  int(in ms)    ","excerpt":"Scopes and Fields Using the Aggregation Function, the requests will be grouped by time and Group …","ref":"/docs/main/v8.5.0/en/concepts-and-designs/scope-definitions/","title":"Scopes and Fields"},{"body":"Send Envoy metrics to SkyWalking with / without Istio Envoy defines a gRPC service to emit the metrics, whatever implements this protocol can be used to receive the metrics. SkyWalking has a built-in receiver that implements this protocol so that you can configure Envoy to emit its metrics to SkyWalking.\nAs an APM system, SkyWalking does not only receive and store the metrics emitted by Envoy, it also analyzes the topology of services and service instances.\nAttention: There are two versions of Envoy metrics service protocol up to date, v2 and v3, SkyWalking (8.3.0+) supports both of them.\nConfigure Envoy to send metrics to SkyWalking without Istio Envoy can be used with / without Istio\u0026rsquo;s control. This section introduces how to configure the standalone Envoy to send the metrics to SkyWalking.\nIn order to let Envoy send metrics to SkyWalking, we need to feed Envoy with a configuration which contains stats_sinks that includes envoy.metrics_service. This envoy.metrics_service should be configured as a config.grpc_service entry.\nThe interesting parts of the config is shown in the config below:\nstats_sinks: - name: envoy.metrics_service config: grpc_service: # Note: we can use google_grpc implementation as well. envoy_grpc: cluster_name: service_skywalking static_resources: ... clusters: - name: service_skywalking connect_timeout: 5s type: LOGICAL_DNS http2_protocol_options: {} dns_lookup_family: V4_ONLY lb_policy: ROUND_ROBIN load_assignment: cluster_name: service_skywalking endpoints: - lb_endpoints: - endpoint: address: socket_address: address: skywalking # This is the port where SkyWalking serving the Envoy Metrics Service gRPC stream. port_value: 11800 A more complete static configuration, can be observed here.\nNote that Envoy can also be configured dynamically through xDS Protocol.\nAs mentioned above, SkyWalking also builds the topology of services from the metrics, this is because Envoy also carries the service metadata along with the metrics, to feed the Envoy such metadata, another configuration part is as follows:\nnode: # ... other configs metadata: LABELS: app: test-app NAME: service-instance-name Configure Envoy to send metrics to SkyWalking with Istio Typically, Envoy can be also used under Istio\u0026rsquo;s control, where the configurations are much more simple because Istio composes the configurations for you and sends them to Envoy via xDS Protocol. Istio also automatically injects the metadata such as service name and instance name into the bootstrap configurations.\nUnder this circumstance, emitting the metrics to SyWalking is as simple as adding the option --set meshConfig.defaultConfig.envoyMetricsService.address=\u0026lt;skywalking.address.port.11800\u0026gt; to Istio install command, for example:\nistioctl install -y \\  --set profile=demo `# replace the profile as per your need` \\ --set meshConfig.defaultConfig.envoyMetricsService.address=\u0026lt;skywalking.address.port.11800\u0026gt; # replace \u0026lt;skywalking.address.port.11800\u0026gt; with your actual SkyWalking OAP address If you already have Istio installed, you can use the following command to apply the config without re-installing Istio:\nistioctl manifest install -y \\  --set profile=demo `# replace the profile as per your need` \\ --set meshConfig.defaultConfig.envoyMetricsService.address=\u0026lt;skywalking.address.port.11800\u0026gt; # replace \u0026lt;skywalking.address.port.11800\u0026gt; with your actual SkyWalking OAP address Metrics data Some Envoy statistics are listed in this list. A sample data that contains identifier can be found here, while the metrics only can be observed here.\n","excerpt":"Send Envoy metrics to SkyWalking with / without Istio Envoy defines a gRPC service to emit the …","ref":"/docs/main/v8.5.0/en/setup/envoy/metrics_service_setting/","title":"Send Envoy metrics to SkyWalking with / without Istio"},{"body":"Sending Envoy Metrics to SkyWalking OAP Server Example This is an example of sending Envoy Stats to SkyWalking OAP server through Metric Service.\nRunning the example The example requires docker and docker-compose to be installed in your local. It fetches images from Docker Hub.\nNote that in ths setup, we override the log4j2.xml config to set the org.apache.skywalking.oap.server.receiver.envoy logger level to DEBUG. This enables us to see the messages sent by Envoy to SkyWalking OAP server.\n$ make up $ docker-compose logs -f skywalking $ # Please wait for a moment until SkyWalking is ready and Envoy starts sending the stats. You will see similar messages like the following: skywalking_1 | 2019-08-31 23:57:40,672 - org.apache.skywalking.oap.server.receiver.envoy.MetricServiceGRPCHandler -26870 [grpc-default-executor-0] DEBUG [] - Received msg identifier { skywalking_1 | node { skywalking_1 | id: \u0026quot;ingress\u0026quot; skywalking_1 | cluster: \u0026quot;envoy-proxy\u0026quot; skywalking_1 | metadata { skywalking_1 | fields { skywalking_1 | key: \u0026quot;skywalking\u0026quot; skywalking_1 | value { skywalking_1 | string_value: \u0026quot;iscool\u0026quot; skywalking_1 | } skywalking_1 | } skywalking_1 | fields { skywalking_1 | key: \u0026quot;envoy\u0026quot; skywalking_1 | value { skywalking_1 | string_value: \u0026quot;isawesome\u0026quot; skywalking_1 | } skywalking_1 | } skywalking_1 | } skywalking_1 | locality { skywalking_1 | region: \u0026quot;ap-southeast-1\u0026quot; skywalking_1 | zone: \u0026quot;zone1\u0026quot; skywalking_1 | sub_zone: \u0026quot;subzone1\u0026quot; skywalking_1 | } skywalking_1 | build_version: \u0026quot;e349fb6139e4b7a59a9a359be0ea45dd61e589c5/1.11.1/Clean/RELEASE/BoringSSL\u0026quot; skywalking_1 | } skywalking_1 | } skywalking_1 | envoy_metrics { skywalking_1 | name: \u0026quot;cluster.service_skywalking.update_success\u0026quot; skywalking_1 | type: COUNTER skywalking_1 | metric { skywalking_1 | counter { skywalking_1 | value: 2.0 skywalking_1 | } skywalking_1 | timestamp_ms: 1567295859556 skywalking_1 | } skywalking_1 | } ... $ # To tear down: $ make down ","excerpt":"Sending Envoy Metrics to SkyWalking OAP Server Example This is an example of sending Envoy Stats to …","ref":"/docs/main/v8.5.0/en/setup/envoy/examples/metrics/readme/","title":"Sending Envoy Metrics to SkyWalking OAP Server Example"},{"body":"Service Auto Grouping SkyWalking supports various default and customized dashboard templates. Each template provides the reasonable layout for the services in the particular field. Such as, services with a language agent installed could have different metrics with service detected by the service mesh observability solution, and different with SkyWalking\u0026rsquo;s self-observability metrics dashboard.\nTherefore, since 8.3.0, SkyWalking OAP would generate the group based on this simple naming format.\n${service name} = [${group name}::]${logic name} Once the service name includes double colons(::), the literal string before the colons would be considered as the group name. In the latest GraphQL query, the group name has been provided as an option parameter.\n getAllServices(duration: Duration!, group: String): [Service!]!\n RocketBot UI dashboards(Standard type) support the group name for default and custom configurations.\n","excerpt":"Service Auto Grouping SkyWalking supports various default and customized dashboard templates. Each …","ref":"/docs/main/v8.5.0/en/setup/backend/service-auto-grouping/","title":"Service Auto Grouping"},{"body":"Service Auto Instrument Agent The service auto instrument agent is a subset of language-based native agents. This kind of agents is based on some language-specific features, especially those of a VM-based language.\nWhat does Auto Instrument mean? Many users learned about these agents when they first heard that \u0026ldquo;Not a single line of code has to be changed\u0026rdquo;. SkyWalking used to mention this in its readme page as well. However, this does not reflect the full picture. For end users, it is true that they no longer have to modify their codes in most cases. But it is important to understand that the codes are in fact still modified by the agent, which is usually known as \u0026ldquo;runtime code manipulation\u0026rdquo;. The underlying logic is that the auto instrument agent uses the VM interface for code modification to dynamically add in the instrument code, such as modifying the class in Java through javaagent premain.\nIn fact, although the SkyWalking team has mentioned that most auto instrument agents are VM-based, you may build such tools during compiling time rather than runtime.\nWhat are the limitations? Auto instrument is very helpful, as you may perform auto instrument during compiling time, without having to depend on VM features. But there are also certain limitations that come with it:\n  Higher possibility of in-process propagation in many cases. Many high-level languages, such as Java and .NET, are used for building business systems. Most business logic codes run in the same thread for each request, which causes propagation to be based on thread ID, in order for the stack module to make sure that the context is safe.\n  Only works in certain frameworks or libraries. Since the agents are responsible for modifying the codes during runtime, the codes are already known to the agent plugin developers. There is usually a list of frameworks or libraries supported by this kind of probes. For example, see the SkyWalking Java agent supported list.\n  Cross-thread operations are not always supported. Like what is mentioned above regarding in-process propagation, most codes (especially business codes) run in a single thread per request. But in some other cases, they operate across different threads, such as assigning tasks to other threads, task pools or batch processes. Some languages may even provide coroutine or similar components like Goroutine, which allows developers to run async process with low payload. In such cases, auto instrument will face problems.\n  So, there\u0026rsquo;s nothing mysterious about auto instrument. In short, agent developers write an activation script to make instrument codes work for you. That\u0026rsquo;s it!\nWhat is next? If you want to learn about manual instrument libs in SkyWalking, see the Manual instrument SDK section.\n","excerpt":"Service Auto Instrument Agent The service auto instrument agent is a subset of language-based native …","ref":"/docs/main/v8.5.0/en/concepts-and-designs/service-agent/","title":"Service Auto Instrument Agent"},{"body":"Service Mesh Probe Service Mesh probes use the extendable mechanism provided in Service Mesh implementor, like Istio.\nWhat is Service Mesh? The following explanation came from Istio documents.\n The term service mesh is often used to describe the network of microservices that make up such applications and the interactions between them. As a service mesh grows in size and complexity, it can become harder to understand and manage. Its requirements can include discovery, load balancing, failure recovery, metrics, and monitoring, and often more complex operational requirements such as A/B testing, canary releases, rate limiting, access control, and end-to-end authentication.\n Where does the probe collect data from? Istio is a very typical Service Mesh design and implementor. It defines Control Panel and Data Panel, which are widely used. Here is Istio Architecture:\nService Mesh probe can choose to collect data from Data Panel. In Istio, it means collecting telemetry data from Envoy sidecar(Data Panel). The probe collects two telemetry entities from client side and server side per request.\nHow does Service Mesh make backend work? From the probe, you can see there must have no trace related in this kind of probe, so why SkyWalking platform still works?\nService Mesh probes collects telemetry data from each request, so it knows the source, destination, endpoint, latency and status. By those, backend can tell the whole topology map by combining these call as lines, and also the metrics of each nodes through their incoming request. Backend asked for the same metrics data from parsing tracing data. So, the right expression is: Service Mesh metrics are exact the metrics, what the traces parsers generate. They are same.\n","excerpt":"Service Mesh Probe Service Mesh probes use the extendable mechanism provided in Service Mesh …","ref":"/docs/main/v8.5.0/en/concepts-and-designs/service-mesh-probe/","title":"Service Mesh Probe"},{"body":"Setting Override SkyWalking backend supports setting overrides by system properties and system environment variables. You could override the settings in application.yml\nSystem properties key rule ModuleName.ProviderName.SettingKey.\n  Example\nOverride restHost in this setting segment\n  core: default: restHost: ${SW_CORE_REST_HOST:0.0.0.0} restPort: ${SW_CORE_REST_PORT:12800} restContextPath: ${SW_CORE_REST_CONTEXT_PATH:/} gRPCHost: ${SW_CORE_GRPC_HOST:0.0.0.0} gRPCPort: ${SW_CORE_GRPC_PORT:11800} Use command arg\n-Dcore.default.restHost=172.0.4.12 System environment variables   Example\nOverride restHost in this setting segment through environment variables\n  core: default: restHost: ${REST_HOST:0.0.0.0} restPort: ${SW_CORE_REST_PORT:12800} restContextPath: ${SW_CORE_REST_CONTEXT_PATH:/} gRPCHost: ${SW_CORE_GRPC_HOST:0.0.0.0} gRPCPort: ${SW_CORE_GRPC_PORT:11800} If the REST_HOST  environment variable exists in your operating system and its value is 172.0.4.12, then the value of restHost here will be overwritten to 172.0.4.12, otherwise, it will be set to 0.0.0.0.\nBy the way, Placeholder nesting is also supported, like ${REST_HOST:${ANOTHER_REST_HOST:127.0.0.1}}. In this case, if the REST_HOST  environment variable not exists, but the REST_ANOTHER_REST_HOSTHOST environment variable exists and its value is 172.0.4.12, then the value of restHost here will be overwritten to 172.0.4.12, otherwise, it will be set to 127.0.0.1.\n","excerpt":"Setting Override SkyWalking backend supports setting overrides by system properties and system …","ref":"/docs/main/v8.5.0/en/setup/backend/backend-setting-override/","title":"Setting Override"},{"body":"Setting Override In default, SkyWalking provide agent.config for agent.\nSetting override means end user can override the settings in these config file, through using system properties or agent options.\nSystem properties Use skywalking. + key in config file as system properties key, to override the value.\n  Why need this prefix?\nThe agent system properties and env share with target application, this prefix can avoid variable conflict.\n  Example\nOverride agent.application_code by this.\n  -Dskywalking.agent.application_code=31200 Agent options Add the properties after the agent path in JVM arguments.\n-javaagent:/path/to/skywalking-agent.jar=[option1]=[value1],[option2]=[value2]   Example\nOverride agent.application_code and logging.level by this.\n  -javaagent:/path/to/skywalking-agent.jar=agent.application_code=31200,logging.level=debug   Special characters\nIf a separator(, or =) in the option or value, it should be wrapped in quotes.\n  -javaagent:/path/to/skywalking-agent.jar=agent.ignore_suffix='.jpg,.jpeg' System environment variables   Example\nOverride agent.application_code and logging.level by this.\n  # The service name in UI agent.service_name=${SW_AGENT_NAME:Your_ApplicationName} # Logging level logging.level=${SW_LOGGING_LEVEL:INFO} If the SW_AGENT_NAME  environment variable exists in your operating system and its value is skywalking-agent-demo, then the value of agent.service_name here will be overwritten to skywalking-agent-demo, otherwise, it will be set to Your_ApplicationName.\nBy the way, Placeholder nesting is also supported, like ${SW_AGENT_NAME:${ANOTHER_AGENT_NAME:Your_ApplicationName}}. In this case, if the SW_AGENT_NAME  environment variable not exists, but the ANOTHER_AGENT_NAME environment variable exists and its value is skywalking-agent-demo, then the value of agent.service_name here will be overwritten to skywalking-agent-demo,otherwise, it will be set to Your_ApplicationName.\nOverride priority Agent Options \u0026gt; System.Properties(-D) \u0026gt; System environment variables \u0026gt; Config file\n","excerpt":"Setting Override In default, SkyWalking provide agent.config for agent.\nSetting override means end …","ref":"/docs/main/v8.5.0/en/setup/service-agent/java-agent/setting-override/","title":"Setting Override"},{"body":"Setup java agent  Agent is available for JDK 8 - 14 in 7.x releases. JDK 1.6 - JDK 12 are supported in all 6.x releases NOTICE¹ Find agent folder in SkyWalking release package Set agent.service_name in config/agent.config. Could be any String in English. Set collector.backend_service in config/agent.config. Default point to 127.0.0.1:11800, only works for local backend. Add -javaagent:/path/to/skywalking-package/agent/skywalking-agent.jar to JVM argument. And make sure to add it before the -jar argument.  The agent release dist is included in Apache official release. New agent package looks like this.\n+-- agent +-- activations apm-toolkit-log4j-1.x-activation.jar apm-toolkit-log4j-2.x-activation.jar apm-toolkit-logback-1.x-activation.jar ... +-- config agent.config +-- plugins apm-dubbo-plugin.jar apm-feign-default-http-9.x.jar apm-httpClient-4.x-plugin.jar ..... +-- optional-plugins apm-gson-2.x-plugin.jar ..... +-- bootstrap-plugins jdk-http-plugin.jar ..... +-- logs skywalking-agent.jar  Start your application.  Supported middleware, framework and library SkyWalking agent has supported various middlewares, frameworks and libraries. Read supported list to get them and supported version. If the plugin is in Optional² catalog, go to optional plugins section to learn how to active it.\nAdvanced features  All plugins are in /plugins folder. The plugin jar is active when it is in there. Remove the plugin jar, it disabled. The default logging output folder is /logs.  Install javaagent FAQs  Linux Tomcat 7, Tomcat 8, Tomcat 9\nChange the first line of tomcat/bin/catalina.sh.  CATALINA_OPTS=\u0026#34;$CATALINA_OPTS-javaagent:/path/to/skywalking-agent/skywalking-agent.jar\u0026#34;; export CATALINA_OPTS  Windows Tomcat 7, Tomcat 8, Tomcat 9\nChange the first line of tomcat/bin/catalina.bat.  set \u0026#34;CATALINA_OPTS=-javaagent:/path/to/skywalking-agent/skywalking-agent.jar\u0026#34;  JAR file\nAdd -javaagent argument to command line in which you start your app. eg:  java -javaagent:/path/to/skywalking-agent/skywalking-agent.jar -jar yourApp.jar  Jetty\nModify jetty.sh, add -javaagent argument to command line in which you start your app. eg:  export JAVA_OPTIONS=\u0026#34;${JAVA_OPTIONS}-javaagent:/path/to/skywalking-agent/skywalking-agent.jar\u0026#34; Table of Agent Configuration Properties This is the properties list supported in agent/config/agent.config.\n   property key Description Default     agent.namespace Namespace isolates headers in cross process propagation. The HEADER name will be HeaderName:Namespace. Not set   agent.service_name The service name to represent a logic group providing the same capabilities/logic. Suggestion: set a unique name for every logic service group, service instance nodes share the same code, Max length is 50(UTF-8 char). Optional, once service_name follows \u0026lt;group name\u0026gt;::\u0026lt;logic name\u0026gt; format, OAP server assigns the group name to the service metadata. Your_ApplicationName   agent.sample_n_per_3_secs Negative or zero means off, by default.SAMPLE_N_PER_3_SECS means sampling N TraceSegment in 3 seconds tops. Not set   agent.authentication Authentication active is based on backend setting, see application.yml for more details.For most scenarios, this needs backend extensions, only basic match auth provided in default implementation. Not set   agent.span_limit_per_segment The max number of spans in a single segment. Through this config item, SkyWalking keep your application memory cost estimated. 300   agent.ignore_suffix If the operation name of the first span is included in this set, this segment should be ignored. Not set   agent.is_open_debugging_class If true, skywalking agent will save all instrumented classes files in /debugging folder. SkyWalking team may ask for these files in order to resolve compatible problem. Not set   agent.is_cache_enhanced_class If true, SkyWalking agent will cache all instrumented classes files to memory or disk files (decided by class cache mode), allow another java agent to enhance those classes that enhanced by SkyWalking agent. To use some Java diagnostic tools (such as BTrace, Arthas) to diagnose applications or add a custom java agent to enhance classes, you need to enable this feature. Read this FAQ for more details false   agent.class_cache_mode The instrumented classes cache mode: MEMORY or FILE. MEMORY: cache class bytes to memory, if instrumented classes is too many or too large, it may take up more memory. FILE: cache class bytes in /class-cache folder, automatically clean up cached class files when the application exits. MEMORY   agent.instance_name Instance name is the identity of an instance, should be unique in the service. If empty, SkyWalking agent will generate an 32-bit uuid. Default, use UUID@hostname as the instance name. Max length is 50(UTF-8 char) \u0026quot;\u0026quot;   agent.instance_properties[key]=value Add service instance custom properties. Not set   agent.cause_exception_depth How depth the agent goes, when log all cause exceptions. 5   agent.force_reconnection_period  Force reconnection period of grpc, based on grpc_channel_check_interval. 1   agent.operation_name_threshold  The operationName max length, setting this value \u0026gt; 190 is not recommended. 150   agent.keep_tracing Keep tracing even the backend is not available if this value is true. false   agent.force_tls Force open TLS for gRPC channel if this value is true. false   osinfo.ipv4_list_size Limit the length of the ipv4 list size. 10   collector.grpc_channel_check_interval grpc channel status check interval. 30   collector.heartbeat_period agent heartbeat report period. Unit, second. 30   collector.properties_report_period_factor The agent sends the instance properties to the backend every collector.heartbeat_period * collector.properties_report_period_factor seconds 10   collector.backend_service Collector SkyWalking trace receiver service addresses. 127.0.0.1:11800   collector.grpc_upstream_timeout How long grpc client will timeout in sending data to upstream. Unit is second. 30 seconds   collector.get_profile_task_interval Sniffer get profile task list interval. 20   collector.get_agent_dynamic_config_interval Sniffer get agent dynamic config interval 20   collector.dns_period_resolve_active If true, skywalking agent will enable periodically resolving DNS to update receiver service addresses. false   logging.level Log level: TRACE, DEBUG, INFO, WARN, ERROR, OFF. Default is info. INFO   logging.file_name Log file name. skywalking-api.log   logging.output Log output. Default is FILE. Use CONSOLE means output to stdout. FILE   logging.dir Log files directory. Default is blank string, means, use \u0026ldquo;{theSkywalkingAgentJarDir}/logs \u0026quot; to output logs. {theSkywalkingAgentJarDir} is the directory where the skywalking agent jar file is located \u0026quot;\u0026quot;   logging.resolver Logger resolver: PATTERN or JSON. The default is PATTERN, which uses logging.pattern to print traditional text logs. JSON resolver prints logs in JSON format. PATTERN   logging.pattern  Logging format. There are all conversion specifiers: * %level means log level. * %timestamp means now of time with format yyyy-MM-dd HH:mm:ss:SSS.\n* %thread means name of current thread.\n* %msg means some message which user logged. * %class means SimpleName of TargetClass. * %throwable means a throwable which user called. * %agent_name means agent.service_name. Only apply to the PatternLogger. %level %timestamp %thread %class : %msg %throwable   logging.max_file_size The max size of log file. If the size is bigger than this, archive the current file, and write into a new file. 300 * 1024 * 1024   logging.max_history_files The max history log files. When rollover happened, if log files exceed this number,then the oldest file will be delete. Negative or zero means off, by default. -1   statuscheck.ignored_exceptions Listed exceptions would not be treated as an error. Because in some codes, the exception is being used as a way of controlling business flow. \u0026quot;\u0026quot;   statuscheck.max_recursive_depth The max recursive depth when checking the exception traced by the agent. Typically, we don\u0026rsquo;t recommend setting this more than 10, which could cause a performance issue. Negative value and 0 would be ignored, which means all exceptions would make the span tagged in error status. 1   correlation.element_max_number Max element count in the correlation context. 3   correlation.value_max_length Max value length of each element. 128   correlation.auto_tag_keys Tag the span by the key/value in the correlation context, when the keys listed here exist. \u0026quot;\u0026quot;   jvm.buffer_size The buffer size of collected JVM info. 60 * 10   buffer.channel_size The buffer channel size. 5   buffer.buffer_size The buffer size. 300   profile.active If true, skywalking agent will enable profile when user create a new profile task. Otherwise disable profile. true   profile.max_parallel Parallel monitor segment count 5   profile.duration Max monitor segment time(minutes), if current segment monitor time out of limit, then stop it. 10   profile.dump_max_stack_depth Max dump thread stack depth 500   profile.snapshot_transport_buffer_size Snapshot transport to backend buffer size 50   meter.active If true, the agent collects and reports metrics to the backend. true   meter.report_interval Report meters interval. The unit is second 20   meter.max_meter_size Max size of the meter pool 500   plugin.mount Mount the specific folders of the plugins. Plugins in mounted folders would work. plugins,activations   plugin.peer_max_length  Peer maximum description limit. 200   plugin.exclude_plugins  Exclude some plugins define in plugins dir.Plugin names is defined in Agent plugin list \u0026quot;\u0026quot;   plugin.mongodb.trace_param If true, trace all the parameters in MongoDB access, default is false. Only trace the operation, not include parameters. false   plugin.mongodb.filter_length_limit If set to positive number, the WriteRequest.params would be truncated to this length, otherwise it would be completely saved, which may cause performance problem. 256   plugin.elasticsearch.trace_dsl If true, trace all the DSL(Domain Specific Language) in ElasticSearch access, default is false. false   plugin.springmvc.use_qualified_name_as_endpoint_name If true, the fully qualified method name will be used as the endpoint name instead of the request URL, default is false. false   plugin.toolit.use_qualified_name_as_operation_name If true, the fully qualified method name will be used as the operation name instead of the given operation name, default is false. false   plugin.jdbc.trace_sql_parameters If set to true, the parameters of the sql (typically java.sql.PreparedStatement) would be collected. false   plugin.jdbc.sql_parameters_max_length If set to positive number, the db.sql.parameters would be truncated to this length, otherwise it would be completely saved, which may cause performance problem. 512   plugin.jdbc.sql_body_max_length If set to positive number, the db.statement would be truncated to this length, otherwise it would be completely saved, which may cause performance problem. 2048   plugin.solrj.trace_statement If true, trace all the query parameters(include deleteByIds and deleteByQuery) in Solr query request, default is false. false   plugin.solrj.trace_ops_params If true, trace all the operation parameters in Solr request, default is false. false   plugin.light4j.trace_handler_chain If true, trace all middleware/business handlers that are part of the Light4J handler chain for a request. false   plugin.opgroup.* Support operation name customize group rules in different plugins. Read Group rule supported plugins Not set   plugin.springtransaction.simplify_transaction_definition_name If true, the transaction definition name will be simplified. false   plugin.jdkthreading.threading_class_prefixes Threading classes (java.lang.Runnable and java.util.concurrent.Callable) and their subclasses, including anonymous inner classes whose name match any one of the THREADING_CLASS_PREFIXES (splitted by ,) will be instrumented, make sure to only specify as narrow prefixes as what you\u0026rsquo;re expecting to instrument, (java. and javax. will be ignored due to safety issues) Not set   plugin.tomcat.collect_http_params This config item controls that whether the Tomcat plugin should collect the parameters of the request. Also, activate implicitly in the profiled trace. false   plugin.springmvc.collect_http_params This config item controls that whether the SpringMVC plugin should collect the parameters of the request, when your Spring application is based on Tomcat, consider only setting either plugin.tomcat.collect_http_params or plugin.springmvc.collect_http_params. Also, activate implicitly in the profiled trace. false   plugin.httpclient.collect_http_params This config item controls that whether the HttpClient plugin should collect the parameters of the request false   plugin.http.http_params_length_threshold When COLLECT_HTTP_PARAMS is enabled, how many characters to keep and send to the OAP backend, use negative values to keep and send the complete parameters, NB. this config item is added for the sake of performance. 1024   plugin.http.http_headers_length_threshold When include_http_headers declares header names, this threshold controls the length limitation of all header values. use negative values to keep and send the complete headers. Note. this config item is added for the sake of performance. 2048   plugin.http.include_http_headers Set the header names, which should be collected by the plugin. Header name must follow javax.servlet.http definition. Multiple names should be split by comma. ``(No header would be collected) |   plugin.feign.collect_request_body This config item controls that whether the Feign plugin should collect the http body of the request. false   plugin.feign.filter_length_limit When COLLECT_REQUEST_BODY is enabled, how many characters to keep and send to the OAP backend, use negative values to keep and send the complete body. 1024   plugin.feign.supported_content_types_prefix When COLLECT_REQUEST_BODY is enabled and content-type start with SUPPORTED_CONTENT_TYPES_PREFIX, collect the body of the request , multiple paths should be separated by , application/json,text/   plugin.influxdb.trace_influxql If true, trace all the influxql(query and write) in InfluxDB access, default is true. true   plugin.dubbo.collect_consumer_arguments Apache Dubbo consumer collect arguments in RPC call, use Object#toString to collect arguments. false   plugin.dubbo.consumer_arguments_length_threshold When plugin.dubbo.collect_consumer_arguments is true, Arguments of length from the front will to the OAP backend 256   plugin.dubbo.collect_provider_arguments Apache Dubbo provider collect arguments in RPC call, use Object#toString to collect arguments. false   plugin.dubbo.consumer_provider_length_threshold When plugin.dubbo.provider_consumer_arguments is true, Arguments of length from the front will to the OAP backend 256   plugin.kafka.bootstrap_servers A list of host/port pairs to use for establishing the initial connection to the Kafka cluster. localhost:9092   plugin.kafka.get_topic_timeout Timeout period of reading topics from the Kafka server, the unit is second. 10   plugin.kafka.consumer_config Kafka producer configuration.    plugin.kafka.producer_config Kafka producer configuration. Read producer configure to get more details. Check Kafka report doc for more details and examples.    plugin.kafka.topic_meter Specify which Kafka topic name for Meter System data to report to. skywalking_meters   plugin.kafka.topic_metrics Specify which Kafka topic name for JVM metrics data to report to. skywalking_metrics   plugin.kafka.topic_segment Specify which Kafka topic name for traces data to report to. skywalking_segments   plugin.kafka.topic_profilings Specify which Kafka topic name for Thread Profiling snapshot to report to. skywalking_profilings   plugin.kafka.topic_management Specify which Kafka topic name for the register or heartbeat data of Service Instance to report to. skywalking_managements   plugin.springannotation.classname_match_regex Match spring beans with regular expression for the class name. Multiple expressions could be separated by a comma. This only works when Spring annotation plugin has been activated. All the spring beans tagged with @Bean,@Service,@Dao, or @Repository.   plugin.toolkit.log.transmit_formatted Whether or not to transmit logged data as formatted or un-formatted. true   plugin.toolkit.log.grpc.reporter.server_host Specify which grpc server\u0026rsquo;s host for log data to report to. 127.0.0.1   plugin.toolkit.log.grpc.reporter.server_port Specify which grpc server\u0026rsquo;s port for log data to report to. 11800   plugin.toolkit.log.grpc.reporter.max_message_size Specify the maximum size of log data for grpc client to report to. 10485760   plugin.toolkit.log.grpc.reporter.upstream_timeout How long grpc client will timeout in sending data to upstream. Unit is second. 30 seconds   plugin.lettuce.trace_redis_parameters If set to true, the parameters of Redis commands would be collected by Lettuce agent. false   plugin.lettuce.redis_parameter_max_length If set to positive number and plugin.lettuce.trace_redis_parameters is set to true, Redis command parameters would be collected and truncated to this length. 128    Dynamic Configurations All configurations above are static, if you need to change some agent settings at runtime, please read CDS - Configuration Discovery Service document for more details.\nOptional Plugins Java agent plugins are all pluggable. Optional plugins could be provided in optional-plugins folder under agent or 3rd party repositories. For using these plugins, you need to put the target plugin jar file into /plugins.\nNow, we have the following known optional plugins.\n Plugin of tracing Spring annotation beans Plugin of tracing Oracle and Resin Filter traces through specified endpoint name patterns Plugin of Gson serialization lib in optional plugin folder. Plugin of Zookeeper 3.4.x in optional plugin folder. The reason of being optional plugin is, many business irrelevant traces are generated, which cause extra payload to agents and backends. At the same time, those traces may be just heartbeat(s). Customize enhance Trace methods based on description files, rather than write plugin or change source codes. Plugin of Spring Cloud Gateway 2.1.x in optional plugin folder. Please only active this plugin when you install agent in Spring Gateway. spring-cloud-gateway-2.x-plugin and spring-webflux-5.x-plugin are both required. Plugin of Spring Transaction in optional plugin folder. The reason of being optional plugin is, many local span are generated, which also spend more CPU, memory and network. Plugin of Kotlin coroutine provides the tracing across coroutines automatically. As it will add local spans to all across routines scenarios, Please assess the performance impact. Plugin of quartz-scheduler-2.x in the optional plugin folder. The reason for being an optional plugin is, many task scheduling systems are based on quartz-scheduler, this will cause duplicate tracing and link different sub-tasks as they share the same quartz level trigger, such as ElasticJob. Plugin of spring-webflux-5.x in the optional plugin folder. Please only activate this plugin when you use webflux alone as a web container. If you are using SpringMVC 5 or Spring Gateway, you don\u0026rsquo;t need this plugin.  Bootstrap class plugins All bootstrap plugins are optional, due to unexpected risk. Bootstrap plugins are provided in bootstrap-plugins folder. For using these plugins, you need to put the target plugin jar file into /plugins.\nNow, we have the following known bootstrap plugins.\n Plugin of JDK HttpURLConnection. Agent is compatible with JDK 1.6+ Plugin of JDK Callable and Runnable. Agent is compatible with JDK 1.6+  The Logic Endpoint In default, all the RPC server-side names as entry spans, such as RESTFul API path and gRPC service name, would be endpoints with metrics. At the same time, SkyWalking introduces the logic endpoint concept, which allows plugins and users to add new endpoints without adding new spans. The following logic endpoints are added automatically by plugins.\n GraphQL Query and Mutation are logic endpoints by using the names of them. Spring\u0026rsquo;s ScheduledMethodRunnable jobs are logic endpoints. The name format is SpringScheduled/${className}/${methodName}. Apache ShardingSphere ElasticJob\u0026rsquo;s jobs are logic endpoints. The name format is ElasticJob/${jobName}. XXLJob\u0026rsquo;s jobs are logic endpoints. The name formats include xxl-job/MethodJob/${className}.${methodName}, xxl-job/ScriptJob/${GlueType}/id/${jobId}, and xxl-job/SimpleJob/${className}. Quartz(optional plugin)\u0026rsquo;s jobs are logic endpoints. the name format is quartz-scheduler/${className}.  User could use the SkyWalking\u0026rsquo;s application toolkits to add the tag into the local span to label the span as a logic endpoint in the analysis stage. The tag is, key=x-le and value = {\u0026quot;logic-span\u0026quot;:true}.\nAdvanced Features  Set the settings through system properties for config file override. Read setting override. Use gRPC TLS to link backend. See open TLS Monitor a big cluster by different SkyWalking services. Use Namespace to isolate the context propagation. Set client token if backend open token authentication. Application Toolkit, are a collection of libraries, provided by SkyWalking APM. Using them, you have a bridge between your application and SkyWalking APM agent.  If you want your codes to interact with SkyWalking agent, including getting trace id, setting tags, propagating custom data etc.. Try SkyWalking manual APIs. If you require customized metrics, try SkyWalking Meter System Toolkit. If you want to print trace context(e.g. traceId) in your logs, or collect logs, choose the log frameworks, log4j, log4j2, logback If you want to continue traces across thread manually, use across thread solution APIs. If you want to forward MicroMeter/Spring Sleuth metrics to Meter System, use SkyWalking MicroMeter Register. If you want to use OpenTracing Java APIs, try SkyWalking OpenTracing compatible tracer. More details you could find at http://opentracing.io If you want to tolerate some exceptions, read tolerate custom exception doc.   If you want to specify the path of your agent.config file. Read set config file through system properties  Advanced Reporters The advanced report provides an alternative way to submit the agent collected data to the backend. All of them are in the optional-reporter-plugins folder, move the one you needed into the reporter-plugins folder for the activation. Notice, don\u0026rsquo;t try to activate multiple reporters, that could cause unexpected fatal errors.\n Use Kafka to transport the traces, JVM metrics, instance properties, and profiled snapshots to the backend. Read the How to enable Kafka Reporter for more details.  Plugin Development Guide SkyWalking java agent supports plugin to extend the supported list. Please follow our Plugin Development Guide.\nTest If you are interested in plugin compatible tests or agent performance, see the following reports.\n Plugin Test in every Pull Request Java Agent Performance Test  ","excerpt":"Setup java agent  Agent is available for JDK 8 - 14 in 7.x releases. JDK 1.6 - JDK 12 are supported …","ref":"/docs/main/v8.5.0/en/setup/service-agent/java-agent/readme/","title":"Setup java agent"},{"body":"Skywalking Agent List  activemq-5.x armeria-063-084 armeria-085 armeria-086 armeria-098 async-http-client-2.x avro-1.x brpc-java canal-1.x cassandra-java-driver-3.x dbcp-2.x dubbo ehcache-2.x elastic-job-2.x elastic-job-3.x elasticsearch-5.x elasticsearch-6.x feign-default-http-9.x feign-pathvar-9.x finagle graphql grpc-1.x gson-2.8.x h2-1.x hbase-1.x/2.x httpasyncclient-4.x httpclient-3.x httpclient-4.x hystrix-1.x influxdb-2.x jdk-http-plugin jdk-threading-plugin jedis-2.x jetty-client-9.0 jetty-client-9.x jetty-server-9.x kafka-0.11.x/1.x/2.x kotlin-coroutine lettuce-5.x light4j mariadb-2.x memcache-2.x mongodb-2.x mongodb-3.x mongodb-4.x motan-0.x mysql-5.x mysql-6.x mysql-8.x netty-socketio nutz-http-1.x nutz-mvc-annotation-1.x okhttp-3.x play-2.x postgresql-8.x pulsar quasar quartz-scheduler-2.x rabbitmq-5.x redisson-3.x resteasy-server-3.x rocketMQ-3.x rocketMQ-4.x servicecomb-0.x servicecomb-1.x sharding-jdbc-1.5.x sharding-sphere-3.x sharding-sphere-4.0.0 sharding-sphere-4.1.0 sharding-sphere-4.x sharding-sphere-4.x-rc3 sofarpc solrj-7.x spring-annotation spring-async-annotation-5.x spring-cloud-feign-1.x spring-cloud-feign-2.x spring-cloud-gateway-2.0.x spring-cloud-gateway-2.1.x spring-concurrent-util-4.x spring-core-patch spring-kafka-1.x spring-kafka-2.x spring-mvc-annotation spring-mvc-annotation-3.x spring-mvc-annotation-4.x spring-mvc-annotation-5.x spring-resttemplate-4.x spring-scheduled-annotation spring-tx spring-webflux-5.x spring-webflux-5.x-webclient spymemcached-2.x struts2-2.x thrift tomcat-7.x/8.x toolkit-counter toolkit-gauge toolkit-histogram toolkit-kafka toolkit-log4j toolkit-log4j2 toolkit-logback toolkit-opentracing toolkit-tag toolkit-trace toolkit-exception undertow-2.x-plugin vertx-core-3.x xxl-job-2.x zookeeper-3.4.x mssql-jtds-1.x mssql-jdbc apache-cxf-3.x  ","excerpt":"Skywalking Agent List  activemq-5.x armeria-063-084 armeria-085 armeria-086 armeria-098 …","ref":"/docs/main/v8.5.0/en/setup/service-agent/java-agent/plugin-list/","title":"Skywalking Agent List"},{"body":"SkyWalking Cross Process Correlation Headers Protocol  Version 1.0  The Cross Process Correlation Headers Protocol is used to transport custom data by leveraging the capability of Cross Process Propagation Headers Protocol.\nThis is an optional and additional protocol for language tracer implementation. All tracer implementation could consider to implement this. Cross Process Correlation Header key is sw8-correlation. The value is the encoded(key):encoded(value) list with elements splitted by , such as base64(string key):base64(string value),base64(string key2):base64(string value2).\nRecommendations of language APIs Recommended implementation in different language API.\n TraceContext#putCorrelation and TraceContext#getCorrelation are recommended to write and read the correlation context, with key/value string. The key should be added if it is absent. The later writes should override the previous value. The total number of all keys should be less than 3, and the length of each value should be less than 128 bytes. The context should be propagated as well when tracing context is propagated across threads and processes.  ","excerpt":"SkyWalking Cross Process Correlation Headers Protocol  Version 1.0  The Cross Process Correlation …","ref":"/docs/main/v8.5.0/en/protocols/skywalking-cross-process-correlation-headers-protocol-v1/","title":"SkyWalking Cross Process Correlation Headers Protocol"},{"body":"SkyWalking Cross Process Propagation Headers Protocol  Version 3.0  SkyWalking is more likely an APM system, rather than the common distributed tracing system. The Headers are much more complex than them in order to improving analysis performance of OAP. You can find many similar mechanism in other commercial APM systems. (Some are even much more complex than our\u0026rsquo;s)\nAbstract SkyWalking Cross Process Propagation Headers Protocol v3 is also named as sw8 protocol, which is for context propagation.\nStandard Header Item The standard header should be the minimal requirement for the context propagation.\n Header Name: sw8. Header Value: 8 fields split by -. The length of header value should be less than 2k(default).  Value format example, XXXXX-XXXXX-XXXX-XXXX\nValues Values include the following segments, all String type values are in BASE64 encoding.\n Required(s)   Sample. 0 or 1. 0 means context exists, but could(most likely will) ignore. 1 means this trace need to be sampled and send to backend. Trace Id. String(BASE64 encoded). Literal String and unique globally. Parent trace segment Id. String(BASE64 encoded). Literal String and unique globally. Parent span Id. Integer. Begin with 0. This span id points to the parent span in parent trace segment. Parent service. String(BASE64 encoded). The length should not be less or equal than 50 UTF-8 characters. Parent service instance. String(BASE64 encoded). The length should be less or equal than 50 UTF-8 characters. Parent endpoint. String(BASE64 encoded). Operation Name of the first entry span in the parent segment. The length should be less than 150 UTF-8 characters. Target address used at client side of this request. String(BASE64 encoded). The network address(not must be IP + port) used at client side to access this target service.   Sample values, 1-TRACEID-SEGMENTID-3-PARENT_SERVICE-PARENT_INSTANCE-PARENT_ENDPOINT-IPPORT  Extension Header Item Extension header item is designed for the advanced features. It provides the interaction capabilities between the agents deployed in upstream and downstream services.\n Header Name: sw8-x Header Value: Split by -. The fields are extendable.  Values The current value includes fields.\n Tracing Mode. empty, 0 or 1. empty or 0 is default. 1 represents all spans generated in this context should skip analysis, spanObject#skipAnalysis=true. This context should be propagated to upstream in the default, unless it is changed in the tracing process. The timestamp of sending at the client-side. This is used in async RPC such as MQ. Once it is set, the consumer side would calculate the latency between sending and receiving, and tag the latency in the span by using key transmission.latency automatically.  ","excerpt":"SkyWalking Cross Process Propagation Headers Protocol  Version 3.0  SkyWalking is more likely an APM …","ref":"/docs/main/v8.5.0/en/protocols/skywalking-cross-process-propagation-headers-protocol-v3/","title":"SkyWalking Cross Process Propagation Headers Protocol"},{"body":"Apache SkyWalking release guide This document guides every committer to release SkyWalking in Apache Way, and also help committers to check the release for vote.\nSetup your development environment Follow Apache maven deployment environment document to set gpg tool and encrypt passwords\nUse the following block as a template and place it in ~/.m2/settings.xml\n\u0026lt;settings\u0026gt; ... \u0026lt;servers\u0026gt; \u0026lt;!-- To publish a snapshot of some part of Maven --\u0026gt; \u0026lt;server\u0026gt; \u0026lt;id\u0026gt;apache.snapshots.https\u0026lt;/id\u0026gt; \u0026lt;username\u0026gt; \u0026lt;!-- YOUR APACHE LDAP USERNAME --\u0026gt; \u0026lt;/username\u0026gt; \u0026lt;password\u0026gt; \u0026lt;!-- YOUR APACHE LDAP PASSWORD (encrypted) --\u0026gt; \u0026lt;/password\u0026gt; \u0026lt;/server\u0026gt; \u0026lt;!-- To stage a release of some part of Maven --\u0026gt; \u0026lt;server\u0026gt; \u0026lt;id\u0026gt;apache.releases.https\u0026lt;/id\u0026gt; \u0026lt;username\u0026gt; \u0026lt;!-- YOUR APACHE LDAP USERNAME --\u0026gt; \u0026lt;/username\u0026gt; \u0026lt;password\u0026gt; \u0026lt;!-- YOUR APACHE LDAP PASSWORD (encrypted) --\u0026gt; \u0026lt;/password\u0026gt; \u0026lt;/server\u0026gt; ... \u0026lt;/servers\u0026gt; \u0026lt;/settings\u0026gt; Add your GPG public key  Add your GPG public key into SkyWalking GPG KEYS file, only if you are a committer, use your Apache id and password login this svn, and update file. Don\u0026rsquo;t override the existing file. Upload your GPG public key to public GPG site. Such as MIT\u0026rsquo;s site. This site should be in Apache maven staging repository check list.  Test your settings This step is only for test, if your env is set right, don\u0026rsquo;t need to check every time.\n./mvnw clean install -Pall (this will build artifacts, sources and sign) Prepare the release ./mvnw release:clean ./mvnw release:prepare -DautoVersionSubmodules=true -Pall  Set version number as x.y.z, and tag as vx.y.z (version tag must start with v, you will find the purpose in next step.)  You could do a GPG sign before doing release, if you need input the password to sign, and the maven don\u0026rsquo;t give the chance, but just failure. Run gpg --sign xxx to any file could remember the password for enough time to do release.\nStage the release ./mvnw release:perform -DskipTests -Pall  The release will automatically be inserted into a temporary staging repository for you.  Build and sign the source code package export RELEASE_VERSION=x.y.z (example: RELEASE_VERSION=5.0.0-alpha) cd tools/releasing bash create_source_release.sh This scripts should do following things\n Use v + RELEASE_VERSION as tag to clone the codes. Make git submodule init/update done. Exclude all unnecessary files in the target source tar, such as .git, .github, .gitmodules. See the script for the details. Do gpg and shasum 512.  The apache-skywalking-apm-x.y.z-src.tgz should be found in tools/releasing folder, with .asc, .sha512.\nFind and download distribution in Apache Nexus Staging repositories  Use ApacheId to login https://repository.apache.org/ Go to https://repository.apache.org/#stagingRepositories Search skywalking and find your staging repository Close the repository and wait for all checks pass. In this step, your GPG KEYS will be checked. See set PGP document, if you haven\u0026rsquo;t done it before. Go to {REPO_URL}/org/apache/skywalking/apache-skywalking-apm/x.y.z Download .tar.gz and .zip with .asc and .sha1  Upload to Apache svn  Use ApacheId to login https://dist.apache.org/repos/dist/dev/skywalking/ Create folder, named by release version and round, such as: x.y.z Upload Source code package to the folder with .asc, .sha512  Package name: apache-skywalking-x.y.z-src.tar.gz See Section \u0026ldquo;Build and sign the source code package\u0026rdquo; for more details   Upload distribution package to the folder with .asc, .sha512  Package name: apache-skywalking-bin-x.y.z.tar.gz, apache-skywalking-bin-x.y.z.zip See Section \u0026ldquo;Find and download distribution in Apache Nexus Staging repositories\u0026rdquo; for more details Create .sha512 package: shasum -a 512 file \u0026gt; file.sha512    Make the internal announcements Send an announcement mail in dev mail list.\nMail title: [ANNOUNCE] SkyWalking x.y.z test build available Mail content: The test build of x.y.z is available. We welcome any comments you may have, and will take all feedback into account if a quality vote is called for this build. Release notes: * https://github.com/apache/skywalking/blob/master/changes/changes-x.y.z.md Release Candidate: * https://dist.apache.org/repos/dist/dev/skywalking/xxxx * sha512 checksums - sha512xxxxyyyzzz apache-skywalking-apm-x.x.x-src.tgz - sha512xxxxyyyzzz apache-skywalking-apm-bin-x.x.x.tar.gz - sha512xxxxyyyzzz apache-skywalking-apm-bin-x.x.x.zip Maven 2 staging repository: * https://repository.apache.org/content/repositories/xxxx/org/apache/skywalking/ Release Tag : * (Git Tag) x.y.z Release CommitID : * https://github.com/apache/skywalking/tree/(Git Commit ID) * Git submodule * skywalking-ui: https://github.com/apache/skywalking-rocketbot-ui/tree/(Git Commit ID) * apm-protocol/apm-network/src/main/proto: https://github.com/apache/skywalking-data-collect-protocol/tree/(Git Commit ID) * oap-server/server-query-plugin/query-graphql-plugin/src/main/resources/query-protocol https://github.com/apache/skywalking-query-protocol/tree/(Git Commit ID) Keys to verify the Release Candidate : * https://dist.apache.org/repos/dist/release/skywalking/KEYS Guide to build the release from source : * https://github.com/apache/skywalking/blob/x.y.z/docs/en/guides/How-to-build.md A vote regarding the quality of this test build will be initiated within the next couple of days. Wait at least 48 hours for test responses Any PMC, committer or contributor can test features for releasing, and feedback. Based on that, PMC will decide whether start a vote.\nCall a vote in dev Call a vote in dev@skywalking.apache.org\nMail title: [VOTE] Release Apache SkyWalking version x.y.z Mail content: Hi All, This is a call for vote to release Apache SkyWalking version x.y.z. Release notes: * https://github.com/apache/skywalking/blob/master/changes/changes-x.y.z.md Release Candidate: * https://dist.apache.org/repos/dist/dev/skywalking/xxxx * sha512 checksums - sha512xxxxyyyzzz apache-skywalking-apm-x.x.x-src.tgz - sha512xxxxyyyzzz apache-skywalking-apm-bin-x.x.x.tar.gz - sha512xxxxyyyzzz apache-skywalking-apm-bin-x.x.x.zip Maven 2 staging repository: * https://repository.apache.org/content/repositories/xxxx/org/apache/skywalking/ Release Tag : * (Git Tag) x.y.z Release CommitID : * https://github.com/apache/skywalking/tree/(Git Commit ID) * Git submodule * skywalking-ui: https://github.com/apache/skywalking-rocketbot-ui/tree/(Git Commit ID) * apm-protocol/apm-network/src/main/proto: https://github.com/apache/skywalking-data-collect-protocol/tree/(Git Commit ID) * oap-server/server-query-plugin/query-graphql-plugin/src/main/resources/query-protocol https://github.com/apache/skywalking-query-protocol/tree/(Git Commit ID) Keys to verify the Release Candidate : * https://dist.apache.org/repos/dist/release/skywalking/KEYS Guide to build the release from source : * https://github.com/apache/skywalking/blob/x.y.z/docs/en/guides/How-to-build.md Voting will start now (xxxx date) and will remain open for at least 72 hours, Request all PMC members to give their vote. [ ] +1 Release this package. [ ] +0 No opinion. [ ] -1 Do not release this package because.... Vote Check All PMC members and committers should check these before vote +1.\n Features test. All artifacts in staging repository are published with .asc, .md5, *sha1 files Source code and distribution package (apache-skywalking-x.y.z-src.tar.gz, apache-skywalking-bin-x.y.z.tar.gz, apache-skywalking-bin-x.y.z.zip) are in https://dist.apache.org/repos/dist/dev/skywalking/x.y.z with .asc, .sha512 LICENSE and NOTICE are in Source code and distribution package. Check shasum -c apache-skywalking-apm-x.y.z-src.tgz.sha512 Check gpg --verify apache-skywalking-apm-x.y.z-src.tgz.asc apache-skywalking-apm-x.y.z-src.tgz Build distribution from source code package (apache-skywalking-x.y.z-src.tar.gz) by following this doc. Check Apache License Header. Run docker run --rm -v $(pwd):/github/workspace apache/skywalking-eyes header check. (No binary in source codes)  Vote result should follow these.\n PMC vote is +1 binding, all others is +1 no binding. In 72 hours, you get at least 3 (+1 binding), and have more +1 than -1. Vote pass.  Publish release  Move source codes tar balls and distributions to https://dist.apache.org/repos/dist/release/skywalking/.  \u0026gt; export SVN_EDITOR=vim \u0026gt; svn mv https://dist.apache.org/repos/dist/dev/skywalking/x.y.z https://dist.apache.org/repos/dist/release/skywalking .... enter your apache password .... Do release in nexus staging repo. Public download source and distribution tar/zip locate in http://www.apache.org/dyn/closer.cgi/skywalking/x.y.z/xxx. We only publish Apache mirror path as release info. Public asc and sha512 locate in https://www.apache.org/dist/skywalking/x.y.z/xxx Public KEYS pointing to https://www.apache.org/dist/skywalking/KEYS Update website download page. http://skywalking.apache.org/downloads/ . Include new download source, distribution, sha512, asc and document links. Links could be found by following above rules(3)-(6). Add a release event on website homepage and event page. Announce the public release with changelog or key features. Send ANNOUNCE email to dev@skywalking.apache.org, announce@apache.org, the sender should use Apache email account.  Mail title: [ANNOUNCE] Apache SkyWalking x.y.z released Mail content: Hi all, Apache SkyWalking Team is glad to announce the first release of Apache SkyWalking x.y.z. SkyWalking: APM (application performance monitor) tool for distributed systems, especially designed for microservices, cloud native and container-based (Docker, Kubernetes, Mesos) architectures. This release contains a number of new features, bug fixes and improvements compared to version a.b.c(last release). The notable changes since x.y.z include: (Highlight key changes) 1. ... 2. ... 3. ... Please refer to the change log for the complete list of changes: https://github.com/apache/skywalking/blob/master/changes/changes-x.y.z.md Apache SkyWalking website: http://skywalking.apache.org/ Downloads: http://skywalking.apache.org/downloads/ Twitter: https://twitter.com/ASFSkyWalking SkyWalking Resources: - GitHub: https://github.com/apache/skywalking - Issue: https://github.com/apache/skywalking/issues - Mailing list: dev@skywalkiing.apache.org - Apache SkyWalking Team Clean old release Once the latest release published, we should clean the old releases from mirror system.\n Update the download links(source, dist, asc, sha512) on the website to archive repo, https://archive.apache.org/dist/skywalking. After (1) deployed, remove previous releases from https://dist.apache.org/repos/dist/release/skywalking/.  ","excerpt":"Apache SkyWalking release guide This document guides every committer to release SkyWalking in Apache …","ref":"/docs/main/v8.5.0/en/guides/how-to-release/","title":"SkyWalking release guide"},{"body":"Skywalking with Kotlin coroutine This Plugin provides an auto instrument support plugin for Kotlin coroutine based on context snapshot.\nDescription SkyWalking provide tracing context propagation inside thread. In order to support Kotlin Coroutine, we provide this additional plugin.\nImplementation principle As we know, Kotlin coroutine switches the execution thread by CoroutineDispatcher.\n Create a snapshot of the current context before dispatch the continuation. Then create a coroutine span after thread switched, mark the span continued with the snapshot. Every new span which created in the new thread will be a child of this coroutine span. So we can link those span together in a tracing. After the original runnable executed, we need to stop the coroutine span for cleaning thread state.  Some screenshots Run without the plugin We run a Kotlin coroutine based gRPC server without this coroutine plugin.\nYou can find, the one call (client -\u0026gt; server1 -\u0026gt; server2) has been split two tracing paths.\n Server1 without exit span and server2 tracing path.  Server2 tracing path.   Run with the plugin Without changing codes manually, just install the plugin. We can find the spans be connected together. We can get all info of one client call.\n","excerpt":"Skywalking with Kotlin coroutine This Plugin provides an auto instrument support plugin for Kotlin …","ref":"/docs/main/v8.5.0/en/setup/service-agent/java-agent/agent-optional-plugins/kotlin-coroutine-plugin/","title":"Skywalking with Kotlin coroutine"},{"body":"Slow Database Statement Slow Database statements are significant important to find out the bottleneck of the system, which relied on Database.\nSlow DB statements are based on sampling, right now, the core samples top 50 slowest in every 10 minutes. But duration of those statements must be slower than threshold.\nThe setting format is following, unit is millisecond.\n database-type:thresholdValue,database-type2:thresholdValue2\n Default setting is default:200,mongodb:100. Reserved DB type is default, which be as default threshold for all database types, except set explicitly.\nNotice, the threshold should not be too small, like 1ms. Functionally, it works, but would cost OAP performance issue, if your system statement access time are mostly more than 1ms.\n","excerpt":"Slow Database Statement Slow Database statements are significant important to find out the …","ref":"/docs/main/v8.5.0/en/setup/backend/slow-db-statement/","title":"Slow Database Statement"},{"body":"Source and Scope extension for new metrics From OAL scope introduction, you should already have understood what the scope is. At here, as you want to do more extension, you need understand deeper, which is the Source.\nSource and Scope are binding concepts. Scope declare the id(int) and name, Source declare the attributes. Please follow these steps to create a new Source and Scope.\n In the OAP core module, it provide SourceReceiver internal service.  public interface SourceReceiver extends Service { void receive(Source source); } All analysis data must be a org.apache.skywalking.oap.server.core.source.Source sub class, tagged by @SourceType annotation, and in org.apache.skywalking package. Then it could be supported by OAL script and OAP core.  Such as existed source, Service.\n@ScopeDeclaration(id = SERVICE_INSTANCE, name = \u0026#34;ServiceInstance\u0026#34;, catalog = SERVICE_INSTANCE_CATALOG_NAME) @ScopeDefaultColumn.VirtualColumnDefinition(fieldName = \u0026#34;entityId\u0026#34;, columnName = \u0026#34;entity_id\u0026#34;, isID = true, type = String.class) public class ServiceInstance extends Source { @Override public int scope() { return DefaultScopeDefine.SERVICE_INSTANCE; } @Override public String getEntityId() { return String.valueOf(id); } @Getter @Setter private int id; @Getter @Setter @ScopeDefaultColumn.DefinedByField(columnName = \u0026#34;service_id\u0026#34;) private int serviceId; @Getter @Setter private String name; @Getter @Setter private String serviceName; @Getter @Setter private String endpointName; @Getter @Setter private int latency; @Getter @Setter private boolean status; @Getter @Setter private int responseCode; @Getter @Setter private RequestType type; }  The scope() method in Source, returns an ID, which is not a random number. This ID need to be declared through @ScopeDeclaration annotation too. The ID in @ScopeDeclaration and ID in scope() method should be same for this Source.\n  The String getEntityId() method in Source, requests the return value representing unique entity which the scope related. Such as, in this Service scope, the id is service id, representing a particular service, like Order service. This value is used in OAL group mechanism.\n  @ScopeDefaultColumn.VirtualColumnDefinition and @ScopeDefaultColumn.DefinedByField are required, all declared fields(virtual/byField) are going to be pushed into persistent entity, mapping to such as ElasticSearch index and Database table column. Such as, include entity id mostly, and service id for endpoint and service instance level scope. Take a reference to all existing scopes. All these fields are detected by OAL Runtime, and required in query stage.\n  Add scope name as keyword to oal grammar definition file, OALLexer.g4, which is at antlr4 folder of generate-tool-grammar module.\n  Add scope name keyword as source in parser definition file, OALParser.g4, which is at same fold of OALLexer.g4.\n   After you done all of these, you could build a receiver, which do\n Get the original data of the metrics, Build the source, send into SourceReceiver. Write your whole OAL scripts. Repackage the project.  ","excerpt":"Source and Scope extension for new metrics From OAL scope introduction, you should already have …","ref":"/docs/main/v8.5.0/en/guides/source-extension/","title":"Source and Scope extension for new metrics"},{"body":"Spring annotation plugin This plugin allows to trace all methods of beans in Spring context, which are annotated with @Bean, @Service, @Component and @Repository.\n Why does this plugin optional?  Tracing all methods in Spring context all creates a lot of spans, which also spend more CPU, memory and network. Of course you want to have spans as many as possible, but please make sure your system payload can support these.\n","excerpt":"Spring annotation plugin This plugin allows to trace all methods of beans in Spring context, which …","ref":"/docs/main/v8.5.0/en/setup/service-agent/java-agent/agent-optional-plugins/spring-annotation-plugin/","title":"Spring annotation plugin"},{"body":"Spring sleuth setup Spring Sleuth provides Spring Boot auto-configuration for distributed tracing. Skywalking integrates it\u0026rsquo;s micrometer part, and it can send metrics to the Skywalking Meter System.\nSet up agent  Add the Micrometer and Skywalking meter registry dependency into project pom.xml file. Also you could found more detail at Toolkit micrometer.  \u0026lt;dependency\u0026gt; \u0026lt;groupId\u0026gt;org.springframework.boot\u0026lt;/groupId\u0026gt; \u0026lt;artifactId\u0026gt;spring-boot-starter-actuator\u0026lt;/artifactId\u0026gt; \u0026lt;/dependency\u0026gt; \u0026lt;dependency\u0026gt; \u0026lt;groupId\u0026gt;org.apache.skywalking\u0026lt;/groupId\u0026gt; \u0026lt;artifactId\u0026gt;apm-toolkit-micrometer-registry\u0026lt;/artifactId\u0026gt; \u0026lt;version\u0026gt;${skywalking.version}\u0026lt;/version\u0026gt; \u0026lt;/dependency\u0026gt; Create the Skywalking meter resgitry into spring bean management.  @Bean SkywalkingMeterRegistry skywalkingMeterRegistry() { // Add rate configs If you need, otherwise using none args construct  SkywalkingConfig config = new SkywalkingConfig(Arrays.asList(\u0026#34;\u0026#34;)); return new SkywalkingMeterRegistry(config); } Set up backend receiver  Make sure enable meter receiver in the applicaiton.yml.  receiver-meter: selector: ${SW_RECEIVER_METER:default} default: Configure the meter config file, It already has the spring sleuth meter config. If you also has some customized meter at the agent side, please read meter document to configure meter.  Add UI dashboard   Open the dashboard view, click edit button to edit the templates.\n  Create a new template. Template type: Standard -\u0026gt; Template Configuration: Spring -\u0026gt; Input the Template Name.\n  Click view button, Finally get the spring sleuth dashboard.\n  Supported meter Supported 3 types information: Application, System, JVM.\n Application: HTTP request count and duration, JDBC max/idle/active connection count, Tomcat session active/reject count. System: CPU system/process usage, OS System load, OS Process file count. JVM: GC pause count and duration, Memory max/used/committed size, Thread peak/live/daemon count, Classes loaded/unloaded count.  ","excerpt":"Spring sleuth setup Spring Sleuth provides Spring Boot auto-configuration for distributed tracing. …","ref":"/docs/main/v8.5.0/en/setup/backend/spring-sleuth-setup/","title":"Spring sleuth setup"},{"body":"Start up mode In different deployment tool, such as k8s, you may need different startup mode. We provide another two optional startup modes.\nDefault mode Default mode. Do initialization works if necessary, start listen and provide service.\nRun /bin/oapService.sh(.bat) to start in this mode. Also when use startup.sh(.bat) to start.\nInit mode In this mode, oap server starts up to do initialization works, then exit. You could use this mode to init your storage, such as ElasticSearch indexes, MySQL and TiDB tables, and init data.\nRun /bin/oapServiceInit.sh(.bat) to start in this mode.\nNo-init mode In this mode, oap server starts up without initialization works, but it waits for ElasticSearch indexes, MySQL and TiDB tables existed, start listen and provide service. Meaning, this oap server expect another oap server to do the initialization.\nRun /bin/oapServiceNoInit.sh(.bat) to start in this mode.\n","excerpt":"Start up mode In different deployment tool, such as k8s, you may need different startup mode. We …","ref":"/docs/main/v8.5.0/en/setup/backend/backend-start-up-mode/","title":"Start up mode"},{"body":"Support custom enhance Here is an optional plugin apm-customize-enhance-plugin\nIntroduce SkyWalking has provided Java agent plugin development guide to help developers to build new plugin.\nThis plugin is not designed for replacement but for user convenience. The behaviour is very similar with @Trace toolkit, but without code change requirement, and more powerful, such as provide tag and log.\nHow to configure Implementing enhancements to custom classes requires two steps.\n Active the plugin, move the optional-plugins/apm-customize-enhance-plugin.jar to plugin/apm-customize-enhance-plugin.jar. Set plugin.customize.enhance_file in agent.config, which targets to rule file, such as /absolute/path/to/customize_enhance.xml. Set enhancement rules in customize_enhance.xml. \u0026lt;?xml version=\u0026#34;1.0\u0026#34; encoding=\u0026#34;UTF-8\u0026#34;?\u0026gt; \u0026lt;enhanced\u0026gt; \u0026lt;class class_name=\u0026#34;test.apache.skywalking.testcase.customize.service.TestService1\u0026#34;\u0026gt; \u0026lt;method method=\u0026#34;staticMethod()\u0026#34; operation_name=\u0026#34;/is_static_method\u0026#34; static=\u0026#34;true\u0026#34;/\u0026gt; \u0026lt;method method=\u0026#34;staticMethod(java.lang.String,int.class,java.util.Map,java.util.List,[Ljava.lang.Object;)\u0026#34; operation_name=\u0026#34;/is_static_method_args\u0026#34; static=\u0026#34;true\u0026#34;\u0026gt; \u0026lt;operation_name_suffix\u0026gt;arg[0]\u0026lt;/operation_name_suffix\u0026gt; \u0026lt;operation_name_suffix\u0026gt;arg[1]\u0026lt;/operation_name_suffix\u0026gt; \u0026lt;operation_name_suffix\u0026gt;arg[3].[0]\u0026lt;/operation_name_suffix\u0026gt; \u0026lt;tag key=\u0026#34;tag_1\u0026#34;\u0026gt;arg[2].[\u0026#39;k1\u0026#39;]\u0026lt;/tag\u0026gt; \u0026lt;tag key=\u0026#34;tag_2\u0026#34;\u0026gt;arg[4].[1]\u0026lt;/tag\u0026gt; \u0026lt;log key=\u0026#34;log_1\u0026#34;\u0026gt;arg[4].[2]\u0026lt;/log\u0026gt; \u0026lt;/method\u0026gt; \u0026lt;method method=\u0026#34;method()\u0026#34; static=\u0026#34;false\u0026#34;/\u0026gt; \u0026lt;method method=\u0026#34;method(java.lang.String,int.class)\u0026#34; operation_name=\u0026#34;/method_2\u0026#34; static=\u0026#34;false\u0026#34;\u0026gt; \u0026lt;operation_name_suffix\u0026gt;arg[0]\u0026lt;/operation_name_suffix\u0026gt; \u0026lt;tag key=\u0026#34;tag_1\u0026#34;\u0026gt;arg[0]\u0026lt;/tag\u0026gt; \u0026lt;log key=\u0026#34;log_1\u0026#34;\u0026gt;arg[1]\u0026lt;/log\u0026gt; \u0026lt;/method\u0026gt; \u0026lt;method method=\u0026#34;method(test.apache.skywalking.testcase.customize.model.Model0,java.lang.String,int.class)\u0026#34; operation_name=\u0026#34;/method_3\u0026#34; static=\u0026#34;false\u0026#34;\u0026gt; \u0026lt;operation_name_suffix\u0026gt;arg[0].id\u0026lt;/operation_name_suffix\u0026gt; \u0026lt;operation_name_suffix\u0026gt;arg[0].model1.name\u0026lt;/operation_name_suffix\u0026gt; \u0026lt;operation_name_suffix\u0026gt;arg[0].model1.getId()\u0026lt;/operation_name_suffix\u0026gt; \u0026lt;tag key=\u0026#34;tag_os\u0026#34;\u0026gt;arg[0].os.[1]\u0026lt;/tag\u0026gt; \u0026lt;log key=\u0026#34;log_map\u0026#34;\u0026gt;arg[0].getM().[\u0026#39;k1\u0026#39;]\u0026lt;/log\u0026gt; \u0026lt;/method\u0026gt; \u0026lt;/class\u0026gt; \u0026lt;class class_name=\u0026#34;test.apache.skywalking.testcase.customize.service.TestService2\u0026#34;\u0026gt; \u0026lt;method method=\u0026#34;staticMethod(java.lang.String,int.class)\u0026#34; operation_name=\u0026#34;/is_2_static_method\u0026#34; static=\u0026#34;true\u0026#34;\u0026gt; \u0026lt;tag key=\u0026#34;tag_2_1\u0026#34;\u0026gt;arg[0]\u0026lt;/tag\u0026gt; \u0026lt;log key=\u0026#34;log_1_1\u0026#34;\u0026gt;arg[1]\u0026lt;/log\u0026gt; \u0026lt;/method\u0026gt; \u0026lt;method method=\u0026#34;method([Ljava.lang.Object;)\u0026#34; operation_name=\u0026#34;/method_4\u0026#34; static=\u0026#34;false\u0026#34;\u0026gt; \u0026lt;tag key=\u0026#34;tag_4_1\u0026#34;\u0026gt;arg[0].[0]\u0026lt;/tag\u0026gt; \u0026lt;/method\u0026gt; \u0026lt;method method=\u0026#34;method(java.util.List,int.class)\u0026#34; operation_name=\u0026#34;/method_5\u0026#34; static=\u0026#34;false\u0026#34;\u0026gt; \u0026lt;tag key=\u0026#34;tag_5_1\u0026#34;\u0026gt;arg[0].[0]\u0026lt;/tag\u0026gt; \u0026lt;log key=\u0026#34;log_5_1\u0026#34;\u0026gt;arg[1]\u0026lt;/log\u0026gt; \u0026lt;/method\u0026gt; \u0026lt;/class\u0026gt; \u0026lt;/enhanced\u0026gt; ``\n   Explanation of the configuration in the file    configuration explanation     class_name The enhanced class   method The interceptor method of the class   operation_name If fill it out, will use it instead of the default operation_name.   operation_name_suffix What it means adding dynamic data after the operation_name.   static Is this method static.   tag Will add a tag in local span. The value of key needs to be represented on the XML node.   log Will add a log in local span. The value of key needs to be represented on the XML node.   arg[x] What it means is to get the input arguments. such as arg[0] is means get first arguments.   .[x] When the parsing object is Array or List, you can use it to get the object at the specified index.   .[\u0026lsquo;key\u0026rsquo;] When the parsing object is Map, you can get the map \u0026lsquo;key\u0026rsquo; through it.      ","excerpt":"Support custom enhance Here is an optional plugin apm-customize-enhance-plugin\nIntroduce SkyWalking …","ref":"/docs/main/v8.5.0/en/setup/service-agent/java-agent/customize-enhance-trace/","title":"Support custom enhance"},{"body":"Support custom trace ignore Here is an optional plugin apm-trace-ignore-plugin\nNotice: Sampling still works when the trace ignores plug-in activation.\nIntroduce  The purpose of this plugin is to filter endpoint which are expected to be ignored by the tracing system. You can setup multiple URL path patterns, The endpoints match these patterns wouldn\u0026rsquo;t be traced. The current matching rules follow Ant Path match style , like /path/*, /path/**, /path/?. Copy apm-trace-ignore-plugin-x.jar to agent/plugins, restarting the agent can effect the plugin.  How to configure There are two ways to configure ignore patterns. Settings through system env has higher priority.\n Set through the system environment variable,you need to add skywalking.trace.ignore_path to the system variables, the value is the path that you need to ignore, multiple paths should be separated by , Copy/agent/optional-plugins/apm-trace-ignore-plugin/apm-trace-ignore-plugin.config to /agent/config/ dir, and add rules to filter traces  trace.ignore_path=/your/path/1/**,/your/path/2/** ","excerpt":"Support custom trace ignore Here is an optional plugin apm-trace-ignore-plugin\nNotice: Sampling …","ref":"/docs/main/v8.5.0/en/setup/service-agent/java-agent/agent-optional-plugins/trace-ignore-plugin/","title":"Support custom trace ignore"},{"body":"Support gRPC SSL transportation for OAP server For OAP communication we are currently using gRPC, a multi-platform RPC framework that uses protocol buffers for message serialization. The nice part about gRPC is that it promotes the use of SSL/TLS to authenticate and encrypt exchanges. Now OAP supports to enable SSL transportation for gRPC receivers.\nYou can follow below steps to enable this feature\nCreating SSL/TLS Certificates It seems like step one is to generate certificates and key files for encrypting communication. I thought this would be fairly straightforward using openssl from the command line.\nUse this script if you are not familiar with how to generate key files.\nWe need below files:\n server.pem a private RSA key to sign and authenticate the public key. It\u0026rsquo;s either a PKCS#8(PEM) or PKCS#1(DER). server.crt self-signed X.509 public keys for distribution. ca.crt a certificate authority public key for a client to validate the server\u0026rsquo;s certificate.  Config OAP server You can enable gRPC SSL by add following lines to application.yml/core/default.\ngRPCSslEnabled: true gRPCSslKeyPath: /path/to/server.pem gRPCSslCertChainPath: /path/to/server.crt gRPCSslTrustedCAPath: /path/to/ca.crt gRPCSslKeyPath and gRPCSslCertChainPath are loaded by OAP server to encrypt the communication. gRPCSslTrustedCAPath helps gRPC client to verify server certificates in cluster mode.\nWhen new files are in place, they can be load dynamically instead of restarting OAP instance.\nIf you enable sharding-server to ingest data from external, add following lines to application.yml/receiver-sharing-server/default:\ngRPCSslEnabled: true gRPCSslKeyPath: /path/to/server.pem gRPCSslCertChainPath: /path/to/server.crt Because sharding-server only receives data from external, so it doesn\u0026rsquo;t need CA at all.\nIf you port to java agent, refer to TLS.md to config java agent to enable TLS.\n","excerpt":"Support gRPC SSL transportation for OAP server For OAP communication we are currently using gRPC, a …","ref":"/docs/main/v8.5.0/en/setup/backend/grpc-ssl/","title":"Support gRPC SSL transportation for OAP server"},{"body":"Support Transport Layer Security (TLS) Transport Layer Security (TLS) is a very common security way when transport data through Internet. In some use cases, end users report the background:\n Target(under monitoring) applications are in a region, which also named VPC, at the same time, the SkyWalking backend is in another region (VPC).\nBecause of that, security requirement is very obvious.\n Authentication Mode Only support no mutual auth.\n Use this script if you are not familiar with how to generate key files. Find ca.crt, and use it at client side Find server.crt ,server.pem and ca.crt. Use them at server side. Please refer to gRPC SSL for more details.  Open and config TLS Agent config   Place ca.crt into /ca folder in agent package. Notice, /ca is not created in distribution, please create it by yourself.\n  Agent open TLS automatically after the /ca/ca.crt file detected.\n  TLS with no CA mode could be activated by this setting.\n  agent.force_tls=${SW_AGENT_FORCE_TLS:false} ","excerpt":"Support Transport Layer Security (TLS) Transport Layer Security (TLS) is a very common security way …","ref":"/docs/main/v8.5.0/en/setup/service-agent/java-agent/tls/","title":"Support Transport Layer Security (TLS)"},{"body":"Telemetry for backend By default, the telemetry is disabled by setting selector to none, like this\ntelemetry: selector: ${SW_TELEMETRY:none} none: prometheus: host: ${SW_TELEMETRY_PROMETHEUS_HOST:0.0.0.0} port: ${SW_TELEMETRY_PROMETHEUS_PORT:1234} sslEnabled: ${SW_TELEMETRY_PROMETHEUS_SSL_ENABLED:false} sslKeyPath: ${SW_TELEMETRY_PROMETHEUS_SSL_KEY_PATH:\u0026#34;\u0026#34;} sslCertChainPath: ${SW_TELEMETRY_PROMETHEUS_SSL_CERT_CHAIN_PATH:\u0026#34;\u0026#34;} but you can set one of prometheus to enable them, for more information, refer to the details below.\nPrometheus Prometheus is supported as telemetry implementor. By using this, prometheus collects metrics from SkyWalking backend.\nSet prometheus to provider. The endpoint open at http://0.0.0.0:1234/ and http://0.0.0.0:1234/metrics.\ntelemetry: selector: ${SW_TELEMETRY:prometheus} prometheus: Set host and port if needed.\ntelemetry: selector: ${SW_TELEMETRY:prometheus} prometheus: host: 127.0.0.1 port: 1543 Set SSL relevant settings to expose a secure endpoint. Notice private key file and cert chain file could be uploaded once changes are applied to them.\ntelemetry: selector: ${SW_TELEMETRY:prometheus} prometheus: host: 127.0.0.1 port: 1543 sslEnabled: true sslKeyPath: /etc/ssl/key.pem sslCertChainPath: /etc/ssl/cert-chain.pem Grafana Visualization Provide the grafana dashboard settings. Check SkyWalking OAP Cluster Monitor Dashboard config and SkyWalking OAP Instance Monitor Dashboard config.\nSelf Observability SkyWalking supports to collect telemetry data into OAP backend directly. Users could check them out through UI or GraphQL API then.\nAdding following configuration to enable self-observability related modules.\n Setting up prometheus telemetry.  telemetry: selector: ${SW_TELEMETRY:prometheus} prometheus: host: 127.0.0.1 port: 1543 Setting up prometheus fetcher  prometheus-fetcher: selector: ${SW_PROMETHEUS_FETCHER:default} default: enabledRules: ${SW_PROMETHEUS_FETCHER_ENABLED_RULES:\u0026#34;self\u0026#34;} Make sure config/fetcher-prom-rules/self.yaml exists.  Once you deploy an oap-server cluster, the target host should be replaced with a dedicated IP or hostname. For instances, there are three oap server in your cluster, their host is service1, service2 and service3 respectively. You should update each self.yaml to twist target host.\nservice1:\nfetcherInterval: PT15S fetcherTimeout: PT10S metricsPath: /metrics staticConfig: # targets will be labeled as \u0026#34;instance\u0026#34; targets: - service1:1234 labels: service: oap-server ... service2:\nfetcherInterval: PT15S fetcherTimeout: PT10S metricsPath: /metrics staticConfig: # targets will be labeled as \u0026#34;instance\u0026#34; targets: - service2:1234 labels: service: oap-server ... service3:\nfetcherInterval: PT15S fetcherTimeout: PT10S metricsPath: /metrics staticConfig: # targets will be labeled as \u0026#34;instance\u0026#34; targets: - service3:1234 labels: service: oap-server ... ","excerpt":"Telemetry for backend By default, the telemetry is disabled by setting selector to none, like this …","ref":"/docs/main/v8.5.0/en/setup/backend/backend-telemetry/","title":"Telemetry for backend"},{"body":"Thread dump merging mechanism The performance profile is an enhancement feature in the APM system. We are using the thread dump to estimate the method execution time, rather than adding many local spans. In this way, the resource cost would be much less than using distributed tracing to locate slow method. This feature is suitable in the production environment. This document introduces how thread dumps are merged into the final report as a stack tree(s).\nThread analyst Read data and transform Read data from the database and convert it to a data structure in gRPC.\nst=\u0026gt;start: Start e=\u0026gt;end: End op1=\u0026gt;operation: Load data using paging op2=\u0026gt;operation: Transform data using parallel st(right)-\u0026gt;op1(right)-\u0026gt;op2 op2(right)-\u0026gt;e Copy code and paste it into this link to generate flow chart.\n Use the stream to read data by page (50 records per page). Convert data into gRPC data structures in the form of parallel streams. Merge into a list of data.  Data analyze Use the group by and collector modes in the Java parallel stream to group according to the first stack element in the database records, and use the collector to perform data aggregation. Generate a multi-root tree.\nst=\u0026gt;start: Start e=\u0026gt;end: End op1=\u0026gt;operation: Group by first stack element sup=\u0026gt;operation: Generate empty stack tree acc=\u0026gt;operation: Accumulator data to stack tree com=\u0026gt;operation: Combine stack trees fin=\u0026gt;operation: Calculate durations and build result st(right)-\u0026gt;op1-\u0026gt;sup(right)-\u0026gt;acc acc(right)-\u0026gt;com(right)-\u0026gt;fin-\u0026gt;e Copy code and paste it into this link to generate flow chart.\n Group by first stack element: Use the first level element in each stack to group, ensuring that the stacks have the same root node. Generate empty stack tree: Generate multiple top-level empty trees for preparation of the following steps, The reason for generating multiple top-level trees is that original data can be add in parallel without generating locks. Accumulator data to stack tree: Add every thread dump into the generated trees.  Iterate through each element in the thread dump to find if there is any child element with the same code signature and same stack depth in the parent element. If not, then add this element. Keep the dump sequences and timestamps in each nodes from the source.   Combine stack trees: Combine all trees structures into one by using the rules as same as Accumulator.  Use LDR to traversal tree node. Use the Stack data structure to avoid recursive calls, each stack element represents the node that needs to be merged. The task of merging two nodes is to merge the list of children nodes. If they have the same code signature and same parents, save the dump sequences and timestamps in this node. Otherwise, the node needs to be added into the target node as a new child.   Calculate durations and build result: Calculate relevant statistics and generate response.  Use the same traversal node logic as in the Combine stack trees step. Convert to a GraphQL data structure, and put all nodes into a list for subsequent duration calculations. Calculate each node\u0026rsquo;s duration in parallel. For each node, sort the sequences, if there are two continuous sequences, the duration should add the duration of these two seq\u0026rsquo;s timestamp. Calculate each node execution in parallel. For each node, the duration of the current node should minus the time consumed by all children.    Profile data debug Please follow the exporter tool to package profile data. Unzip the profile data and using analyzer main function to run it.\n","excerpt":"Thread dump merging mechanism The performance profile is an enhancement feature in the APM system. …","ref":"/docs/main/v8.5.0/en/guides/backend-profile/","title":"Thread dump merging mechanism"},{"body":"Token Authentication Supported version 7.0.0+\nWhy need token authentication after we have TLS? TLS is about transport security, which makes sure the network can be trusted. The token authentication is about monitoring application data can be trusted.\nToken In current version, Token is considered as a simple string.\nSet Token  Set token in agent.config file  # Authentication active is based on backend setting, see application.yml for more details. agent.authentication = ${SW_AGENT_AUTHENTICATION:xxxx} Set token in application.yml file  ······ receiver-sharing-server: default: authentication: ${SW_AUTHENTICATION:\u0026#34;\u0026#34;} ······ Authentication fails The Skywalking OAP verifies every request from agent, only allows requests whose token matches the one configured in application.yml.\nIf the token is not right, you will see the following log in agent\norg.apache.skywalking.apm.dependencies.io.grpc.StatusRuntimeException: PERMISSION_DENIED FAQ Can I use token authentication instead of TLS? No, you shouldn\u0026rsquo;t. In tech way, you can of course, but token and TLS are used for untrusted network env. In that circumstance, TLS has higher priority than this. Token can be trusted only under TLS protection.Token can be stolen easily if you send it through a non-TLS network.\nDo you support other authentication mechanisms? Such as ak/sk? For now, no. But we appreciate someone contributes this feature.\n","excerpt":"Token Authentication Supported version 7.0.0+\nWhy need token authentication after we have TLS? TLS …","ref":"/docs/main/v8.5.0/en/setup/backend/backend-token-auth/","title":"Token Authentication"},{"body":"Token Authentication Token In current version, Token is considered as a simple string.\nSet Token Set token in agent.config file\n# Authentication active is based on backend setting, see application.yml for more details. agent.authentication = xxxx Meanwhile, open the backend token authentication.\nAuthentication fails The Collector verifies every request from agent, allowed only the token match.\nIf the token is not right, you will see the following log in agent\norg.apache.skywalking.apm.dependencies.io.grpc.StatusRuntimeException: PERMISSION_DENIED FAQ Can I use token authentication instead of TLS? No, you shouldn\u0026rsquo;t. In tech way, you can of course, but token and TLS are used for untrusted network env. In that circumstance, TLS has higher priority than this. Token can be trusted only under TLS protection.Token can be stolen easily if you send it through a non-TLS network.\nDo you support other authentication mechanisms? Such as ak/sk? For now, no. But we appreciate someone contributes this feature.\n","excerpt":"Token Authentication Token In current version, Token is considered as a simple string.\nSet Token Set …","ref":"/docs/main/v8.5.0/en/setup/service-agent/java-agent/token-auth/","title":"Token Authentication"},{"body":"trace cross thread  Dependency the toolkit, such as using maven or gradle  \u0026lt;dependency\u0026gt; \u0026lt;groupId\u0026gt;org.apache.skywalking\u0026lt;/groupId\u0026gt; \u0026lt;artifactId\u0026gt;apm-toolkit-trace\u0026lt;/artifactId\u0026gt; \u0026lt;version\u0026gt;${skywalking.version}\u0026lt;/version\u0026gt; \u0026lt;/dependency\u0026gt;  usage 1.  @TraceCrossThread public static class MyCallable\u0026lt;String\u0026gt; implements Callable\u0026lt;String\u0026gt; { @Override public String call() throws Exception { return null; } } ... ExecutorService executorService = Executors.newFixedThreadPool(1); executorService.submit(new MyCallable());  usage 2.  ExecutorService executorService = Executors.newFixedThreadPool(1); executorService.submit(CallableWrapper.of(new Callable\u0026lt;String\u0026gt;() { @Override public String call() throws Exception { return null; } })); or\nExecutorService executorService = Executors.newFixedThreadPool(1); executorService.execute(RunnableWrapper.of(new Runnable() { @Override public void run() { //your code  } }));  usage 3.  @TraceCrossThread public class MySupplier\u0026lt;String\u0026gt; implements Supplier\u0026lt;String\u0026gt; { @Override public String get() { return null; } } ... CompletableFuture.supplyAsync(new MySupplier\u0026lt;String\u0026gt;()); or\nCompletableFuture.supplyAsync(SupplierWrapper.of(()-\u0026gt;{ return \u0026#34;SupplierWrapper\u0026#34;; })).thenAccept(System.out::println); Sample codes only\n","excerpt":"trace cross thread  Dependency the toolkit, such as using maven or gradle  \u0026lt;dependency\u0026gt; …","ref":"/docs/main/v8.5.0/en/setup/service-agent/java-agent/application-toolkit-trace-cross-thread/","title":"trace cross thread"},{"body":"Trace Data Protocol v3 Trace Data Protocol describes the data format between SkyWalking agent/sniffer and backend.\nOverview Trace data protocol is defined and provided in gRPC format, also implemented in HTTP 1.1\nReport service instance status   Service Instance Properties Service instance has more information than a name, once the agent wants to report this, use ManagementService#reportInstanceProperties service providing a string-key/string-value pair list as the parameter. language of target instance is expected at least.\n  Service Ping Service instance should keep alive with the backend. The agent should set a scheduler using ManagementService#keepAlive service in every minute.\n  Send trace and metrics After you have service id and service instance id, you could send traces and metrics. Now we have\n TraceSegmentReportService#collect for skywalking native trace format JVMMetricReportService#collect for skywalking native jvm format  For trace format, there are some notices\n Segment is a concept in SkyWalking, it should include all span for per request in a single OS process, usually single thread based on language. Span has 3 different groups.    EntrySpan EntrySpan represents a service provider, also the endpoint of server side. As an APM system, we are targeting the application servers. So almost all the services and MQ-consumer are EntrySpan(s).\n  LocalSpan LocalSpan represents a normal Java method, which don\u0026rsquo;t relate with remote service, neither a MQ producer/consumer nor a service(e.g. HTTP service) provider/consumer.\n  ExitSpan ExitSpan represents a client of service or MQ-producer, as named as LeafSpan at early age of SkyWalking. e.g. accessing DB by JDBC, reading Redis/Memcached are cataloged an ExitSpan.\n   Span across thread or process parent info is called Reference. Reference carries trace id, segment id, span id, service name, service instance name, endpoint name and target address used at client side(not required in across thread) of this request in the parent. Follow Cross Process Propagation Headers Protocol v3 to get more details.\n  Span#skipAnalysis could be TRUE, if this span doesn\u0026rsquo;t require backend analysis.\n  ","excerpt":"Trace Data Protocol v3 Trace Data Protocol describes the data format between SkyWalking …","ref":"/docs/main/v8.5.0/en/protocols/trace-data-protocol-v3/","title":"Trace Data Protocol v3"},{"body":"Trace Sampling at server side When we run a distributed tracing system, the trace bring us detailed info, but cost a lot at storage. Open server side trace sampling mechanism, the metrics of service, service instance, endpoint and topology are all accurate as before, but only don\u0026rsquo;t save all the traces into storage.\nOf course, even you open sampling, the traces will be kept as consistent as possible. Consistent means, once the trace segments have been collected and reported by agents, the backend would do their best to don\u0026rsquo;t break the trace. See Recommendation to understand why we called it as consistent as possible and do their best to don't break the trace.\nSet the sample rate In agent-analyzer module, you will find sampleRate setting.\nagent-analyzer: default: ... sampleRate: ${SW_TRACE_SAMPLE_RATE:10000} # The sample rate precision is 1/10000. 10000 means 100% sample in default. forceSampleErrorSegment: ${SW_FORCE_SAMPLE_ERROR_SEGMENT:true} # When sampling mechanism activated, this config would make the error status segment sampled, ignoring the sampling rate. slowTraceSegmentThreshold: ${SW_SLOW_TRACE_SEGMENT_THRESHOLD:-1} # Setting this threshold about the latency would make the slow trace segments sampled if they cost more time, even the sampling mechanism activated. The default value is `-1`, which means would not sample slow traces. Unit, millisecond. sampleRate is for you to set sample rate to this backend. The sample rate precision is 1/10000. 10000 means 100% sample in default.\nforceSampleErrorSegment is for you to save all error segments when sampling mechanism actived. When sampling mechanism activated, this config would make the error status segment sampled, ignoring the sampling rate.\nslowTraceSegmentThreshold is for you to save all slow trace segments when sampling mechanism actived. Setting this threshold about the latency would make the slow trace segments sampled if they cost more time, even the sampling mechanism activated. The default value is -1, which means would not sample slow traces. Unit, millisecond.\nRecommendation You could set different backend instances with different sampleRate values, but we recommend you to set the same.\nWhen you set the rate different, let\u0026rsquo;s say\n Backend-InstanceA.sampleRate = 35 Backend-InstanceB.sampleRate = 55  And we assume the agents reported all trace segments to backend, Then the 35% traces in the global will be collected and saved in storage consistent/complete, with all spans. 20% trace segments, which reported to Backend-InstanceB, will saved in storage, maybe miss some trace segments, because they are reported to Backend-InstanceA and ignored.\nNote When you open sampling, the actual sample rate could be over sampleRate. Because currently, all error/slow segments will be saved, meanwhile, the upstream and downstream may not be sampled. This feature is going to make sure you could have the error/slow stacks and segments, but don\u0026rsquo;t guarantee you would have the whole trace.\nAlso, the side effect would be, if most of the accesses are fail/slow, the sampling rate would be closing to 100%, which could crash the backend or storage clusters.\n","excerpt":"Trace Sampling at server side When we run a distributed tracing system, the trace bring us detailed …","ref":"/docs/main/v8.5.0/en/setup/backend/trace-sampling/","title":"Trace Sampling at server side"},{"body":"Tracing and Tracing based Metrics Analyze Plugins The following plugins provide the distributed tracing capability, and the OAP backend would analyze the topology and metrics based on the tracing data.\n HTTP Server  Tomcat 7 Tomcat 8 Tomcat 9 Spring Boot Web 4.x Spring MVC 3.x, 4.x 5.x with servlet 3.x Nutz Web Framework 1.x Struts2 MVC 2.3.x -\u0026gt; 2.5.x Resin 3 (Optional¹) Resin 4 (Optional¹) Jetty Server 9 Spring WebFlux 5.x (Optional¹) Undertow 1.3.0.Final -\u0026gt; 2.0.27.Final RESTEasy 3.1.0.Final -\u0026gt; 3.7.0.Final Play Framework 2.6.x -\u0026gt; 2.8.x Light4J Microservices Framework 1.6.x -\u0026gt; 2.x Netty SocketIO 1.x   HTTP Client  Feign 9.x Netflix Spring Cloud Feign 1.1.x -\u0026gt; 2.x Okhttp 3.x Apache httpcomponent HttpClient 2.0 -\u0026gt; 3.1, 4.2, 4.3 Spring RestTemplete 4.x Jetty Client 9 Apache httpcomponent AsyncClient 4.x AsyncHttpClient 2.x JRE HttpURLConnection (Optional²)   HTTP Gateway  Spring Cloud Gateway 2.0.2.RELEASE -\u0026gt; 2.2.x.RELEASE (Optional²)   JDBC  Mysql Driver 5.x, 6.x, 8.x Oracle Driver (Optional¹) H2 Driver 1.3.x -\u0026gt; 1.4.x Sharding-JDBC 1.5.x ShardingSphere 3.0.0, 4.0.0-RC1, 4.0.0, 4.0.1, 4.1.0, 4.1.1 PostgreSQL Driver 8.x, 9.x, 42.x Mariadb Driver 2.x, 1.8 InfluxDB 2.5 -\u0026gt; 2.17 Mssql-Jtds 1.x Mssql-jdbc 6.x -\u0026gt; 8.x   RPC Frameworks  Dubbo 2.5.4 -\u0026gt; 2.6.0 Dubbox 2.8.4 Apache Dubbo 2.7.0 Motan 0.2.x -\u0026gt; 1.1.0 gRPC 1.x Apache ServiceComb Java Chassis 0.1 -\u0026gt; 0.5,1.x SOFARPC 5.4.0 Armeria 0.63.0 -\u0026gt; 0.98.0 Apache Avro 1.7.0 - 1.8.x Finagle 6.44.0 -\u0026gt; 20.1.0 (6.25.0 -\u0026gt; 6.44.0 not tested) Brpc-Java 2.3.7 -\u0026gt; 2.5.3 Thrift 0.10.0 -\u0026gt; 0.12.0 Apache CXF 3.x   MQ  RocketMQ 4.x Kafka 0.11.0.0 -\u0026gt; 2.6.1 Spring-Kafka Spring Kafka Consumer 1.3.x -\u0026gt; 2.3.x (2.0.x and 2.1.x not tested and not recommended by the official document) ActiveMQ 5.10.0 -\u0026gt; 5.15.4 RabbitMQ 5.x Pulsar 2.2.x -\u0026gt; 2.4.x Aliyun ONS 1.x (Optional¹)   NoSQL  Redis  Jedis 2.x Redisson Easy Java Redis client 3.5.2+ Lettuce 5.x   MongoDB Java Driver 2.13-2.14, 3.4.0-3.12.7, 4.0.0-4.1.0 Memcached Client  Spymemcached 2.x Xmemcached 2.x   Elasticsearch  transport-client 5.2.x-5.6.x transport-client 6.7.1-6.8.4 rest-high-level-client 6.7.1-6.8.4 rest-high-level-client 7.0.0-7.5.2   Solr  SolrJ 7.x   Cassandra 3.x  cassandra-java-driver 3.7.0-3.7.2   HBase  hbase-client HTable 1.0.0-2.4.2     Service Discovery  Netflix Eureka   Distributed Coordination  Zookeeper 3.4.x (Optional² \u0026amp; Except 3.4.4)   Spring Ecosystem  Spring Bean annotations(@Bean, @Service, @Component, @Repository) 3.x and 4.x (Optional²) Spring Core Async SuccessCallback/FailureCallback/ListenableFutureCallback 4.x Spring Transaction 4.x and 5.x (Optional²)   Hystrix: Latency and Fault Tolerance for Distributed Systems 1.4.20 -\u0026gt; 1.5.18 Scheduler  Elastic Job 2.x Apache ShardingSphere-Elasticjob 3.0.0-alpha Spring @Scheduled 3.1+ Quartz Scheduler 2.x (Optional²) XXL Job 2.x   OpenTracing community supported Canal: Alibaba mysql database binlog incremental subscription \u0026amp; consumer components 1.0.25 -\u0026gt; 1.1.2 JSON  GSON 2.8.x (Optional²)   Vert.x Ecosystem  Vert.x Eventbus 3.2+ Vert.x Web 3.x   Thread Schedule Framework  Spring @Async 4.x and 5.x Quasar 0.7.x JRE Callable and Runnable (Optional²)   Cache  Ehcache 2.x   Kotlin  Coroutine 1.0.1 -\u0026gt; 1.3.x (Optional²)   GraphQL  Graphql 8.0 -\u0026gt; 15.x   Pool  Apache Commons DBCP 2.x   Logging Framework  log4j 2.x log4j2 1.2.x logback 1.2.x    Meter Plugins The meter plugin provides the advanced metrics collections, which are not a part of tracing.\n ¹Due to license incompatibilities/restrictions these plugins are hosted and released in 3rd part repository, go to SkyAPM java plugin extension repository to get these.\n²These plugins affect the performance or must be used under some conditions, from experiences. So only released in /optional-plugins or /bootstrap-plugins, copy to /plugins in order to make them work.\n","excerpt":"Tracing and Tracing based Metrics Analyze Plugins The following plugins provide the distributed …","ref":"/docs/main/v8.5.0/en/setup/service-agent/java-agent/supported-list/","title":"Tracing and Tracing based Metrics Analyze Plugins"},{"body":"TTL In SkyWalking, there are two types of observability data, besides metadata.\n Record, including trace and alarm. Maybe log in the future. Metric, including such as percentile, heat map, success rate, cpm(rpm) etc.  You have following settings for different types.\n# Set a timeout on metrics data. After the timeout has expired, the metrics data will automatically be deleted. recordDataTTL: ${SW_CORE_RECORD_DATA_TTL:3} # Unit is day metricsDataTTL: ${SW_CORE_METRICS_DATA_TTL:7} # Unit is day  recordDataTTL affects Record data, including tracing and alarm. metricsDataTTL affects all metrics, including service, instance, endpoint metrics and topology map metrics.  ","excerpt":"TTL In SkyWalking, there are two types of observability data, besides metadata.\n Record, including …","ref":"/docs/main/v8.5.0/en/setup/backend/ttl/","title":"TTL"},{"body":"UI SkyWalking UI distribution is already included in our Apache official release.\nStartup Startup script is also in /bin/webappService.sh(.bat). UI runs as an OS Java process, powered-by Zuul.\nSettings Setting file of UI is webapp/webapp.yml in distribution package. It is constituted by three parts.\n Listening port. Backend connect info.  server: port: 8080 collector: path: /graphql ribbon: ReadTimeout: 10000 # Point to all backend\u0026#39;s restHost:restPort, split by ,  listOfServers: 10.2.34.1:12800,10.2.34.2:12800 ","excerpt":"UI SkyWalking UI distribution is already included in our Apache official release.\nStartup Startup …","ref":"/docs/main/v8.5.0/en/setup/backend/ui-setup/","title":"UI"},{"body":"UI Introduction SkyWalking official UI provides the default and powerful visualization capabilities for SkyWalking observing distributed cluster.\nThe latest introduction video could be found on the Youtube\n\nSkyWalking dashboard includes the following part.\n Feature Tab Selector Zone. The key features are list there. The more details will be introduced below. Reload Zone. Control the reload mechanism, including reload periodically or manually. Time Selector Zone. Control the timezone and time range. And a Chinese/English switch button here, default, the UI uses the browser language setting. We also welcome to contribute more languages.  Dashboard Dashboard provide metrics of service, service instance and endpoint. There are a few metrics terms you need to understand\n Throughput CPM , represents calls per minute. Apdex score, Read Apdex in WIKI Response Time Percentile, including p99, p95, p90, p75, p50. Read percentile in WIKI SLA, represents the successful rate. For HTTP, it means the rate of 200 response code.  Service, Instance and Dashboard selector could reload manually rather than reload the whole page. NOTICE, the Reload Zone wouldn\u0026rsquo;t reload these selectors.\nTwo default dashboards are provided to visualize the metrics of service and database.\nUser could click the lock button left aside the Service/Instance/Endpoint Reload button to custom your own dashboard.\nCustom Dashboard Users could customize the dashboard. The default dashboards are provided through the default templates located in /ui-initialized-templates folders.\nThe template file follows this format.\ntemplates: - name: template name # The unique name # The type includes DASHBOARD, TOPOLOGY_INSTANCE, TOPOLOGY_ENDPOINT. # DASHBOARD type templates could have multiple definitions, by using different names. # TOPOLOGY_INSTANCE, TOPOLOGY_ENDPOINT type templates should be defined once,  # as they are used in the topology page only. type: \u0026#34;DASHBOARD\u0026#34; # Custom the dashboard or create a new one on the UI, set the metrics as you like in the edit mode. # Then, you could export this configuration through the page and add it here. configuration: |-[ { \u0026#34;name\u0026#34;:\u0026#34;Spring Sleuth\u0026#34;, \u0026#34;type\u0026#34;:\u0026#34;service\u0026#34;, \u0026#34;children\u0026#34;:[ { \u0026#34;name\u0026#34;:\u0026#34;Sleuth\u0026#34;, \u0026#34;children\u0026#34;: [{ \u0026#34;width\u0026#34;: \u0026#34;3\u0026#34;, \u0026#34;title\u0026#34;: \u0026#34;HTTP Request\u0026#34;, \u0026#34;height\u0026#34;: \u0026#34;200\u0026#34;, \u0026#34;entityType\u0026#34;: \u0026#34;ServiceInstance\u0026#34;, \u0026#34;independentSelector\u0026#34;: false, \u0026#34;metricType\u0026#34;: \u0026#34;REGULAR_VALUE\u0026#34;, \u0026#34;metricName\u0026#34;: \u0026#34;meter_http_server_requests_count\u0026#34;, \u0026#34;queryMetricType\u0026#34;: \u0026#34;readMetricsValues\u0026#34;, \u0026#34;chartType\u0026#34;: \u0026#34;ChartLine\u0026#34;, \u0026#34;unit\u0026#34;: \u0026#34;Count\u0026#34; } ... ] } ] } ] # Activated means this templates added into the UI page automatically. # False means providing a basic template, user needs to add it manually on the page. activated: false # True means wouldn\u0026#39;t show up on the dashboard. Only keeps the definition in the storage. disabled: false NOTE, UI initialized templates would only be initialized if there is no template in the storage has the same name. Check the entity named as ui_template in your storage.\nTopology Topology map shows the relationship among the services and instances with metrics.\n Topology shows the default global topology including all services. Service Selector provides 2 level selectors, service group list and service name list. The group name is separated from the service name if it follows \u0026lt;group name\u0026gt;::\u0026lt;logic name\u0026gt; format. Topology map is available for single group, single service, or global(include all services). Custom Group provides the any sub topology capability of service group. Service Deep Dive opens when you click any service. The honeycomb could do metrics, trace and alarm query of the selected service. Service Relationship Metrics gives the metrics of service RPC interactions and instances of these two services.  Trace Query Trace query is a typical feature as SkyWalking provided distributed agents.\n Trace Segment List is not the trace list. Every trace has several segments belonging to different services. If\nquery by all services or by trace id, different segments with same trace id could be list there. Span is clickable, the detail of each span will pop up on the left side. Trace Views provides 3 typical and different usage views to visualize the trace.  Profile Profile is an interaction feature. It provides the method level performance diagnosis.\nTo start the profile analysis, user need to create the profile task\n Select the specific service. Set the endpoint name. This endpoint name typically is the operation name of the first span. Find this on the trace segment list view. Monitor time could start right now or from any given future time. Monitor duration defines the observation time window to find the suitable request to do performance analysis. Even the profile add a very limited performance impact to the target system, but it is still an additional load. This duration make the impact controllable. Min duration threshold provides a filter mechanism, if a request of the given endpoint response quickly, it wouldn\u0026rsquo;t be profiled. This could make sure, the profiled data is the expected one. Max sampling count gives the max dataset of agent will collect. It helps to reduce the memory and network load. One implicit condition, in any moment, SkyWalking only accept one profile task for each service. Agent could have different settings to control or limit this feature, read document setup for more details. Not all SkyWalking ecosystem agent supports this feature, java agent from 7.0.0 supports this in default.  Once the profile done, the profiled trace segments would show up. And you could request for analysis for any span. Typically, we analysis spans having long self duration, if the span and its children both have long duration, you could choose include children or exclude childrend to set the analysis boundaries.\nAfter choose the right span, and click the analysis button, you will see the stack based analysis result. The slowest methods have been highlighted.\nAdvanced features  Since 7.1.0, the profiled trace collects the HTTP request parameters for Tomcat and SpringMVC Controller automatically.  Log Since 8.3.0, SkyWalking provides log query for the browser monitoring. Use Apache SkyWalking Client JS agent would collect metrics and error logs.\nAlarm Alarm page lists all triggered alarm. Read the backend setup documentation to know how to set up the alarm rule or integrate with 3rd party system.\n","excerpt":"UI Introduction SkyWalking official UI provides the default and powerful visualization capabilities …","ref":"/docs/main/v8.5.0/en/ui/readme/","title":"UI Introduction"},{"body":"Uninstrumented Gateways/Proxies The uninstrumented gateways are not instrumented by SkyWalking agent plugin when they are started, but they can be configured in gateways.yml file or via Dynamic Configuration. The reason why they can\u0026rsquo;t register to backend automatically is that there\u0026rsquo;re no suitable agent plugins, for example, there is no agent plugins for Nginx, haproxy, etc. So in order to visualize the real topology, we provide a way to configure the gateways/proxies manually.\nConfiguration Format The configuration content includes the gateways' names and their instances:\ngateways: - name: proxy0 # the name is not used for now instances: - host: 127.0.0.1 # the host/ip of this gateway instance port: 9099 # the port of this gateway instance, defaults to 80 Note that the host of the instance must be the one that is actually used in client side, for example, if the instance proxyA has 2 IPs, say 192.168.1.110 and 192.168.1.111, both of which delegates the target service, and the client connects to 192.168.1.110, then configuring 192.168.1.111 as the host won\u0026rsquo;t work properly.\n","excerpt":"Uninstrumented Gateways/Proxies The uninstrumented gateways are not instrumented by SkyWalking agent …","ref":"/docs/main/v8.5.0/en/setup/backend/uninstrumented-gateways/","title":"Uninstrumented Gateways/Proxies"},{"body":"Using E2E local remote debugging The E2E remote debugging port of service containers is 5005. If the developer wants to use remote debugging, he needs to add remote debugging parameters to the start service command, and then expose the port 5005.\nFor example, this is the configuration of a container in the skywalking/test/e2e/e2e-test/docker/base-compose.yml. JAVA_OPTS is a preset variable for passing additional parameters in the AOP service startup command, so we only need to add the JAVA remote debugging parameters agentlib:jdwp=transport=dt_socket,server=y,suspend=n,address=5005 to the configuration and exposes the port 5005.\noap: image: skywalking/oap:latest expose: ... - 5005 ... environment: ... JAVA_OPTS: \u0026gt;-... -agentlib:jdwp=transport=dt_socket,server=y,suspend=n,address=5005 ... At last, if the E2E test failed and is retrying, the developer can get the ports mapping in the file skywalking/test/e2e/e2e-test/remote_real_port and selects the host port of the corresponding service for remote debugging. For example,\n#remote_real_port #The remote debugging port on the host is 32783 oap-localhost:32783 #The remote debugging port on the host is 32782 provider-localhost:32782 ","excerpt":"Using E2E local remote debugging The E2E remote debugging port of service containers is 5005. If the …","ref":"/docs/main/v8.5.0/en/guides/e2e-local-remote-debug/","title":"Using E2E local remote debugging"},{"body":"V6 upgrade SkyWalking v6 is widely used in many production environments. Follow the steps in the guide below to learn how to upgrade to a new release.\nNOTE: The ways to upgrade are not limited to the steps below.\nUse Canary Release Like all applications, you may upgrade SkyWalking using the canary release method through the following steps.\n Deploy a new cluster by using the latest version of SkyWalking OAP cluster with the new database cluster. Once the target service (i.e. the service being monitored) has upgraded the agent.jar (or simply by rebooting), have collector.backend_service pointing to the new OAP backend, and use/add a new namespace(agent.namespace in Table of Agent Configuration Properties). The namespace will prevent conflicts from arising between different versions. When all target services have been rebooted, the old OAP clusters could be discarded.  The Canary Release method works for any version upgrades.\nOnline Hot Reboot Upgrade The reason we require Canary Release is that the SkyWalking agent has cache mechanisms, and switching to a new cluster causes the cache to become unavailable for new OAP clusters. In version 6.5.0+ (especially for agent versions), we have Agent hot reboot trigger mechanism. This streamlines the upgrade process as we deploy a new cluster by using the latest version of SkyWalking OAP cluster with the new database cluster, and shift the traffic to the new cluster once and for all. Based on the mechanism, all agents will enter the cool_down mode, and come back online. For more details, see the backend setup documentation.\nNOTE: A known bug in 6.4.0 is that its agent may have re-connection issues; therefore, even though this bot reboot mechanism has been included in 6.4.0, it may not work under some network scenarios, especially in Kubernetes.\nAgent Compatibility All versions of SkyWalking 6.x (and even 7.x) are compatible with each other, so users could simply upgrade the OAP servers. As the agent has also been enhanced in the latest versions, according to the SkyWalking team\u0026rsquo;s recommendation, upgrade the agent as soon as practicable.\n","excerpt":"V6 upgrade SkyWalking v6 is widely used in many production environments. Follow the steps in the …","ref":"/docs/main/v8.5.0/en/faq/v6-version-upgrade/","title":"V6 upgrade"},{"body":"V8 upgrade Starting from SkyWalking v8, the v3 protocol has been used. This makes it incompatible with previous releases. Users who intend to upgrade in v8 series releases could follow the steps below.\nRegisters in v6 and v7 have been removed in v8 for better scaling out performance. Please upgrade following the instructions below.\n Use a different storage or a new namespace. You may also consider erasing the whole storage indexes or tables related to SkyWalking. Deploy the whole SkyWalking cluster, and expose it in a new network address. If you are using language agents, upgrade the new agents too; meanwhile, make sure the agents are supported in a different language. Then, set up the backend address to the new SkyWalking OAP cluster.  ","excerpt":"V8 upgrade Starting from SkyWalking v8, the v3 protocol has been used. This makes it incompatible …","ref":"/docs/main/v8.5.0/en/faq/v8-version-upgrade/","title":"V8 upgrade"},{"body":"Version 3.x -\u0026gt; 5.0.0-alpha Upgrade FAQs Collector Problem There is no information showing in the UI.\nCause In the upgrade from version 3.2.6 to 5.0.0, the existing Elasticsearch indexes are kept, but aren\u0026rsquo;t compatible with 5.0.0-alpha. When service name is registered, ElasticSearch will create this column by default type string, which will lead to an error.\nSolution Clean the data folder in ElasticSearch and restart ElasticSearch, collector and your application under monitoring.\n","excerpt":"Version 3.x -\u0026gt; 5.0.0-alpha Upgrade FAQs Collector Problem There is no information showing in the …","ref":"/docs/main/v8.5.0/en/faq/v3-version-upgrade/","title":"Version 3.x -\u003e 5.0.0-alpha Upgrade FAQs"},{"body":"Visualization SkyWalking native UI provides the default visualization solution. It provides observability related graphs about overview, service, service instance, endpoint, trace and alarm, including topology, dependency graph, heatmap, etc.\nAlso, we have already known, many of our users have integrated SkyWalking into their products. If you want to do that too, please use SkyWalking query protocol.\n","excerpt":"Visualization SkyWalking native UI provides the default visualization solution. It provides …","ref":"/docs/main/v8.5.0/en/concepts-and-designs/ui-overview/","title":"Visualization"},{"body":"VMs monitoring SkyWalking leverages Prometheus node-exporter for collecting metrics data from the VMs, and leverages OpenTelemetry Collector to transfer the metrics to OpenTelemetry receiver and into the Meter System.\nWe defined the VM entity as a Service in OAP, use vm:: as a prefix to identify.\nData flow  Prometheus node-exporter collects metrics data from the VMs. OpenTelemetry Collector fetches metrics from node-exporter via Prometheus Receiver and pushes metrics to SkyWalking OAP Server via the OpenCensus GRPC Exporter. The SkyWalking OAP Server parses the expression with MAL to filter/calculate/aggregate and store the results.  Setup  Setup Prometheus node-exporter. Setup OpenTelemetry Collector . This is an example for OpenTelemetry Collector configuration otel-collector-config.yaml. Config SkyWalking OpenTelemetry receiver.  Supported Metrics    Monitoring Panel Unit Metric Name Description Data Source     CPU Usage % cpu_total_percentage The CPU cores total used percentage, if there are 2 cores the max usage is 200% Prometheus node-exporter   Memory RAM Usage MB meter_vm_memory_used The RAM total usage Prometheus node-exporter   Memory Swap Usage % meter_vm_memory_swap_percentage The swap memory used percentage Prometheus node-exporter   CPU Average Used % meter_vm_cpu_average_used The CPU cores used percentage in each mode Prometheus node-exporter   CPU Load  meter_vm_cpu_load1\nmeter_vm_cpu_load5\nmeter_vm_cpu_load15 The CPU 1m / 5m / 15m average load Prometheus node-exporter   Memory RAM MB meter_vm_memory_total\nmeter_vm_memory_available\nmeter_vm_memory_used The RAM statistics, include Total / Available / Used Prometheus node-exporter   Memory Swap MB meter_vm_memory_swap_free\nmeter_vm_memory_swap_total The Swap Memory statistics, include Free / Total Prometheus node-exporter   File System Mountpoint Usage % meter_vm_filesystem_percentage The File System used percentage in each mount point Prometheus node-exporter   Disk R/W KB/s meter_vm_disk_read,meter_vm_disk_written The Disk read and written Prometheus node-exporter   Network Bandwidth Usage KB/s meter_vm_network_receive\nmeter_vm_network_transmit The Network receive and transmit Prometheus node-exporter   Network Status  meter_vm_tcp_curr_estab\nmeter_vm_tcp_tw\nmeter_vm_tcp_alloc\nmeter_vm_sockets_used\nmeter_vm_udp_inuse The number of the TCP establish / TCP time wait / TCP allocated / Sockets inuse / UDP inuse Prometheus node-exporter   Filefd Allocated  meter_vm_filefd_allocated The number of the File Descriptor allocated Prometheus node-exporter    Customizing You can customize your own metrics/expression/dashboard panel.\nThe metrics definition and expression rules are in /config/otel-oc-rules/vm.yaml.\nThe dashboard panel confirmations are in /config/ui-initialized-templates/vm.yml.\nBlog A related blog can see: SkyWalking 8.4 provides infrastructure monitoring\nK8s monitoring SkyWalking leverages K8s kube-state-metrics and cAdvisor for collecting metrics data from the K8s, and leverages OpenTelemetry Collector to transfer the metrics to OpenTelemetry receiver and into the Meter System. This feature requires authorizing the OAP Server to access K8s\u0026rsquo;s API Server.\nWe defined the k8s-cluster as a Service in OAP, use k8s-cluster:: as a prefix to identify.\nDefined the k8s-node as an Instance in OAP, the name is k8s node name.\nDefined the k8s-service as an Endpoint in OAP, the name is $serviceName.$namespace.\nData flow  K8s kube-state-metrics and cAdvisor collects metrics data from the K8s. OpenTelemetry Collector fetches metrics from kube-state-metrics and cAdvisor via Prometheus Receiver and pushes metrics to SkyWalking OAP Server via the OpenCensus GRPC Exporter. The SkyWalking OAP Server access to K8s\u0026rsquo;s API Server gets meta info and parses the expression with MAL to filter/calculate/aggregate and store the results.  Setup  Setup kube-state-metric. cAdvisor is integrated into kubelet by default. Setup OpenTelemetry Collector . Prometheus Receiver in OpenTelemetry Collector for K8s can reference here. For a quick start, we provided a full example for OpenTelemetry Collector configuration otel-collector-config.yaml. Config SkyWalking OpenTelemetry receiver.  Supported Metrics From the different point of view to monitor the K8s, there are 3 kinds of metrics: Cluster / Node / Service\nCLuster These metrics are related to the selected cluster(Current Service in the dashboard).\n   Monitoring Panel Unit Metric Name Description Data Source     Node Total  k8s_cluster_node_total The number of the nodes K8s kube-state-metrics   Namespace Total  k8s_cluster_namespace_total The number of the namespaces K8s kube-state-metrics   Deployment Total  k8s_cluster_deployment_total The number of the deployments K8s kube-state-metrics   Service Total  k8s_cluster_service_total The number of the services K8s kube-state-metrics   Pod Total  k8s_cluster_pod_total The number of the pods K8s kube-state-metrics   Container Total  k8s_cluster_container_total The number of the containers K8s kube-state-metrics   CPU Resources m k8s_cluster_cpu_cores\nk8s_cluster_cpu_cores_requests\nk8s_cluster_cpu_cores_limits\nk8s_cluster_cpu_cores_allocatable The capacity and the Requests / Limits / Allocatable of the CPU K8s kube-state-metrics   Memory Resources GB k8s_cluster_memory_total\nk8s_cluster_memory_requests\nk8s_cluster_memory_limits\nk8s_cluster_memory_allocatable The capacity and the Requests / Limits / Allocatable of the memory K8s kube-state-metrics   Storage Resources GB k8s_cluster_storage_total\nk8s_cluster_storage_allocatable The capacity and allocatable of the storage K8s kube-state-metrics   Node Status  k8s_cluster_node_status The current status of the nodes K8s kube-state-metrics   Deployment Status  k8s_cluster_deployment_status The current status of the deployment K8s kube-state-metrics   Deployment Spec Replicas  k8s_cluster_deployment_spec_replicas The number of desired pods for a deployment K8s kube-state-metrics   Service Status  k8s_cluster_service_pod_status The services current status, depending on the related pods' status K8s kube-state-metrics   Pod Status Not Running  k8s_cluster_pod_status_not_running The pods which the current phase is not running K8s kube-state-metrics   Pod Status Waiting  k8s_cluster_pod_status_waiting The pods and containers which currently in the waiting status, and show the reason K8s kube-state-metrics   Pod Status Terminated  k8s_cluster_container_status_terminated The pods and containers which currently in the terminated status, and show the reason K8s kube-state-metrics    Node These metrics are related to the selected node (Current Instance in the dashboard).\n   Monitoring Panel Unit Metric Name Description Data Source     Pod Total  k8s_node_pod_total The number of the pods which in this node K8s kube-state-metrics   Node Status  k8s_node_node_status The current status of this node K8s kube-state-metrics   CPU Resources m k8s_node_cpu_cores\nk8s_node_cpu_cores_allocatable\nk8s_node_cpu_cores_requests\nk8s_node_cpu_cores_limits The capacity and the Requests / Limits / Allocatable of the CPU K8s kube-state-metrics   Memory Resources GB k8s_node_memory_total\nk8s_node_memory_allocatable\nk8s_node_memory_requests\nk8s_node_memory_limits The capacity and the Requests / Limits / Allocatable of the memory K8s kube-state-metrics   Storage Resources GB k8s_node_storage_total\nk8s_node_storage_allocatable The capacity and allocatable of the storage K8s kube-state-metrics   CPU Usage m k8s_node_cpu_usage The CPU cores total usage, if there are 2 cores the max usage is 2000m cAdvisor   Memory Usage GB k8s_node_memory_usage The memory total usage cAdvisor   Network I/O KB/s k8s_node_network_receive\nk8s_node_network_transmit The Network receive and transmit cAdvisor    Service In these metrics, the pods are related to the selected service (Current Endpoint in the dashboard).\n   Monitoring Panel Unit Metric Name Description Data Source     Service Pod Total  k8s_service_pod_total The number of the pods K8s kube-state-metrics   Service Pod Status  k8s_service_pod_status The current status of pods K8s kube-state-metrics   Service CPU Resources m k8s_service_cpu_cores_requests\nk8s_service_cpu_cores_limits The CPU resources Requests / Limits of this service K8s kube-state-metrics   Service Memory Resources MB k8s_service_memory_requests\nk8s_service_memory_limits The memory resources Requests / Limits of this service K8s kube-state-metrics   Pod CPU Usage m k8s_service_pod_cpu_usage The CPU resources total usage of pods cAdvisor   Pod Memory Usage MB k8s_service_pod_memory_usage The memory resources total usage of pods cAdvisor   Pod Waiting  k8s_service_pod_status_waiting The pods and containers which currently in the waiting status, and show the reason K8s kube-state-metrics   Pod Terminated  k8s_service_pod_status_terminated The pods and containers which currently in the terminated status, and show the reason K8s kube-state-metrics   Pod Restarts  k8s_service_pod_status_restarts_total The number of per container restarts that related to the pod K8s kube-state-metrics   Pod Network Receive KB/s k8s_service_pod_network_receive The Network receive of the pods cAdvisor   Pod Network Transmit KB/s k8s_service_pod_network_transmit The Network transmit of the pods cAdvisor   Pod Storage Usage MB k8s_service_pod_fs_usage The storage resources total usage of pods which related to this service cAdvisor    Customizing You can customize your own metrics/expression/dashboard panel.\nThe metrics definition and expression rules are in /config/otel-oc-rules/k8s-cluster.yaml，/config/otel-oc-rules/k8s-node.yaml, /config/otel-oc-rules/k8s-service.yaml.\nThe dashboard panel confirmations are in /config/ui-initialized-templates/k8s.yml.\n","excerpt":"VMs monitoring SkyWalking leverages Prometheus node-exporter for collecting metrics data from the …","ref":"/docs/main/v8.5.0/en/setup/backend/backend-infrastructure-monitoring/","title":"VMs monitoring"},{"body":"Welcome This is the official documentation of SkyWalking 8. Welcome to the SkyWalking community!\nHere you can learn all you need to know about SkyWalking’s architecture, understand how to deploy and use SkyWalking, and contribute to the project based on SkyWalking\u0026rsquo;s contributing guidelines.\nNOTE: SkyWalking 8 uses brand new tracing APIs which are incompatible with all previous releases.\n  Concepts and Designs. You\u0026rsquo;ll find the core logic behind SkyWalking. You may start from here if you want to understand what is going on under our cool features and visualization.\n  Setup. A guide to installing SkyWalking for different use cases. It is an observability platform that supports multiple observability modes.\n  UI Introduction. An introduction to the UI components and their features.\n  Contributing Guides. If you are a PMC member, a committer, or a new contributor, learn how to start contributing with these guides!\n  Protocols. The protocols show how agents/probes and the backend communicate with one another. Anyone interested in uplink telemetry data should definitely read this.\n  FAQs. A manifest of known issues with setup and secondary developments processes. Should you encounter any problems, check here first.\n  You might also find these links interesting:\n  The latest and old releases are all available at Apache SkyWalking release page. The change logs can be found here.\n  SkyWalking WIKI hosts the context of some changes and events.\n  You can find the conference schedules, video recordings, and articles about SkyWalking in the community resource catalog.\n  We\u0026rsquo;re always looking for help to improve our documentation and codes, so please don’t hesitate to file an issue if you see any problems. Or better yet, directly contribute by submitting a pull request to help us get better!\n","excerpt":"Welcome This is the official documentation of SkyWalking 8. Welcome to the SkyWalking community! …","ref":"/docs/main/v8.5.0/readme/","title":"Welcome"},{"body":"What is VNode? On the trace page, you may sometimes find nodes with their spans named VNode, and that there are no attributes for such spans.\nVNode is created by the UI itself, rather than being reported by the agent or tracing SDK. It indicates that some spans are missed in the trace data in this query.\nHow does the UI detect the missing span(s)? The UI checks the parent spans and reference segments of all spans in real time. If no parent id(segment id + span id) could be found, then it creates a VNode automatically.\nHow did this happen? The VNode appears when the trace data is incomplete.\n The agent fail-safe mechanism has been activated. The SkyWalking agent could abandon the trace data if there are any network issues between the agent and the OAP (e.g. failure to connect, slow network speeds, etc.), or if the OAP cluster is not capable of processing all traces. Some plug-ins may have bugs, and some segments in the trace do not stop correctly and are held in the memory.  In such case, the trace would not exist in the query, thus the VNode shows up.\n","excerpt":"What is VNode? On the trace page, you may sometimes find nodes with their spans named VNode, and …","ref":"/docs/main/v8.5.0/en/faq/vnode/","title":"What is VNode?"},{"body":"Why can\u0026rsquo;t I see any data in the UI? There are three main reasons no data can be shown by the UI:\n No traces have been sent to the collector. Traces have been sent, but the timezone of your containers is incorrect. Traces are in the collector, but you\u0026rsquo;re not watching the correct timeframe in the UI.  No traces Be sure to check the logs of your agents to see if they are connected to the collector and traces are being sent.\nIncorrect timezone in containers Be sure to check the time in your containers.\nThe UI isn\u0026rsquo;t showing any data Be sure to configure the timeframe shown by the UI.\n","excerpt":"Why can\u0026rsquo;t I see any data in the UI? There are three main reasons no data can be shown by the …","ref":"/docs/main/v8.5.0/en/faq/time-and-timezone/","title":"Why can't I see any data in the UI?"},{"body":"Why do metrics indexes with Hour and Day precisions stop updating after upgrade to 7.x? This issue is to be expected with an upgrade from 6.x to 7.x. See the Downsampling Data Packing feature of the ElasticSearch storage.\nYou may simply delete all expired *-day_xxxxx and *-hour_xxxxx(xxxxx is a timestamp) indexes. Currently, SkyWalking uses the metrics name-xxxxx and metrics name-month_xxxxx indexes only.\n","excerpt":"Why do metrics indexes with Hour and Day precisions stop updating after upgrade to 7.x? This issue …","ref":"/docs/main/v8.5.0/en/faq/hour-day-metrics-stopping/","title":"Why do metrics indexes with Hour and Day precisions stop updating after upgrade to 7.x?"},{"body":"Why doesn\u0026rsquo;t SkyWalking involve MQ in its architecture? This is often asked by those who are first introduced to SkyWalking. Many believe that MQ should have better performance and should be able to support higher throughput, like the following:\nHere\u0026rsquo;s what we think.\nIs MQ appropriate for communicating with the OAP backend? This question arises when users consider the circumstances where the OAP cluster may not be powerful enough or becomes offline. But the following issues must first be addressed:\n Why do you think that the OAP is not powerful enough? Were it not powerful, the speed of data analysis wouldn\u0026rsquo;t have caught up with the producers (or agents). Then what is the point of adding new deployment requirements? Some may argue that the payload is sometimes higher than usual during peak times. But we must consider how much higher the payload really is. If it is higher by less than 40%, how many resources would you use for the new MQ cluster? How about moving them to new OAP and ES nodes? Say it is higher by 40% or more, such as by 70% to 200%. Then, it is likely that your MQ would use up more resources than it saves. Your MQ would support 2 to 3 times the payload using 10%-20% of the cost during usual times. Furthermore, in this case, if the payload/throughput are so high, how long would it take for the OAP cluster to catch up? The challenge here is that well before it catches up, the next peak times would have come.  With the analysis above in mind, why would you still want the traces to be 100%, given the resources they would cost? The preferred way to do this would be adding a better dynamic trace sampling mechanism at the backend. When throughput exceeds the threshold, gradually modify the active sampling rate from 100% to 10%, which means you could get the OAP and ES 3 times more powerful than usual, while ignoring the traces at peak times.\nIs MQ transport recommended despite its side effects? Even though MQ transport is not recommended from the production perspective, SkyWalking still provides optional plugins named kafka-reporter and kafka-fetcher for this feature since 8.1.0.\nHow about MQ metrics data exporter? The answer is that the MQ metrics data exporter is already readily available. The exporter module with gRPC default mechanism is there, and you can easily provide a new implementor of this module.\n","excerpt":"Why doesn\u0026rsquo;t SkyWalking involve MQ in its architecture? This is often asked by those who are …","ref":"/docs/main/v8.5.0/en/faq/why_mq_not_involved/","title":"Why doesn't SkyWalking involve MQ in its architecture?"},{"body":"Work with Istio Instructions for transport Istio\u0026rsquo;s metrics to the SkyWalking OAP server.\nPrerequisites Istio should be installed in the Kubernetes cluster. Follow Istio getting start to finish it.\nDeploy SkyWalking backend Follow the deploying backend in Kubernetes to install the OAP server in the kubernetes cluster. Refer to OpenTelemetry receiver to ingest metrics. otel-receiver defaults to be inactive. Set env var SW_OTEL_RECEIVER to default to enable it.\nDeploy OpenTelemetry collector OpenTelemetry collector is the location Istio telemetry sends metrics, then processing and sending them to SkyWalking backend.\nFollowing the Getting Started to deploy this collector. There are several components available in the collector, and they could be combined for different scenarios. For the sake of brevity, we use the Prometheus receiver to retrieve metrics from Istio control and data plane, then send them to SkyWalking by OpenCensus exporter.\nPrometheus receiver Refer to Prometheus Receiver to set up this receiver. you could find more configuration details in Prometheus Integration of Istio to figure out how to direct Prometheus receiver to query Istio metrics.\nSkyWalking supports receiving multi-cluster metrics in a single OAP cluster. A cluster label should be appended to every metric fetched by this receiver even there\u0026rsquo;s only a single cluster needed to be collected. You could leverage relabel to add it like below:\nrelabel_configs: - source_labels: [] target_label: cluster replacement: \u0026lt;cluster name\u0026gt; or opt to Resource Processor:\nprocessors: resource: attributes: - key: cluster value: \u0026quot;\u0026lt;cluster name\u0026gt;\u0026quot; action: upsert Notice, if you try the sample of istio Prometheus Kubernetes configuration, the issues described here might block you. Try to use the solution indicated in this issue if it\u0026rsquo;s not fixed.\nOpenCensus exporter Follow OpenCensus exporter configuration to set up a connection between OpenTelemetry collector and OAP cluster. endpoint is the address of OAP gRPC service.\nObserve Istio Open Istio Dashboard in SkyWaling UI by clicking Dashboard -\u0026gt; Istio, then you\u0026rsquo;re able to view charts and diagrams generated by Istio metrics. You also could view them by swctl and set up alarm rules based on them.\nNOTICE, if you want metrics of Istio managed services, including topology among them, we recommend you to consider our ALS solution\n","excerpt":"Work with Istio Instructions for transport Istio\u0026rsquo;s metrics to the SkyWalking OAP server. …","ref":"/docs/main/v8.5.0/en/setup/istio/readme/","title":"Work with Istio"},{"body":"Zabbix Receiver Zabbix receiver is accepting the metrics of Zabbix Agent Active Checks protocol format into the Meter System. Zabbix Agent is base on GPL-2.0 License.\nModule define receiver-zabbix: selector: ${SW_RECEIVER_ZABBIX:default} default: # Export tcp port, Zabbix agent could connected and transport data port: 10051 # Bind to host host: 0.0.0.0 # Enable config when receive agent request activeFiles: agent Configuration file Zabbix receiver is configured via a configuration file. The configuration file defines everything related to receiving from agents, as well as which rule files to load.\nOAP can load the configuration at bootstrap. If the new configuration is not well-formed, OAP fails to start up. The files are located at $CLASSPATH/zabbix-rules.\nThe file is written in YAML format, defined by the scheme described below. Square brackets indicate that a parameter is optional.\nAn example for zabbix agent configuration could be found here. You could find the Zabbix agent detail items from Zabbix Agent documentation.\nConfiguration file # insert metricPrefix into metric name: \u0026lt;metricPrefix\u0026gt;_\u0026lt;raw_metric_name\u0026gt; metricPrefix: \u0026lt;string\u0026gt; # expSuffix is appended to all expression in this file. expSuffix: \u0026lt;string\u0026gt; # Datasource from Zabbix Item keys. requiredZabbixItemKeys: - \u0026lt;zabbix item keys\u0026gt; # Support agent entities information. entities: # Allow hostname patterns to build metrics. hostPatterns: - \u0026lt;regex string\u0026gt; # Customized metrics label before parse to meter system. labels: [- \u0026lt;labels\u0026gt; ] # Metrics rule allow you to recompute queries. metrics: [ - \u0026lt;metrics_rules\u0026gt; ]  # Define the label name. The label value must query from `value` or `fromItem` attribute. name: \u0026lt;string\u0026gt; # Appoint value to label. [value: \u0026lt;string\u0026gt;] # Query label value from Zabbix Agent Item key. [fromItem: \u0026lt;string\u0026gt;] \u0026lt;metric_rules\u0026gt; # The name of rule, which combinates with a prefix \u0026#39;meter_\u0026#39; as the index/table name in storage. name: \u0026lt;string\u0026gt; # MAL expression. exp: \u0026lt;string\u0026gt; More about MAL, please refer to mal.md.\n","excerpt":"Zabbix Receiver Zabbix receiver is accepting the metrics of Zabbix Agent Active Checks protocol …","ref":"/docs/main/v8.5.0/en/setup/backend/backend-zabbix/","title":"Zabbix Receiver"},{"body":"Advanced deployment OAP servers inter communicate with each other in a cluster environment. In the cluster mode, you could run in different roles.\n Mixed(default) Receiver Aggregator  In some time, users want to deploy cluster nodes with explicit role. Then could use this.\nMixed Default role, the OAP should take responsibilities of\n Receive agent traces or metrics. Do L1 aggregation Internal communication(send/receive) Do L2 aggregation Persistence Alarm  Receiver The OAP should take responsibilities of\n Receive agent traces or metrics. Do L1 aggregation Internal communication(send)  Aggregator The OAP should take responsibilities of\n Internal communication(receive) Do L2 aggregation Persistence Alarm   These roles are designed for complex deployment requirements based on security and network policy.\nKubernetes If you are using our native Kubernetes coordinator, the labelSelector setting is used for Aggregator choose rules. Choose the right OAP deployment based on your requirements.\n","excerpt":"Advanced deployment OAP servers inter communicate with each other in a cluster environment. In the …","ref":"/docs/main/v8.4.0/en/setup/backend/advanced-deployment/","title":"Advanced deployment"},{"body":"Alarm Alarm core is driven by a collection of rules, which are defined in config/alarm-settings.yml. There are three parts in alarm rule definition.\n Alarm rules. They define how metrics alarm should be triggered, what conditions should be considered. Webhooks. The list of web service endpoint, which should be called after the alarm is triggered. gRPCHook. The host and port of remote gRPC method, which should be called after the alarm is triggered.  Entity name Define the relation between scope and entity name.\n Service: Service name Instance: {Instance name} of {Service name} Endpoint: {Endpoint name} in {Service name} Database: Database service name Service Relation: {Source service name} to {Dest service name} Instance Relation: {Source instance name} of {Source service name} to {Dest instance name} of {Dest service name} Endpoint Relation: {Source endpoint name} in {Source Service name} to {Dest endpoint name} in {Dest service name}  Rules There are two types of rules, individual rule and composite rule, composite rule is the combination of individual rules\nIndividual rules Alarm rule is constituted by following keys\n Rule name. Unique name, show in alarm message. Must end with _rule. Metrics name. A.K.A. metrics name in oal script. Only long, double, int types are supported. See List of all potential metrics name. Include names. The following entity names are included in this rule. Please follow Entity name define. Exclude names. The following entity names are excluded in this rule. Please follow Entity name define. Include names regex. Provide a regex to include the entity names. If both setting the include name list and include name regex, both rules will take effect. Exclude names regex. Provide a regex to exclude the exclude names. If both setting the exclude name list and exclude name regex, both rules will take effect. Include labels. The following labels of the metric are included in this rule. Exclude labels. The following labels of the metric are excluded in this rule. Include labels regex. Provide a regex to include labels. If both setting the include label list and include label regex, both rules will take effect. Exclude labels regex. Provide a regex to exclude labels. If both setting the exclude label list and exclude label regex, both rules will take effect.  The settings of labels is required by meter-system which intends to store metrics from label-system platform, just like Prometheus, Micrometer, etc. The function supports the above four settings should implement LabeledValueHolder.\n Threshold. The target value. For multiple values metrics, such as percentile, the threshold is an array. Described like value1, value2, value3, value4, value5. Each value could the threshold for each value of the metrics. Set the value to - if don\u0026rsquo;t want to trigger alarm by this or some of the values.\nSuch as in percentile, value1 is threshold of P50, and -, -, value3, value4, value5 means, there is no threshold for P50 and P75 in percentile alarm rule. OP. Operator, support \u0026gt;, \u0026gt;=, \u0026lt;, \u0026lt;=, =. Welcome to contribute all OPs. Period. How long should the alarm rule should be checked. This is a time window, which goes with the backend deployment env time. Count. In the period window, if the number of values over threshold(by OP), reaches count, alarm should send. Only as condition. Specify if the rule can send notification or just as an condition of composite rule. Silence period. After alarm is triggered in Time-N, then keep silence in the TN -\u0026gt; TN + period. By default, it is as same as Period, which means in a period, same alarm(same ID in same metrics name) will be trigger once.  Composite rules NOTE. Composite rules only work for alarm rules targeting the same entity level, such as alarm rules of the service level. For example, service_percent_rule \u0026amp;\u0026amp; service_resp_time_percentile_rule. You shouldn\u0026rsquo;t compose alarm rules of different entity levels. such as one alarm rule of the service metrics with another rule of the endpoint metrics.\nComposite rule is constituted by the following keys\n Rule name. Unique name, show in alarm message. Must end with _rule. Expression. Specify how to compose rules, support \u0026amp;\u0026amp;, ||, (). Message. Specify the notification message when rule triggered.  rules: # Rule unique name, must be ended with `_rule`. endpoint_percent_rule: # Metrics value need to be long, double or int metrics-name: endpoint_percent threshold: 75 op: \u0026lt; # The length of time to evaluate the metrics period: 10 # How many times after the metrics match the condition, will trigger alarm count: 3 # How many times of checks, the alarm keeps silence after alarm triggered, default as same as period. silence-period: 10 # Specify if the rule can send notification or just as an condition of composite rule only-as-condition: false service_percent_rule: metrics-name: service_percent # [Optional] Default, match all services in this metrics include-names: - service_a - service_b exclude-names: - service_c # Single value metrics threshold. threshold: 85 op: \u0026lt; period: 10 count: 4 only-as-condition: false service_resp_time_percentile_rule: # Metrics value need to be long, double or int metrics-name: service_percentile op: \u0026#34;\u0026gt;\u0026#34; # Multiple value metrics threshold. Thresholds for P50, P75, P90, P95, P99. threshold: 1000,1000,1000,1000,1000 period: 10 count: 3 silence-period: 5 message: Percentile response time of service {name} alarm in 3 minutes of last 10 minutes, due to more than one condition of p50 \u0026gt; 1000, p75 \u0026gt; 1000, p90 \u0026gt; 1000, p95 \u0026gt; 1000, p99 \u0026gt; 1000 only-as-condition: false meter_service_status_code_rule: metrics-name: meter_status_code exclude-labels: - \u0026#34;200\u0026#34; op: \u0026#34;\u0026gt;\u0026#34; threshold: 10 period: 10 count: 3 silence-period: 5 message: The request number of entity {name} non-200 status is more than expected. only-as-condition: false composite-rules: comp_rule: # Must satisfied percent rule and resp time rule  expression: service_percent_rule \u0026amp;\u0026amp; service_resp_time_percentile_rule message: Service {name} successful rate is less than 80% and P50 of response time is over 1000ms Default alarm rules We provided a default alarm-setting.yml in our distribution only for convenience, which including following rules\n Service average response time over 1s in last 3 minutes. Service success rate lower than 80% in last 2 minutes. Percentile of service response time is over 1s in last 3 minutes Service Instance average response time over 1s in last 2 minutes, and the instance name matches the regex. Endpoint average response time over 1s in last 2 minutes. Database access average response time over 1s in last 2 minutes. Endpoint relation average response time over 1s in last 2 minutes.  List of all potential metrics name The metrics names are defined in official OAL scripts, right now metrics from Service, Service Instance, Endpoint, Service Relation, Service Instance Relation, Endpoint Relation scopes could be used in Alarm, and the Database access same with Service scope.\nSubmit issue or pull request if you want to support any other scope in alarm.\nWebhook Webhook requires the peer is a web container. The alarm message will send through HTTP post by application/json content type. The JSON format is based on List\u0026lt;org.apache.skywalking.oap.server.core.alarm.AlarmMessage\u0026gt; with following key information.\n scopeId, scope. All scopes are defined in org.apache.skywalking.oap.server.core.source.DefaultScopeDefine. name. Target scope entity name. Please follow Entity name define. id0. The ID of the scope entity matched the name. When using relation scope, it is the source entity ID. id1. When using relation scope, it will be the dest entity ID. Otherwise, it is empty. ruleName. The rule name you configured in alarm-settings.yml. alarmMessage. Alarm text message. startTime. Alarm time measured in milliseconds, between the current time and midnight, January 1, 1970 UTC.  Example as following\n[{ \u0026#34;scopeId\u0026#34;: 1, \u0026#34;scope\u0026#34;: \u0026#34;SERVICE\u0026#34;, \u0026#34;name\u0026#34;: \u0026#34;serviceA\u0026#34;, \u0026#34;id0\u0026#34;: \u0026#34;12\u0026#34;, \u0026#34;id1\u0026#34;: \u0026#34;\u0026#34;, \u0026#34;ruleName\u0026#34;: \u0026#34;service_resp_time_rule\u0026#34;, \u0026#34;alarmMessage\u0026#34;: \u0026#34;alarmMessage xxxx\u0026#34;, \u0026#34;startTime\u0026#34;: 1560524171000 }, { \u0026#34;scopeId\u0026#34;: 1, \u0026#34;scope\u0026#34;: \u0026#34;SERVICE\u0026#34;, \u0026#34;name\u0026#34;: \u0026#34;serviceB\u0026#34;, \u0026#34;id0\u0026#34;: \u0026#34;23\u0026#34;, \u0026#34;id1\u0026#34;: \u0026#34;\u0026#34;, \u0026#34;ruleName\u0026#34;: \u0026#34;service_resp_time_rule\u0026#34;, \u0026#34;alarmMessage\u0026#34;: \u0026#34;alarmMessage yyy\u0026#34;, \u0026#34;startTime\u0026#34;: 1560524171000 }] gRPCHook The alarm message will send through remote gRPC method by Protobuf content type. The message format with following key information which are defined in oap-server/server-alarm-plugin/src/main/proto/alarm-hook.proto.\nPart of protocol looks as following:\nmessage AlarmMessage { int64 scopeId = 1; string scope = 2; string name = 3; string id0 = 4; string id1 = 5; string ruleName = 6; string alarmMessage = 7; int64 startTime = 8;}Slack Chat Hook To do this you need to follow the Getting Started with Incoming Webhooks guide and create new Webhooks.\nThe alarm message will send through HTTP post by application/json content type if you configured Slack Incoming Webhooks as following:\nslackHooks: textTemplate: |-{ \u0026#34;type\u0026#34;: \u0026#34;section\u0026#34;, \u0026#34;text\u0026#34;: { \u0026#34;type\u0026#34;: \u0026#34;mrkdwn\u0026#34;, \u0026#34;text\u0026#34;: \u0026#34;:alarm_clock: *Apache Skywalking Alarm* \\n **%s**.\u0026#34; } } webhooks: - https://hooks.slack.com/services/x/y/z WeChat Hook Note, only WeCom(WeChat Company Edition) supports webhook. To use the WeChat webhook you need to follow the Wechat Webhooks guide. The alarm message would send through HTTP post by application/json content type after you set up Wechat Webhooks as following:\nwechatHooks: textTemplate: |-{ \u0026#34;msgtype\u0026#34;: \u0026#34;text\u0026#34;, \u0026#34;text\u0026#34;: { \u0026#34;content\u0026#34;: \u0026#34;Apache SkyWalking Alarm: \\n %s.\u0026#34; } } webhooks: - https://qyapi.weixin.qq.com/cgi-bin/webhook/send?key=dummy_key Dingtalk Hook To do this you need to follow the Dingtalk Webhooks guide and create new Webhooks. For security issue, you can config optional secret for individual webhook url. The alarm message will send through HTTP post by application/json content type if you configured Dingtalk Webhooks as following:\ndingtalkHooks: textTemplate: |-{ \u0026#34;msgtype\u0026#34;: \u0026#34;text\u0026#34;, \u0026#34;text\u0026#34;: { \u0026#34;content\u0026#34;: \u0026#34;Apache SkyWalking Alarm: \\n %s.\u0026#34; } } webhooks: - url: https://oapi.dingtalk.com/robot/send?access_token=dummy_token secret: dummysecret Feishu Hook To do this you need to follow the Feishu Webhooks guide and create new Webhooks. For security issue, you can config optional secret for individual webhook url. if you want to at someone, you can config ats which is the feishu\u0026rsquo;s user_id and separated by \u0026ldquo;,\u0026rdquo; . The alarm message will send through HTTP post by application/json content type if you configured Feishu Webhooks as following:\nfeishuHooks: textTemplate: |-{ \u0026#34;msg_type\u0026#34;: \u0026#34;text\u0026#34;, \u0026#34;content\u0026#34;: { \u0026#34;text\u0026#34;: \u0026#34;Apache SkyWalking Alarm: \\n %s.\u0026#34; }, \u0026#34;ats\u0026#34;:\u0026#34;feishu_user_id_1,feishu_user_id_2\u0026#34; } webhooks: - url: https://open.feishu.cn/open-apis/bot/v2/hook/dummy_token secret: dummysecret Update the settings dynamically Since 6.5.0, the alarm settings can be updated dynamically at runtime by Dynamic Configuration, which will override the settings in alarm-settings.yml.\nIn order to determine that whether an alarm rule is triggered or not, SkyWalking needs to cache the metrics of a time window for each alarm rule, if any attribute (metrics-name, op, threshold, period, count, etc.) of a rule is changed, the sliding window will be destroyed and re-created, causing the alarm of this specific rule to restart again.\n","excerpt":"Alarm Alarm core is driven by a collection of rules, which are defined in config/alarm-settings.yml. …","ref":"/docs/main/v8.4.0/en/setup/backend/backend-alarm/","title":"Alarm"},{"body":"Apache SkyWalking committer SkyWalking Project Management Committee(PMC) takes the responsibilities to assess the contributions of candidates.\nIn the SkyWalking, like many Apache projects, we treat contributions including, but not limited to, code contributions. Such as writing blog, guiding new users, give public speak, prompting project in various ways, are all treated as significant contributions.\nCommitter New committer nomination In the SkyWalking, new committer nomination could only be started by existing PMC members officially. The new contributor could contact any existing PMC member if he/she feels he/she is qualified. Talk with the PMC member, if some members agree, they could start the process.\nThe following steps are recommended, and could only be started by existing PMC member.\n Send the [DISCUSS] Promote xxx as new committer mail to private@skywalking.a.o. List the important contributions of the candidates, in order to help the PMC members supporting your proposal. Keep discussion open in more than 3 days, but not more than 1 week, unless there is any explicit objection or concern. Send the [VOTE] Promote xxx as new committer mail to private@skywalking.a.o, when the PMC seems to agree the proposal. Keep vote more than 3 days, but not more than 1 week. Consider the result as Consensus Approval if there 3 +1 votes and +1 votes \u0026gt; -1 votes Send the [RESULT][VOTE] Promote xxx as new committer mail to private@skywalking.a.o, and list the vote detail including the voters.  Invite new committer The PMC member, who start the promotion, takes the responsibilities to send the invitation to new committer and guide him/her to set up the ASF env.\nYou should send the mail like the following template to new committer\nTo: JoeBloggs@foo.net Cc: private@skywalking.apache.org Subject: Invitation to become SkyWalking committer: Joe Bloggs Hello [invitee name], The SkyWalking Project Management Committee] (PMC) hereby offers you committer privileges to the project . These privileges are offered on the understanding that you'll use them reasonably and with common sense. We like to work on trust rather than unnecessary constraints. Being a committer enables you to more easily make changes without needing to go through the patch submission process. Being a committer does not require you to participate any more than you already do. It does tend to make one even more committed. You will probably find that you spend more time here. Of course, you can decline and instead remain as a contributor, participating as you do now. A. This personal invitation is a chance for you to accept or decline in private. Either way, please let us know in reply to the [private@skywalking.apache.org] address only. B. If you accept, the next step is to register an iCLA: 1. Details of the iCLA and the forms are found through this link: http://www.apache.org/licenses/#clas 2. Instructions for its completion and return to the Secretary of the ASF are found at http://www.apache.org/licenses/#submitting 3. When you transmit the completed iCLA, request to notify the Apache SkyWalking and choose a unique Apache id. Look to see if your preferred id is already taken at http://people.apache.org/committer-index.html This will allow the Secretary to notify the PMC when your iCLA has been recorded. When recording of your iCLA is noticed, you will receive a follow-up message with the next steps for establishing you as a committer. Invitation acceptance process And the new committer should reply the mail to private@skywalking.apache.org(Choose reply all), and express the will to accept the invitation explicitly. Then this invitation will be treated as accepted by project PMC. Of course, the new committer could just say NO, and reject the invitation.\nIf they accepted, then they need to do the following things.\n Make sure they have subscribed the dev@skywalking.apache.org. Usually they already have. Choose a Apache ID that is not in the apache committers list page. Download the ICLA (If they are going to contribute to the project as day job, CCLA is expected). After filling the icla.pdf (or ccla.pdf) with information correctly, print, sign it manually (by hand), scan it as an pdf, and send it in mail as an attachment to the secretary@apache.org. (If they prefer to sign electronically, please follow the steps of this page) Then the PMC will wait the Apache secretary confirmed the ICLA (or CCLA) filed. The new committer and PMC will receive the mail like following  Dear XXX, This message acknowledges receipt of your ICLA, which has been filed in the Apache Software Foundation records. Your account has been requested for you and you should receive email with next steps within the next few days (can take up to a week). Please refer to https://www.apache.org/foundation/how-it-works.html#developers for more information about roles at Apache. If in some case, the account has not be requested(rarely to see), the PMC member should contact the project V.P.. The V.P. could request through the Apache Account Submission Helper Form.\nAfter several days, the new committer will receive the account created mail, as this title, Welcome to the Apache Software Foundation (ASF)!. At this point, congratulate! You have the official Apache ID.\nThe PMC member should add the new committer to official committer list through roster.\nSet up the Apache ID and dev env  Go to Apache Account Utility Platform, initial your password, set up your personal mailbox(Forwarding email address) and GitHub account(Your GitHub Username). An organisational invite will be sent to you via email shortly thereafter (within 2 hours). If you want to use xxx@apache.org to send mail, please refer to here. Gmail is recommended, because in other mailbox service settings, this forwarding mode is not easy to find. Following the authorized GitHub 2FA wiki to enable two-factors authorization (2FA) on github. When you set 2FA to \u0026ldquo;off\u0026rdquo;, it will be delisted by the corresponding Apache committer write permission group until you set it up again. (NOTE: Treat your recovery codes with the same level of attention as you would your password !) Use GitBox Account Linking Utility to obtain write permission of the SkyWalking project. Follow this doc to update the website.  If you want others could see you are in the Apache GitHub org, you need to go to Apache GitHub org people page, search for yourself, and choose Organization visibility to Public.\nCommitter rights, duties and responsibilities SkyWalking project doesn\u0026rsquo;t require the continue contributions after you become a committer, but we hope and truly want you could.\nBeing a committer, you could\n Review and merge the pull request to the master branch in the Apache repo. A pull request often contains multiple commits. Those commits must be squashed and merged into a single commit with explanatory comments. For new committer, we hope you could request some senior committer to recheck the pull request. Create and push codes to new branch in the Apache repo. Follow the Release process to process new release. Of course, you need to ask committer team to confirm it is the right time to release.  The PMC hope the new committer to take part in the release and release vote, even still be consider +1 no binding. But be familiar with the release is one of the key to be promoted as a PMC member.\nProject Management Committee Project Management Committee(PMC) member has no special rights in code contributions. They just cover and make sure the project following the Apache requirement, including\n Release binding vote and license check New committer and PMC member recognition Identify branding issue and do branding protection. Response the ASF board question, take necessary actions.  V.P. and chair of the PMC is the secretary, take responsibility of initializing the board report.\nIn the normal case, the new PMC member should be nominated from committer team. But becoming a PMC member directly is not forbidden, if the PMC could agree and be confidence that the candidate is ready, such as he/she has been a PMC member of another project, Apache member or Apache officer.\nThe process of new PMC vote should also follow the same [DISCUSS], [VOTE] and [RESULT][VOTE] in private mail list as new committer vote. One more step before sending the invitation, the PMC need to send NOTICE mail to Apache board.\nTo: board@apache.org Cc: private@skywalking.apache.org Subject: [NOTICE] Jane Doe for SkyWalking PMC SkyWalking proposes to invite Jane Doe (janedoe) to join the PMC. (include if a vote was held) The vote result is available here: https://lists.apache.org/... After 72 hours, if the board doesn\u0026rsquo;t object(usually it wouldn\u0026rsquo;t be), send the invitation.\nAfter the committer accepted the invitation, The PMC member should add the new committer to official PMC list through roster.\n","excerpt":"Apache SkyWalking committer SkyWalking Project Management Committee(PMC) takes the responsibilities …","ref":"/docs/main/v8.4.0/en/guides/asf/committer/","title":"Apache SkyWalking committer"},{"body":"Apdex threshold Apdex is a measure of response time based against a set threshold. It measures the ratio of satisfactory response times to unsatisfactory response times. The response time is measured from an asset request to completed delivery back to the requestor.\nA user defines a response time threshold T. All responses handled in T or less time satisfy the user.\nFor example, if T is 1.2 seconds and a response completes in 0.5 seconds, then the user is satisfied. All responses greater than 1.2 seconds dissatisfy the user. Responses greater than 4.8 seconds frustrate the user.\nThe apdex threshold T can be configured in service-apdex-threshold.yml file or via Dynamic Configuration. The default item will be apply to a service isn\u0026rsquo;t defined in this configuration as the default threshold.\nConfiguration Format The configuration content includes the service' names and their threshold:\n# default threshold is 500ms default: 500 # example: # the threshold of service \u0026#34;tomcat\u0026#34; is 1s # tomcat: 1000 # the threshold of service \u0026#34;springboot1\u0026#34; is 50ms # springboot1: 50 ","excerpt":"Apdex threshold Apdex is a measure of response time based against a set threshold. It measures the …","ref":"/docs/main/v8.4.0/en/setup/backend/apdex-threshold/","title":"Apdex threshold"},{"body":"Backend setup First and most important thing is, SkyWalking backend startup behaviours are driven by config/application.yml. Understood the setting file will help you to read this document.\nStartup script The default startup scripts are /bin/oapService.sh(.bat). Read start up mode document to know other options of starting backend.\napplication.yml The core concept behind this setting file is, SkyWalking collector is based on pure modularization design. End user can switch or assemble the collector features by their own requirements.\nSo, in application.yml, there are three levels.\n Level 1, module name. Meaning this module is active in running mode. Level 2, provider option list and provider selector. Available providers are listed here with a selector to indicate which one will actually take effect, if there is only one provider listed, the selector is optional and can be omitted. Level 3. settings of the provider.  Example:\nstorage: selector: mysql # the mysql storage will actually be activated, while the h2 storage takes no effect h2: driver: ${SW_STORAGE_H2_DRIVER:org.h2.jdbcx.JdbcDataSource} url: ${SW_STORAGE_H2_URL:jdbc:h2:mem:skywalking-oap-db} user: ${SW_STORAGE_H2_USER:sa} metadataQueryMaxSize: ${SW_STORAGE_H2_QUERY_MAX_SIZE:5000} mysql: properties: jdbcUrl: ${SW_JDBC_URL:\u0026#34;jdbc:mysql://localhost:3306/swtest\u0026#34;} dataSource.user: ${SW_DATA_SOURCE_USER:root} dataSource.password: ${SW_DATA_SOURCE_PASSWORD:root@1234} dataSource.cachePrepStmts: ${SW_DATA_SOURCE_CACHE_PREP_STMTS:true} dataSource.prepStmtCacheSize: ${SW_DATA_SOURCE_PREP_STMT_CACHE_SQL_SIZE:250} dataSource.prepStmtCacheSqlLimit: ${SW_DATA_SOURCE_PREP_STMT_CACHE_SQL_LIMIT:2048} dataSource.useServerPrepStmts: ${SW_DATA_SOURCE_USE_SERVER_PREP_STMTS:true} metadataQueryMaxSize: ${SW_STORAGE_MYSQL_QUERY_MAX_SIZE:5000} # other configurations  storage is the module. selector selects one out of the all providers listed below, the unselected ones take no effect as if they were deleted. default is the default implementor of core module. driver, url, \u0026hellip; metadataQueryMaxSize are all setting items of the implementor.  At the same time, modules includes required and optional, the required modules provide the skeleton of backend, even modularization supported pluggable, removing those modules are meaningless, for optional modules, some of them have a provider implementation called none, meaning it only provides a shell with no actual logic, typically such as telemetry. Setting - to the selector means this whole module will be excluded at runtime. We highly recommend you don\u0026rsquo;t try to change APIs of those modules, unless you understand SkyWalking project and its codes very well.\nList the required modules here\n Core. Do basic and major skeleton of all data analysis and stream dispatch. Cluster. Manage multiple backend instances in a cluster, which could provide high throughputs process capabilities. Storage. Make the analysis result persistence. Query. Provide query interfaces to UI.  For Cluster and Storage have provided multiple implementors(providers), see Cluster management and Choose storage documents in the link list.\nAlso, several receiver modules are provided. Receiver is the module in charge of accepting incoming data requests to backend. Most(all) provide service by some network(RPC) protocol, such as gRPC, HTTPRestful.\nThe receivers have many different module names, you could read Set receivers document in the link list.\nConfiguration Vocabulary All available configurations in application.yml could be found in Configuration Vocabulary.\nAdvanced feature document link list After understand the setting file structure, you could choose your interesting feature document. We recommend you to read the feature documents in our following order.\n Overriding settings in application.yml is supported IP and port setting. Introduce how IP and port set and be used. Backend init mode startup. How to init the environment and exit graciously. Read this before you try to initial a new cluster. Cluster management. Guide you to set backend server in cluster mode. Deploy in kubernetes. Guide you to build and use SkyWalking image, and deploy in k8s. Choose storage. As we know, in default quick start, backend is running with H2 DB. But clearly, it doesn\u0026rsquo;t fit the product env. In here, you could find what other choices do you have. Choose the ones you like, we are also welcome anyone to contribute new storage implementor. Set receivers. You could choose receivers by your requirements, most receivers are harmless, at least our default receivers are. You would set and active all receivers provided. Open fetchers. You could open different fetchers to read metrics from the target applications. These ones work like receivers, but in pulling mode, typically like Prometheus. Token authentication. You could add token authentication mechanisms to avoid OAP receiving untrusted data. Do trace sampling at backend. This sample keep the metrics accurate, only don\u0026rsquo;t save some of traces in storage based on rate. Follow slow DB statement threshold config document to understand that, how to detect the Slow database statements(including SQL statements) in your system. Official OAL scripts. As you have known from our OAL introduction, most of backend analysis capabilities based on the scripts. Here is the description of official scripts, which helps you to understand which metrics data are in process, also could be used in alarm. Alarm. Alarm provides a time-series based check mechanism. You could set alarm rules targeting the analysis oal metrics objects. Advanced deployment options. If you want to deploy backend in very large scale and support high payload, you may need this. Metrics exporter. Use metrics data exporter to forward metrics data to 3rd party system. Time To Live (TTL). Metrics and trace are time series data, TTL settings affect the expired time of them. Dynamic Configuration. Make configuration of OAP changed dynamic, from remote service or 3rd party configuration management system. Uninstrumented Gateways. Configure gateways/proxies that are not supported by SkyWalking agent plugins, to reflect the delegation in topology graph. Apdex threshold. Configure the thresholds for different services if Apdex calculation is activated in the OAL. Service Grouping. An automatic grouping mechanism for all services based on name. Group Parameterized Endpoints. Configure the grouping rules for parameterized endpoints, to improve the meaning of the metrics. OpenTelemetry Metrics Analysis. Activate built-in configurations to convert the metrics forwarded from OpenTelemetry collector. And learn how to write your own conversion rules. Meter Analysis. Set up the backend analysis rules, when use SkyWalking Meter System Toolkit or meter plugins. Spring Sleuth Metrics Analysis. Configure the agent and backend to receiver metrics from micrometer.  Telemetry for backend OAP backend cluster itself underlying is a distributed streaming process system. For helping the Ops team, we provide the telemetry for OAP backend itself. Follow document to use it.\nAt the same time, we provide Health Check to get a score for the health status.\n 0 means healthy, more than 0 means unhealthy and less than 0 means oap doesn\u0026rsquo;t startup.\n FAQs When and why do we need to set Timezone? SkyWalking provides downsampling time series metrics features. Query and storage at each time dimension(minute, hour, day, month metrics indexes) related to timezone when doing time format.\nFor example, metrics time will be formatted like YYYYMMDDHHmm in minute dimension metrics, which format process is timezone related.\nIn default, SkyWalking OAP backend choose the OS default timezone. If you want to override it, please follow Java and OS documents to do so.\nHow to query the storage directly from 3rd party tool? SkyWalking provides browser UI, CLI and GraphQL ways to support extensions. But some users may have the idea to query data directly from the storage. Such as in ElasticSearch case, Kibana is a great tool to do this.\nIn default, due to reduce memory, network and storage space usages, SkyWalking saves based64-encoded id(s) only in the metrics entities. But these tools usually don\u0026rsquo;t support nested query, or don\u0026rsquo;t work conveniently. In this special case, SkyWalking provide a config to add all necessary name column(s) into the final metrics entities with ID as a trade-off.\nTake a look at core/default/activeExtraModelColumns config in the application.yaml, and set it as true to open this feature.\nThis feature wouldn\u0026rsquo;t provide any new feature to the native SkyWalking scenarios, just for the 3rd party integration.\n","excerpt":"Backend setup First and most important thing is, SkyWalking backend startup behaviours are driven by …","ref":"/docs/main/v8.4.0/en/setup/backend/backend-setup/","title":"Backend setup"},{"body":"Backend storage SkyWalking storage is pluggable, we have provided the following storage solutions, you could easily use one of them by specifying it as the selector in the application.yml：\nstorage: selector: ${SW_STORAGE:elasticsearch7} Native supported storage\n H2 ElasticSearch 6, 7 MySQL TiDB InfluxDB  Redistribution version with supported storage.\n ElasticSearch 5  H2 Active H2 as storage, set storage provider to H2 In-Memory Databases. Default in distribution package. Please read Database URL Overview in H2 official document, you could set the target to H2 in Embedded, Server and Mixed modes.\nSetting fragment example\nstorage: selector: ${SW_STORAGE:h2} h2: driver: org.h2.jdbcx.JdbcDataSource url: jdbc:h2:mem:skywalking-oap-db user: sa ElasticSearch  In order to activate ElasticSearch 6 as storage, set storage provider to elasticsearch In order to activate ElasticSearch 7 as storage, set storage provider to elasticsearch7  Required ElasticSearch 6.3.2 or higher. HTTP RestHighLevelClient is used to connect server.\n For ElasticSearch 6.3.2 ~ 7.0.0 (excluded), please download the apache-skywalking-bin.tar.gz or apache-skywalking-bin.zip, For ElasticSearch 7.0.0 ~ 8.0.0 (excluded), please download the apache-skywalking-bin-es7.tar.gz or apache-skywalking-bin-es7.zip.  For now, ElasticSearch 6 and ElasticSearch 7 share the same configurations, as follows:\nstorage: selector: ${SW_STORAGE:elasticsearch} elasticsearch: nameSpace: ${SW_NAMESPACE:\u0026#34;\u0026#34;} clusterNodes: ${SW_STORAGE_ES_CLUSTER_NODES:localhost:9200} protocol: ${SW_STORAGE_ES_HTTP_PROTOCOL:\u0026#34;http\u0026#34;} trustStorePath: ${SW_STORAGE_ES_SSL_JKS_PATH:\u0026#34;\u0026#34;} trustStorePass: ${SW_STORAGE_ES_SSL_JKS_PASS:\u0026#34;\u0026#34;} user: ${SW_ES_USER:\u0026#34;\u0026#34;} password: ${SW_ES_PASSWORD:\u0026#34;\u0026#34;} secretsManagementFile: ${SW_ES_SECRETS_MANAGEMENT_FILE:\u0026#34;\u0026#34;} # Secrets management file in the properties format includes the username, password, which are managed by 3rd party tool. dayStep: ${SW_STORAGE_DAY_STEP:1} # Represent the number of days in the one minute/hour/day index. indexShardsNumber: ${SW_STORAGE_ES_INDEX_SHARDS_NUMBER:1} # Shard number of new indexes indexReplicasNumber: ${SW_STORAGE_ES_INDEX_REPLICAS_NUMBER:1} # Replicas number of new indexes # Super data set has been defined in the codes, such as trace segments.The following 3 config would be improve es performance when storage super size data in es. superDatasetDayStep: ${SW_SUPERDATASET_STORAGE_DAY_STEP:-1} # Represent the number of days in the super size dataset record index, the default value is the same as dayStep when the value is less than 0 superDatasetIndexShardsFactor: ${SW_STORAGE_ES_SUPER_DATASET_INDEX_SHARDS_FACTOR:5} # This factor provides more shards for the super data set, shards number = indexShardsNumber * superDatasetIndexShardsFactor. Also, this factor effects Zipkin and Jaeger traces. superDatasetIndexReplicasNumber: ${SW_STORAGE_ES_SUPER_DATASET_INDEX_REPLICAS_NUMBER:0} # Represent the replicas number in the super size dataset record index, the default value is 0. bulkActions: ${SW_STORAGE_ES_BULK_ACTIONS:1000} # Execute the async bulk record data every ${SW_STORAGE_ES_BULK_ACTIONS} requests syncBulkActions: ${SW_STORAGE_ES_SYNC_BULK_ACTIONS:50000} # Execute the sync bulk metrics data every ${SW_STORAGE_ES_SYNC_BULK_ACTIONS} requests flushInterval: ${SW_STORAGE_ES_FLUSH_INTERVAL:10} # flush the bulk every 10 seconds whatever the number of requests concurrentRequests: ${SW_STORAGE_ES_CONCURRENT_REQUESTS:2} # the number of concurrent requests resultWindowMaxSize: ${SW_STORAGE_ES_QUERY_MAX_WINDOW_SIZE:10000} metadataQueryMaxSize: ${SW_STORAGE_ES_QUERY_MAX_SIZE:5000} segmentQueryMaxSize: ${SW_STORAGE_ES_QUERY_SEGMENT_SIZE:200} profileTaskQueryMaxSize: ${SW_STORAGE_ES_QUERY_PROFILE_TASK_SIZE:200} oapAnalyzer: ${SW_STORAGE_ES_OAP_ANALYZER:\u0026#34;{\\\u0026#34;analyzer\\\u0026#34;:{\\\u0026#34;oap_analyzer\\\u0026#34;:{\\\u0026#34;type\\\u0026#34;:\\\u0026#34;stop\\\u0026#34;}}}\u0026#34;} # the oap analyzer. oapLogAnalyzer: ${SW_STORAGE_ES_OAP_LOG_ANALYZER:\u0026#34;{\\\u0026#34;analyzer\\\u0026#34;:{\\\u0026#34;oap_log_analyzer\\\u0026#34;:{\\\u0026#34;type\\\u0026#34;:\\\u0026#34;standard\\\u0026#34;}}}\u0026#34;} # the oap log analyzer. It could be customized by the ES analyzer configuration to support more language log formats, such as Chinese log, Japanese log and etc. advanced: ${SW_STORAGE_ES_ADVANCED:\u0026#34;\u0026#34;} ElasticSearch 6 With Https SSL Encrypting communications. example:\nstorage: selector: ${SW_STORAGE:elasticsearch} elasticsearch: # nameSpace: ${SW_NAMESPACE:\u0026#34;\u0026#34;} user: ${SW_ES_USER:\u0026#34;\u0026#34;} # User needs to be set when Http Basic authentication is enabled password: ${SW_ES_PASSWORD:\u0026#34;\u0026#34;} # Password to be set when Http Basic authentication is enabled clusterNodes: ${SW_STORAGE_ES_CLUSTER_NODES:localhost:443} trustStorePath: ${SW_SW_STORAGE_ES_SSL_JKS_PATH:\u0026#34;../es_keystore.jks\u0026#34;} trustStorePass: ${SW_SW_STORAGE_ES_SSL_JKS_PASS:\u0026#34;\u0026#34;} protocol: ${SW_STORAGE_ES_HTTP_PROTOCOL:\u0026#34;https\u0026#34;} ...  File at trustStorePath is being monitored, once it is changed, the ElasticSearch client will do reconnecting. trustStorePass could be changed on the runtime through Secrets Management File Of ElasticSearch Authentication.  Daily Index Step Daily index step(storage/elasticsearch/dayStep, default 1) represents the index creation period. In this period, several days(dayStep value)' metrics are saved.\nMostly, users don\u0026rsquo;t need to change the value manually. As SkyWalking is designed to observe large scale distributed system. But in some specific cases, users want to set a long TTL value, such as more than 60 days, but their ElasticSearch cluster isn\u0026rsquo;t powerful due to the low traffic in the production environment. This value could be increased to 5(or more), if users could make sure single one index could support these days(5 in this case) metrics and traces.\nSuch as, if dayStep == 11,\n data in [2000-01-01, 2000-01-11] will be merged into the index-20000101. data in [2000-01-12, 2000-01-22] will be merged into the index-20000112.  storage/elasticsearch/superDatasetDayStep override the storage/elasticsearch/dayStep if the value is positive. This would affect the record related entities, such as the trace segment. In some cases, the size of metrics is much less than the record(trace), this would help the shards balance in the ElasticSearch cluster.\nNOTICE, TTL deletion would be affected by these. You should set an extra more dayStep in your TTL. Such as you want to TTL == 30 days and dayStep == 10, you actually need to set TTL = 40;\nSecrets Management File Of ElasticSearch Authentication The value of secretsManagementFile should point to the secrets management file absolute path. The file includes username, password and JKS password of ElasticSearch server in the properties format.\nuser=xxx password=yyy trustStorePass=zzz The major difference between using user, password, trustStorePass configs in the application.yaml file is, the Secrets Management File is being watched by the OAP server. Once it is changed manually or through 3rd party tool, such as Vault, the storage provider will use the new username, password and JKS password to establish the connection and close the old one. If the information exist in the file, the user/password will be overrided.\nAdvanced Configurations For Elasticsearch Index You can add advanced configurations in JSON format to set ElasticSearch index settings by following ElasticSearch doc\nFor example, set translog settings:\nstorage: elasticsearch: # ...... advanced: ${SW_STORAGE_ES_ADVANCED:\u0026#34;{\\\u0026#34;index.translog.durability\\\u0026#34;:\\\u0026#34;request\\\u0026#34;,\\\u0026#34;index.translog.sync_interval\\\u0026#34;:\\\u0026#34;5s\\\u0026#34;}\u0026#34;} Recommended ElasticSearch server-side configurations You could add following config to elasticsearch.yml, set the value based on your env.\n# In tracing scenario, consider to set more than this at least. thread_pool.index.queue_size: 1000 # Only suitable for ElasticSearch 6 thread_pool.write.queue_size: 1000 # Suitable for ElasticSearch 6 and 7 # When you face query error at trace page, remember to check this. index.max_result_window: 1000000 We strongly advice you to read more about these configurations from ElasticSearch official document. This effects the performance of ElasticSearch very much.\nElasticSearch 6 with Zipkin trace extension This implementation shares most of elasticsearch, just extend to support zipkin span storage. It has all same configs.\nstorage: selector: ${SW_STORAGE:zipkin-elasticsearch} zipkin-elasticsearch: nameSpace: ${SW_NAMESPACE:\u0026#34;\u0026#34;} clusterNodes: ${SW_STORAGE_ES_CLUSTER_NODES:localhost:9200} protocol: ${SW_STORAGE_ES_HTTP_PROTOCOL:\u0026#34;http\u0026#34;} user: ${SW_ES_USER:\u0026#34;\u0026#34;} password: ${SW_ES_PASSWORD:\u0026#34;\u0026#34;} indexShardsNumber: ${SW_STORAGE_ES_INDEX_SHARDS_NUMBER:2} indexReplicasNumber: ${SW_STORAGE_ES_INDEX_REPLICAS_NUMBER:0} # Batch process setting, refer to https://www.elastic.co/guide/en/elasticsearch/client/java-api/5.5/java-docs-bulk-processor.html bulkActions: ${SW_STORAGE_ES_BULK_ACTIONS:2000} # Execute the bulk every 2000 requests bulkSize: ${SW_STORAGE_ES_BULK_SIZE:20} # flush the bulk every 20mb flushInterval: ${SW_STORAGE_ES_FLUSH_INTERVAL:10} # flush the bulk every 10 seconds whatever the number of requests concurrentRequests: ${SW_STORAGE_ES_CONCURRENT_REQUESTS:2} # the number of concurrent requests ElasticSearch 6 with Jaeger trace extension This implementation shares most of elasticsearch, just extend to support jaeger span storage. It has all same configs.\nstorage: selector: ${SW_STORAGE:jaeger-elasticsearch} jaeger-elasticsearch: nameSpace: ${SW_NAMESPACE:\u0026#34;\u0026#34;} clusterNodes: ${SW_STORAGE_ES_CLUSTER_NODES:localhost:9200} protocol: ${SW_STORAGE_ES_HTTP_PROTOCOL:\u0026#34;http\u0026#34;} user: ${SW_ES_USER:\u0026#34;\u0026#34;} password: ${SW_ES_PASSWORD:\u0026#34;\u0026#34;} indexShardsNumber: ${SW_STORAGE_ES_INDEX_SHARDS_NUMBER:2} indexReplicasNumber: ${SW_STORAGE_ES_INDEX_REPLICAS_NUMBER:0} # Batch process setting, refer to https://www.elastic.co/guide/en/elasticsearch/client/java-api/5.5/java-docs-bulk-processor.html bulkActions: ${SW_STORAGE_ES_BULK_ACTIONS:2000} # Execute the bulk every 2000 requests bulkSize: ${SW_STORAGE_ES_BULK_SIZE:20} # flush the bulk every 20mb flushInterval: ${SW_STORAGE_ES_FLUSH_INTERVAL:10} # flush the bulk every 10 seconds whatever the number of requests concurrentRequests: ${SW_STORAGE_ES_CONCURRENT_REQUESTS:2} # the number of concurrent requests About Namespace When namespace is set, names of all indexes in ElasticSearch will use it as prefix.\nMySQL Active MySQL as storage, set storage provider to mysql.\nNOTICE: MySQL driver is NOT allowed in Apache official distribution and source codes. Please download MySQL driver by yourself. Copy the connection driver jar to oap-libs.\nstorage: selector: ${SW_STORAGE:mysql} mysql: properties: jdbcUrl: ${SW_JDBC_URL:\u0026#34;jdbc:mysql://localhost:3306/swtest\u0026#34;} dataSource.user: ${SW_DATA_SOURCE_USER:root} dataSource.password: ${SW_DATA_SOURCE_PASSWORD:root@1234} dataSource.cachePrepStmts: ${SW_DATA_SOURCE_CACHE_PREP_STMTS:true} dataSource.prepStmtCacheSize: ${SW_DATA_SOURCE_PREP_STMT_CACHE_SQL_SIZE:250} dataSource.prepStmtCacheSqlLimit: ${SW_DATA_SOURCE_PREP_STMT_CACHE_SQL_LIMIT:2048} dataSource.useServerPrepStmts: ${SW_DATA_SOURCE_USE_SERVER_PREP_STMTS:true} metadataQueryMaxSize: ${SW_STORAGE_MYSQL_QUERY_MAX_SIZE:5000} All connection related settings including link url, username and password are in application.yml. Here are some of the settings, please follow HikariCP connection pool document for all the settings.\nTiDB Tested TiDB Server 4.0.8 version and Mysql Client driver 8.0.13 version currently. Active TiDB as storage, set storage provider to tidb.\nstorage: selector: ${SW_STORAGE:tidb} tidb: properties: jdbcUrl: ${SW_JDBC_URL:\u0026#34;jdbc:mysql://localhost:4000/swtest\u0026#34;} dataSource.user: ${SW_DATA_SOURCE_USER:root} dataSource.password: ${SW_DATA_SOURCE_PASSWORD:\u0026#34;\u0026#34;} dataSource.cachePrepStmts: ${SW_DATA_SOURCE_CACHE_PREP_STMTS:true} dataSource.prepStmtCacheSize: ${SW_DATA_SOURCE_PREP_STMT_CACHE_SQL_SIZE:250} dataSource.prepStmtCacheSqlLimit: ${SW_DATA_SOURCE_PREP_STMT_CACHE_SQL_LIMIT:2048} dataSource.useServerPrepStmts: ${SW_DATA_SOURCE_USE_SERVER_PREP_STMTS:true} dataSource.useAffectedRows: ${SW_DATA_SOURCE_USE_AFFECTED_ROWS:true} metadataQueryMaxSize: ${SW_STORAGE_MYSQL_QUERY_MAX_SIZE:5000} maxSizeOfArrayColumn: ${SW_STORAGE_MAX_SIZE_OF_ARRAY_COLUMN:20} numOfSearchableValuesPerTag: ${SW_STORAGE_NUM_OF_SEARCHABLE_VALUES_PER_TAG:2} All connection related settings including link url, username and password are in application.yml. These settings can refer to the configuration of MySQL above.\nInfluxDB InfluxDB storage provides a time-series database as a new storage option.\nstorage: selector: ${SW_STORAGE:influxdb} influxdb: url: ${SW_STORAGE_INFLUXDB_URL:http://localhost:8086} user: ${SW_STORAGE_INFLUXDB_USER:root} password: ${SW_STORAGE_INFLUXDB_PASSWORD:} database: ${SW_STORAGE_INFLUXDB_DATABASE:skywalking} actions: ${SW_STORAGE_INFLUXDB_ACTIONS:1000} # the number of actions to collect duration: ${SW_STORAGE_INFLUXDB_DURATION:1000} # the time to wait at most (milliseconds) fetchTaskLogMaxSize: ${SW_STORAGE_INFLUXDB_FETCH_TASK_LOG_MAX_SIZE:5000} # the max number of fetch task log in a request All connection related settings including link url, username and password are in application.yml. The Metadata storage provider settings can refer to the configuration of H2/MySQL above.\nElasticSearch 5 ElasticSearch 5 is incompatible with ElasticSearch 6 Java client jar, so it could not be included in native distribution. OpenSkyWalking/SkyWalking-With-Es5x-Storage repo includes the distribution version.\nMore storage solution extension Follow Storage extension development guide in Project Extensions document in development guide.\n","excerpt":"Backend storage SkyWalking storage is pluggable, we have provided the following storage solutions, …","ref":"/docs/main/v8.4.0/en/setup/backend/backend-storage/","title":"Backend storage"},{"body":"Backend, UI, and CLI setup SkyWalking backend distribution package includes the following parts:\n  bin/cmd scripts, in /bin folder. Includes startup linux shell and Windows cmd scripts for Backend server and UI startup.\n  Backend config, in /config folder. Includes settings files of the backend, which are:\n application.yml log4j.xml alarm-settings.yml    Libraries of backend, in /oap-libs folder. All the dependencies of the backend are in it.\n  Webapp env, in webapp folder. UI frontend jar file is here, with its webapp.yml setting file.\n  Quick start Requirements and default settings Requirement: JDK8 to JDK12 are tested, other versions are not tested and may or may not work.\nBefore you start, you should know that the quickstart aims to get you a basic configuration mostly for previews/demo, performance and long-term running are not our goals.\nFor production/QA/tests environments, you should head to Backend and UI deployment documents.\nYou can use bin/startup.sh (or cmd) to startup the backend and UI with their default settings, which are:\n Backend storage uses H2 by default (for an easier start) Backend listens 0.0.0.0/11800 for gRPC APIs and 0.0.0.0/12800 for http rest APIs.  In Java, .NetCore, Node.js, Istio agents/probe, you should set the gRPC service address to ip/host:11800, with ip/host where your backend is.\n UI listens on 8080 port and request 127.0.0.1/12800 to do GraphQL query.  Deploy Backend and UI Before deploying Skywalking in your distributed environment, you should know how agents/probes, backend, UI communicates with each other:\n All native agents and probes, either language based or mesh probe, are using gRPC service (core/default/gRPC* in application.yml) to report data to the backend. Also, jetty service supported in JSON format. UI uses GraphQL (HTTP) query to access the backend also in Jetty service (core/default/rest* in application.yml).  Now, let\u0026rsquo;s continue with the backend, UI and CLI setting documents.\nBackend setup document UI setup document CLI set up document ","excerpt":"Backend, UI, and CLI setup SkyWalking backend distribution package includes the following parts: …","ref":"/docs/main/v8.4.0/en/setup/backend/backend-ui-setup/","title":"Backend, UI, and CLI setup"},{"body":"Browser Protocol Browser protocol describes the data format between skywalking-client-js and backend.\nOverview Browser protocol is defined and provided in gRPC format, also implemented in HTTP 1.1\nSend performance data and error log You can send performance data and error logs via the following services:\n BrowserPerfService#collectPerfData for performance data format. BrowserPerfService#collectErrorLogs for error log format.  For error log format, there are some notices\n BrowserErrorLog#uniqueId should be unique in the whole distributed environments.  ","excerpt":"Browser Protocol Browser protocol describes the data format between skywalking-client-js and …","ref":"/docs/main/v8.4.0/en/protocols/browser-protocol/","title":"Browser Protocol"},{"body":"CDS - Configuration Discovery Service CDS - Configuration Discovery Service provides the dynamic configuration for the agent, defined in gRPC.\nConfiguration Format The configuration content includes the service name and their configs. The\nconfigurations: //service name serviceA: // Configurations of service A // Key and Value are determined by the agent side. // Check the agent setup doc for all available configurations. key1: value1 key2: value2 ... serviceB: ... Available key(s) and value(s) in Java Agent. Java agent supports the following dynamic configurations.\n   Config Key Value Description Value Format Example Required Plugin(s)     agent.sample_n_per_3_secs The number of sampled traces per 3 seconds -1 -   agent.ignore_suffix If the operation name of the first span is included in this set, this segment should be ignored. Multiple values should be separated by , .txt,.log -   agent.trace.ignore_path The value is the path that you need to ignore, multiple paths should be separated by , more details /your/path/1/**,/your/path/2/** apm-trace-ignore-plugin     Required plugin(s), the configuration affects only when the required plugins activated.  ","excerpt":"CDS - Configuration Discovery Service CDS - Configuration Discovery Service provides the dynamic …","ref":"/docs/main/v8.4.0/en/setup/service-agent/java-agent/configuration-discovery/","title":"CDS - Configuration Discovery Service"},{"body":"Choose receiver Receiver is a concept in SkyWalking backend. All modules, which are responsible for receiving telemetry or tracing data from other being monitored system, are all being called Receiver. If you are looking for the pull mode, Take a look at fetcher document.\nWe have following receivers, and default implementors are provided in our Apache distribution.\n receiver-trace. gRPC and HTTPRestful services to accept SkyWalking format traces. receiver-register. gRPC and HTTPRestful services to provide service, service instance and endpoint register. service-mesh. gRPC services accept data from inbound mesh probes. receiver-jvm. gRPC services accept JVM metrics data. envoy-metric. Envoy metrics_service and ALS(access log service) supported by this receiver. OAL script support all GAUGE type metrics. receiver-profile. gRPC services accept profile task status and snapshot reporter. receiver-otel. See details. Receiver for analyzing metrics data from OpenTelemetry receiver-meter. See details. Receiver for analyzing metrics in SkyWalking native meter format. receiver-browser. gRPC services to accept browser performance data and error log. receiver-log. gRPC services accept log data. configuration-discovery. gRPC services handle configurationDiscovery. Experimental receivers. All following receivers are in the POC stage, not production ready.  receiver_zipkin. See details. (Experimental) receiver_jaeger. See details. (Experimental)    The sample settings of these receivers should be already in default application.yml, and also list here\nreceiver-register: selector: ${SW_RECEIVER_REGISTER:default} default: receiver-trace: selector: ${SW_RECEIVER_TRACE:default} default: receiver-jvm: selector: ${SW_RECEIVER_JVM:default} default: service-mesh: selector: ${SW_SERVICE_MESH:default} default: envoy-metric: selector: ${SW_ENVOY_METRIC:default} default: acceptMetricsService: ${SW_ENVOY_METRIC_SERVICE:true} alsHTTPAnalysis: ${SW_ENVOY_METRIC_ALS_HTTP_ANALYSIS:\u0026#34;\u0026#34;} receiver_zipkin: selector: ${SW_RECEIVER_ZIPKIN:-} default: host: ${SW_RECEIVER_ZIPKIN_HOST:0.0.0.0} port: ${SW_RECEIVER_ZIPKIN_PORT:9411} contextPath: ${SW_RECEIVER_ZIPKIN_CONTEXT_PATH:/} jettyMinThreads: ${SW_RECEIVER_ZIPKIN_JETTY_MIN_THREADS:1} jettyMaxThreads: ${SW_RECEIVER_ZIPKIN_JETTY_MAX_THREADS:200} jettyIdleTimeOut: ${SW_RECEIVER_ZIPKIN_JETTY_IDLE_TIMEOUT:30000} jettyAcceptorPriorityDelta: ${SW_RECEIVER_ZIPKIN_JETTY_DELTA:0} jettyAcceptQueueSize: ${SW_RECEIVER_ZIPKIN_QUEUE_SIZE:0} receiver-profile: selector: ${SW_RECEIVER_PROFILE:default} default: receiver-browser: selector: ${SW_RECEIVER_BROWSER:default} default: sampleRate: ${SW_RECEIVER_BROWSER_SAMPLE_RATE:10000} receiver-log: selector: ${SW_RECEIVER_LOG:default} default: configuration-discovery: selector: ${SW_CONFIGURATION_DISCOVERY:default} default: gRPC/HTTP server for receiver In default, all gRPC/HTTP services should be served at core/gRPC and core/rest. But the receiver-sharing-server module provide a way to make all receivers serving at different ip:port, if you set them explicitly.\nreceiver-sharing-server: selector: ${SW_RECEIVER_SHARING_SERVER:default} default: host: ${SW_RECEIVER_JETTY_HOST:0.0.0.0} contextPath: ${SW_RECEIVER_JETTY_CONTEXT_PATH:/} authentication: ${SW_AUTHENTICATION:\u0026#34;\u0026#34;} jettyMinThreads: ${SW_RECEIVER_SHARING_JETTY_MIN_THREADS:1} jettyMaxThreads: ${SW_RECEIVER_SHARING_JETTY_MAX_THREADS:200} jettyIdleTimeOut: ${SW_RECEIVER_SHARING_JETTY_IDLE_TIMEOUT:30000} jettyAcceptorPriorityDelta: ${SW_RECEIVER_SHARING_JETTY_DELTA:0} jettyAcceptQueueSize: ${SW_RECEIVER_SHARING_JETTY_QUEUE_SIZE:0} Notice, if you add these settings, make sure they are not as same as core module, because gRPC/HTTP servers of core are still used for UI and OAP internal communications.\nOpenTelemetry receiver OpenTelemetry receiver supports to ingest agent metrics by meter-system. OAP can load the configuration at bootstrap. If the new configuration is not well-formed, OAP fails to start up. The files are located at $CLASSPATH/otel-\u0026lt;handler\u0026gt;-rules. Eg, the oc handler loads fules from $CLASSPATH/otel-oc-rules,\nSupported handlers: * oc: OpenCensus gRPC service handler.\nThe rule file should be in YAML format, defined by the scheme described in prometheus-fetcher. Notice, receiver-otel only support group, defaultMetricLevel and metricsRules nodes of scheme due to the push mode it opts to.\nTo active the oc handler and istio relevant rules:\nreceiver-otel: selector: ${SW_OTEL_RECEIVER:default} default: enabledHandlers: ${SW_OTEL_RECEIVER_ENABLED_HANDLERS:\u0026#34;oc\u0026#34;} enabledOcRules: ${SW_OTEL_RECEIVER_ENABLED_OC_RULES:\u0026#34;istio-controlplane\u0026#34;} The receiver adds labels with key = node_identifier_host_name and key = node_identifier_pid to the collected data samples， and values from Node.identifier.host_name and Node.identifier.pid defined in opencensus agent proto, to be the identification of the metric data.\n   Rule Name Description Configuration File Data Source     istio-controlplane Metrics of Istio control panel otel-oc-rules/istio-controlplane.yaml Istio Control Panel -\u0026gt; OpenTelemetry Collector \u0026ndash;OC format\u0026ndash;\u0026gt; SkyWalking OAP Server   oap Metrics of SkyWalking OAP server itself otel-oc-rules/oap.yaml SkyWalking OAP Server(SelfObservability) -\u0026gt; OpenTelemetry Collector \u0026ndash;OC format\u0026ndash;\u0026gt; SkyWalking OAP Server   vm Metrics of VMs otel-oc-rules/vm.yaml Prometheus node-exporter(VMs) -\u0026gt; OpenTelemetry Collector \u0026ndash;OC format\u0026ndash;\u0026gt; SkyWalking OAP Server    Meter receiver Meter receiver supports accept the metrics into the meter-system. OAP can load the configuration at bootstrap.\nThe file is written in YAML format, defined by the scheme described in backend-meter.\nTo active the default implementation:\nreceiver-meter: selector: ${SW_RECEIVER_METER:default} default: Experimental receivers All following receivers are in the POC stage, not production ready.\nZipkin receiver Zipkin receiver could work in two different mode.\n Tracing mode(default). Tracing mode is that, skywalking OAP acts like zipkin collector, fully supports Zipkin v1/v2 formats through HTTP service, also provide persistence and query in skywalking UI. But it wouldn\u0026rsquo;t analysis metrics from them. In most case, I suggest you could use this feature, when metrics come from service mesh. Notice, in this mode, Zipkin receiver requires zipkin-elasticsearch storage implementation active. Read this to know how to active.  Use following config to active.\nreceiver_zipkin: selector: ${SW_RECEIVER_ZIPKIN:-} default: host: ${SW_RECEIVER_ZIPKIN_HOST:0.0.0.0} port: ${SW_RECEIVER_ZIPKIN_PORT:9411} contextPath: ${SW_RECEIVER_ZIPKIN_CONTEXT_PATH:/} jettyMinThreads: ${SW_RECEIVER_ZIPKIN_JETTY_MIN_THREADS:1} jettyMaxThreads: ${SW_RECEIVER_ZIPKIN_JETTY_MAX_THREADS:200} jettyIdleTimeOut: ${SW_RECEIVER_ZIPKIN_JETTY_IDLE_TIMEOUT:30000} jettyAcceptorPriorityDelta: ${SW_RECEIVER_ZIPKIN_JETTY_DELTA:0} jettyAcceptQueueSize: ${SW_RECEIVER_ZIPKIN_QUEUE_SIZE:0} Analysis mode(Not production ready), receive Zipkin v1/v2 formats through HTTP service. Transform the trace to skywalking native format, and analysis like skywalking trace. This feature can\u0026rsquo;t work in production env right now, because of Zipkin tag/endpoint value unpredictable, we can\u0026rsquo;t make sure it fits production env requirements.  Active analysis mode, you should set needAnalysis config.\nreceiver_zipkin: selector: ${SW_RECEIVER_ZIPKIN:-} default: host: ${SW_RECEIVER_ZIPKIN_HOST:0.0.0.0} port: ${SW_RECEIVER_ZIPKIN_PORT:9411} contextPath: ${SW_RECEIVER_ZIPKIN_CONTEXT_PATH:/} jettyMinThreads: ${SW_RECEIVER_ZIPKIN_JETTY_MIN_THREADS:1} jettyMaxThreads: ${SW_RECEIVER_ZIPKIN_JETTY_MAX_THREADS:200} jettyIdleTimeOut: ${SW_RECEIVER_ZIPKIN_JETTY_IDLE_TIMEOUT:30000} jettyAcceptorPriorityDelta: ${SW_RECEIVER_ZIPKIN_JETTY_DELTA:0} jettyAcceptQueueSize: ${SW_RECEIVER_ZIPKIN_QUEUE_SIZE:0} needAnalysis: true NOTICE, Zipkin receiver is only provided in apache-skywalking-apm-x.y.z.tar.gz tar.\nJaeger receiver Jaeger receiver right now only works in Tracing Mode, and no analysis. Jaeger receiver provides extra gRPC host/port, if absent, sharing-server host/port will be used, then core gRPC host/port. Receiver requires jaeger-elasticsearch storage implementation active. Read this to know how to active.\nRight now, you need jaeger agent to batch send spans to SkyWalking oap server. Read Jaeger Architecture to get more details.\nActive the receiver.\nreceiver_jaeger: selector: ${SW_RECEIVER_JAEGER:-} default: gRPCHost: ${SW_RECEIVER_JAEGER_HOST:0.0.0.0} gRPCPort: ${SW_RECEIVER_JAEGER_PORT:14250} NOTICE, Jaeger receiver is only provided in apache-skywalking-apm-x.y.z.tar.gz tar.\n","excerpt":"Choose receiver Receiver is a concept in SkyWalking backend. All modules, which are responsible for …","ref":"/docs/main/v8.4.0/en/setup/backend/backend-receivers/","title":"Choose receiver"},{"body":"Cluster Management In many product environments, backend needs to support high throughput and provides HA to keep robustness, so you should need cluster management always in product env.\nBackend provides several ways to do cluster management. Choose the one you need/want.\n Zookeeper coordinator. Use Zookeeper to let backend instance detects and communicates with each other. Kubernetes. When backend cluster are deployed inside kubernetes, you could choose this by using k8s native APIs to manage cluster. Consul. Use Consul as backend cluster management implementor, to coordinate backend instances. Etcd. Use Etcd to coordinate backend instances. Nacos. Use Nacos to coordinate backend instances. In the application.yml, there\u0026rsquo;re default configurations for the aforementioned coordinators under the section cluster, you can specify one of them in the selector property to enable it.  Zookeeper coordinator Zookeeper is a very common and wide used cluster coordinator. Set the cluster/selector to zookeeper in the yml to enable.\nRequired Zookeeper version, 3.4+\ncluster: selector: ${SW_CLUSTER:zookeeper} # other configurations  hostPort is the list of zookeeper servers. Format is IP1:PORT1,IP2:PORT2,...,IPn:PORTn enableACL enable Zookeeper ACL to control access to its znode. schema is Zookeeper ACL schemas. expression is a expression of ACL. The format of the expression is specific to the schema. hostPort, baseSleepTimeMs and maxRetries are settings of Zookeeper curator client.  Note:\n If Zookeeper ACL is enabled and /skywalking existed, must be sure SkyWalking has CREATE, READ and WRITE permissions. If /skywalking is not exists, it will be created by SkyWalking and grant all permissions to the specified user. Simultaneously, znode is granted READ to anyone. If set schema as digest, the password of expression is set in clear text.  In some cases, oap default gRPC host and port in core are not suitable for internal communication among the oap nodes. The following setting are provided to set the host and port manually, based on your own LAN env.\n internalComHost, the host registered and other oap node use this to communicate with current node. internalComPort, the port registered and other oap node use this to communicate with current node.  zookeeper: nameSpace: ${SW_NAMESPACE:\u0026#34;\u0026#34;} hostPort: ${SW_CLUSTER_ZK_HOST_PORT:localhost:2181} #Retry Policy baseSleepTimeMs: ${SW_CLUSTER_ZK_SLEEP_TIME:1000} # initial amount of time to wait between retries maxRetries: ${SW_CLUSTER_ZK_MAX_RETRIES:3} # max number of times to retry internalComHost: 172.10.4.10 internalComPort: 11800 # Enable ACL enableACL: ${SW_ZK_ENABLE_ACL:false} # disable ACL in default schema: ${SW_ZK_SCHEMA:digest} # only support digest schema expression: ${SW_ZK_EXPRESSION:skywalking:skywalking} Kubernetes Require backend cluster are deployed inside kubernetes, guides are in Deploy in kubernetes. Set the selector to kubernetes.\ncluster: selector: ${SW_CLUSTER:kubernetes} # other configurations Consul Now, consul is becoming a famous system, many of companies and developers using consul to be their service discovery solution. Set the cluster/selector to consul in the yml to enable.\ncluster: selector: ${SW_CLUSTER:consul} # other configurations Same as Zookeeper coordinator, in some cases, oap default gRPC host and port in core are not suitable for internal communication among the oap nodes. The following setting are provided to set the host and port manually, based on your own LAN env.\n internalComHost, the host registered and other oap node use this to communicate with current node. internalComPort, the port registered and other oap node use this to communicate with current node.  Etcd Set the cluster/selector to etcd in the yml to enable.\ncluster: selector: ${SW_CLUSTER:etcd} # other configurations Same as Zookeeper coordinator, in some cases, oap default gRPC host and port in core are not suitable for internal communication among the oap nodes. The following setting are provided to set the host and port manually, based on your own LAN env.\n internalComHost, the host registered and other oap node use this to communicate with current node. internalComPort, the port registered and other oap node use this to communicate with current node.  Nacos Set the cluster/selector to nacos in the yml to enable.\ncluster: selector: ${SW_CLUSTER:nacos} # other configurations Nacos support authenticate by username or accessKey, empty means no need auth. extra config is bellow:\nnacos: username: password: accessKey: secretKey: Same as Zookeeper coordinator, in some cases, oap default gRPC host and port in core are not suitable for internal communication among the oap nodes. The following setting are provided to set the host and port manually, based on your own LAN env.\n internalComHost, the host registered and other oap node use this to communicate with current node. internalComPort, the port registered and other oap node use this to communicate with current node.  ","excerpt":"Cluster Management In many product environments, backend needs to support high throughput and …","ref":"/docs/main/v8.4.0/en/setup/backend/backend-cluster/","title":"Cluster Management"},{"body":"Compatible with other javaagent bytecode processing Problem   When use skywalking agent, some other agent, such as Arthas, can\u0026rsquo;t work well https://github.com/apache/skywalking/pull/4858\n  Java agent retransforming class fails with Skywalking agent, such as in this demo\n  Reason SkyWalking agent uses ByteBuddy to transform classes when the Java application starts. ByteBuddy generates auxiliary classes with different random names every time.\nWhen another java agent retransforms the same class, it triggers the SkyWalking agent to enhance the class again. The bytecode regenerated by ByteBuddy is changed, the fields and imported class names are modified, the JVM verifications about class bytecode fail, causing the retransform fails.\nResolve 1.Enable the class cache feature\nAdd JVM parameters:\n-Dskywalking.agent.is_cache_enhanced_class=true -Dskywalking.agent.class_cache_mode=MEMORY\nOr uncomment options in agent.conf:\n# If true, SkyWalking agent will cache all instrumented classes files to memory or disk files (decided by class cache mode), # allow other javaagent to enhance those classes that enhanced by SkyWalking agent. agent.is_cache_enhanced_class = ${SW_AGENT_CACHE_CLASS:false} # The instrumented classes cache mode: MEMORY or FILE # MEMORY: cache class bytes to memory, if instrumented classes is too many or too large, it may take up more memory # FILE: cache class bytes to user temp folder starts with 'class-cache', automatically clean up cached class files when the application exits agent.class_cache_mode = ${SW_AGENT_CLASS_CACHE_MODE:MEMORY} If the class cache feature is enabled, save the instrumented class bytecode to memory or a temporary file. When other java agents retransform the same class, SkyWalking agent first attempts to load from the cache.\nIf the cached class is found, it will be used directly without regenerating a new random name auxiliary class, which will not affect the processing of the subsequent java agent.\n2.Class cache save mode\nIt is recommended to put the cache class in memory, meanwhile if it costs more memory resources. Another option is using the local file system. Set the class cache mode through the following options:\n-Dskywalking.agent.class_cache_mode=MEMORY : save cache classes to java memory. -Dskywalking.agent.class_cache_mode=FILE : save cache classes to SkyWalking agent path \u0026lsquo;/class-cache\u0026rsquo;.\nOr modify the option in agent.conf:\nagent.class_cache_mode = ${SW_AGENT_CLASS_CACHE_MODE:MEMORY} agent.class_cache_mode = ${SW_AGENT_CLASS_CACHE_MODE:FILE}\n","excerpt":"Compatible with other javaagent bytecode processing Problem   When use skywalking agent, some other …","ref":"/docs/main/v8.4.0/en/faq/compatible-with-other-javaagent-bytecode-processing/","title":"Compatible with other javaagent bytecode processing"},{"body":"Component library settings Component library settings are about your own or 3rd part libraries used in monitored application.\nIn agent or SDK, no matter library name collected as ID or String(literally, e.g. SpringMVC), collector formats data in ID for better performance and less storage requirements.\nAlso, collector conjectures the remote service based on the component library, such as: the component library is MySQL Driver library, then the remote service should be MySQL Server.\nFor those two reasons, collector require two parts of settings in this file:\n Component Library id, name and languages. Remote server mapping, based on local library.  All component names and IDs must be defined in this file.\nComponent Library id Define all component libraries' names and IDs, used in monitored application. This is a both-way mapping, agent or SDK could use the value(ID) to represent the component name in uplink data.\n Name: the component name used in agent and UI id: Unique ID. All IDs are reserved, once it is released. languages: Program languages may use this component. Multi languages should be separated by ,  ID rules  Java and multi languages shared: (0, 3000) .NET Platform reserved: [3000, 4000) Node.js Platform reserved: [4000, 5000) Go reserved: [5000, 6000) Lua reserved: [6000, 7000) Python reserved: [7000, 8000) PHP reserved: [8000, 9000) C++ reserved: [9000, 10000)  Example\nTomcat: id: 1 languages: Java HttpClient: id: 2 languages: Java,C#,Node.js Dubbo: id: 3 languages: Java H2: id: 4 languages: Java Remote server mapping Remote server will be conjectured by the local component. The mappings are based on Component library names.\n Key: client component library name Value: server component name  Component-Server-Mappings: Jedis: Redis StackExchange.Redis: Redis Redisson: Redis Lettuce: Redis Zookeeper: Zookeeper SqlClient: SqlServer Npgsql: PostgreSQL MySqlConnector: Mysql EntityFrameworkCore.InMemory: InMemoryDatabase ","excerpt":"Component library settings Component library settings are about your own or 3rd part libraries used …","ref":"/docs/main/v8.4.0/en/guides/component-library-settings/","title":"Component library settings"},{"body":"Concepts and Designs Concepts and Designs help you to learn and understand the SkyWalking and the landscape.\n What is SkyWalking?  Overview and Core concepts. Provides a high-level description and introduction, including the problems the project solves. Project Goals. Provides the goals, which SkyWalking is trying to focus and provide features about them.    After you read the above documents, you should understand the SkyWalking basic goals. Now, you can choose which following parts you are interested, then dive in.\n Probe  Introduction. Lead readers to understand what the probe is, how many different probes existed and why need them. Service auto instrument agent. Introduce what the auto instrument agents do and which languages does SkyWalking already support. Manual instrument SDK. Introduce the role of the manual instrument SDKs in SkyWalking ecosystem. Service Mesh probe. Introduce why and how SkyWalking receive telemetry data from Service mesh and proxy probe.   Backend  Overview. Provides a high level introduction about the OAP backend. Observability Analysis Language. Introduces the core languages, which is designed for aggregation behaviour definition. Query in OAP. A set of query protocol provided, based on the Observability Analysis Language metrics definition.   UI  Overview. A simple brief about SkyWalking UI.   CLI  SkyWalking CLI. A command line interface for SkyWalking.    ","excerpt":"Concepts and Designs Concepts and Designs help you to learn and understand the SkyWalking and the …","ref":"/docs/main/v8.4.0/en/concepts-and-designs/readme/","title":"Concepts and Designs"},{"body":"Configuration Vocabulary Configuration Vocabulary lists all available configurations provided by application.yml.\n   Module Provider Settings Value(s) and Explanation System Environment Variable¹ Default     core default role Option values, Mixed/Receiver/Aggregator. Receiver mode OAP open the service to the agents, analysis and aggregate the results and forward the results for distributed aggregation. Aggregator mode OAP receives data from Mixer and Receiver role OAP nodes, and do 2nd level aggregation. Mixer means being Receiver and Aggregator both. SW_CORE_ROLE Mixed   - - restHost Binding IP of restful service. Services include GraphQL query and HTTP data report SW_CORE_REST_HOST 0.0.0.0   - - restPort Binding port of restful service SW_CORE_REST_PORT 12800   - - restContextPath Web context path of restful service SW_CORE_REST_CONTEXT_PATH /   - - restMinThreads Min threads number of restful service SW_CORE_REST_JETTY_MIN_THREADS 1   - - restMaxThreads Max threads number of restful service SW_CORE_REST_JETTY_MAX_THREADS 200   - - restIdleTimeOut Connector idle timeout in milliseconds of restful service SW_CORE_REST_JETTY_IDLE_TIMEOUT 30000   - - restAcceptorPriorityDelta Thread priority delta to give to acceptor threads of restful service SW_CORE_REST_JETTY_DELTA 0   - - restAcceptQueueSize ServerSocketChannel backlog of restful service SW_CORE_REST_JETTY_QUEUE_SIZE 0   - - gRPCHost Binding IP of gRPC service. Services include gRPC data report and internal communication among OAP nodes SW_CORE_GRPC_HOST 0.0.0.0   - - gRPCPort Binding port of gRPC service SW_CORE_GRPC_PORT 11800   - - gRPCSslEnabled Activate SSL for gRPC service SW_CORE_GRPC_SSL_ENABLED false   - - gRPCSslKeyPath The file path of gRPC SSL key SW_CORE_GRPC_SSL_KEY_PATH -   - - gRPCSslCertChainPath The file path of gRPC SSL cert chain SW_CORE_GRPC_SSL_CERT_CHAIN_PATH -   - - gRPCSslTrustedCAPath The file path of gRPC trusted CA SW_CORE_GRPC_SSL_TRUSTED_CA_PATH -   - - downsampling The activated level of down sampling aggregation  Hour,Day   - - enableDataKeeperExecutor Controller of TTL scheduler. Once disabled, TTL wouldn\u0026rsquo;t work. SW_CORE_ENABLE_DATA_KEEPER_EXECUTOR true   - - dataKeeperExecutePeriod The execution period of TTL scheduler, unit is minute. Execution doesn\u0026rsquo;t mean deleting data. The storage provider could override this, such as ElasticSearch storage. SW_CORE_DATA_KEEPER_EXECUTE_PERIOD 5   - - recordDataTTL The lifecycle of record data. Record data includes traces, top n sampled records, and logs. Unit is day. Minimal value is 2. SW_CORE_RECORD_DATA_TTL 3   - - metricsDataTTL The lifecycle of metrics data, including the metadata. Unit is day. Recommend metricsDataTTL \u0026gt;= recordDataTTL. Minimal value is 2. SW_CORE_METRICS_DATA_TTL 7   - - enableDatabaseSession Cache metrics data for 1 minute to reduce database queries, and if the OAP cluster changes within that minute. SW_CORE_ENABLE_DATABASE_SESSION true   - - topNReportPeriod The execution period of top N sampler, which saves sampled data into the storage. Unit is minute SW_CORE_TOPN_REPORT_PERIOD 10   - - activeExtraModelColumns Append the names of entity, such as service name, into the metrics storage entities. SW_CORE_ACTIVE_EXTRA_MODEL_COLUMNS false   - - serviceNameMaxLength Max length limitation of service name. SW_SERVICE_NAME_MAX_LENGTH 70   - - instanceNameMaxLength Max length limitation of service instance name. The max length of service + instance names should be less than 200. SW_INSTANCE_NAME_MAX_LENGTH 70   - - endpointNameMaxLength Max length limitation of endpoint name. The max length of service + endpoint names should be less than 240. SW_ENDPOINT_NAME_MAX_LENGTH 150   - - searchableTracesTags Define the set of span tag keys, which should be searchable through the GraphQL. Multiple values should be separated through the comma. SW_SEARCHABLE_TAG_KEYS http.method,status_code,db.type,db.instance,mq.queue,mq.topic,mq.broker   - - searchableLogsTags Define the set of log tag keys, which should be searchable through the GraphQL. Multiple values should be separated through the comma. SW_SEARCHABLE_LOGS_TAG_KEYS level   - - gRPCThreadPoolSize Pool size of gRPC server SW_CORE_GRPC_THREAD_POOL_SIZE CPU core * 4   - - gRPCThreadPoolQueueSize The queue size of gRPC server SW_CORE_GRPC_POOL_QUEUE_SIZE 10000   - - maxConcurrentCallsPerConnection The maximum number of concurrent calls permitted for each incoming connection. Defaults to no limit. SW_CORE_GRPC_MAX_CONCURRENT_CALL -   - - maxMessageSize Sets the maximum message size allowed to be received on the server. Empty means 4 MiB SW_CORE_GRPC_MAX_MESSAGE_SIZE 4M(based on Netty)   - - remoteTimeout Timeout for cluster internal communication, in seconds. - 20   - - maxSizeOfNetworkAddressAlias Max size of network address detected in the be monitored system. - 1_000_000   - - maxPageSizeOfQueryProfileSnapshot The max size in every OAP query for snapshot analysis - 500   - - maxSizeOfAnalyzeProfileSnapshot The max number of snapshots analyzed by OAP - 12000   cluster standalone - standalone is not suitable for one node running, no available configuration. - -   - zookeeper nameSpace The namespace, represented by root path, isolates the configurations in the zookeeper. SW_NAMESPACE /, root path   - - hostPort hosts and ports of Zookeeper Cluster SW_CLUSTER_ZK_HOST_PORT localhost:2181   - - baseSleepTimeMs The period of Zookeeper client between two retries. Unit is ms. SW_CLUSTER_ZK_SLEEP_TIME 1000   - - maxRetries The max retry time of re-trying. SW_CLUSTER_ZK_MAX_RETRIES 3   - - enableACL Open ACL by using schema and expression SW_ZK_ENABLE_ACL false   - - schema schema for the authorization SW_ZK_SCHEMA digest   - - expression expression for the authorization SW_ZK_EXPRESSION skywalking:skywalking   - - internalComHost The hostname registered in the Zookeeper for the internal communication of OAP cluster. - -   - - internalComPort The port registered in the Zookeeper for the internal communication of OAP cluster. - -1   - kubernetes namespace Namespace SkyWalking deployed in the k8s SW_CLUSTER_K8S_NAMESPACE default   - - labelSelector Labels used for filtering the OAP deployment in the k8s SW_CLUSTER_K8S_LABEL app=collector,release=skywalking   - - uidEnvName Environment variable name for reading uid. SW_CLUSTER_K8S_UID SKYWALKING_COLLECTOR_UID   - consul serviceName Service name used for SkyWalking cluster. SW_SERVICE_NAME SkyWalking_OAP_Cluster   - - hostPort hosts and ports used of Consul cluster. SW_CLUSTER_CONSUL_HOST_PORT localhost:8500   - - aclToken ALC Token of Consul. Empty string means without ALC token. SW_CLUSTER_CONSUL_ACLTOKEN -   - - internalComHost The hostname registered in the Consul for the internal communication of OAP cluster. - -   - - internalComPort The port registered in the Consul for the internal communication of OAP cluster. - -1   - etcd serviceName Service name used for SkyWalking cluster. SW_SERVICE_NAME SkyWalking_OAP_Cluster   - - hostPort hosts and ports used of etcd cluster. SW_CLUSTER_ETCD_HOST_PORT localhost:2379   - - isSSL Open SSL for the connection between SkyWalking and etcd cluster. - -   - - internalComHost The hostname registered in the etcd for the internal communication of OAP cluster. - -   - - internalComPort The port registered in the etcd for the internal communication of OAP cluster. - -1   - Nacos serviceName Service name used for SkyWalking cluster. SW_SERVICE_NAME SkyWalking_OAP_Cluster   - - hostPort hosts and ports used of Nacos cluster. SW_CLUSTER_NACOS_HOST_PORT localhost:8848   - - namespace Namespace used by SkyWalking node coordination. SW_CLUSTER_NACOS_NAMESPACE public   - - internalComHost The hostname registered in the Nacos for the internal communication of OAP cluster. - -   - - internalComPort The port registered in the Nacos for the internal communication of OAP cluster. - -1   - - username Nacos Auth username SW_CLUSTER_NACOS_USERNAME -   - - password Nacos Auth password SW_CLUSTER_NACOS_PASSWORD -   - - accessKey Nacos Auth accessKey SW_CLUSTER_NACOS_ACCESSKEY -   - - secretKey Nacos Auth secretKey SW_CLUSTER_NACOS_SECRETKEY -   storage elasticsearch - ElasticSearch 6 storage implementation - -   - - nameSpace Prefix of indexes created and used by SkyWalking. SW_NAMESPACE -   - - clusterNodes ElasticSearch cluster nodes for client connection. SW_STORAGE_ES_CLUSTER_NODES localhost   - - protocol HTTP or HTTPs. SW_STORAGE_ES_HTTP_PROTOCOL HTTP   - - user User name of ElasticSearch cluster SW_ES_USER -   - - password Password of ElasticSearch cluster SW_ES_PASSWORD -   - - trustStorePath Trust JKS file path. Only work when user name and password opened SW_STORAGE_ES_SSL_JKS_PATH -   - - trustStorePass Trust JKS file password. Only work when user name and password opened SW_STORAGE_ES_SSL_JKS_PASS -   - - secretsManagementFile Secrets management file in the properties format includes the username, password, which are managed by 3rd party tool. Provide the capability to update them in the runtime. SW_ES_SECRETS_MANAGEMENT_FILE -   - - dayStep Represent the number of days in the one minute/hour/day index. SW_STORAGE_DAY_STEP 1   - - indexShardsNumber Shard number of new indexes SW_STORAGE_ES_INDEX_SHARDS_NUMBER 1   - - indexReplicasNumber Replicas number of new indexes SW_STORAGE_ES_INDEX_REPLICAS_NUMBER 0   - - superDatasetDayStep Represent the number of days in the super size dataset record index, the default value is the same as dayStep when the value is less than 0. SW_SUPERDATASET_STORAGE_DAY_STEP -1   - - superDatasetIndexShardsFactor Super data set has been defined in the codes, such as trace segments. This factor provides more shards for the super data set, shards number = indexShardsNumber * superDatasetIndexShardsFactor. Also, this factor effects Zipkin and Jaeger traces. SW_STORAGE_ES_SUPER_DATASET_INDEX_SHARDS_FACTOR 5   - - superDatasetIndexReplicasNumber Represent the replicas number in the super size dataset record index. SW_STORAGE_ES_SUPER_DATASET_INDEX_REPLICAS_NUMBER 0   - - bulkActions Async bulk size of the record data batch execution. SW_STORAGE_ES_BULK_ACTIONS 1000   - - syncBulkActions Sync bulk size of the metrics data batch execution. SW_STORAGE_ES_SYNC_BULK_ACTIONS 50000   - - flushInterval Period of flush, no matter bulkActions reached or not. Unit is second. SW_STORAGE_ES_FLUSH_INTERVAL 10   - - concurrentRequests The number of concurrent requests allowed to be executed. SW_STORAGE_ES_CONCURRENT_REQUESTS 2   - - resultWindowMaxSize The max size of dataset when OAP loading cache, such as network alias. SW_STORAGE_ES_QUERY_MAX_WINDOW_SIZE 10000   - - metadataQueryMaxSize The max size of metadata per query. SW_STORAGE_ES_QUERY_MAX_SIZE 5000   - - segmentQueryMaxSize The max size of trace segments per query. SW_STORAGE_ES_QUERY_SEGMENT_SIZE 200   - - profileTaskQueryMaxSize The max size of profile task per query. SW_STORAGE_ES_QUERY_PROFILE_TASK_SIZE 200   - - advanced All settings of ElasticSearch index creation. The value should be in JSON format SW_STORAGE_ES_ADVANCED -   - elasticsearch7 - ElasticSearch 7 storage implementation - -   - - nameSpace Prefix of indexes created and used by SkyWalking. SW_NAMESPACE -   - - clusterNodes ElasticSearch cluster nodes for client connection. SW_STORAGE_ES_CLUSTER_NODES localhost   - - protocol HTTP or HTTPs. SW_STORAGE_ES_HTTP_PROTOCOL HTTP   - - user User name of ElasticSearch cluster SW_ES_USER -   - - password Password of ElasticSearch cluster SW_ES_PASSWORD -   - - trustStorePath Trust JKS file path. Only work when user name and password opened SW_STORAGE_ES_SSL_JKS_PATH -   - - trustStorePass Trust JKS file password. Only work when user name and password opened SW_STORAGE_ES_SSL_JKS_PASS -   - - secretsManagementFile Secrets management file in the properties format includes the username, password, which are managed by 3rd party tool. Provide the capability to update them in the runtime. SW_ES_SECRETS_MANAGEMENT_FILE -   - - dayStep Represent the number of days in the one minute/hour/day index. SW_STORAGE_DAY_STEP 1   - - indexShardsNumber Shard number of new indexes SW_STORAGE_ES_INDEX_SHARDS_NUMBER 1   - - indexReplicasNumber Replicas number of new indexes SW_STORAGE_ES_INDEX_REPLICAS_NUMBER 0   - - superDatasetDayStep Represent the number of days in the super size dataset record index, the default value is the same as dayStep when the value is less than 0. SW_SUPERDATASET_STORAGE_DAY_STEP -1   - - superDatasetIndexShardsFactor Super data set has been defined in the codes, such as trace segments. This factor provides more shards for the super data set, shards number = indexShardsNumber * superDatasetIndexShardsFactor. Also, this factor effects Zipkin and Jaeger traces. SW_STORAGE_ES_SUPER_DATASET_INDEX_SHARDS_FACTOR 5   - - superDatasetIndexReplicasNumber Represent the replicas number in the super size dataset record index. SW_STORAGE_ES_SUPER_DATASET_INDEX_REPLICAS_NUMBER 0   - - bulkActions Async bulk size of the record data batch execution. SW_STORAGE_ES_BULK_ACTIONS 1000   - - syncBulkActions Sync bulk size of the metrics data batch execution. SW_STORAGE_ES_SYNC_BULK_ACTIONS 50000   - - flushInterval Period of flush, no matter bulkActions reached or not. Unit is second. SW_STORAGE_ES_FLUSH_INTERVAL 10   - - concurrentRequests The number of concurrent requests allowed to be executed. SW_STORAGE_ES_CONCURRENT_REQUESTS 2   - - resultWindowMaxSize The max size of dataset when OAP loading cache, such as network alias. SW_STORAGE_ES_QUERY_MAX_WINDOW_SIZE 10000   - - metadataQueryMaxSize The max size of metadata per query. SW_STORAGE_ES_QUERY_MAX_SIZE 5000   - - segmentQueryMaxSize The max size of trace segments per query. SW_STORAGE_ES_QUERY_SEGMENT_SIZE 200   - - profileTaskQueryMaxSize The max size of profile task per query. SW_STORAGE_ES_QUERY_PROFILE_TASK_SIZE 200   - - advanced All settings of ElasticSearch index creation. The value should be in JSON format SW_STORAGE_ES_ADVANCED -   - h2 - H2 storage is designed for demonstration and running in short term(1-2 hours) only - -   - - driver H2 JDBC driver. SW_STORAGE_H2_DRIVER org.h2.jdbcx.JdbcDataSource   - - url H2 connection URL. Default is H2 memory mode SW_STORAGE_H2_URL jdbc:h2:mem:skywalking-oap-db   - - user User name of H2 database. SW_STORAGE_H2_USER sa   - - password Password of H2 database. - -   - - metadataQueryMaxSize The max size of metadata per query. SW_STORAGE_H2_QUERY_MAX_SIZE 5000   - - maxSizeOfArrayColumn Some entities, such as trace segment, include the logic column with multiple values. In the H2, we use multiple physical columns to host the values, such as, Change column_a with values [1,2,3,4,5] to column_a_0 = 1, column_a_1 = 2, column_a_2 = 3 , column_a_3 = 4, column_a_4 = 5 SW_STORAGE_MAX_SIZE_OF_ARRAY_COLUMN 20   - - numOfSearchableValuesPerTag In a trace segment, it includes multiple spans with multiple tags. Different spans could have same tag keys, such as multiple HTTP exit spans all have their own http.method tag. This configuration set the limitation of max num of values for the same tag key. SW_STORAGE_NUM_OF_SEARCHABLE_VALUES_PER_TAG 2   - mysql - MySQL Storage. The MySQL JDBC Driver is not in the dist, please copy it into oap-lib folder manually - -   - - properties Hikari connection pool configurations - Listed in the application.yaml.   - - metadataQueryMaxSize The max size of metadata per query. SW_STORAGE_MYSQL_QUERY_MAX_SIZE 5000   - - maxSizeOfArrayColumn Some entities, such as trace segment, include the logic column with multiple values. In the MySQL, we use multiple physical columns to host the values, such as, Change column_a with values [1,2,3,4,5] to column_a_0 = 1, column_a_1 = 2, column_a_2 = 3 , column_a_3 = 4, column_a_4 = 5 SW_STORAGE_MAX_SIZE_OF_ARRAY_COLUMN 20   - - numOfSearchableValuesPerTag In a trace segment, it includes multiple spans with multiple tags. Different spans could have same tag keys, such as multiple HTTP exit spans all have their own http.method tag. This configuration set the limitation of max num of values for the same tag key. SW_STORAGE_NUM_OF_SEARCHABLE_VALUES_PER_TAG 2   - influxdb - InfluxDB storage. - -   - - url InfluxDB connection URL. SW_STORAGE_INFLUXDB_URL http://localhost:8086   - - user User name of InfluxDB. SW_STORAGE_INFLUXDB_USER root   - - password Password of InfluxDB. SW_STORAGE_INFLUXDB_PASSWORD -   - - database Database of InfluxDB. SW_STORAGE_INFLUXDB_DATABASE skywalking   - - actions The number of actions to collect. SW_STORAGE_INFLUXDB_ACTIONS 1000   - - duration The time to wait at most (milliseconds). SW_STORAGE_INFLUXDB_DURATION 1000   - - batchEnabled If true, write points with batch api. SW_STORAGE_INFLUXDB_BATCH_ENABLED true   - - fetchTaskLogMaxSize The max number of fetch task log in a request. SW_STORAGE_INFLUXDB_FETCH_TASK_LOG_MAX_SIZE 5000   - - connectionResponseFormat The response format of connection to influxDB, cannot be anything but MSGPACK or JSON. SW_STORAGE_INFLUXDB_CONNECTION_RESPONSE_FORMAT MSGPACK   agent-analyzer default Agent Analyzer. SW_AGENT_ANALYZER default    - - sampleRate Sampling rate for receiving trace. The precision is 1/10000. 10000 means 100% sample in default. SW_TRACE_SAMPLE_RATE 10000   - - slowDBAccessThreshold The slow database access thresholds. Unit ms. SW_SLOW_DB_THRESHOLD default:200,mongodb:100   - - forceSampleErrorSegment When sampling mechanism activated, this config would make the error status segment sampled, ignoring the sampling rate. SW_FORCE_SAMPLE_ERROR_SEGMENT true   - - segmentStatusAnalysisStrategy Determine the final segment status from the status of spans. Available values are FROM_SPAN_STATUS , FROM_ENTRY_SPAN and FROM_FIRST_SPAN. FROM_SPAN_STATUS represents the segment status would be error if any span is in error status. FROM_ENTRY_SPAN means the segment status would be determined by the status of entry spans only. FROM_FIRST_SPAN means the segment status would be determined by the status of the first span only. SW_SEGMENT_STATUS_ANALYSIS_STRATEGY FROM_SPAN_STATUS   - - noUpstreamRealAddressAgents Exit spans with the component in the list would not generate the client-side instance relation metrics. As some tracing plugins can\u0026rsquo;t collect the real peer ip address, such as Nginx-LUA and Envoy. SW_NO_UPSTREAM_REAL_ADDRESS 6000,9000   - - slowTraceSegmentThreshold Setting this threshold about the latency would make the slow trace segments sampled if they cost more time, even the sampling mechanism activated. The default value is -1, which means would not sample slow traces. Unit, millisecond. SW_SLOW_TRACE_SEGMENT_THRESHOLD -1   - - meterAnalyzerActiveFiles Which files could be meter analyzed, files split by \u0026ldquo;,\u0026rdquo; SW_METER_ANALYZER_ACTIVE_FILES    receiver-sharing-server default Sharing server provides new gRPC and restful servers for data collection. Ana make the servers in the core module working for internal communication only. - -    - - restHost Binding IP of restful service. Services include GraphQL query and HTTP data report SW_RECEIVER_SHARING_REST_HOST -   - - restPort Binding port of restful service SW_RECEIVER_SHARING_REST_PORT -   - - restContextPath Web context path of restful service SW_RECEIVER_SHARING_REST_CONTEXT_PATH -   - - restMinThreads Min threads number of restful service SW_RECEIVER_SHARING_JETTY_MIN_THREADS 1   - - restMaxThreads Max threads number of restful service SW_RECEIVER_SHARING_JETTY_MAX_THREADS 200   - - restIdleTimeOut Connector idle timeout in milliseconds of restful service SW_RECEIVER_SHARING_JETTY_IDLE_TIMEOUT 30000   - - restAcceptorPriorityDelta Thread priority delta to give to acceptor threads of restful service SW_RECEIVER_SHARING_JETTY_DELTA 0   - - restAcceptQueueSize ServerSocketChannel backlog of restful service SW_RECEIVER_SHARING_JETTY_QUEUE_SIZE 0   - - gRPCHost Binding IP of gRPC service. Services include gRPC data report and internal communication among OAP nodes SW_RECEIVER_GRPC_HOST 0.0.0.0. Not Activated   - - gRPCPort Binding port of gRPC service SW_RECEIVER_GRPC_PORT Not Activated   - - gRPCThreadPoolSize Pool size of gRPC server SW_RECEIVER_GRPC_THREAD_POOL_SIZE CPU core * 4   - - gRPCThreadPoolQueueSize The queue size of gRPC server SW_RECEIVER_GRPC_POOL_QUEUE_SIZE 10000   - - gRPCSslEnabled Activate SSL for gRPC service SW_RECEIVER_GRPC_SSL_ENABLED false   - - gRPCSslKeyPath The file path of gRPC SSL key SW_RECEIVER_GRPC_SSL_KEY_PATH -   - - gRPCSslCertChainPath The file path of gRPC SSL cert chain SW_RECEIVER_GRPC_SSL_CERT_CHAIN_PATH -   - - maxConcurrentCallsPerConnection The maximum number of concurrent calls permitted for each incoming connection. Defaults to no limit. SW_RECEIVER_GRPC_MAX_CONCURRENT_CALL -   - - authentication The token text for the authentication. Work for gRPC connection only. Once this is set, the client is required to use the same token. SW_AUTHENTICATION -   receiver-register default Read receiver doc for more details - -    receiver-trace default Read receiver doc for more details - -    receiver-jvm default Read receiver doc for more details - -    receiver-clr default Read receiver doc for more details - -    receiver-profile default Read receiver doc for more details - -    service-mesh default Read receiver doc for more details - -    envoy-metric default Read receiver doc for more details - -    - - acceptMetricsService Open Envoy Metrics Service analysis SW_ENVOY_METRIC_SERVICE true   - - alsHTTPAnalysis Open Envoy Access Log Service analysis. Value = k8s-mesh means open the analysis SW_ENVOY_METRIC_ALS_HTTP_ANALYSIS -   - - k8sServiceNameRule k8sServiceNameRule allows you to customize the service name in ALS via Kubernetes metadata, the available variables are pod, service, e.g., you can use ${service.metadata.name}-${pod.metadata.labels.version} to append the version number to the service name. Be careful, when using environment variables to pass this configuration, use single quotes('') to avoid it being evaluated by the shell. -    receiver-otel default Read receiver doc for more details - -    - - enabledHandlers Enabled handlers for otel SW_OTEL_RECEIVER_ENABLED_HANDLERS -   - - enabledOcRules Enabled metric rules for OC handler SW_OTEL_RECEIVER_ENABLED_OC_RULES -   receiver_zipkin default Read receiver doc - -    - - restHost Binding IP of restful service. SW_RECEIVER_ZIPKIN_HOST 0.0.0.0   - - restPort Binding port of restful service SW_RECEIVER_ZIPKIN_PORT 9411   - - restContextPath Web context path of restful service SW_RECEIVER_ZIPKIN_CONTEXT_PATH /   - - needAnalysis Analysis zipkin span to generate metrics - false   - - maxCacheSize Max cache size for span analysis - 1_000_000   - - expireTime The expire time of analysis cache, unit is second. - 20   receiver_jaeger default Read receiver doc - -    - - gRPCHost Binding IP of gRPC service. Services include gRPC data report and internal communication among OAP nodes SW_RECEIVER_JAEGER_HOST -   - - gRPCPort Binding port of gRPC service SW_RECEIVER_JAEGER_PORT -   - - gRPCThreadPoolSize Pool size of gRPC server - CPU core * 4   - - gRPCThreadPoolQueueSize The queue size of gRPC server - 10000   - - maxConcurrentCallsPerConnection The maximum number of concurrent calls permitted for each incoming connection. Defaults to no limit. - -   - - maxMessageSize Sets the maximum message size allowed to be received on the server. Empty means 4 MiB - 4M(based on Netty)   prometheus-fetcher default Read fetcher doc for more details - -    - - active Activate the Prometheus fetcher. SW_PROMETHEUS_FETCHER_ACTIVE false   kafka-fetcher default Read fetcher doc for more details - -    - - bootstrapServers A list of host/port pairs to use for establishing the initial connection to the Kafka cluster. SW_KAFKA_FETCHER_SERVERS localhost:9092   - - groupId A unique string that identifies the consumer group this consumer belongs to. - skywalking-consumer   - - consumePartitions Which PartitionId(s) of the topics assign to the OAP server. If more than one, is separated by commas. SW_KAFKA_FETCHER_CONSUME_PARTITIONS -   - - isSharding it was true when OAP Server in cluster. SW_KAFKA_FETCHER_IS_SHARDING false   - - createTopicIfNotExist If true, create the Kafka topic when it does not exist. - true   - - partitions The number of partitions for the topic being created. SW_KAFKA_FETCHER_PARTITIONS 3   - - enableMeterSystem To enable to fetch and handle Meter System data. SW_KAFKA_FETCHER_ENABLE_METER_SYSTEM false   - - enableLog To enable to fetch and handle log data. SW_KAFKA_FETCHER_ENABLE_LOG false   - - replicationFactor The replication factor for each partition in the topic being created. SW_KAFKA_FETCHER_PARTITIONS_FACTOR 2   - - kafkaHandlerThreadPoolSize Pool size of kafka message handler executor. SW_KAFKA_HANDLER_THREAD_POOL_SIZE CPU core * 2   - - kafkaHandlerThreadPoolQueueSize The queue size of kafka message handler executor. SW_KAFKA_HANDLER_THREAD_POOL_QUEUE_SIZE 10000   - - topicNameOfMeters Specifying Kafka topic name for Meter system data. - skywalking-meters   - - topicNameOfMetrics Specifying Kafka topic name for JVM Metrics data. - skywalking-metrics   - - topicNameOfProfiling Specifying Kafka topic name for Profiling data. - skywalking-profilings   - - topicNameOfTracingSegments Specifying Kafka topic name for Tracing data. - skywalking-segments   - - topicNameOfManagements Specifying Kafka topic name for service instance reporting and registering. - skywalking-managements   - - topicNameOfLogs Specifying Kafka topic name for log data. - skywalking-logs   receiver-browser default Read receiver doc for more details - - -   - - sampleRate Sampling rate for receiving trace. The precision is 1/10000. 10000 means 100% sample in default. SW_RECEIVER_BROWSER_SAMPLE_RATE 10000   query graphql - GraphQL query implementation -    - - path Root path of GraphQL query and mutation. SW_QUERY_GRAPHQL_PATH /graphql   alarm default - Read alarm doc for more details. -    telemetry - - Read telemetry doc for more details. -    - none - No op implementation -    - prometheus host Binding host for Prometheus server fetching data SW_TELEMETRY_PROMETHEUS_HOST 0.0.0.0   - - port Binding port for Prometheus server fetching data SW_TELEMETRY_PROMETHEUS_PORT 1234   configuration - - Read dynamic configuration doc for more details. -    - grpc host DCS server binding hostname SW_DCS_SERVER_HOST -   - - port DCS server binding port SW_DCS_SERVER_PORT 80   - - clusterName Cluster name when reading latest configuration from DSC server. SW_DCS_CLUSTER_NAME SkyWalking   - - period The period of OAP reading data from DSC server. Unit is second. SW_DCS_PERIOD 20   - apollo apolloMeta apollo.meta in Apollo SW_CONFIG_APOLLO http://106.12.25.204:8080   - - apolloCluster apollo.cluster in Apollo SW_CONFIG_APOLLO_CLUSTER default   - - apolloEnv env in Apollo SW_CONFIG_APOLLO_ENV -   - - appId app.id in Apollo SW_CONFIG_APOLLO_APP_ID skywalking   - - period The period of data sync. Unit is second. SW_CONFIG_APOLLO_PERIOD 60   - zookeeper nameSpace The namespace, represented by root path, isolates the configurations in the zookeeper. SW_CONFIG_ZK_NAMESPACE /, root path   - - hostPort hosts and ports of Zookeeper Cluster SW_CONFIG_ZK_HOST_PORT localhost:2181   - - baseSleepTimeMs The period of Zookeeper client between two retries. Unit is ms. SW_CONFIG_ZK_BASE_SLEEP_TIME_MS 1000   - - maxRetries The max retry time of re-trying. SW_CONFIG_ZK_MAX_RETRIES 3   - - period The period of data sync. Unit is second. SW_CONFIG_ZK_PERIOD 60   - etcd clusterName Service name used for SkyWalking cluster. SW_CONFIG_ETCD_CLUSTER_NAME default   - - serverAddr hosts and ports used of etcd cluster. SW_CONFIG_ETCD_SERVER_ADDR localhost:2379   - - group Additional prefix of the configuration key SW_CONFIG_ETCD_GROUP skywalking   - - period The period of data sync. Unit is second. SW_CONFIG_ZK_PERIOD 60   - consul hostPort hosts and ports used of Consul cluster. SW_CONFIG_CONSUL_HOST_AND_PORTS localhost:8500   - - aclToken ALC Token of Consul. Empty string means without ALC token. SW_CONFIG_CONSUL_ACL_TOKEN -   - - period The period of data sync. Unit is second. SW_CONFIG_CONSUL_PERIOD 60   - k8s-configmap namespace Deployment namespace of the config map. SW_CLUSTER_K8S_NAMESPACE default   - - labelSelector Labels used for locating configmap. SW_CLUSTER_K8S_LABEL app=collector,release=skywalking   - - period The period of data sync. Unit is second. SW_CONFIG_ZK_PERIOD 60   - nacos serverAddr Nacos Server Host SW_CONFIG_NACOS_SERVER_ADDR 127.0.0.1   - - port Nacos Server Port SW_CONFIG_NACOS_SERVER_PORT 8848   - - group Nacos Configuration namespace SW_CONFIG_NACOS_SERVER_NAMESPACE -   - - period The period of data sync. Unit is second. SW_CONFIG_CONFIG_NACOS_PERIOD 60   - - username Nacos Auth username SW_CONFIG_NACOS_USERNAME -   - - password Nacos Auth password SW_CONFIG_NACOS_PASSWORD -   - - accessKey Nacos Auth accessKey SW_CONFIG_NACOS_ACCESSKEY -   - - secretKey Nacos Auth secretKey SW_CONFIG_NACOS_SECRETKEY -   exporter grpc targetHost The host of target grpc server for receiving export data. SW_EXPORTER_GRPC_HOST 127.0.0.1   - - targetPort The port of target grpc server for receiving export data. SW_EXPORTER_GRPC_PORT 9870   health-checker default checkIntervalSeconds The period of check OAP internal health status. Unit is second. SW_HEALTH_CHECKER_INTERVAL_SECONDS 5   configuration-discovery default disableMessageDigest If true, agent receives the latest configuration every time even without change. In default, OAP uses SHA512 message digest mechanism to detect changes of configuration. SW_DISABLE_MESSAGE_DIGEST false    Notice ¹ System Environment Variable name could be declared and changed in the application.yml. The names listed here, are just provided in the default application.yml file.\n","excerpt":"Configuration Vocabulary Configuration Vocabulary lists all available configurations provided by …","ref":"/docs/main/v8.4.0/en/setup/backend/configuration-vocabulary/","title":"Configuration Vocabulary"},{"body":" Dependency the toolkit, such as using maven or gradle  \u0026lt;dependency\u0026gt; \u0026lt;groupId\u0026gt;org.apache.skywalking\u0026lt;/groupId\u0026gt; \u0026lt;artifactId\u0026gt;apm-toolkit-log4j-1.x\u0026lt;/artifactId\u0026gt; \u0026lt;version\u0026gt;{project.release.version}\u0026lt;/version\u0026gt; \u0026lt;/dependency\u0026gt; Print trace ID in your logs  Config a layout  log4j.appender.CONSOLE.layout=TraceIdPatternLayout  set %T in layout.ConversionPattern ( In 2.0-2016, you should use %x, Why change? )  log4j.appender.CONSOLE.layout.ConversionPattern=%d [%T] %-5p %c{1}:%L - %m%n  When you use -javaagent to active the sky-walking tracer, log4j will output traceId, if it existed. If the tracer is inactive, the output will be TID: N/A.  gRPC reporter The gRPC report could forward the collected logs to SkyWalking OAP server, or SkyWalking Satellite sidecar. Trace id, segment id, and span id will attach to logs automatically. You don\u0026rsquo;t need to change the layout.\n Add GRPCLogClientAppender in log4j.properties  log4j.rootLogger=INFO,CustomAppender log4j.appender.CustomAppender=org.apache.skywalking.apm.toolkit.log.log4j.v1.x.log.GRPCLogClientAppender  Add config of the plugin or use default  plugin.toolkit.log.grpc.reporter.server_host=${SW_GRPC_LOG_SERVER_HOST:127.0.0.1} plugin.toolkit.log.grpc.reporter.server_port=${SW_GRPC_LOG_SERVER_PORT:11800} plugin.toolkit.log.grpc.reporter.max_message_size=${SW_GRPC_LOG_MAX_MESSAGE_SIZE:10485760} plugin.toolkit.log.grpc.reporter.upstream_timeout=${SW_GRPC_LOG_GRPC_UPSTREAM_TIMEOUT:30} ","excerpt":"Dependency the toolkit, such as using maven or gradle  \u0026lt;dependency\u0026gt; …","ref":"/docs/main/v8.4.0/en/setup/service-agent/java-agent/application-toolkit-log4j-1.x/","title":"Dependency the toolkit, such as using maven or gradle"},{"body":" Dependency the toolkit, such as using maven or gradle  \u0026lt;dependency\u0026gt; \u0026lt;groupId\u0026gt;org.apache.skywalking\u0026lt;/groupId\u0026gt; \u0026lt;artifactId\u0026gt;apm-toolkit-log4j-2.x\u0026lt;/artifactId\u0026gt; \u0026lt;version\u0026gt;{project.release.version}\u0026lt;/version\u0026gt; \u0026lt;/dependency\u0026gt; Print trace ID in your logs  Config the [%traceId] pattern in your log4j2.xml  \u0026lt;Appenders\u0026gt; \u0026lt;Console name=\u0026#34;Console\u0026#34; target=\u0026#34;SYSTEM_OUT\u0026#34;\u0026gt; \u0026lt;PatternLayout pattern=\u0026#34;%d [%traceId] %-5p %c{1}:%L - %m%n\u0026#34;/\u0026gt; \u0026lt;/Console\u0026gt; \u0026lt;/Appenders\u0026gt;  Support log4j2 AsyncRoot , No additional configuration is required. Refer to the demo of log4j2.xml below. For details: Log4j2 Async Loggers  \u0026lt;Configuration\u0026gt; \u0026lt;Appenders\u0026gt; \u0026lt;Console name=\u0026#34;Console\u0026#34; target=\u0026#34;SYSTEM_OUT\u0026#34;\u0026gt; \u0026lt;PatternLayout pattern=\u0026#34;%d [%traceId] %-5p %c{1}:%L - %m%n\u0026#34;/\u0026gt; \u0026lt;/Console\u0026gt; \u0026lt;/Appenders\u0026gt; \u0026lt;Loggers\u0026gt; \u0026lt;AsyncRoot level=\u0026#34;INFO\u0026#34;\u0026gt; \u0026lt;AppenderRef ref=\u0026#34;Console\u0026#34;/\u0026gt; \u0026lt;/AsyncRoot\u0026gt; \u0026lt;/Loggers\u0026gt; \u0026lt;/Configuration\u0026gt;   Support log4j2 AsyncAppender , No additional configuration is required. Refer to the demo of log4j2.xml below.\nFor details: All Loggers Async\nLog4j-2.9 and higher require disruptor-3.3.4.jar or higher on the classpath. Prior to Log4j-2.9, disruptor-3.0.0.jar or higher was required. This is simplest to configure and gives the best performance. To make all loggers asynchronous, add the disruptor jar to the classpath and set the system property log4j2.contextSelector to org.apache.logging.log4j.core.async.AsyncLoggerContextSelector.\n\u0026lt;Configuration status=\u0026#34;WARN\u0026#34;\u0026gt; \u0026lt;Appenders\u0026gt; \u0026lt;!-- Async Loggers will auto-flush in batches, so switch off immediateFlush. --\u0026gt; \u0026lt;RandomAccessFile name=\u0026#34;RandomAccessFile\u0026#34; fileName=\u0026#34;async.log\u0026#34; immediateFlush=\u0026#34;false\u0026#34; append=\u0026#34;false\u0026#34;\u0026gt; \u0026lt;PatternLayout\u0026gt; \u0026lt;Pattern\u0026gt;%d %p %c{1.} [%t] [%traceId] %m %ex%n\u0026lt;/Pattern\u0026gt; \u0026lt;/PatternLayout\u0026gt; \u0026lt;/RandomAccessFile\u0026gt; \u0026lt;/Appenders\u0026gt; \u0026lt;Loggers\u0026gt; \u0026lt;Root level=\u0026#34;info\u0026#34; includeLocation=\u0026#34;false\u0026#34;\u0026gt; \u0026lt;AppenderRef ref=\u0026#34;RandomAccessFile\u0026#34;/\u0026gt; \u0026lt;/Root\u0026gt; \u0026lt;/Loggers\u0026gt; \u0026lt;/Configuration\u0026gt; For details: Mixed Sync \u0026amp; Async\nLog4j-2.9 and higher require disruptor-3.3.4.jar or higher on the classpath. Prior to Log4j-2.9, disruptor-3.0.0.jar or higher was required. There is no need to set system property Log4jContextSelector to any value.\n\u0026lt;Configuration status=\u0026#34;WARN\u0026#34;\u0026gt; \u0026lt;Appenders\u0026gt; \u0026lt;!-- Async Loggers will auto-flush in batches, so switch off immediateFlush. --\u0026gt; \u0026lt;RandomAccessFile name=\u0026#34;RandomAccessFile\u0026#34; fileName=\u0026#34;asyncWithLocation.log\u0026#34; immediateFlush=\u0026#34;false\u0026#34; append=\u0026#34;false\u0026#34;\u0026gt; \u0026lt;PatternLayout\u0026gt; \u0026lt;Pattern\u0026gt;%d %p %class{1.} [%t] [%traceId] %location %m %ex%n\u0026lt;/Pattern\u0026gt; \u0026lt;/PatternLayout\u0026gt; \u0026lt;/RandomAccessFile\u0026gt; \u0026lt;/Appenders\u0026gt; \u0026lt;Loggers\u0026gt; \u0026lt;!-- pattern layout actually uses location, so we need to include it --\u0026gt; \u0026lt;AsyncLogger name=\u0026#34;com.foo.Bar\u0026#34; level=\u0026#34;trace\u0026#34; includeLocation=\u0026#34;true\u0026#34;\u0026gt; \u0026lt;AppenderRef ref=\u0026#34;RandomAccessFile\u0026#34;/\u0026gt; \u0026lt;/AsyncLogger\u0026gt; \u0026lt;Root level=\u0026#34;info\u0026#34; includeLocation=\u0026#34;true\u0026#34;\u0026gt; \u0026lt;AppenderRef ref=\u0026#34;RandomAccessFile\u0026#34;/\u0026gt; \u0026lt;/Root\u0026gt; \u0026lt;/Loggers\u0026gt; \u0026lt;/Configuration\u0026gt;   Support log4j2 AsyncAppender, For details: Log4j2 AsyncAppender\n  \u0026lt;Configuration\u0026gt; \u0026lt;Appenders\u0026gt; \u0026lt;Console name=\u0026#34;Console\u0026#34; target=\u0026#34;SYSTEM_OUT\u0026#34;\u0026gt; \u0026lt;PatternLayout pattern=\u0026#34;%d [%traceId] %-5p %c{1}:%L - %m%n\u0026#34;/\u0026gt; \u0026lt;/Console\u0026gt; \u0026lt;Async name=\u0026#34;Async\u0026#34;\u0026gt; \u0026lt;AppenderRef ref=\u0026#34;Console\u0026#34;/\u0026gt; \u0026lt;/Async\u0026gt; \u0026lt;/Appenders\u0026gt; \u0026lt;Loggers\u0026gt; \u0026lt;Root level=\u0026#34;INFO\u0026#34;\u0026gt; \u0026lt;AppenderRef ref=\u0026#34;Async\u0026#34;/\u0026gt; \u0026lt;/Root\u0026gt; \u0026lt;/Loggers\u0026gt; \u0026lt;/Configuration\u0026gt;  When you use -javaagent to active the sky-walking tracer, log4j2 will output traceId, if it existed. If the tracer is inactive, the output will be TID: N/A.  gRPC reporter The gRPC report could forward the collected logs to SkyWalking OAP server, or SkyWalking Satellite sidecar. Trace id, segment id, and span id will attach to logs automatically. You don\u0026rsquo;t need to change the layout.\n Add GRPCLogClientAppender in log4j2.xml  \u0026lt;GRPCLogClientAppender name=\u0026#34;grpc-log\u0026#34;/\u0026gt;  Add config of the plugin or use default  plugin.toolkit.log.grpc.reporter.server_host=${SW_GRPC_LOG_SERVER_HOST:127.0.0.1} plugin.toolkit.log.grpc.reporter.server_port=${SW_GRPC_LOG_SERVER_PORT:11800} plugin.toolkit.log.grpc.reporter.max_message_size=${SW_GRPC_LOG_MAX_MESSAGE_SIZE:10485760} plugin.toolkit.log.grpc.reporter.upstream_timeout=${SW_GRPC_LOG_GRPC_UPSTREAM_TIMEOUT:30} Transmitting un-formatted messages The log4j 2.x gRPC reporter supports transmitting logs as formatted or un-formatted. Transmitting formatted data is the default but can be disabled by adding the following to the agent config:\nplugin.toolkit.log.transmit_formatted=false The above will result in the content field being used for the log pattern with additional log tags of argument.0, argument.1, and so on representing each logged argument as well as an additional exception tag which is only present if a throwable is also logged.\nFor example, the following code:\nlog.info(\u0026#34;{} {} {}\u0026#34;, 1, 2, 3); Will result in:\n{ \u0026#34;content\u0026#34;: \u0026#34;{} {} {}\u0026#34;, \u0026#34;tags\u0026#34;: [ { \u0026#34;key\u0026#34;: \u0026#34;argument.0\u0026#34;, \u0026#34;value\u0026#34;: \u0026#34;1\u0026#34; }, { \u0026#34;key\u0026#34;: \u0026#34;argument.1\u0026#34;, \u0026#34;value\u0026#34;: \u0026#34;2\u0026#34; }, { \u0026#34;key\u0026#34;: \u0026#34;argument.2\u0026#34;, \u0026#34;value\u0026#34;: \u0026#34;3\u0026#34; } ] } ","excerpt":"Dependency the toolkit, such as using maven or gradle  \u0026lt;dependency\u0026gt; …","ref":"/docs/main/v8.4.0/en/setup/service-agent/java-agent/application-toolkit-log4j-2.x/","title":"Dependency the toolkit, such as using maven or gradle"},{"body":" Dependency the toolkit, such as using maven or gradle  \u0026lt;dependency\u0026gt; \u0026lt;groupId\u0026gt;org.apache.skywalking\u0026lt;/groupId\u0026gt; \u0026lt;artifactId\u0026gt;apm-toolkit-meter\u0026lt;/artifactId\u0026gt; \u0026lt;version\u0026gt;${skywalking.version}\u0026lt;/version\u0026gt; \u0026lt;/dependency\u0026gt; If you\u0026rsquo;re using Spring sleuth, you could use Spring Sleuth Setup.\n Counter API represents a single monotonically increasing counter, automatic collect data and report to backend.  import org.apache.skywalking.apm.toolkit.meter.MeterFactory; Counter counter = MeterFactory.counter(meterName).tag(\u0026#34;tagKey\u0026#34;, \u0026#34;tagValue\u0026#34;).mode(Counter.Mode.INCREMENT).build(); counter.increment(1d);  MeterFactory.counter Create a new counter builder with the meter name. Counter.Builder.tag(String key, String value) Mark a tag key/value pair. Counter.Builder.mode(Counter.Mode mode) Change the counter mode, RATE mode means reporting rate to the backend. Counter.Builder.build() Build a new Counter which is collected and reported to the backend. Counter.increment(double count) Increment count to the Counter, It could be a positive value.   Gauge API represents a single numerical value.  import org.apache.skywalking.apm.toolkit.meter.MeterFactory; ThreadPoolExecutor threadPool = ...; Gauge gauge = MeterFactory.gauge(meterName, () -\u0026gt; threadPool.getActiveCount()).tag(\u0026#34;tagKey\u0026#34;, \u0026#34;tagValue\u0026#34;).build();  MeterFactory.gauge(String name, Supplier\u0026lt;Double\u0026gt; getter) Create a new gauge builder with the meter name and supplier function, this function need to return a double value. Gauge.Builder.tag(String key, String value) Mark a tag key/value pair. Gauge.Builder.build() Build a new Gauge which is collected and reported to the backend.   Histogram API represents a summary sample observations with customize buckets.  import org.apache.skywalking.apm.toolkit.meter.MeterFactory; Histogram histogram = MeterFactory.histogram(\u0026#34;test\u0026#34;).tag(\u0026#34;tagKey\u0026#34;, \u0026#34;tagValue\u0026#34;).steps(Arrays.asList(1, 5, 10)).minValue(0).build(); histogram.addValue(3);  MeterFactory.histogram(String name) Create a new histogram builder with the meter name. Histogram.Builder.tag(String key, String value) Mark a tag key/value pair. Histogram.Builder.steps(List\u0026lt;Double\u0026gt; steps) Set up the max values of every histogram buckets. Histogram.Builder.minValue(double value) Set up the minimal value of this histogram, default is 0. Histogram.Builder.build() Build a new Histogram which is collected and reported to the backend. Histogram.addValue(double value) Add value into the histogram, automatically analyze what bucket count needs to be increment. rule: count into [step1, step2).  ","excerpt":"Dependency the toolkit, such as using maven or gradle  \u0026lt;dependency\u0026gt; …","ref":"/docs/main/v8.4.0/en/setup/service-agent/java-agent/application-toolkit-meter/","title":"Dependency the toolkit, such as using maven or gradle"},{"body":" Dependency the toolkit, such as using maven or gradle  \u0026lt;dependency\u0026gt; \u0026lt;groupId\u0026gt;org.apache.skywalking\u0026lt;/groupId\u0026gt; \u0026lt;artifactId\u0026gt;apm-toolkit-micrometer-registry\u0026lt;/artifactId\u0026gt; \u0026lt;version\u0026gt;${skywalking.version}\u0026lt;/version\u0026gt; \u0026lt;/dependency\u0026gt;  Using org.apache.skywalking.apm.meter.micrometer.SkywalkingMeterRegistry as the registry, it could forward the MicroMeter collected metrics to OAP server.  import org.apache.skywalking.apm.meter.micrometer.SkywalkingMeterRegistry; SkywalkingMeterRegistry registry = new SkywalkingMeterRegistry(); // If you has some counter want to rate by agent side SkywalkingConfig config = new SkywalkingConfig(Arrays.asList(\u0026#34;test_rate_counter\u0026#34;)); new SkywalkingMeterRegistry(config); // Also you could using composite registry to combine multiple meter registry, such as collect to Skywalking and prometheus CompositeMeterRegistry compositeRegistry = new CompositeMeterRegistry(); compositeRegistry.add(new PrometheusMeterRegistry(PrometheusConfig.DEFAULT)); compositeRegistry.add(new SkywalkingMeterRegistry());   Using snake case as the naming convention. Such as test.meter will be send to test_meter.\n  Using Millisecond as the time unit.\n  Adapt micrometer data convention.\n     Micrometer data type Transform to meter name Skywalking data type Description     Counter Counter name Counter Same with counter   Gauges Gauges name Gauges Same with gauges   Timer Timer name + \u0026ldquo;_count\u0026rdquo; Counter Execute finished count    Timer name + \u0026ldquo;_sum\u0026rdquo; Counter Total execute finished duration    Timer name + \u0026ldquo;_max\u0026rdquo; Gauges Max duration of execute finished time    Timer name + \u0026ldquo;_histogram\u0026rdquo; Histogram Histogram of execute finished duration   LongTaskTimer Timer name + \u0026ldquo;_active_count\u0026rdquo; Gauges Executing task count    Timer name + \u0026ldquo;_duration_sum\u0026rdquo; Counter All of executing task sum duration    Timer name + \u0026ldquo;_max\u0026rdquo; Counter Current longest running task execute duration   Function Timer Timer name + \u0026ldquo;_count\u0026rdquo; Gauges Execute finished timer count    Timer name + \u0026ldquo;_sum\u0026rdquo; Gauges Execute finished timer total duration   Function Counter Counter name Counter Custom counter value   Distribution summary Summary name + \u0026ldquo;_count\u0026rdquo; Counter Total record count    Summary name + \u0026ldquo;_sum\u0026rdquo; Counter Total record amount sum    Summary name + \u0026ldquo;_max\u0026rdquo; Gauges Max record amount    Summary name + \u0026ldquo;_histogram\u0026rdquo; Gauges Histogram of the amount     Not Adapt data convention.     Micrometer data type Data type     LongTaskTimer Histogram    ","excerpt":"Dependency the toolkit, such as using maven or gradle  \u0026lt;dependency\u0026gt; …","ref":"/docs/main/v8.4.0/en/setup/service-agent/java-agent/application-toolkit-micrometer/","title":"Dependency the toolkit, such as using maven or gradle"},{"body":" Dependency the toolkit, such as using maven or gradle  \u0026lt;dependency\u0026gt; \u0026lt;groupId\u0026gt;org.apache.skywalking\u0026lt;/groupId\u0026gt; \u0026lt;artifactId\u0026gt;apm-toolkit-trace\u0026lt;/artifactId\u0026gt; \u0026lt;version\u0026gt;${skywalking.version}\u0026lt;/version\u0026gt; \u0026lt;/dependency\u0026gt;  Use TraceContext.traceId() API to obtain traceId.  import TraceContext; ... modelAndView.addObject(\u0026#34;traceId\u0026#34;, TraceContext.traceId());  Use TraceContext.segmentId() API to obtain segmentId.  import TraceContext; ... modelAndView.addObject(\u0026#34;segmentId\u0026#34;, TraceContext.segmentId());  Use TraceContext.spanId() API to obtain spanId.  import TraceContext; ... modelAndView.addObject(\u0026#34;spanId\u0026#34;, TraceContext.spanId()); Sample codes only\n  Add @Trace to any method you want to trace. After that, you can see the span in the Stack.\n  Methods annotated with @Tag will try to tag the current active span with the given key (Tag#key()) and (Tag#value()), if there is no active span at all, this annotation takes no effect. @Tag can be repeated, and can be used in companion with @Trace, see examples below. The value of Tag is the same as what are supported in Customize Enhance Trace.\n  Add custom tag in the context of traced method, ActiveSpan.tag(\u0026quot;key\u0026quot;, \u0026quot;val\u0026quot;).\n  ActiveSpan.error() Mark the current span as error status.\n  ActiveSpan.error(String errorMsg) Mark the current span as error status with a message.\n  ActiveSpan.error(Throwable throwable) Mark the current span as error status with a Throwable.\n  ActiveSpan.debug(String debugMsg) Add a debug level log message in the current span.\n  ActiveSpan.info(String infoMsg) Add an info level log message in the current span.\n  ActiveSpan.setOperationName(String operationName) Customize an operation name.\n  ActiveSpan.tag(\u0026#34;my_tag\u0026#34;, \u0026#34;my_value\u0026#34;); ActiveSpan.error(); ActiveSpan.error(\u0026#34;Test-Error-Reason\u0026#34;); ActiveSpan.error(new RuntimeException(\u0026#34;Test-Error-Throwable\u0026#34;)); ActiveSpan.info(\u0026#34;Test-Info-Msg\u0026#34;); ActiveSpan.debug(\u0026#34;Test-debug-Msg\u0026#34;); /** * The codes below will generate a span, * and two types of tags, one type tag: keys are `tag1` and `tag2`, values are the passed-in parameters, respectively, the other type tag: keys are `username` and `age`, values are the return value in User, respectively */ @Trace @Tag(key = \u0026#34;tag1\u0026#34;, value = \u0026#34;arg[0]\u0026#34;) @Tag(key = \u0026#34;tag2\u0026#34;, value = \u0026#34;arg[1]\u0026#34;) @Tag(key = \u0026#34;username\u0026#34;, value = \u0026#34;returnedObj.username\u0026#34;) @Tag(key = \u0026#34;age\u0026#34;, value = \u0026#34;returnedObj.age\u0026#34;) public User methodYouWantToTrace(String param1, String param2) { // ActiveSpan.setOperationName(\u0026#34;Customize your own operation name, if this is an entry span, this would be an endpoint name\u0026#34;);  // ... }  Use TraceContext.putCorrelation() API to put custom data in tracing context.  Optional\u0026lt;String\u0026gt; previous = TraceContext.putCorrelation(\u0026#34;customKey\u0026#34;, \u0026#34;customValue\u0026#34;); CorrelationContext will remove the item when the value is null or empty.\n Use TraceContext.getCorrelation() API to get custom data.  Optional\u0026lt;String\u0026gt; value = TraceContext.getCorrelation(\u0026#34;customKey\u0026#34;); CorrelationContext configuration descriptions could be found in the agent configuration documentation, with correlation. as the prefix.\n","excerpt":"Dependency the toolkit, such as using maven or gradle  \u0026lt;dependency\u0026gt; …","ref":"/docs/main/v8.4.0/en/setup/service-agent/java-agent/application-toolkit-trace/","title":"Dependency the toolkit, such as using maven or gradle"},{"body":" Dependency the toolkit, such as using maven or gradle  \u0026lt;dependency\u0026gt; \u0026lt;groupId\u0026gt;org.apache.skywalking\u0026lt;/groupId\u0026gt; \u0026lt;artifactId\u0026gt;apm-toolkit-opentracing\u0026lt;/artifactId\u0026gt; \u0026lt;version\u0026gt;{project.release.version}\u0026lt;/version\u0026gt; \u0026lt;/dependency\u0026gt;  Use our OpenTracing tracer implementation  Tracer tracer = new SkywalkingTracer(); Tracer.SpanBuilder spanBuilder = tracer.buildSpan(\u0026#34;/yourApplication/yourService\u0026#34;); ","excerpt":" Dependency the toolkit, such as using maven or gradle  \u0026lt;dependency\u0026gt; …","ref":"/docs/main/v8.4.0/en/setup/service-agent/java-agent/opentracing/","title":"Dependency the toolkit, such as using maven or gradle"},{"body":"Deploy SkyWalking backend and UI in kubernetes Follow instructions in the deploying SkyWalking backend to Kubernetes cluster to deploy oap and ui to a kubernetes cluster.\nPlease read the Readme file.\n","excerpt":"Deploy SkyWalking backend and UI in kubernetes Follow instructions in the deploying SkyWalking …","ref":"/docs/main/v8.4.0/en/setup/backend/backend-k8s/","title":"Deploy SkyWalking backend and UI in kubernetes"},{"body":"Design Goals The document outlines the core design goals for SkyWalking project.\n  Keep Observability. No matter how does the target system deploy, SkyWalking could provide a solution or integration way to keep observability for it. Based on this, SkyWalking provides several runtime forms and probes.\n  Topology, Metrics and Trace Together. The first step to see and understand a distributed system should be from topology map. It visualizes the whole complex system as an easy map. Under that topology, OSS people requires more about metrics for service, instance, endpoint and calls. Trace exists as detail logs for making sense of those metrics. Such as when endpoint latency becomes long, you want to see the slowest the trace to find out why. So you can see, they are from big picture to details, they are all needed. SkyWalking integrates and provides a lot of features to make this possible and easy understand.\n  Light Weight. There two parts of light weight are needed. (1) In probe, we just depend on network communication framework, prefer gRPC. By that, the probe should be as small as possible, to avoid the library conflicts and the payload of VM, such as permsize requirement in JVM. (2) As an observability platform, it is secondary and third level system in your project environment. So we are using our own light weight framework to build the backend core. Then you don\u0026rsquo;t need to deploy big data tech platform and maintain them. SkyWalking should be simple in tech stack.\n  Pluggable. SkyWalking core team provides many default implementations, but definitely it is not enough, and also don\u0026rsquo;t fit every scenario. So, we provide a lot of features for being pluggable.\n  Portability. SkyWalking can run in multiple environments, including: (1) Use traditional register center like eureka. (2) Use RPC framework including service discovery, like Spring Cloud, Apache Dubbo. (3) Use Service Mesh in modern infrastructure. (4) Use cloud services. (5) Across cloud deployment. SkyWalking should run well in all these cases.\n  Interop. Observability is a big landscape, SkyWalking is impossible to support all, even by its community. As that, it supports to interop with other OSS system, mostly probes, such as Zipkin, Jaeger, OpenTracing, OpenCensus. To accept and understand their data formats makes sure SkyWalking more useful for end users. And don\u0026rsquo;t require the users to switch their libraries.\n  What is next?  See probe Introduction to know SkyWalking\u0026rsquo;s probe groups. From backend overview, you can understand what backend does after it received probe data. If you want to customize UI, start with UI overview document.  ","excerpt":"Design Goals The document outlines the core design goals for SkyWalking project.\n  Keep …","ref":"/docs/main/v8.4.0/en/concepts-and-designs/project-goals/","title":"Design Goals"},{"body":"Disable plugins Delete or remove the specific libraries / jars in skywalking-agent/plugins/*.jar\n+-- skywalking-agent +-- activations apm-toolkit-log4j-1.x-activation.jar apm-toolkit-log4j-2.x-activation.jar apm-toolkit-logback-1.x-activation.jar ... +-- config agent.config +-- plugins apm-dubbo-plugin.jar apm-feign-default-http-9.x.jar apm-httpClient-4.x-plugin.jar ..... skywalking-agent.jar ","excerpt":"Disable plugins Delete or remove the specific libraries / jars in skywalking-agent/plugins/*.jar\n+-- …","ref":"/docs/main/v8.4.0/en/setup/service-agent/java-agent/how-to-disable-plugin/","title":"Disable plugins"},{"body":"Dynamic Configuration SkyWalking Configurations mostly are set through application.yml and OS system environment variables. At the same time, some of them are supporting dynamic settings from upstream management system.\nRight now, SkyWalking supports following dynamic configurations.\n   Config Key Value Description Value Format Example     agent-analyzer.default.slowDBAccessThreshold Thresholds of slow Database statement, override receiver-trace/default/slowDBAccessThreshold of application.yml. default:200,mongodb:50   agent-analyzer.default.uninstrumentedGateways The uninstrumented gateways, override gateways.yml. same as gateways.yml   alarm.default.alarm-settings The alarm settings, will override alarm-settings.yml. same as alarm-settings.yml   core.default.apdexThreshold The apdex threshold settings, will override service-apdex-threshold.yml. same as service-apdex-threshold.yml   core.default.endpoint-name-grouping The endpoint name grouping setting, will override endpoint-name-grouping.yml. same as endpoint-name-grouping.yml   agent-analyzer.default.sampleRate Trace sampling , override receiver-trace/default/sampleRate of application.yml. 10000   agent-analyzer.default.slowTraceSegmentThreshold Setting this threshold about the latency would make the slow trace segments sampled if they cost more time, even the sampling mechanism activated. The default value is -1, which means would not sample slow traces. Unit, millisecond. override receiver-trace/default/slowTraceSegmentThreshold of application.yml. -1   configuration-discovery.default.agentConfigurations The ConfigurationDiscovery settings look at configuration-discovery.md    This feature depends on upstream service, so it is DISABLED by default.\nconfiguration: selector: ${SW_CONFIGURATION:none} none: grpc: host: ${SW_DCS_SERVER_HOST:\u0026#34;\u0026#34;} port: ${SW_DCS_SERVER_PORT:80} clusterName: ${SW_DCS_CLUSTER_NAME:SkyWalking} period: ${SW_DCS_PERIOD:20} # ... other implementations Dynamic Configuration Service, DCS Dynamic Configuration Service is a gRPC service, which requires the upstream system implemented. The SkyWalking OAP fetches the configuration from the implementation(any system), after you open this implementation like this.\nconfiguration: selector: ${SW_CONFIGURATION:grpc} grpc: host: ${SW_DCS_SERVER_HOST:\u0026#34;\u0026#34;} port: ${SW_DCS_SERVER_PORT:80} clusterName: ${SW_DCS_CLUSTER_NAME:SkyWalking} period: ${SW_DCS_PERIOD:20} Dynamic Configuration Zookeeper Implementation Zookeeper is also supported as DCC(Dynamic Configuration Center), to use it, please configure as follows:\nconfiguration: selector: ${SW_CONFIGURATION:zookeeper} zookeeper: period: ${SW_CONFIG_ZK_PERIOD:60} # Unit seconds, sync period. Default fetch every 60 seconds. nameSpace: ${SW_CONFIG_ZK_NAMESPACE:/default} hostPort: ${SW_CONFIG_ZK_HOST_PORT:localhost:2181} # Retry Policy baseSleepTimeMs: ${SW_CONFIG_ZK_BASE_SLEEP_TIME_MS:1000} # initial amount of time to wait between retries maxRetries: ${SW_CONFIG_ZK_MAX_RETRIES:3} # max number of times to retry The nameSpace is the ZooKeeper path. The config key and value are the properties of the namespace folder.\nDynamic Configuration Etcd Implementation Etcd is also supported as DCC(Dynamic Configuration Center), to use it, please configure as follows:\nconfiguration: selector: ${SW_CONFIGURATION:etcd} etcd: period: ${SW_CONFIG_ETCD_PERIOD:60} # Unit seconds, sync period. Default fetch every 60 seconds. group: ${SW_CONFIG_ETCD_GROUP:skywalking} serverAddr: ${SW_CONFIG_ETCD_SERVER_ADDR:localhost:2379} clusterName: ${SW_CONFIG_ETCD_CLUSTER_NAME:default} Dynamic Configuration Consul Implementation Consul is also supported as DCC(Dynamic Configuration Center), to use it, please configure as follows:\nconfiguration: selector: ${SW_CONFIGURATION:consul} consul: # Consul host and ports, separated by comma, e.g. 1.2.3.4:8500,2.3.4.5:8500 hostAndPorts: ${SW_CONFIG_CONSUL_HOST_AND_PORTS:1.2.3.4:8500} # Sync period in seconds. Defaults to 60 seconds. period: ${SW_CONFIG_CONSUL_PERIOD:1} # Consul aclToken aclToken: ${SW_CONFIG_CONSUL_ACL_TOKEN:\u0026#34;\u0026#34;} Dynamic Configuration Apollo Implementation Apollo is also supported as DCC(Dynamic Configuration Center), to use it, just configured as follows:\nconfiguration: selector: ${SW_CONFIGURATION:apollo} apollo: apolloMeta: ${SW_CONFIG_APOLLO:http://106.12.25.204:8080} apolloCluster: ${SW_CONFIG_APOLLO_CLUSTER:default} apolloEnv: ${SW_CONFIG_APOLLO_ENV:\u0026#34;\u0026#34;} appId: ${SW_CONFIG_APOLLO_APP_ID:skywalking} period: ${SW_CONFIG_APOLLO_PERIOD:5} Dynamic Configuration Kuberbetes Configmap Implementation configmap is also supported as DCC(Dynamic Configuration Center), to use it, just configured as follows:\nconfiguration: selector: ${SW_CONFIGURATION:k8s-configmap} # [example] (../../../../oap-server/server-configuration/configuration-k8s-configmap/src/test/resources/skywalking-dynamic-configmap.example.yaml) k8s-configmap: # Sync period in seconds. Defaults to 60 seconds. period: ${SW_CONFIG_CONFIGMAP_PERIOD:60} # Which namespace is confiigmap deployed in. namespace: ${SW_CLUSTER_K8S_NAMESPACE:default} # Labelselector is used to locate specific configmap labelSelector: ${SW_CLUSTER_K8S_LABEL:app=collector,release=skywalking} Dynamic Configuration Nacos Implementation Nacos is also supported as DCC(Dynamic Configuration Center), to use it, please configure as follows:\nconfiguration: selector: ${SW_CONFIGURATION:nacos} nacos: # Nacos Server Host serverAddr: ${SW_CONFIG_NACOS_SERVER_ADDR:127.0.0.1} # Nacos Server Port port: ${SW_CONFIG_NACOS_SERVER_PORT:8848} # Nacos Configuration Group group: ${SW_CONFIG_NACOS_SERVER_GROUP:skywalking} # Nacos Configuration namespace namespace: ${SW_CONFIG_NACOS_SERVER_NAMESPACE:} # Unit seconds, sync period. Default fetch every 60 seconds. period: ${SW_CONFIG_NACOS_PERIOD:60} # the name of current cluster, set the name if you want to upstream system known. clusterName: ${SW_CONFIG_NACOS_CLUSTER_NAME:default} ","excerpt":"Dynamic Configuration SkyWalking Configurations mostly are set through application.yml and OS system …","ref":"/docs/main/v8.4.0/en/setup/backend/dynamic-config/","title":"Dynamic Configuration"},{"body":"ElasticSearch Some new users may face\n ElasticSearch performance is not as good as expected. Such as, can\u0026rsquo;t have latest data after a while.  Or\n ERROR CODE 429.   Suppressed: org.elasticsearch.client.ResponseException: method [POST], host [http://127.0.0.1:9200], URI [/service_instance_inventory/type/6_tcc-app-gateway-77b98ff6ff-crblx.cards_0_0/_update?refresh=true\u0026amp;timeout=1m], status line [HTTP/1.1 429 Too Many Requests] {\u0026quot;error\u0026quot;:{\u0026quot;root_cause\u0026quot;:[{\u0026quot;type\u0026quot;:\u0026quot;remote_transport_exception\u0026quot;,\u0026quot;reason\u0026quot;:\u0026quot;[elasticsearch-0][10.16.9.130:9300][indices:data/write/update[s]]\u0026quot;}],\u0026quot;type\u0026quot;:\u0026quot;es_rejected_execution_exception\u0026quot;,\u0026quot;reason\u0026quot;:\u0026quot;rejected execution of org.elasticsearch.transport.TransportService$7@19a5cf02 on EsThreadPoolExecutor[name = elasticsearch-0/write, queue capacity = 200, org.elasticsearch.common.util.concurrent.EsThreadPoolExecutor@389297ad[Running, pool size = 2, active threads = 2, queued tasks = 200, completed tasks = 147611]]\u0026quot;},\u0026quot;status\u0026quot;:429} at org.elasticsearch.client.RestClient$SyncResponseListener.get(RestClient.java:705) ~[elasticsearch-rest-client-6.3.2.jar:6.3.2] at org.elasticsearch.client.RestClient.performRequest(RestClient.java:235) ~[elasticsearch-rest-client-6.3.2.jar:6.3.2] at org.elasticsearch.client.RestClient.performRequest(RestClient.java:198) ~[elasticsearch-rest-client-6.3.2.jar:6.3.2] at org.elasticsearch.client.RestHighLevelClient.performRequest(RestHighLevelClient.java:522) ~[elasticsearch You could add following config to elasticsearch.yml, set the value based on your env.\n# In tracing scenario, consider to set more than this at least. thread_pool.index.queue_size: 1000 thread_pool.write.queue_size: 1000 # When you face query error at trace page, remember to check this. index.max_result_window: 1000000 Read ElasticSearch official documents to get more information.\n","excerpt":"ElasticSearch Some new users may face\n ElasticSearch performance is not as good as expected. Such …","ref":"/docs/main/v8.4.0/en/faq/es-server-faq/","title":"ElasticSearch"},{"body":"Exporter tool of profile raw data When the visualization doesn\u0026rsquo;t work well through the official UI, users could submit the issue to report. This tool helps the users to package the original profile data for helping the community to locate the issue in the user case. NOTICE, this report includes the class name, method name, line number, etc. Before submit this, please make sure this wouldn\u0026rsquo;t become your system vulnerability.\nExport command line Usage  Set the storage in tools/profile-exporter/application.yml file by following your use case. Prepare data  Profile task id: Profile task id Trace id: Wrong profiled trace id Export dir: Directory of the data will export   Enter the Skywalking root path Execute shell command bash tools/profile-exporter/profile_exporter.sh --taskid={profileTaskId} --traceid={traceId} {exportDir}  The file {traceId}.tar.gz will be generated after execution shell.  Exported data content  basic.yml: Contains the complete information of the profiled segments in the trace. snapshot.data: All monitored thread snapshot data in the current segment.  Report profile issue  Provide exported data generated from this tool. Provide span operation name, analyze mode(include/exclude children). Issue description. (If there have the UI screenshots, it\u0026rsquo;s better)  ","excerpt":"Exporter tool of profile raw data When the visualization doesn\u0026rsquo;t work well through the …","ref":"/docs/main/v8.4.0/en/guides/backend-profile-export/","title":"Exporter tool of profile raw data"},{"body":"Extend storage SkyWalking has already provided several storage solutions. In this document, you could learn how to implement a new storage easily.\nDefine your storage provider  Define a class extends org.apache.skywalking.oap.server.library.module.ModuleProvider. Set this provider targeting to Storage module.  @Override public Class\u0026lt;? extends ModuleDefine\u0026gt; module() { return StorageModule.class; } Implement all DAOs Here is the list of all DAO interfaces in storage\n IServiceInventoryCacheDAO IServiceInstanceInventoryCacheDAO IEndpointInventoryCacheDAO INetworkAddressInventoryCacheDAO IBatchDAO StorageDAO IRegisterLockDAO ITopologyQueryDAO IMetricsQueryDAO ITraceQueryDAO IMetadataQueryDAO IAggregationQueryDAO IAlarmQueryDAO IHistoryDeleteDAO IMetricsDAO IRecordDAO IRegisterDAO ILogQueryDAO ITopNRecordsQueryDAO IBrowserLogQueryDAO  Register all service implementations In public void prepare(), use this#registerServiceImplementation method to do register binding your implementation with the above interfaces.\nExample Take org.apache.skywalking.oap.server.storage.plugin.elasticsearch.StorageModuleElasticsearchProvider or org.apache.skywalking.oap.server.storage.plugin.jdbc.mysql.MySQLStorageProvider as a good example.\nRedistribution with new storage implementation. You don\u0026rsquo;t have to clone the main repo just for implementing the storage. You could just easy depend our Apache releases. Take a look at SkyAPM/SkyWalking-With-Es5x-Storage repo, SkyWalking v6 redistribution with ElasticSearch 5 TCP connection storage implementation.\n","excerpt":"Extend storage SkyWalking has already provided several storage solutions. In this document, you …","ref":"/docs/main/v8.4.0/en/guides/storage-extention/","title":"Extend storage"},{"body":"FAQs These are known and common FAQs. We welcome you to contribute yours.\nDesign  Why doesn\u0026rsquo;t SkyWalking involve MQ in the architecture?  Compiling  Protoc plugin fails in maven build Required items could not be found, when import project into Eclipse Maven compilation failure with python2 not found error Fix compiling on Mac M1 chip  Runtime  8.x+ upgrade Why metrics indexes(ElasticSearch) in Hour and Day precisions stop update after upgrade to 7.x? 6.x version upgrade Why only traces in UI? The trace doesn\u0026rsquo;t continue in kafka consumer side Agent or collector version upgrade, 3.x -\u0026gt; 5.0.0-alpha EnhanceRequireObjectCache class cast exception ElasticSearch server performance FAQ, including ERROR CODE:429 IllegalStateException when install Java agent on WebSphere 7 \u0026ldquo;FORBIDDEN/12/index read-only / allow delete (api)\u0026rdquo; appears in the log No data shown and backend replies with \u0026ldquo;Variable \u0026lsquo;serviceId\u0026rsquo; has coerced Null value for NonNull type \u0026lsquo;ID!'\u0026quot; Unexpected endpoint register warning after 6.6.0 Use the profile exporter tool if the profile analysis is not right Compatible with other javaagent bytecode processing Java agent memory leak when enhance Worker thread at use Thread Pool Thrift plugin  UI  What is VNode? And why does SkyWalking have that?  ","excerpt":"FAQs These are known and common FAQs. We welcome you to contribute yours.\nDesign  Why doesn\u0026rsquo;t …","ref":"/docs/main/v8.4.0/en/faq/readme/","title":"FAQs"},{"body":"Fix compiling on MacBook M1 chip Problem  When compiling according to How-to-build, The following problems will occur, causing the build to fail.  [ERROR] Failed to execute goal org.xolstice.maven.plugins:protobuf-maven-plugin:0.6.1:compile (grpc-build) on project apm-network: Unable to resolve artifact: Missing: [ERROR] ---------- [ERROR] 1) com.google.protobuf:protoc:exe:osx-aarch_64:3.12.0 [ERROR] [ERROR] Try downloading the file manually from the project website. [ERROR] [ERROR] Then, install it using the command: [ERROR] mvn install:install-file -DgroupId=com.google.protobuf -DartifactId=protoc -Dversion=3.12.0 -Dclassifier=osx-aarch_64 -Dpackaging=exe -Dfile=/path/to/file [ERROR] [ERROR] Alternatively, if you host your own repository you can deploy the file there: [ERROR] mvn deploy:deploy-file -DgroupId=com.google.protobuf -DartifactId=protoc -Dversion=3.12.0 -Dclassifier=osx-aarch_64 -Dpackaging=exe -Dfile=/path/to/file -Durl=[url] -DrepositoryId=[id] [ERROR] [ERROR] Path to dependency: [ERROR] 1) org.apache.skywalking:apm-network:jar:8.4.0-SNAPSHOT [ERROR] 2) com.google.protobuf:protoc:exe:osx-aarch_64:3.12.0 [ERROR] [ERROR] ---------- [ERROR] 1 required artifact is missing. Reason Because the dependent Protocol Buffers v3.14.0 does not have an osx-aarch_64 version, Protocol Buffers Releases link: https://github.com/protocolbuffers/protobuf/releases, fortunately, mac m1 is compatible with the osx-x86_64 version, before this version is available for download, you need to manually specify the osx-x86_64 version.\nResolve We can add -Dos.detected.classifier=osx-x86_64 after the original compilation parameters, for example: ./mvnw clean package -DskipTests -Dos.detected.classifier=osx-x86_64, After specifying, compile and run normally.\n","excerpt":"Fix compiling on MacBook M1 chip Problem  When compiling according to How-to-build, The following …","ref":"/docs/main/v8.4.0/en/faq/how-to-build-with-mac-m1/","title":"Fix compiling on MacBook M1 chip"},{"body":"Group Parameterized Endpoints In most cases, the endpoint should be detected automatically through the language agents, service mesh observability solution, or configuration of meter system.\nThere are some special cases, especially when people use REST style URI, the application codes put the parameter in the endpoint name, such as putting order id in the URI, like /prod/ORDER123 and /prod/ORDER123. But logically, people expect they could have an endpoint name like prod/{order-id}. This is the feature of parameterized endpoint grouping designed for.\nCurrent, user could set up grouping rules through the static YAML file, named endpoint-name-grouping.yml, or use Dynamic Configuration to initial and update the endpoint grouping rule.\nConfiguration Format No matter in static local file or dynamic configuration value, they are sharing the same YAML format.\ngrouping: # Endpoint of the service would follow the following rules - service-name: serviceA rules: # Logic name when the regex expression matched. - endpoint-name: /prod/{id} regex: \\/prod\\/.+ ","excerpt":"Group Parameterized Endpoints In most cases, the endpoint should be detected automatically through …","ref":"/docs/main/v8.4.0/en/setup/backend/endpoint-grouping-rules/","title":"Group Parameterized Endpoints"},{"body":"Guides There are many ways that you can help the SkyWalking community.\n Go through our documents, point out or fix unclear things. Translate the documents to other languages. Download our releases, try to monitor your applications, and feedback to us about what you think. Read our source codes, Ask questions for details. Find some bugs, submit issue, and try to fix it. Find help wanted issues, which are good for you to start. Submit issue or start discussion through GitHub issue. See all mail list discussion through website list review. If you are a SkyWalking committer, could login and use the mail list in browser mode. Otherwise, follow the next step to subscribe. Issue report and discussion also could take place in dev@skywalking.apache.org. Mail to dev-subscribe@skywalking.apache.org, follow the reply to subscribe the mail list.  Contact Us All the following channels are open to the community, you could choose the way you like.\n Submit an issue Mail list: dev@skywalking.apache.org. Mail to dev-subscribe@skywalking.apache.org, follow the reply to subscribe the mail list. Gitter QQ Group: 392443393  Become official Apache SkyWalking Committer The PMC will assess the contributions of every contributor, including, but not limited to, code contributions, and follow the Apache guides to promote, vote and invite new committer and PMC member. Read Become official Apache SkyWalking Committer to get details.\nFor code developer For developers, first step, read Compiling Guide. It teaches developer how to build the project in local and set up the environment.\nIntegration Tests After setting up the environment and writing your codes, in order to make it more easily accepted by SkyWalking project, you\u0026rsquo;ll need to run the tests locally to verify that your codes don\u0026rsquo;t break any existed features, and write some unit test (UT) codes to verify that the new codes work well, preventing them being broke by future contributors. If the new codes involve other components or libraries, you\u0026rsquo;re also supposed to write integration tests (IT).\nSkyWalking leverages plugin maven-surefire-plugin to run the UTs while using maven-failsafe-plugin to run the ITs, maven-surefire-plugin will exclude ITs (whose class name starts with IT) and leave them for maven-failsafe-plugin to run, which is bound to the verify goal, CI-with-IT profile. Therefore, to run the UTs, try ./mvnw clean test, this will only run the UTs, not including ITs.\nIf you want to run the ITs please activate the CI-with-IT profile as well as the the profiles of the modules whose ITs you want to run. e.g. if you want to run the ITs in oap-server, try ./mvnw -Pbackend,CI-with-IT clean verify, and if you\u0026rsquo;d like to run all the ITs, simply run ./mvnw -Pall,CI-with-IT clean verify.\nPlease be advised that if you\u0026rsquo;re writing integration tests, name it with the pattern IT* to make them only run in CI-with-IT profile.\nEnd to End Tests (E2E for short) Since version 6.3.0, we have introduced more automatic tests to perform software quality assurance, E2E is one of the most important parts.\n End-to-end testing is a methodology used to test whether the flow of an application is performing as designed from start to finish. The purpose of carrying out end-to-end tests is to identify system dependencies and to ensure that the right information is passed between various system components and systems.\n The e2e test involves some/all of the OAP server, storage, coordinator, webapp, and the instrumented services, all of which are orchestrated by docker-compose, besides, there is a test controller(JUnit test) running outside of the container that sends traffics to the instrumented service, and then verifies the corresponding results after those requests, by GraphQL API of the SkyWalking Web App.\nWriting E2E Cases  Set up environment in IntelliJ IDEA  The e2e test is an separated project under the SkyWalking root directory and the IDEA cannot recognize it by default, right click on the file test/e2e/pom.xml and click Add as Maven Project, things should be ready now. But we recommend to open the directory skywalking/test/e2e in a separated IDE window for better experience because there may be shaded classes issues.\n Orchestrate the components  Our goal of E2E tests is to test the SkyWalking project in a whole, including the OAP server, storage, coordinator, webapp, and even the frontend UI(not now), in single node mode as well as cluster mode, therefore the first step is to determine what case we are going to verify and orchestrate the components.\nIn order to make it more easily to orchestrate, we\u0026rsquo;re using a docker-compose that provides a convenient file format (docker-compose.yml) to orchestrate the needed containers, and gives us possibilities to define the dependencies of the components.\nBasically you will need:\n Decide what (and how many) containers will be needed, e.g. for cluster testing, you\u0026rsquo;ll need \u0026gt; 2 OAP nodes, coordinators like zookeeper, storage like ElasticSearch, and instrumented services; Define the containers in docker-compose.yml, and carefully specify the dependencies, starting orders, and most importantly, link them together, e.g. set correct OAP address in the agent side, set correct coordinator address in OAP, etc. Write (or hopefully reuse) the test codes, to verify the results is correct.  As for the last step, we have a friendly framework to help you get started more quickly, which provides annotation @DockerCompose(\u0026quot;docker-compose.yml\u0026quot;) to load/parse and start up all the containers in a proper order, @ContainerHost/@ContainerPort to get the real host/port of the container, @ContainerHostAndPort to get both, @DockerContainer to get the running container.\n Write test controller  To put it simple, test controllers are basically tests that can be bound to the Maven integration-test/verify phase. They send designed requests to the instrumented service, and expect to get corresponding traces/metrics/metadata from the SkyWalking webapp GraphQL API.\nIn the test framework, we provide a TrafficController to periodically send traffic data to the instrumented services, you can simply enable it by giving a url and traffic data, refer to [../../../test/e2e/e2e-test/src/test/java/org/apache/skywalking/e2e/base/TrafficController.java].\n Troubleshooting  We expose all the logs from all containers to the stdout in non-CI (local) mode, but save/and upload them all to the GitHub server and you can download them (only when tests failed) in the right-up button \u0026ldquo;Artifacts/Download artifacts/logs\u0026rdquo; for debugging.\nNOTE: Please verify the newly-added E2E test case locally first, however, if you find it passed locally but failed in the PR check status, make sure all the updated/newly-added files (especially those in submodules) are committed and included in that PR, or reset the git HEAD to the remote and verify locally again.\nE2E local remote debugging When the E2E test is executed locally, if any test case fails, the E2E local remote debugging function can be used to quickly troubleshoot the bug.\nProject Extensions SkyWalking project supports many ways to extend existing features. If you are interesting in these ways, read the following guides.\n Java agent plugin development guide. This guide helps you to develop SkyWalking agent plugin to support more frameworks. Both open source plugin and private plugin developer should read this. If you want to build a new probe or plugin in any language, please read Component library definition and extension document. Storage extension development guide. Help potential contributors to build a new storage implementor besides the official. Customize analysis by oal script. OAL scripts locate in config/oal/*.oal. You could change it and reboot the OAP server. Read Observability Analysis Language Introduction if you need to learn about OAL script. Source and scope extension for new metrics. If you want to analysis a new metrics, which SkyWalking haven\u0026rsquo;t provide. You need to add a new receiver rather than choosing existed receiver. At that moment, you most likely need to add a new source and scope. This document will teach you how to do.  UI developer Our UI is constituted by static pages and web container.\n RocketBot UI is SkyWalking primary UI since 6.1 release. It is built with vue + typescript. You could know more at the rocketbot repository. Web container source codes are in apm-webapp module. This is a just an easy zuul proxy to host static resources and send GraphQL query requests to backend. Legacy UI repository is still there, but not included in SkyWalking release, after 6.0.0-GA.  OAP backend dependency management  This section is only applicable to the dependencies of the backend module\n Being one of the Top Level Projects of The Apache Software Foundation (ASF), SkyWalking is supposed to follow the ASF 3RD PARTY LICENSE POLICY, so if you\u0026rsquo;re adding new dependencies to the project, you\u0026rsquo;re responsible to check the newly-added dependencies won\u0026rsquo;t break the policy, and add their LICENSE\u0026rsquo;s and NOTICES\u0026rsquo;s to the project.\nWe have a simple script to help you make sure that you didn\u0026rsquo;t miss any newly-added dependency:\n Build a distribution package and unzip/untar it to folder dist. Run the script in the root directory, it will print out all newly-added dependencies. Check the LICENSE\u0026rsquo;s and NOTICE\u0026rsquo;s of those dependencies, if they can be included in an ASF project, add them in the apm-dist/release-docs/{LICENSE,NOTICE} file. Add those dependencies' names to the tools/dependencies/known-oap-backend-dependencies.txt file (alphabetical order), the next run of check-LICENSE.sh should pass.  Profile The performance profile is an enhancement feature in the APM system. We are using the thread dump to estimate the method execution time, rather than adding many local spans. In this way, the resource cost would be much less than using distributed tracing to locate slow method. This feature is suitable in the production environment. The following documents are important for developers to understand the key parts of this feature\n Profile data report procotol is provided like other trace, JVM data through gRPC. Thread dump merging mechanism introduces the merging mechanism, which helps the end users to understand the profile report. Exporter tool of profile raw data introduces when the visualization doesn\u0026rsquo;t work well through the official UI, how to package the original profile data, which helps the users report the issue.  For release Apache Release Guide introduces to the committer team about doing official Apache version release, to avoid breaking any Apache rule. Apache license allows everyone to redistribute if you keep our licenses and NOTICE in your redistribution.\n","excerpt":"Guides There are many ways that you can help the SkyWalking community.\n Go through our documents, …","ref":"/docs/main/v8.4.0/en/guides/readme/","title":"Guides"},{"body":"Health Check Health check intends to provide a unique approach to check the healthy status of OAP server. It includes the health status of modules, GraphQL and gRPC services readiness.\nHealth Checker Module. Health Checker module could solute how to observe the health status of modules. We can active it by below:\nhealth-checker: selector: ${SW_HEALTH_CHECKER:default} default: checkIntervalSeconds: ${SW_HEALTH_CHECKER_INTERVAL_SECONDS:5} Notice, we should enable telemetry module at the same time. That means the provider should not be - and none.\nAfter that, we can query OAP server health status by querying GraphQL:\nquery{ checkHealth{ score details } } If the OAP server is healthy, the response should be\n{ \u0026#34;data\u0026#34;: { \u0026#34;checkHealth\u0026#34;: { \u0026#34;score\u0026#34;: 0, \u0026#34;details\u0026#34;: \u0026#34;\u0026#34; } } } Once some modules are unhealthy, for instance, storage H2 is down. The result might be like below:\n{ \u0026#34;data\u0026#34;: { \u0026#34;checkHealth\u0026#34;: { \u0026#34;score\u0026#34;: 1, \u0026#34;details\u0026#34;: \u0026#34;storage_h2,\u0026#34; } } } You could refer to checkHealth query for more details.\nThe readiness of GraphQL and gRPC We could opt to above query to check the readiness of GraphQL.\nOAP has implemented gRPC Health Checking Protocol. We could use grpc-health-probe or any other tools to check the health of OAP gRPC services.\nCLI tool Please follow the CLI doc to get the health status score directly through the checkhealth command.\n","excerpt":"Health Check Health check intends to provide a unique approach to check the healthy status of OAP …","ref":"/docs/main/v8.4.0/en/setup/backend/backend-health-check/","title":"Health Check"},{"body":"How to build project This document helps people to compile and build the project in your maven and set your IDE.\nBuild Project Because we are using Git submodule, we recommend don\u0026rsquo;t use GitHub tag or release page to download source codes for compiling.\nMaven behind Proxy If you need to execute build behind the proxy, edit the .mvn/jvm.config and put the follow properties:\n-Dhttp.proxyHost=proxy_ip -Dhttp.proxyPort=proxy_port -Dhttps.proxyHost=proxy_ip -Dhttps.proxyPort=proxy_port -Dhttp.proxyUser=username -Dhttp.proxyPassword=password Build from GitHub   Prepare git, JDK8+ and Maven 3.6+\n  Clone project\nIf you want to build a release from source codes, provide a tag name by using git clone -b [tag_name] ... while cloning.\ngit clone --recurse-submodules https://github.com/apache/skywalking.git cd skywalking/ OR git clone https://github.com/apache/skywalking.git cd skywalking/ git submodule init git submodule update   Run ./mvnw clean package -DskipTests\n  All packages are in /dist (.tar.gz for Linux and .zip for Windows).\n  Build from Apache source code release  What is Apache source code release?  For each official Apache release, there is a complete and independent source code tar, which is including all source codes. You could download it from SkyWalking Apache download page. No git related stuff required when compiling this. Just follow these steps.\n Prepare JDK8+ and Maven 3.6+ Run ./mvnw clean package -DskipTests All packages are in /dist.(.tar.gz for Linux and .zip for Windows).  Advanced compile SkyWalking is a complex maven project, including many modules, which could cause long compiling time. If you just want to recompile part of the project, you have following options\n Compile agent and package   ./mvnw package -Pagent,dist\n or\n make build.agent\n If you intend to compile a single one plugin, such as in the dev stage, you could\n cd plugin_module_dir \u0026amp; mvn clean package\n  Compile backend and package   ./mvnw package -Pbackend,dist\n or\n make build.backend\n  Compile UI and package   ./mvnw package -Pui,dist\n or\n make build.ui\n Build docker images We can build docker images of backend and ui with Makefile located in root folder.\nRefer to Build docker image for more details.\nSetup your IntelliJ IDEA NOTICE: If you clone the codes from GitHub, please make sure that you had finished step 1 to 3 in section Build from GitHub, if you download the source codes from the official website of SkyWalking, please make sure that you had followed the steps in section Build from Apache source code release.\n Import the project as a maven project Run ./mvnw compile -Dmaven.test.skip=true to compile project and generate source codes. Because we use gRPC and protobuf. Set Generated Source Codes folders.  grpc-java and java folders in apm-protocol/apm-network/target/generated-sources/protobuf grpc-java and java folders in oap-server/server-core/target/generated-sources/protobuf grpc-java and java folders in oap-server/server-receiver-plugin/receiver-proto/target/generated-sources/fbs grpc-java and java folders in oap-server/server-receiver-plugin/receiver-proto/target/generated-sources/protobuf grpc-java and java folders in oap-server/exporter/target/generated-sources/protobuf grpc-java and java folders in oap-server/server-configuration/grpc-configuration-sync/target/generated-sources/protobuf grpc-java and java folders in oap-server/server-alarm-plugin/target/generated-sources/protobuf antlr4 folder in oap-server/oal-grammar/target/generated-sources    ","excerpt":"How to build project This document helps people to compile and build the project in your maven and …","ref":"/docs/main/v8.4.0/en/guides/how-to-build/","title":"How to build project"},{"body":"How to enable Kafka Reporter The Kafka reporter plugin support report traces, JVM metrics, Instance Properties, and profiled snapshots to Kafka cluster, which is disabled in default. Move the jar of the plugin, kafka-reporter-plugin-x.y.z.jar, from agent/optional-reporter-plugins to agent/plugins for activating.\nNotice, currently, the agent still needs to configure GRPC receiver for delivering the task of profiling. In other words, the following configure cannot be omitted.\n# Backend service addresses. collector.backend_service=${SW_AGENT_COLLECTOR_BACKEND_SERVICES:127.0.0.1:11800} # Kafka producer configuration plugin.kafka.bootstrap_servers=${SW_KAFKA_BOOTSTRAP_SERVERS:localhost:9092} plugin.kafka.producer_config[delivery.timeout.ms]=12000 plugin.kafka.get_topic_timeout=${SW_GET_TOPIC_TIMEOUT:10} Kafka reporter plugin support to customize all configurations of listed in here.\nBefore you activated the Kafka reporter, you have to make sure that Kafka fetcher has been opened in service.\n","excerpt":"How to enable Kafka Reporter The Kafka reporter plugin support report traces, JVM metrics, Instance …","ref":"/docs/main/v8.4.0/en/setup/service-agent/java-agent/how-to-enable-kafka-reporter/","title":"How to enable Kafka Reporter"},{"body":"How to tolerate custom exceptions In some codes, the exception is being used as a way of controlling business flow. Skywalking provides 2 ways to tolerate an exception which is traced in a span.\n Set the names of exception classes in the agent config Use our annotation in the codes.  Set the names of exception classes in the agent config The property named \u0026ldquo;statuscheck.ignored_exceptions\u0026rdquo; is used to set up class names in the agent configuration file. if the exception listed here are detected in the agent, the agent core would flag the related span as the error status.\nDemo   A custom exception.\n TestNamedMatchException  package org.apache.skywalking.apm.agent.core.context.status; public class TestNamedMatchException extends RuntimeException { public TestNamedMatchException() { } public TestNamedMatchException(final String message) { super(message); } ... }  TestHierarchyMatchException  package org.apache.skywalking.apm.agent.core.context.status; public class TestHierarchyMatchException extends TestNamedMatchException { public TestHierarchyMatchException() { } public TestHierarchyMatchException(final String message) { super(message); } ... }   When the above exceptions traced in some spans, the status is like the following.\n   The traced exception Final span status     org.apache.skywalking.apm.agent.core.context.status.TestNamedMatchException true   org.apache.skywalking.apm.agent.core.context.status.TestHierarchyMatchException true      After set these class names through \u0026ldquo;statuscheck.ignored_exceptions\u0026rdquo;, the status of spans would be changed.\nstatuscheck.ignored_exceptions=org.apache.skywalking.apm.agent.core.context.status.TestNamedMatchException    The traced exception Final span status     org.apache.skywalking.apm.agent.core.context.status.TestNamedMatchException false   org.apache.skywalking.apm.agent.core.context.status.TestHierarchyMatchException false      Use our annotation in the codes. If an exception has the @IgnoredException annotation, the exception wouldn\u0026rsquo;t be marked as error status when tracing. Because the annotation supports inheritance, also affects the subclasses.\nDependency  Dependency the toolkit, such as using maven or gradle. Since 8.2.0.  \u0026lt;dependency\u0026gt; \u0026lt;groupId\u0026gt;org.apache.skywalking\u0026lt;/groupId\u0026gt; \u0026lt;artifactId\u0026gt;apm-toolkit-log4j-1.x\u0026lt;/artifactId\u0026gt; \u0026lt;version\u0026gt;{project.release.version}\u0026lt;/version\u0026gt; \u0026lt;/dependency\u0026gt; Demo   A custom exception.\npackage org.apache.skywalking.apm.agent.core.context.status; public class TestAnnotatedException extends RuntimeException { public TestAnnotatedException() { } public TestAnnotatedException(final String message) { super(message); } ... }   When the above exception traced in some spans, the status is like the following.\n   The traced exception Final span status     org.apache.skywalking.apm.agent.core.context.status.TestAnnotatedException true      However, when the exception annotated with the annotation, the status would be changed.\npackage org.apache.skywalking.apm.agent.core.context.status; @IgnoredException public class TestAnnotatedException extends RuntimeException { public TestAnnotatedException() { } public TestAnnotatedException(final String message) { super(message); } ... }    The traced exception Final span status     org.apache.skywalking.apm.agent.core.context.status.TestAnnotatedException false      Recursive check Due to the wrapper nature of Java exceptions, sometimes users need recursive checking. Skywalking also supports it. Typically, we don\u0026rsquo;t recommend setting this more than 10, which could cause a performance issue. Negative value and 0 would be ignored, which means all exceptions would make the span tagged in error status.\n statuscheck.max_recursive_depth=${SW_STATUSCHECK_MAX_RECURSIVE_DEPTH:1} ","excerpt":"How to tolerate custom exceptions In some codes, the exception is being used as a way of controlling …","ref":"/docs/main/v8.4.0/en/setup/service-agent/java-agent/how-to-tolerate-exceptions/","title":"How to tolerate custom exceptions"},{"body":"HTTP API Protocol HTTP API Protocol defines the API data format, including api request and response data format. They use the HTTP1.1 wrapper of the official SkyWalking Browser Protocol. Read it for more details.\nPerformance Data Report Detail information about data format can be found in BrowserPerf.proto.\nPOST http://localhost:12800/browser/perfData Send a performance data object with JSON format.\nInput:\n{ \u0026#34;service\u0026#34;: \u0026#34;web\u0026#34;, \u0026#34;serviceVersion\u0026#34;: \u0026#34;v0.0.1\u0026#34;, \u0026#34;pagePath\u0026#34;: \u0026#34;/index.html\u0026#34;, \u0026#34;redirectTime\u0026#34;: 10, \u0026#34;dnsTime\u0026#34;: 10, \u0026#34;ttfbTime\u0026#34;: 10, \u0026#34;tcpTime\u0026#34;: 10, \u0026#34;transTime\u0026#34;: 10, \u0026#34;domAnalysisTime\u0026#34;: 10, \u0026#34;fptTime\u0026#34;: 10, \u0026#34;domReadyTime\u0026#34;: 10, \u0026#34;loadPageTime\u0026#34;: 10, \u0026#34;resTime\u0026#34;: 10, \u0026#34;sslTime\u0026#34;: 10, \u0026#34;ttlTime\u0026#34;: 10, \u0026#34;firstPackTime\u0026#34;: 10, \u0026#34;fmpTime\u0026#34;: 10 } OutPut:\nHttp Status: 204\nError Log Report Detail information about data format can be found in BrowserPerf.proto.\nPOST http://localhost:12800/browser/errorLogs Send an error log object list with JSON format.\nInput:\n[ { \u0026#34;uniqueId\u0026#34;: \u0026#34;55ec6178-3fb7-43ef-899c-a26944407b01\u0026#34;, \u0026#34;service\u0026#34;: \u0026#34;web\u0026#34;, \u0026#34;serviceVersion\u0026#34;: \u0026#34;v0.0.1\u0026#34;, \u0026#34;pagePath\u0026#34;: \u0026#34;/index.html\u0026#34;, \u0026#34;category\u0026#34;: \u0026#34;ajax\u0026#34;, \u0026#34;message\u0026#34;: \u0026#34;error\u0026#34;, \u0026#34;line\u0026#34;: 1, \u0026#34;col\u0026#34;: 1, \u0026#34;stack\u0026#34;: \u0026#34;error\u0026#34;, \u0026#34;errorUrl\u0026#34;: \u0026#34;/index.html\u0026#34; }, { \u0026#34;uniqueId\u0026#34;: \u0026#34;55ec6178-3fb7-43ef-899c-a26944407b02\u0026#34;, \u0026#34;service\u0026#34;: \u0026#34;web\u0026#34;, \u0026#34;serviceVersion\u0026#34;: \u0026#34;v0.0.1\u0026#34;, \u0026#34;pagePath\u0026#34;: \u0026#34;/index.html\u0026#34;, \u0026#34;category\u0026#34;: \u0026#34;ajax\u0026#34;, \u0026#34;message\u0026#34;: \u0026#34;error\u0026#34;, \u0026#34;line\u0026#34;: 1, \u0026#34;col\u0026#34;: 1, \u0026#34;stack\u0026#34;: \u0026#34;error\u0026#34;, \u0026#34;errorUrl\u0026#34;: \u0026#34;/index.html\u0026#34; } ] OutPut:\nHttp Status: 204\nPOST http://localhost:12800/browser/errorLog Send a single error log object with JSON format.\nInput:\n{ \u0026#34;uniqueId\u0026#34;: \u0026#34;55ec6178-3fb7-43ef-899c-a26944407b01\u0026#34;, \u0026#34;service\u0026#34;: \u0026#34;web\u0026#34;, \u0026#34;serviceVersion\u0026#34;: \u0026#34;v0.0.1\u0026#34;, \u0026#34;pagePath\u0026#34;: \u0026#34;/index.html\u0026#34;, \u0026#34;category\u0026#34;: \u0026#34;ajax\u0026#34;, \u0026#34;message\u0026#34;: \u0026#34;error\u0026#34;, \u0026#34;line\u0026#34;: 1, \u0026#34;col\u0026#34;: 1, \u0026#34;stack\u0026#34;: \u0026#34;error\u0026#34;, \u0026#34;errorUrl\u0026#34;: \u0026#34;/index.html\u0026#34; } OutPut:\nHttp Status: 204\n","excerpt":"HTTP API Protocol HTTP API Protocol defines the API data format, including api request and response …","ref":"/docs/main/v8.4.0/en/protocols/browser-http-api-protocol/","title":"HTTP API Protocol"},{"body":"HTTP API Protocol HTTP API Protocol defines the API data format, including api request and response data format. They use the HTTP1.1 wrapper of the official SkyWalking Trace Data Protocol v3. Read it for more details.\nInstance Management Detail information about data format can be found in Instance Management.\n Report service instance properties   POST http://localhost:12800/v3/management/reportProperties\n Input:\n{ \u0026#34;service\u0026#34;: \u0026#34;User Service Name\u0026#34;, \u0026#34;serviceInstance\u0026#34;: \u0026#34;User Service Instance Name\u0026#34;, \u0026#34;properties\u0026#34;: [{ \u0026#34;language\u0026#34;: \u0026#34;Lua\u0026#34; }] } Output JSON Array:\n{}  Service instance ping   POST http://localhost:12800/v3/management/keepAlive\n Input:\n{ \u0026#34;service\u0026#34;: \u0026#34;User Service Name\u0026#34;, \u0026#34;serviceInstance\u0026#34;: \u0026#34;User Service Instance Name\u0026#34; } OutPut:\n{} Trace Report Detail information about data format can be found in Instance Management. There are two ways to report segment data, one segment per request or segment array in the bulk mode.\nPOST http://localhost:12800/v3/segment Send a single segment object with JSON format.\nInput:\n{ \u0026#34;traceId\u0026#34;: \u0026#34;a12ff60b-5807-463b-a1f8-fb1c8608219e\u0026#34;, \u0026#34;serviceInstance\u0026#34;: \u0026#34;User_Service_Instance_Name\u0026#34;, \u0026#34;spans\u0026#34;: [{ \u0026#34;operationName\u0026#34;: \u0026#34;/ingress\u0026#34;, \u0026#34;startTime\u0026#34;: 1588664577013, \u0026#34;endTime\u0026#34;: 1588664577028, \u0026#34;spanType\u0026#34;: \u0026#34;Exit\u0026#34;, \u0026#34;spanId\u0026#34;: 1, \u0026#34;isError\u0026#34;: false, \u0026#34;parentSpanId\u0026#34;: 0, \u0026#34;componentId\u0026#34;: 6000, \u0026#34;peer\u0026#34;: \u0026#34;upstream service\u0026#34;, \u0026#34;spanLayer\u0026#34;: \u0026#34;Http\u0026#34; }, { \u0026#34;operationName\u0026#34;: \u0026#34;/ingress\u0026#34;, \u0026#34;startTime\u0026#34;: 1588664577013, \u0026#34;tags\u0026#34;: [{ \u0026#34;key\u0026#34;: \u0026#34;http.method\u0026#34;, \u0026#34;value\u0026#34;: \u0026#34;GET\u0026#34; }, { \u0026#34;key\u0026#34;: \u0026#34;http.params\u0026#34;, \u0026#34;value\u0026#34;: \u0026#34;http://localhost/ingress\u0026#34; }], \u0026#34;endTime\u0026#34;: 1588664577028, \u0026#34;spanType\u0026#34;: \u0026#34;Entry\u0026#34;, \u0026#34;spanId\u0026#34;: 0, \u0026#34;parentSpanId\u0026#34;: -1, \u0026#34;isError\u0026#34;: false, \u0026#34;spanLayer\u0026#34;: \u0026#34;Http\u0026#34;, \u0026#34;componentId\u0026#34;: 6000 }], \u0026#34;service\u0026#34;: \u0026#34;User_Service_Name\u0026#34;, \u0026#34;traceSegmentId\u0026#34;: \u0026#34;a12ff60b-5807-463b-a1f8-fb1c8608219e\u0026#34; } OutPut:\nPOST http://localhost:12800/v3/segments Send a segment object list with JSON format.\nInput:\n[{ \u0026#34;traceId\u0026#34;: \u0026#34;a12ff60b-5807-463b-a1f8-fb1c8608219e\u0026#34;, \u0026#34;serviceInstance\u0026#34;: \u0026#34;User_Service_Instance_Name\u0026#34;, \u0026#34;spans\u0026#34;: [{ \u0026#34;operationName\u0026#34;: \u0026#34;/ingress\u0026#34;, \u0026#34;startTime\u0026#34;: 1588664577013, \u0026#34;endTime\u0026#34;: 1588664577028, \u0026#34;spanType\u0026#34;: \u0026#34;Exit\u0026#34;, \u0026#34;spanId\u0026#34;: 1, \u0026#34;isError\u0026#34;: false, \u0026#34;parentSpanId\u0026#34;: 0, \u0026#34;componentId\u0026#34;: 6000, \u0026#34;peer\u0026#34;: \u0026#34;upstream service\u0026#34;, \u0026#34;spanLayer\u0026#34;: \u0026#34;Http\u0026#34; }, { \u0026#34;operationName\u0026#34;: \u0026#34;/ingress\u0026#34;, \u0026#34;startTime\u0026#34;: 1588664577013, \u0026#34;tags\u0026#34;: [{ \u0026#34;key\u0026#34;: \u0026#34;http.method\u0026#34;, \u0026#34;value\u0026#34;: \u0026#34;GET\u0026#34; }, { \u0026#34;key\u0026#34;: \u0026#34;http.params\u0026#34;, \u0026#34;value\u0026#34;: \u0026#34;http://localhost/ingress\u0026#34; }], \u0026#34;endTime\u0026#34;: 1588664577028, \u0026#34;spanType\u0026#34;: \u0026#34;Entry\u0026#34;, \u0026#34;spanId\u0026#34;: 0, \u0026#34;parentSpanId\u0026#34;: -1, \u0026#34;isError\u0026#34;: false, \u0026#34;spanLayer\u0026#34;: \u0026#34;Http\u0026#34;, \u0026#34;componentId\u0026#34;: 6000 }], \u0026#34;service\u0026#34;: \u0026#34;User_Service_Name\u0026#34;, \u0026#34;traceSegmentId\u0026#34;: \u0026#34;a12ff60b-5807-463b-a1f8-fb1c8608219e\u0026#34; }, { \u0026#34;traceId\u0026#34;: \u0026#34;f956699e-5106-4ea3-95e5-da748c55bac1\u0026#34;, \u0026#34;serviceInstance\u0026#34;: \u0026#34;User_Service_Instance_Name\u0026#34;, \u0026#34;spans\u0026#34;: [{ \u0026#34;operationName\u0026#34;: \u0026#34;/ingress\u0026#34;, \u0026#34;startTime\u0026#34;: 1588664577250, \u0026#34;endTime\u0026#34;: 1588664577250, \u0026#34;spanType\u0026#34;: \u0026#34;Exit\u0026#34;, \u0026#34;spanId\u0026#34;: 1, \u0026#34;isError\u0026#34;: false, \u0026#34;parentSpanId\u0026#34;: 0, \u0026#34;componentId\u0026#34;: 6000, \u0026#34;peer\u0026#34;: \u0026#34;upstream service\u0026#34;, \u0026#34;spanLayer\u0026#34;: \u0026#34;Http\u0026#34; }, { \u0026#34;operationName\u0026#34;: \u0026#34;/ingress\u0026#34;, \u0026#34;startTime\u0026#34;: 1588664577250, \u0026#34;tags\u0026#34;: [{ \u0026#34;key\u0026#34;: \u0026#34;http.method\u0026#34;, \u0026#34;value\u0026#34;: \u0026#34;GET\u0026#34; }, { \u0026#34;key\u0026#34;: \u0026#34;http.params\u0026#34;, \u0026#34;value\u0026#34;: \u0026#34;http://localhost/ingress\u0026#34; }], \u0026#34;endTime\u0026#34;: 1588664577250, \u0026#34;spanType\u0026#34;: \u0026#34;Entry\u0026#34;, \u0026#34;spanId\u0026#34;: 0, \u0026#34;parentSpanId\u0026#34;: -1, \u0026#34;isError\u0026#34;: false, \u0026#34;spanLayer\u0026#34;: \u0026#34;Http\u0026#34;, \u0026#34;componentId\u0026#34;: 6000 }], \u0026#34;service\u0026#34;: \u0026#34;User_Service_Name\u0026#34;, \u0026#34;traceSegmentId\u0026#34;: \u0026#34;f956699e-5106-4ea3-95e5-da748c55bac1\u0026#34; }] OutPut:\n","excerpt":"HTTP API Protocol HTTP API Protocol defines the API data format, including api request and response …","ref":"/docs/main/v8.4.0/en/protocols/http-api-protocol/","title":"HTTP API Protocol"},{"body":"IllegalStateException when install Java agent on WebSphere This FAQ came from community discussion and feedback. This user installed SkyWalking Java agent on WebSphere 7.0.0.11 and ibm jdk 1.8_20160719 and 1.7.0_20150407, and had following error logs\nWARN 2019-05-09 17:01:35:905 SkywalkingAgent-1-GRPCChannelManager-0 ProtectiveShieldMatcher : Byte-buddy occurs exception when match type. java.lang.IllegalStateException: Cannot resolve type description for java.security.PrivilegedAction at org.apache.skywalking.apm.dependencies.net.bytebuddy.pool.TypePool$Resolution$Illegal.resolve(TypePool.java:144) at org.apache.skywalking.apm.dependencies.net.bytebuddy.pool.TypePool$Default$WithLazyResolution$LazyTypeDescription.delegate(TypePool.java:1392) at org.apache.skywalking.apm.dependencies.net.bytebuddy.description.type.TypeDescription$AbstractBase$OfSimpleType$WithDelegation.getInterfaces(TypeDescription.java:8016) at org.apache.skywalking.apm.dependencies.net.bytebuddy.description.type.TypeDescription$Generic$OfNonGenericType.getInterfaces(TypeDescription.java:3621) at org.apache.skywalking.apm.dependencies.net.bytebuddy.matcher.HasSuperTypeMatcher.hasInterface(HasSuperTypeMatcher.java:53) at org.apache.skywalking.apm.dependencies.net.bytebuddy.matcher.HasSuperTypeMatcher.hasInterface(HasSuperTypeMatcher.java:54) at org.apache.skywalking.apm.dependencies.net.bytebuddy.matcher.HasSuperTypeMatcher.matches(HasSuperTypeMatcher.java:38) at org.apache.skywalking.apm.dependencies.net.bytebuddy.matcher.HasSuperTypeMatcher.matches(HasSuperTypeMatcher.java:15) at org.apache.skywalking.apm.dependencies.net.bytebuddy.matcher.ElementMatcher$Junction$Conjunction.matches(ElementMatcher.java:107) at org.apache.skywalking.apm.dependencies.net.bytebuddy.matcher.ElementMatcher$Junction$Disjunction.matches(ElementMatcher.java:147) at org.apache.skywalking.apm.dependencies.net.bytebuddy.matcher.ElementMatcher$Junction$Disjunction.matches(ElementMatcher.java:147) at org.apache.skywalking.apm.dependencies.net.bytebuddy.matcher.ElementMatcher$Junction$Disjunction.matches(ElementMatcher.java:147) at org.apache.skywalking.apm.dependencies.net.bytebuddy.matcher.ElementMatcher$Junction$Disjunction.matches(ElementMatcher.java:147) at org.apache.skywalking.apm.dependencies.net.bytebuddy.matcher.ElementMatcher$Junction$Disjunction.matches(ElementMatcher.java:147) ... The exception has been addressed as access grant required in WebSphere. You could follow these steps.\n Set the agent\u0026rsquo;s owner to the owner of WebSphere. Add \u0026ldquo;grant codeBase \u0026ldquo;file:${agent_dir}/-\u0026rdquo; { permission java.security.AllPermission; };\u0026rdquo; in the file of \u0026ldquo;server.policy\u0026rdquo;.  ","excerpt":"IllegalStateException when install Java agent on WebSphere This FAQ came from community discussion …","ref":"/docs/main/v8.4.0/en/faq/install_agent_on_websphere/","title":"IllegalStateException when install Java agent on WebSphere"},{"body":"Init mode SkyWalking backend supports multiple storage implementors. Most of them could initialize the storage, such as Elastic Search, Database automatically when the backend startup in first place.\nBut there are some unexpected happens based on the storage, such as When create Elastic Search indexes concurrently, because of several backend instances startup at the same time., there is a change, the APIs of Elastic Search would be blocked without any exception. And this has more chances happen in container management platform, like k8s.\nThat is where you need Init mode startup.\nSolution Only one single instance should run in Init mode before other instances start up. And this instance will exit graciously after all initialization steps are done.\nUse oapServiceInit.sh/oapServiceInit.bat to start up backend. You should see the following logs\n 2018-11-09 23:04:39,465 - org.apache.skywalking.oap.server.starter.OAPServerStartUp -2214 [main] INFO [] - OAP starts up in init mode successfully, exit now\u0026hellip;\n Kubernetes Initialization in this mode would be included in our Kubernetes scripts and Helm.\n","excerpt":"Init mode SkyWalking backend supports multiple storage implementors. Most of them could initialize …","ref":"/docs/main/v8.4.0/en/setup/backend/backend-init-mode/","title":"Init mode"},{"body":"IP and port setting Backend is using IP and port binding, in order to support the OS having multiple IPs. The binding/listening IP and port are specified by core module\ncore: default: restHost: 0.0.0.0 restPort: 12800 restContextPath: / gRPCHost: 0.0.0.0 gRPCPort: 11800 There are two IP/port pair for gRPC and HTTP rest services.\n Most agents and probes use gRPC service for better performance and code readability. Few agent use rest service, because gRPC may be not supported in that language. UI uses rest service, but data in GraphQL format, always.  Notice IP binding In case some users are not familiar with IP binding, you should know, after you did that, the client could only use this IP to access the service. For example, bind 172.09.13.28, even you are in this machine, must use 172.09.13.28 rather than 127.0.0.1 or localhost to access the service.\nModule provider specified IP and port The IP and port in core are only default provided by core. But some module provider may provide other IP and port settings, this is common. Such as many receiver modules provide this.\n","excerpt":"IP and port setting Backend is using IP and port binding, in order to support the OS having multiple …","ref":"/docs/main/v8.4.0/en/setup/backend/backend-ip-port/","title":"IP and port setting"},{"body":"JVM Metrics Service Abstract Uplink the JVM metrics, including PermSize, HeapSize, CPU, Memory, etc., every second.\ngRPC service define\n","excerpt":"JVM Metrics Service Abstract Uplink the JVM metrics, including PermSize, HeapSize, CPU, Memory, …","ref":"/docs/main/v8.4.0/en/protocols/jvm-protocol/","title":"JVM Metrics Service"},{"body":"Local span and Exit span should not be register Since 6.6.0, SkyWalking cancelled the local span and exit span register. If old java agent(before 6.6.0) is still running, and do register to 6.6.0+ backend, you will face the following warning message.\nclass=RegisterServiceHandler, message = Unexpected endpoint register, endpoint isn't detected from server side. This will not harm the backend or cause any issue. This is a reminder that, your agent or other client should follow the new protocol requirements.\nYou could simply use log4j2.xml to filter this warning message out.\n","excerpt":"Local span and Exit span should not be register Since 6.6.0, SkyWalking cancelled the local span and …","ref":"/docs/main/v8.4.0/en/faq/unexpected-endpoint-register/","title":"Local span and Exit span should not be register"},{"body":"Locate agent config file by system property Supported version 5.0.0-RC+\nWhat is Locate agent config file by system property ？ In Default. The agent will try to locate agent.config, which should be in the /config dictionary of agent package. If User sets the specified agent config file through system properties, The agent will try to load file from there. By the way, This function has no conflict with Setting Override\nOverride priority The specified agent config \u0026gt; The default agent config\nHow to use The content formats of the specified config must be same as the default config.\nUsing System.Properties(-D) to set the specified config path\n-Dskywalking_config=/path/to/agent.config /path/to/agent.config is the absolute path of the specified config file\n","excerpt":"Locate agent config file by system property Supported version 5.0.0-RC+\nWhat is Locate agent config …","ref":"/docs/main/v8.4.0/en/setup/service-agent/java-agent/specified-agent-config/","title":"Locate agent config file by system property"},{"body":"Log Data Protocol Report log data via protocol.\ngRPC service define\n","excerpt":"Log Data Protocol Report log data via protocol.\ngRPC service define","ref":"/docs/main/v8.4.0/en/protocols/log-data-protocol/","title":"Log Data Protocol"},{"body":"logback plugin  Dependency the toolkit, such as using maven or gradle  \u0026lt;dependency\u0026gt; \u0026lt;groupId\u0026gt;org.apache.skywalking\u0026lt;/groupId\u0026gt; \u0026lt;artifactId\u0026gt;apm-toolkit-logback-1.x\u0026lt;/artifactId\u0026gt; \u0026lt;version\u0026gt;{project.release.version}\u0026lt;/version\u0026gt; \u0026lt;/dependency\u0026gt; Print trace ID in your logs  set %tid in Pattern section of logback.xml  \u0026lt;appender name=\u0026#34;STDOUT\u0026#34; class=\u0026#34;ch.qos.logback.core.ConsoleAppender\u0026#34;\u0026gt; \u0026lt;encoder class=\u0026#34;ch.qos.logback.core.encoder.LayoutWrappingEncoder\u0026#34;\u0026gt; \u0026lt;layout class=\u0026#34;org.apache.skywalking.apm.toolkit.log.logback.v1.x.TraceIdPatternLogbackLayout\u0026#34;\u0026gt; \u0026lt;Pattern\u0026gt;%d{yyyy-MM-dd HH:mm:ss.SSS} [%tid] [%thread] %-5level %logger{36} -%msg%n\u0026lt;/Pattern\u0026gt; \u0026lt;/layout\u0026gt; \u0026lt;/encoder\u0026gt; \u0026lt;/appender\u0026gt;  with the MDC, set %X{tid} in Pattern section of logback.xml  \u0026lt;appender name=\u0026#34;STDOUT\u0026#34; class=\u0026#34;ch.qos.logback.core.ConsoleAppender\u0026#34;\u0026gt; \u0026lt;encoder class=\u0026#34;ch.qos.logback.core.encoder.LayoutWrappingEncoder\u0026#34;\u0026gt; \u0026lt;layout class=\u0026#34;org.apache.skywalking.apm.toolkit.log.logback.v1.x.mdc.TraceIdMDCPatternLogbackLayout\u0026#34;\u0026gt; \u0026lt;Pattern\u0026gt;%d{yyyy-MM-dd HH:mm:ss.SSS} [%X{tid}] [%thread] %-5level %logger{36} -%msg%n\u0026lt;/Pattern\u0026gt; \u0026lt;/layout\u0026gt; \u0026lt;/encoder\u0026gt; \u0026lt;/appender\u0026gt;  Support logback AsyncAppender(MDC also support), No additional configuration is required. Refer to the demo of logback.xml below. For details: Logback AsyncAppender  \u0026lt;configuration scan=\u0026#34;true\u0026#34; scanPeriod=\u0026#34; 5 seconds\u0026#34;\u0026gt; \u0026lt;appender name=\u0026#34;STDOUT\u0026#34; class=\u0026#34;ch.qos.logback.core.ConsoleAppender\u0026#34;\u0026gt; \u0026lt;encoder class=\u0026#34;ch.qos.logback.core.encoder.LayoutWrappingEncoder\u0026#34;\u0026gt; \u0026lt;layout class=\u0026#34;org.apache.skywalking.apm.toolkit.log.logback.v1.x.mdc.TraceIdMDCPatternLogbackLayout\u0026#34;\u0026gt; \u0026lt;Pattern\u0026gt;%d{yyyy-MM-dd HH:mm:ss.SSS} [%X{tid}] [%thread] %-5level %logger{36} -%msg%n\u0026lt;/Pattern\u0026gt; \u0026lt;/layout\u0026gt; \u0026lt;/encoder\u0026gt; \u0026lt;/appender\u0026gt; \u0026lt;appender name=\u0026#34;ASYNC\u0026#34; class=\u0026#34;ch.qos.logback.classic.AsyncAppender\u0026#34;\u0026gt; \u0026lt;discardingThreshold\u0026gt;0\u0026lt;/discardingThreshold\u0026gt; \u0026lt;queueSize\u0026gt;1024\u0026lt;/queueSize\u0026gt; \u0026lt;neverBlock\u0026gt;true\u0026lt;/neverBlock\u0026gt; \u0026lt;appender-ref ref=\u0026#34;STDOUT\u0026#34;/\u0026gt; \u0026lt;/appender\u0026gt; \u0026lt;root level=\u0026#34;INFO\u0026#34;\u0026gt; \u0026lt;appender-ref ref=\u0026#34;ASYNC\u0026#34;/\u0026gt; \u0026lt;/root\u0026gt; \u0026lt;/configuration\u0026gt;  When you use -javaagent to active the sky-walking tracer, logback will output traceId, if it existed. If the tracer is inactive, the output will be TID: N/A.  logstash logback plugin  Dependency the toolkit, such as using maven or gradle  \u0026lt;dependency\u0026gt; \u0026lt;groupId\u0026gt;org.apache.skywalking\u0026lt;/groupId\u0026gt; \u0026lt;artifactId\u0026gt;apm-toolkit-logback-1.x\u0026lt;/artifactId\u0026gt; \u0026lt;version\u0026gt;${skywalking.version}\u0026lt;/version\u0026gt; \u0026lt;/dependency\u0026gt;  set LogstashEncoder of logback.xml  \u0026lt;encoder charset=\u0026#34;UTF-8\u0026#34; class=\u0026#34;net.logstash.logback.encoder.LogstashEncoder\u0026#34;\u0026gt; \u0026lt;provider class=\u0026#34;org.apache.skywalking.apm.toolkit.log.logback.v1.x.logstash.TraceIdJsonProvider\u0026#34;\u0026gt; \u0026lt;/provider\u0026gt; \u0026lt;/encoder\u0026gt;  set LoggingEventCompositeJsonEncoder of logstash in logback-spring.xml for custom json format  1.add converter for %tid as child of  node\n\u0026lt;!--add converter for %tid --\u0026gt; \u0026lt;conversionRule conversionWord=\u0026#34;tid\u0026#34; converterClass=\u0026#34;org.apache.skywalking.apm.toolkit.log.logback.v1.x.LogbackPatternConverter\u0026#34;/\u0026gt; 2.add json encoder for custom json format\n\u0026lt;encoder class=\u0026#34;net.logstash.logback.encoder.LoggingEventCompositeJsonEncoder\u0026#34;\u0026gt; \u0026lt;providers\u0026gt; \u0026lt;timestamp\u0026gt; \u0026lt;timeZone\u0026gt;UTC\u0026lt;/timeZone\u0026gt; \u0026lt;/timestamp\u0026gt; \u0026lt;pattern\u0026gt; \u0026lt;pattern\u0026gt; { \u0026#34;level\u0026#34;: \u0026#34;%level\u0026#34;, \u0026#34;tid\u0026#34;: \u0026#34;%tid\u0026#34;, \u0026#34;thread\u0026#34;: \u0026#34;%thread\u0026#34;, \u0026#34;class\u0026#34;: \u0026#34;%logger{1.}:%L\u0026#34;, \u0026#34;message\u0026#34;: \u0026#34;%message\u0026#34;, \u0026#34;stackTrace\u0026#34;: \u0026#34;%exception{10}\u0026#34; } \u0026lt;/pattern\u0026gt; \u0026lt;/pattern\u0026gt; \u0026lt;/providers\u0026gt; \u0026lt;/encoder\u0026gt; gRPC reporter The gRPC reporter could forward the collected logs to SkyWalking OAP server, or SkyWalking Satellite sidecar. Trace id, segment id, and span id will attach to logs automatically. There is no need to modify existing layouts.\n Add GRPCLogClientAppender in logback.xml  \u0026lt;appender name=\u0026#34;grpc-log\u0026#34; class=\u0026#34;org.apache.skywalking.apm.toolkit.log.logback.v1.x.log.GRPCLogClientAppender\u0026#34;/\u0026gt;  Add config of the plugin or use default  plugin.toolkit.log.grpc.reporter.server_host=${SW_GRPC_LOG_SERVER_HOST:127.0.0.1} plugin.toolkit.log.grpc.reporter.server_port=${SW_GRPC_LOG_SERVER_PORT:11800} plugin.toolkit.log.grpc.reporter.max_message_size=${SW_GRPC_LOG_MAX_MESSAGE_SIZE:10485760} plugin.toolkit.log.grpc.reporter.upstream_timeout=${SW_GRPC_LOG_GRPC_UPSTREAM_TIMEOUT:30} Transmitting un-formatted messages The logback 1.x gRPC reporter supports transmitting logs as formatted or un-formatted. Transmitting formatted data is the default but can be disabled by adding the following to the agent config:\nplugin.toolkit.log.transmit_formatted=false The above will result in the content field being used for the log pattern with additional log tags of argument.0, argument.1, and so on representing each logged argument as well as an additional exception tag which is only present if a throwable is also logged.\nFor example, the following code:\nlog.info(\u0026#34;{} {} {}\u0026#34;, 1, 2, 3); Will result in:\n{ \u0026#34;content\u0026#34;: \u0026#34;{} {} {}\u0026#34;, \u0026#34;tags\u0026#34;: [ { \u0026#34;key\u0026#34;: \u0026#34;argument.0\u0026#34;, \u0026#34;value\u0026#34;: \u0026#34;1\u0026#34; }, { \u0026#34;key\u0026#34;: \u0026#34;argument.1\u0026#34;, \u0026#34;value\u0026#34;: \u0026#34;2\u0026#34; }, { \u0026#34;key\u0026#34;: \u0026#34;argument.2\u0026#34;, \u0026#34;value\u0026#34;: \u0026#34;3\u0026#34; } ] } ","excerpt":"logback plugin  Dependency the toolkit, such as using maven or gradle  \u0026lt;dependency\u0026gt; …","ref":"/docs/main/v8.4.0/en/setup/service-agent/java-agent/application-toolkit-logback-1.x/","title":"logback plugin"},{"body":"Manual instrument SDK We have manual instrument SDK contributed from the community.\n Go2Sky. Go SDK follows SkyWalking format. C++. C++ SDK follows SkyWalking format.  What is SkyWalking formats and propagation protocols? See these protocols in protocols document.\nEnvoy tracer Envoy has its internal tracer implementation for SkyWalking. Read SkyWalking Tracer doc and SkyWalking tracing sandbox for more details.\n","excerpt":"Manual instrument SDK We have manual instrument SDK contributed from the community.\n Go2Sky. Go SDK …","ref":"/docs/main/v8.4.0/en/concepts-and-designs/manual-sdk/","title":"Manual instrument SDK"},{"body":"Meter Analysis Language Meter system provides a functional analysis language called MAL(Meter Analysis Language) that lets the user analyze and aggregate meter data in OAP streaming system. The result of an expression can either be ingested by agent analyzer, or OC/Prometheus analyzer.\nLanguage data type In MAL, an expression or sub-expression can evaluate to one of two types:\n Sample family - a set of samples(metrics) containing a range of metrics whose name is identical. Scalar - a simple numeric value. it supports integer/long, floating/double,  Sample family A set of samples, which is as the basic unit in MAL. For example:\ninstance_trace_count The above sample family might contains following simples which are provided by external modules, for instance, agent analyzer:\ninstance_trace_count{region=\u0026quot;us-west\u0026quot;,az=\u0026quot;az-1\u0026quot;} 100 instance_trace_count{region=\u0026quot;us-east\u0026quot;,az=\u0026quot;az-3\u0026quot;} 20 instance_trace_count{region=\u0026quot;asia-north\u0026quot;,az=\u0026quot;az-1\u0026quot;} 33 Tag filter MAL support four type operations to filter samples in a sample family:\n tagEqual: Filter tags that are exactly equal to the provided string. tagNotEqual: Filter tags that are not equal to the provided string. tagMatch: Filter tags that regex-match the provided string. tagNotMatch: Filter labels that do not regex-match the provided string.  For example, this filters all instance_trace_count samples for us-west and asia-north region and az-1 az:\ninstance_trace_count.tagMatch(\u0026quot;region\u0026quot;, \u0026quot;us-west|asia-north\u0026quot;).tagEqual(\u0026quot;az\u0026quot;, \u0026quot;az-1\u0026quot;) Binary operators The following binary arithmetic operators are available in MAL:\n + (addition) - (subtraction) * (multiplication) / (division)  Binary operators are defined between scalar/scalar, sampleFamily/scalar and sampleFamily/sampleFamily value pairs.\nBetween two scalars: they evaluate to another scalar that is the result of the operator applied to both scalar operands:\n1 + 2 Between a sample family and a scalar, the operator is applied to the value of every sample in the sample family. For example:\ninstance_trace_count + 2 or\n2 + instance_trace_count results in\ninstance_trace_count{region=\u0026quot;us-west\u0026quot;,az=\u0026quot;az-1\u0026quot;} 102 // 100 + 2 instance_trace_count{region=\u0026quot;us-east\u0026quot;,az=\u0026quot;az-3\u0026quot;} 22 // 20 + 2 instance_trace_count{region=\u0026quot;asia-north\u0026quot;,az=\u0026quot;az-1\u0026quot;} 35 // 33 + 2 Between two sample families, a binary operator is applied to each sample in the left-hand side sample family and its matching sample in the right-hand sample family. A new sample family with empty name will be generated. Only the matched tags will be reserved. Samples for which no matching sample in the right-hand sample family are not in the result.\nAnother sample family instance_trace_analysis_error_count is\ninstance_trace_analysis_error_count{region=\u0026quot;us-west\u0026quot;,az=\u0026quot;az-1\u0026quot;} 20 instance_trace_analysis_error_count{region=\u0026quot;asia-north\u0026quot;,az=\u0026quot;az-1\u0026quot;} 11 Example expression:\ninstance_trace_analysis_error_count / instance_trace_count This returns a result sample family containing the error rate of trace analysis. The samples with region us-west and az az-3 have no match and will not show up in the result:\n{region=\u0026quot;us-west\u0026quot;,az=\u0026quot;az-1\u0026quot;} 0.8 // 20 / 100 {region=\u0026quot;asia-north\u0026quot;,az=\u0026quot;az-1\u0026quot;} 0.3333 // 11 / 33 Aggregation Operation Sample family supports the following aggregation operations that can be used to aggregate the samples of a single sample family, resulting in a new sample family of fewer samples(even single one) with aggregated values:\n sum (calculate sum over dimensions) min (select minimum over dimensions) max (select maximum over dimensions) avg (calculate the average over dimensions)  These operations can be used to aggregate over all label dimensions or preserve distinct dimensions by inputting by parameter.\n\u0026lt;aggr-op\u0026gt;(by: \u0026lt;tag1, tag2, ...\u0026gt;) Example expression:\ninstance_trace_count.sum(by: ['az']) will output a result:\ninstance_trace_count{az=\u0026quot;az-1\u0026quot;} 133 // 100 + 33 instance_trace_count{az=\u0026quot;az-3\u0026quot;} 20 Function Duraton is a textual representation of a time range. The formats accepted are based on the ISO-8601 duration format {@code PnDTnHnMn.nS} with days considered to be exactly 24 hours.\nExamples:\n \u0026ldquo;PT20.345S\u0026rdquo; \u0026ndash; parses as \u0026ldquo;20.345 seconds\u0026rdquo; \u0026ldquo;PT15M\u0026rdquo; \u0026ndash; parses as \u0026ldquo;15 minutes\u0026rdquo; (where a minute is 60 seconds) \u0026ldquo;PT10H\u0026rdquo; \u0026ndash; parses as \u0026ldquo;10 hours\u0026rdquo; (where an hour is 3600 seconds) \u0026ldquo;P2D\u0026rdquo; \u0026ndash; parses as \u0026ldquo;2 days\u0026rdquo; (where a day is 24 hours or 86400 seconds) \u0026ldquo;P2DT3H4M\u0026rdquo; \u0026ndash; parses as \u0026ldquo;2 days, 3 hours and 4 minutes\u0026rdquo; \u0026ldquo;P-6H3M\u0026rdquo; \u0026ndash; parses as \u0026ldquo;-6 hours and +3 minutes\u0026rdquo; \u0026ldquo;-P6H3M\u0026rdquo; \u0026ndash; parses as \u0026ldquo;-6 hours and -3 minutes\u0026rdquo; \u0026ldquo;-P-6H+3M\u0026rdquo; \u0026ndash; parses as \u0026ldquo;+6 hours and -3 minutes\u0026rdquo;  increase increase(Duration). Calculates the increase in the time range.\nrate rate(Duration). Calculates the per-second average rate of increase of the time range.\nirate irate(). Calculates the per-second instant rate of increase of the time range.\ntag tag({allTags -\u0026gt; }). Update tags of samples. User can add, drop, rename and update tags.\nhistogram histogram(le: '\u0026lt;the tag name of le\u0026gt;'). Transforms less based histogram buckets to meter system histogram buckets. le parameter hints the tag name of a bucket.\nhistogram_percentile histogram_percentile([\u0026lt;p scalar\u0026gt;]). Hints meter-system to calculates the p-percentile (0 ≤ p ≤ 100) from the buckets.\ntime time(). returns the number of seconds since January 1, 1970 UTC.\nDown Sampling Operation MAL should instruct meter-system how to do downsampling for metrics. It doesn\u0026rsquo;t only refer to aggregate raw samples to minute level, but also hints data from minute to higher levels, for instance, hour and day.\nDown sampling operations are as global function in MAL:\n avg latest (TODO) min (TODO) max (TODO) mean (TODO) sum (TODO) count (TODO)  The default one is avg if not specific an operation.\nIf user want get latest time from last_server_state_sync_time_in_seconds:\nlatest(last_server_state_sync_time_in_seconds.tagEqual('production', 'catalog')) or latest last_server_state_sync_time_in_seconds.tagEqual('production', 'catalog') Metric level function Metric has three level, service, instance and endpoint. They extract level relevant labels from metric labels, then hints meter-system which level this metrics should be.\n servcie([svc_label1, svc_label2...]) extracts service level labels from the array argument. instance([svc_label1, svc_label2...], [ins_label1, ins_label2...]) extracts service level labels from the first array argument, extracts instance level labels from the second array argument. endpoint([svc_label1, svc_label2...], [ep_label1, ep_label2...]) extracts service level labels from the first array argument, extracts endpoint level labels from the second array argument.  More Examples Please refer to OAP Self-Observability\n","excerpt":"Meter Analysis Language Meter system provides a functional analysis language called MAL(Meter …","ref":"/docs/main/v8.4.0/en/concepts-and-designs/mal/","title":"Meter Analysis Language"},{"body":"Meter Receiver Meter receiver is accepting the metrics of meter protocol format into the Meter System.\nModule define receiver-meter: selector: ${SW_RECEIVER_METER:default} default: In Kafka Fetcher, we need to follow the configuration to enable it.\nkafka-fetcher: selector: ${SW_KAFKA_FETCHER:default} default: bootstrapServers: ${SW_KAFKA_FETCHER_SERVERS:localhost:9092} enableMeterSystem: ${SW_KAFKA_FETCHER_ENABLE_METER_SYSTEM:true} Configuration file Meter receiver is configured via a configuration file. The configuration file defines everything related to receiving from agents, as well as which rule files to load.\nOAP can load the configuration at bootstrap. If the new configuration is not well-formed, OAP fails to start up. The files are located at $CLASSPATH/meter-analyzer-config.\nThe file is written in YAML format, defined by the scheme described below. Brackets indicate that a parameter is optional.\nAn example can be found here. If you\u0026rsquo;re using Spring sleuth, you could use Spring Sleuth Setup.\nMeters configure # expSuffix is appended to all expression in this file. expSuffix: \u0026lt;string\u0026gt; # insert metricPrefix into metric name: \u0026lt;metricPrefix\u0026gt;_\u0026lt;raw_metric_name\u0026gt; metricPrefix: \u0026lt;string\u0026gt; # Metrics rule allow you to recompute queries. metricsRules: # The name of rule, which combinates with a prefix \u0026#39;meter_\u0026#39; as the index/table name in storage. name: \u0026lt;string\u0026gt; # MAL expression. exp: \u0026lt;string\u0026gt; More about MAL, please refer to mal.md\nAbout rate, irate, increase Even we supported rate, irate, increase function in the backend, but we still recommend user to consider using client-side APIs to do these. Because\n The OAP has to set up caches to calculate the value. Once the agent reconnected to another OAP instance, the time windows of rate calculation will break. Then, the result would not be accurate.  ","excerpt":"Meter Receiver Meter receiver is accepting the metrics of meter protocol format into the Meter …","ref":"/docs/main/v8.4.0/en/setup/backend/backend-meter/","title":"Meter Receiver"},{"body":"Meter System Meter system is another streaming calculation mode, especially for metrics data. In the OAL, there are clear Scope Definitions, including native objects. Meter system is focusing on the data type itself, and provides more flexible to the end user to define the scope entity.\nThe meter system is open to different receivers and fetchers in the backend, follow the backend setup document for more details.\nEvery metrics is declared in the meter system should include following attribute\n Metrics Name. An unique name globally, should avoid overlap the OAL variable names. Function Name. The function used for this metrics, distributed aggregation, value calculation and down sampling calculation based on the function implementation. Also, the data structure is determined by the function too, such as function Avg is for Long. Scope Type. Unlike inside the OAL, there are plenty of logic scope definitions, in meter system, only type is required. Type values include service, instance, and endpoint, like we introduced in the Overview. The values of scope entity name, such as service name, are required when metrics data generated with the metrics data value.  NOTICE, the metrics must be declared in the bootstrap stage, no runtime changed.\nMeter System supports following binding functions\n avg. Calculate the avg value for every entity in the same metrics name. histogram. Aggregate the counts in the configurable buckets, buckets is configurable but must be assigned in the declaration stage. percentile. Read percentile in WIKI. Unlike in the OAL, we provide 50/75/90/95/99 in default, in the meter system function, percentile function accepts several ranks, which should be in the (0, 100) range.  ","excerpt":"Meter System Meter system is another streaming calculation mode, especially for metrics data. In the …","ref":"/docs/main/v8.4.0/en/concepts-and-designs/meter/","title":"Meter System"},{"body":"Metrics Exporter SkyWalking provides basic and most important metrics aggregation, alarm and analysis. In real world, people may want to forward the data to their 3rd party system, for deeper analysis or anything else. Metrics Exporter makes that possible.\nMetrics exporter is an independent module, you need manually active it.\nRight now, we provide the following exporters\n gRPC exporter  gRPC exporter gRPC exporter uses SkyWalking native exporter service definition. Here is proto definition.\nservice MetricExportService { rpc export (stream ExportMetricValue) returns (ExportResponse) { } rpc subscription (SubscriptionReq) returns (SubscriptionsResp) { }}message ExportMetricValue { string metricName = 1; string entityName = 2; string entityId = 3; ValueType type = 4; int64 timeBucket = 5; int64 longValue = 6; double doubleValue = 7; repeated int64 longValues = 8;}message SubscriptionsResp { repeated string metricNames = 1;}enum ValueType { LONG = 0; DOUBLE = 1; MULTI_LONG = 2;}message SubscriptionReq {}message ExportResponse {}To active the exporter, you should add this into your application.yml\nexporter: grpc: targetHost: 127.0.0.1 targetPort: 9870  targetHost:targetPort is the expected target service address. You could set any gRPC server to receive the data. Target gRPC service needs to be standby, otherwise, the OAP starts up failure.  For target exporter service subscription implementation Return the expected metrics name list, all the names must match the OAL script definition. Return empty list, if you want to export all metrics.\nexport implementation Stream service, all subscribed metrics will be sent to here, based on OAP core schedule. Also, if the OAP deployed as cluster, then this method will be called concurrently. For metrics value, you need follow #type to choose #longValue or #doubleValue.\n","excerpt":"Metrics Exporter SkyWalking provides basic and most important metrics aggregation, alarm and …","ref":"/docs/main/v8.4.0/en/setup/backend/metrics-exporter/","title":"Metrics Exporter"},{"body":"Namespace Background SkyWalking is a monitoring tool, which collects metrics from a distributed system. In the real world, a very large distributed system includes hundreds of services, thousands of service instances. In that case, most likely, more than one group, even more than one company are maintaining and monitoring the distributed system. Each one of them takes charge of different parts, don\u0026rsquo;t want or shouldn\u0026rsquo;t share there metrics.\nNamespace is the proposal from this.It is used for tracing and monitoring isolation.\nSet the namespace Set agent.namespace in agent config # The agent namespace # agent.namespace=default-namespace The default value of agent.namespace is empty.\nInfluence The default header key of SkyWalking is sw8, more in this document. After agent.namespace is set, the key changes to namespace-sw8.\nThe across process propagation chain breaks, when the two sides are using different namespace.\n","excerpt":"Namespace Background SkyWalking is a monitoring tool, which collects metrics from a distributed …","ref":"/docs/main/v8.4.0/en/setup/service-agent/java-agent/namespace/","title":"Namespace"},{"body":"Observability Analysis Language Provide OAL(Observability Analysis Language) to analysis incoming data in streaming mode.\nOAL focuses on metrics in Service, Service Instance and Endpoint. Because of that, the language is easy to learn and use.\nSince 6.3, the OAL engine is embedded in OAP server runtime, as oal-rt(OAL Runtime). OAL scripts now locate in /config folder, user could simply change and reboot the server to make it effective. But still, OAL script is compile language, OAL Runtime generates java codes dynamically.\nYou could open set SW_OAL_ENGINE_DEBUG=Y at system env, to see which classes generated.\nGrammar Scripts should be named as *.oal\n// Declare the metrics. METRICS_NAME = from(SCOPE.(* | [FIELD][,FIELD ...])) [.filter(FIELD OP [INT | STRING])] .FUNCTION([PARAM][, PARAM ...]) // Disable hard code disable(METRICS_NAME); Scope Primary SCOPEs are All, Service, ServiceInstance, Endpoint, ServiceRelation, ServiceInstanceRelation, EndpointRelation. Also there are some secondary scopes, which belongs to one primary scope.\nRead Scope Definitions, you can find all existing Scopes and Fields.\nFilter Use filter to build the conditions for the value of fields, by using field name and expression.\nThe expressions support to link by and, or and (...). The OPs support ==, !=, \u0026gt;, \u0026lt;, \u0026gt;=, \u0026lt;=, in [...] ,like %..., like ...% , like %...% , contain and not contain, with type detection based of field type. Trigger compile or code generation error if incompatible.\nAggregation Function The default functions are provided by SkyWalking OAP core, and could implement more.\nProvided functions\n longAvg. The avg of all input per scope entity. The input field must be a long.   instance_jvm_memory_max = from(ServiceInstanceJVMMemory.max).longAvg();\n In this case, input are request of each ServiceInstanceJVMMemory scope, avg is based on field max.\n doubleAvg. The avg of all input per scope entity. The input field must be a double.   instance_jvm_cpu = from(ServiceInstanceJVMCPU.usePercent).doubleAvg();\n In this case, input are request of each ServiceInstanceJVMCPU scope, avg is based on field usePercent.\n percent. The number or ratio expressed as a fraction of 100, for the condition matched input.   endpoint_percent = from(Endpoint.*).percent(status == true);\n In this case, all input are requests of each endpoint, condition is endpoint.status == true.\n rate. The rate expressed as a fraction of 100, for the condition matched input.   browser_app_error_rate = from(BrowserAppTraffic.*).rate(trafficCategory == BrowserAppTrafficCategory.FIRST_ERROR, trafficCategory == BrowserAppTrafficCategory.NORMAL);\n In this case, all input are requests of each browser app traffic, numerator condition is trafficCategory == BrowserAppTrafficCategory.FIRST_ERROR and denominator condition is trafficCategory == BrowserAppTrafficCategory.NORMAL. The parameter (1) is the numerator condition. The parameter (2) is the denominator condition.\n count. The sum calls per scope entity.   service_calls_sum = from(Service.*).count();\n In this case, calls of each service.\n histogram. Read Heatmap in WIKI   all_heatmap = from(All.latency).histogram(100, 20);\n In this case, thermodynamic heatmap of all incoming requests. The parameter (1) is the precision of latency calculation, such as in above case, 113ms and 193ms are considered same in the 101-200ms group. The parameter (2) is the group amount. In above case, 21(param value + 1) groups are 0-100ms, 101-200ms, \u0026hellip; 1901-2000ms, 2000+ms\n apdex. Read Apdex in WIKI   service_apdex = from(Service.latency).apdex(name, status);\n In this case, apdex score of each service. The parameter (1) is the service name, which effects the Apdex threshold value loaded from service-apdex-threshold.yml in the config folder. The parameter (2) is the status of this request. The status(success/failure) effects the Apdex calculation.\n p99, p95, p90, p75, p50. Read percentile in WIKI   all_percentile = from(All.latency).percentile(10);\n percentile is the first multiple value metrics, introduced since 7.0.0. As having multiple values, it could be query through getMultipleLinearIntValues GraphQL query. In this case, p99, p95, p90, p75, p50 of all incoming request. The parameter is the precision of p99 latency calculation, such as in above case, 120ms and 124 are considered same. Before 7.0.0, use p99, p95, p90, p75, p50 func(s) to calculate metrics separately. Still supported in 7.x, but don\u0026rsquo;t be recommended, and don\u0026rsquo;t be included in official OAL script.\n all_p99 = from(All.latency).p99(10);\n In this case, p99 value of all incoming requests. The parameter is the precision of p99 latency calculation, such as in above case, 120ms and 124 are considered same.\nMetrics name The metrics name for storage implementor, alarm and query modules. The type inference supported by core.\nGroup All metrics data will be grouped by Scope.ID and min-level TimeBucket.\n In Endpoint scope, the Scope.ID = Endpoint id (the unique id based on service and its Endpoint)  Disable Disable is an advanced statement in OAL, which is only used in certain case. Some of the aggregation and metrics are defined through core hard codes, this disable statement is designed for make them de-active, such as segment, top_n_database_statement. In default, no one is being disable.\nExamples // Calculate p99 of both Endpoint1 and Endpoint2 endpoint_p99 = from(Endpoint.latency).filter(name in (\u0026quot;Endpoint1\u0026quot;, \u0026quot;Endpoint2\u0026quot;)).summary(0.99) // Calculate p99 of Endpoint name started with `serv` serv_Endpoint_p99 = from(Endpoint.latency).filter(name like \u0026quot;serv%\u0026quot;).summary(0.99) // Calculate the avg response time of each Endpoint endpoint_avg = from(Endpoint.latency).avg() // Calculate the p50, p75, p90, p95 and p99 of each Endpoint by 50 ms steps. endpoint_percentile = from(Endpoint.latency).percentile(10) // Calculate the percent of response status is true, for each service. endpoint_success = from(Endpoint.*).filter(status == true).percent() // Calculate the sum of response code in [404, 500, 503], for each service. endpoint_abnormal = from(Endpoint.*).filter(responseCode in [404, 500, 503]).count() // Calculate the sum of request type in [RequestType.RPC, RequestType.gRPC], for each service. endpoint_rpc_calls_sum = from(Endpoint.*).filter(type in [RequestType.RPC, RequestType.gRPC]).count() // Calculate the sum of endpoint name in [\u0026quot;/v1\u0026quot;, \u0026quot;/v2\u0026quot;], for each service. endpoint_url_sum = from(Endpoint.*).filter(name in [\u0026quot;/v1\u0026quot;, \u0026quot;/v2\u0026quot;]).count() // Calculate the sum of calls for each service. endpoint_calls = from(Endpoint.*).count() // Calculate the CPM with the GET method for each service.The value is made up with `tagKey:tagValue`. service_cpm_http_get = from(Service.*).filter(tags contain \u0026quot;http.method:GET\u0026quot;).cpm() // Calculate the CPM with the HTTP method except for the GET method for each service.The value is made up with `tagKey:tagValue`. service_cpm_http_other = from(Service.*).filter(tags not contain \u0026quot;http.method:GET\u0026quot;).cpm() disable(segment); disable(endpoint_relation_server_side); disable(top_n_database_statement); ","excerpt":"Observability Analysis Language Provide OAL(Observability Analysis Language) to analysis incoming …","ref":"/docs/main/v8.4.0/en/concepts-and-designs/oal/","title":"Observability Analysis Language"},{"body":"Observability Analysis Platform OAP(Observability Analysis Platform) is a new concept, which starts in SkyWalking 6.x. OAP replaces the old SkyWalking whole backend. The capabilities of the platform are following.\nOAP capabilities OAP accepts data from more sources, which belongs two groups: Tracing and Metrics.\n Tracing. Including, SkyWalking native data formats. Zipkin v1,v2 data formats and Jaeger data formats. Metrics. SkyWalking integrates with Service Mesh platforms, such as Istio, Envoy, Linkerd, to provide observability from data panel or control panel. Also, SkyWalking native agents can run in metrics mode, which highly improve the performance.  At the same time by using any integration solution provided, such as SkyWalking log plugin or toolkits, SkyWalking provides visualization integration for binding tracing and logging together by using the trace id and span id.\nAs usual, all services provided by gRPC and HTTP protocol to make integration easier for unsupported ecosystem.\nTracing in OAP Tracing in OAP has two ways to process.\n Traditional way in SkyWalking 5 series. Format tracing data in SkyWalking trace segment and span formats, even for Zipkin data format. The OAP analysis the segments to get metrics, and push the metrics data into the streaming aggregation. Consider tracing as some kinds of logging only. Just provide save and visualization capabilities for trace.  Also, SkyWalking accepts trace formats from other project, such as Zipkin, Jaeger, OpenCensus. These formats could be processed in the two ways too.\nMetrics in OAP Metrics in OAP is totally new feature in 6 series. Build observability for a distributed system based on metrics of connected nodes. No tracing data is required.\nMetrics data are aggregated inside OAP cluster in streaming mode. See about Observability Analysis Language, which provides the easy way to do aggregation and analysis in script style.\n","excerpt":"Observability Analysis Platform OAP(Observability Analysis Platform) is a new concept, which starts …","ref":"/docs/main/v8.4.0/en/concepts-and-designs/backend-overview/","title":"Observability Analysis Platform"},{"body":"Observe Service Mesh through ALS Envoy Access Log Service (ALS) provides full logs about RPC routed, including HTTP and TCP.\nBackground The solution was initialized and firstly implemented by Sheng Wu, Hongtao Gao, Lizan Zhou, and Dhi Aurrahman at 17 May. 2019, and was presented on KubeCon China 2019. Here is the recorded video.\nSkyWalking is the first open source project introducing this ALS based solution to the world. This provides a new way with very low payload to service mesh, but the same observability.\nEnable ALS and SkyWalking Receiver You need the following steps to set up ALS.\n  Enable envoyAccessLogService in ProxyConfig and set the ALS address to where SkyWalking OAP listens. On Istio version 1.6.0+, if Istio is installed with demo profile, you can enable ALS with command:\nistioctl manifest apply \\  --set profile=demo \\  --set meshConfig.enableEnvoyAccessLogService=true \\  --set meshConfig.defaultConfig.envoyAccessLogService.address=\u0026lt;skywalking-oap.skywalking.svc:11800\u0026gt; Note: Replace \u0026lt;skywalking-oap.skywalking.svc:11800\u0026gt; with the real address where SkyWalking OAP is deployed.\n  Activate SkyWalking Envoy Receiver. This is activated by default.\n  Choose an ALS analyzer. There are two available analyzers, k8s-mesh and mx-mesh. Set the system environment variable SW_ENVOY_METRIC_ALS_HTTP_ANALYSIS such as SW_ENVOY_METRIC_ALS_HTTP_ANALYSIS=k8s-mesh or in the application.yaml to activate the analyzer. For more about the analyzers, see SkyWalking ALS Analyzers\nenvoy-metric: selector: ${SW_ENVOY_METRIC:default} default: acceptMetricsService: ${SW_ENVOY_METRIC_SERVICE:true} alsHTTPAnalysis: ${SW_ENVOY_METRIC_ALS_HTTP_ANALYSIS:\u0026#34;\u0026#34;} # Setting the system env variable would override this.  To use multiple analyzers as a fallback，please use , to concatenate.\n  Example Here\u0026rsquo;s an example to install Istio and deploy SkyWalking by Helm chart.\nistioctl install \\  --set profile=demo \\  --set meshConfig.enableEnvoyAccessLogService=true \\  --set meshConfig.defaultConfig.envoyAccessLogService.address=skywalking-oap.istio-system:11800 git clone https://github.com/apache/skywalking-kubernetes.git cd skywalking-kubernetes/chart helm repo add elastic https://helm.elastic.co helm dep up skywalking helm install 8.1.0 skywalking -n istio-system \\  --set oap.env.SW_ENVOY_METRIC_ALS_HTTP_ANALYSIS=k8s-mesh \\  --set fullnameOverride=skywalking \\  --set oap.envoy.als.enabled=true You can use kubectl -n istio-system logs -l app=skywalking | grep \u0026quot;K8sALSServiceMeshHTTPAnalysis\u0026quot; to ensure OAP ALS k8s-mesh analyzer has been activated.\nSkyWalking ALS Analyzers There are two available analyzers, k8s-mesh and mx-mesh, you can specify one or more analyzers to analyze the access logs. When multiple analyzers are specified, it acts as a fast-success mechanism: SkyWalking loops over the analyzers and use it to analyze the logs, once there is an analyzer that is able to produce a result, it stops the loop.\nk8s-mesh k8s-mesh uses the metadata from Kubernetes cluster, hence in this analyzer OAP needs access roles to Pod, Service, and Endpoints.\nThe blog illustrates the detail of how it works, and a step-by-step tutorial to apply it into the bookinfo application.\nmx-mesh mx-mesh uses the Envoy metadata exchange mechanism to get the service name, etc., this analyzer requires Istio to enable the metadata exchange plugin (you can enable it by --set values.telemetry.v2.enabled=true, or if you\u0026rsquo;re using Istio 1.7+ and installing it with profile demo/preview, it should be enabled then).\n","excerpt":"Observe Service Mesh through ALS Envoy Access Log Service (ALS) provides full logs about RPC routed, …","ref":"/docs/main/v8.4.0/en/setup/envoy/als_setting/","title":"Observe Service Mesh through ALS"},{"body":"Official OAL script First, read OAL introduction.\nFind OAL script at the /config/oal/*.oal of SkyWalking dist, since 8.0.0. You could change it(such as adding filter condition, or add new metrics) and reboot the OAP server, then it will affect.\nAll metrics named in this script could be used in alarm and UI query.\nNotice,\nIf you try to add or remove some metrics, UI may break, we only recommend you to do this when you plan to build your own UI based on the customization analysis core.\n","excerpt":"Official OAL script First, read OAL introduction.\nFind OAL script at the /config/oal/*.oal of …","ref":"/docs/main/v8.4.0/en/guides/backend-oal-scripts/","title":"Official OAL script"},{"body":"Open Fetcher Fetcher is a concept in SkyWalking backend. It uses pulling mode rather than receiver, which read the data from the target systems. This mode is typically in some metrics SDKs, such as Prometheus.\nPrometheus Fetcher Suppose you want to enable some metric-custom.yaml files stored at fetcher-prom-rules, append its name to enabledRules of promethues-fetcher as below:\nprometheus-fetcher: selector: ${SW_PROMETHEUS_FETCHER:default} default: enabledRules: ${SW_PROMETHEUS_FETCHER_ENABLED_RULES:\u0026#34;self,metric-custom\u0026#34;} Configuration file Prometheus fetcher is configured via a configuration file. The configuration file defines everything related to fetching services and their instances, as well as which rule files to load.\nOAP can load the configuration at bootstrap. If the new configuration is not well-formed, OAP fails to start up. The files are located at $CLASSPATH/fetcher-prom-rules.\nThe file is written in YAML format, defined by the scheme described below. Brackets indicate that a parameter is optional.\nA full example can be found here\nGeneric placeholders are defined as follows:\n \u0026lt;duration\u0026gt;: a duration This will parse a textual representation of a duration. The formats accepted are based on the ISO-8601 duration format PnDTnHnMn.nS with days considered to be exactly 24 hours. \u0026lt;labelname\u0026gt;: a string matching the regular expression [a-zA-Z_][a-zA-Z0-9_]* \u0026lt;labelvalue\u0026gt;: a string of unicode characters \u0026lt;host\u0026gt;: a valid string consisting of a hostname or IP followed by an optional port number \u0026lt;path\u0026gt;: a valid URL path \u0026lt;string\u0026gt;: a regular string  # How frequently to fetch targets. fetcherInterval: \u0026lt;duration\u0026gt; # Per-fetch timeout when fetching this target. fetcherTimeout: \u0026lt;duration\u0026gt; # The HTTP resource path on which to fetch metrics from targets. metricsPath: \u0026lt;path\u0026gt; #Statically configured targets. staticConfig: # The targets specified by the static config. targets: [ - \u0026lt;target\u0026gt; ] # Labels assigned to all metrics fetched from the targets. labels: [ \u0026lt;labelname\u0026gt;: \u0026lt;labelvalue\u0026gt; ... ] # expSuffix is appended to all expression in this file. expSuffix: \u0026lt;string\u0026gt; # insert metricPrefix into metric name: \u0026lt;metricPrefix\u0026gt;_\u0026lt;raw_metric_name\u0026gt; metricPrefix: \u0026lt;string\u0026gt; # Metrics rule allow you to recompute queries. metricsRules: [ - \u0026lt;metric_rules\u0026gt; ]  # The url of target exporter. the format should be complied with \u0026#34;java.net.URI\u0026#34; url: \u0026lt;string\u0026gt; # The path of root CA file. sslCaFilePath: \u0026lt;string\u0026gt; \u0026lt;metric_rules\u0026gt; # The name of rule, which combinates with a prefix \u0026#39;meter_\u0026#39; as the index/table name in storage. name: \u0026lt;string\u0026gt; # MAL expression. exp: \u0026lt;string\u0026gt; More about MAL, please refer to mal.md\nKafka Fetcher Kafka Fetcher pulls messages from Kafka Broker(s) what is the Agent delivered. Check the agent documentation about the details. Typically Tracing Segments, Service/Instance properties, JVM Metrics, and Meter system data are supported. Kafka Fetcher can work with gRPC/HTTP Receivers at the same time for adopting different transport protocols.\nKafka Fetcher is disabled in default, and we configure as following to enable.\nkafka-fetcher: selector: ${SW_KAFKA_FETCHER:default} default: bootstrapServers: ${SW_KAFKA_FETCHER_SERVERS:localhost:9092} skywalking-segments, skywalking-metrics, skywalking-profile, skywalking-managements and skywalking-meters topics are required by kafka-fetcher. If they do not exist, Kafka Fetcher will create them in default. Also, you can create them by yourself before the OAP server started.\nWhen using the OAP server automatical creation mechanism, you could modify the number of partitions and replications of the topics through the following configurations:\nkafka-fetcher: selector: ${SW_KAFKA_FETCHER:default} default: bootstrapServers: ${SW_KAFKA_FETCHER_SERVERS:localhost:9092} partitions: ${SW_KAFKA_FETCHER_PARTITIONS:3} replicationFactor: ${SW_KAFKA_FETCHER_PARTITIONS_FACTOR:2} enableMeterSystem: ${SW_KAFKA_FETCHER_ENABLE_METER_SYSTEM:false} isSharding: ${SW_KAFKA_FETCHER_IS_SHARDING:false} consumePartitions: ${SW_KAFKA_FETCHER_CONSUME_PARTITIONS:\u0026#34;\u0026#34;} In cluster mode, all topics have the same number of partitions. Then we have to set \u0026quot;isSharding\u0026quot; to \u0026quot;true\u0026quot; and assign the partitions to consume for OAP server. The OAP server can use commas to separate multiple partitions.\nKafka Fetcher allows to configure all the Kafka producers listed here in property kafkaConsumerConfig. Such as:\nkafka-fetcher: selector: ${SW_KAFKA_FETCHER:default} default: bootstrapServers: ${SW_KAFKA_FETCHER_SERVERS:localhost:9092} partitions: ${SW_KAFKA_FETCHER_PARTITIONS:3} replicationFactor: ${SW_KAFKA_FETCHER_PARTITIONS_FACTOR:2} enableMeterSystem: ${SW_KAFKA_FETCHER_ENABLE_METER_SYSTEM:false} isSharding: ${SW_KAFKA_FETCHER_IS_SHARDING:true} consumePartitions: ${SW_KAFKA_FETCHER_CONSUME_PARTITIONS:1,3,5} kafkaConsumerConfig: enable.auto.commit: true ... When use Kafka MirrorMaker 2.0 to replicate topics between Kafka clusters, you can set the source Kafka Cluster alias(mm2SourceAlias) and separator(mm2SourceSeparator) according to your Kafka MirrorMaker config.\nkafka-fetcher: selector: ${SW_KAFKA_FETCHER:default} default: bootstrapServers: ${SW_KAFKA_FETCHER_SERVERS:localhost:9092} partitions: ${SW_KAFKA_FETCHER_PARTITIONS:3} replicationFactor: ${SW_KAFKA_FETCHER_PARTITIONS_FACTOR:2} enableMeterSystem: ${SW_KAFKA_FETCHER_ENABLE_METER_SYSTEM:false} isSharding: ${SW_KAFKA_FETCHER_IS_SHARDING:true} consumePartitions: ${SW_KAFKA_FETCHER_CONSUME_PARTITIONS:1,3,5} mm2SourceAlias: ${SW_KAFKA_MM2_SOURCE_ALIAS:\u0026#34;\u0026#34;} mm2SourceSeparator: ${SW_KAFKA_MM2_SOURCE_SEPARATOR:\u0026#34;\u0026#34;} kafkaConsumerConfig: enable.auto.commit: true ... ","excerpt":"Open Fetcher Fetcher is a concept in SkyWalking backend. It uses pulling mode rather than receiver, …","ref":"/docs/main/v8.4.0/en/setup/backend/backend-fetcher/","title":"Open Fetcher"},{"body":"Oracle and Resin plugins These plugins can\u0026rsquo;t be provided in Apache release because of Oracle and Resin Licenses. If you want to know details, please read Apache license legal document\nDue to license incompatibilities/restrictions these plugins are hosted and released in 3rd part repository, go to OpenSkywalking java plugin extension repository to get these.\n","excerpt":"Oracle and Resin plugins These plugins can\u0026rsquo;t be provided in Apache release because of Oracle …","ref":"/docs/main/v8.4.0/en/setup/service-agent/java-agent/agent-optional-plugins/oracle-resin-plugins/","title":"Oracle and Resin plugins"},{"body":"Overview SkyWalking: an open source observability platform used to collect, analyze, aggregate and visualize data from services and cloud native infrastructures. SkyWalking provides an easy way to maintain a clear view of your distributed systems, even across Clouds. It is a modern APM, specially designed for cloud native, container based distributed systems.\nWhy use SkyWalking? SkyWalking provides solutions for observing and monitoring distributed systems, in many different scenarios. First of all, like traditional approaches, SkyWalking provides auto instrument agents for services, such as Java, C#, Node.js, Go, PHP and Nginx LUA. (with calls out for Python and C++ SDK contributions). In multilanguage, continuously deployed environments, cloud native infrastructures grow more powerful but also more complex. SkyWalking\u0026rsquo;s service mesh receiver allows SkyWalking to receive telemetry data from service mesh frameworks such as Istio/Envoy and Linkerd, allowing users to understanding the entire distributed system.\nSkyWalking provides observability capabilities for service(s), service instance(s), endpoint(s). The terms Service, Instance and Endpoint are used everywhere today, so it is worth defining their specific meanings in the context of SkyWalking:\n Service. Represents a set/group of workloads which provide the same behaviours for incoming requests. You can define the service name when you are using instrument agents or SDKs. SkyWalking can also use the name you define in platforms such as Istio. Service Instance. Each individual workload in the Service group is known as an instance. Like pods in Kubernetes, it doesn\u0026rsquo;t need to be a single OS process, however, if you are using instrument agents, an instance is actually a real OS process. Endpoint. A path in a service for incoming requests, such as an HTTP URI path or a gRPC service class + method signature.  SkyWalking allows users to understand the topology relationship between Services and Endpoints, to view the metrics of every Service/Service Instance/Endpoint and to set alarm rules.\nIn addition, you can integrate\n Other distributed tracing using SkyWalking native agents and SDKs with Zipkin, Jaeger and OpenCensus. Other metrics systems, such as Prometheus, Sleuth(Micrometer).  Architecture SkyWalking is logically split into four parts: Probes, Platform backend, Storage and UI.\n Probes collect data and reformat them for SkyWalking requirements (different probes support different sources). Platform backend, supports data aggregation, analysis and drives process flow from probes to the UI. The analysis includes SkyWalking natives traces and metrics, 3rd party, including Istio and Envoy telemetry, Zipkin trace format, etc. You even can customize aggregation and analysis by using Observability Analysis Language for native metrics and Meter System for extension metrics. Storage houses SkyWalking data through an open/plugable interface. You can choose an existing implementation, such as ElasticSearch, H2 or a MySQL cluster managed by Sharding-Sphere, or implement your own. Patches for new storage implementors welcome! UI a highly customizable web based interface allowing SkyWalking end users to visualize and manage SkyWalking data.  What is next?  Learn SkyWalking\u0026rsquo;s Project Goals FAQ, Why doesn\u0026rsquo;t SkyWalking involve MQ in the architecture?  ","excerpt":"Overview SkyWalking: an open source observability platform used to collect, analyze, aggregate and …","ref":"/docs/main/v8.4.0/en/concepts-and-designs/overview/","title":"Overview"},{"body":"Plugin automatic test framework Plugin test framework is designed for verifying the plugins' function and compatible status. As there are dozens of plugins and hundreds of versions need to be verified, it is impossible to do manually. The test framework uses container based tech stack, requires a set of real services with agent installed, then the test mock OAP backend is running to check the segments data sent from agents.\nEvery plugin maintained in the main repo requires corresponding test cases, also matching the versions in the supported list doc.\nEnvironment Requirements  MacOS/Linux JDK 8+ Docker Docker Compose  Case Base Image Introduction The test framework provides JVM-container and Tomcat-container base images including JDK8, JDK14. You could choose the suitable one for your test case, if both are suitable, JVM-container is preferred.\nJVM-container Image Introduction JVM-container uses openjdk:8 as the base image. JVM-container has supported JDK14, which inherits openjdk:14. The test case project is required to be packaged as project-name.zip, including startup.sh and uber jar, by using mvn clean package.\nTake the following test projects as good examples\n sofarpc-scenario as a single project case. webflux-scenario as a case including multiple projects. jdk14-with-gson-scenario as a single project case with JDK14.  Tomcat-container Image Introduction Tomcat-container uses tomcat:8.5.57-jdk8-openjdk or tomcat:8.5.57-jdk14-openjdk as the base image. The test case project is required to be packaged as project-name.war by using mvn package.\nTake the following test project as a good example\n spring-4.3.x-scenario  Test project hierarchical structure The test case is an independent maven project, and it is required to be packaged as a war tar ball or zip file, depends on the chosen base image. Also, two external accessible endpoints, mostly two URLs, are required.\nAll test case codes should be in org.apache.skywalking.apm.testcase.* package, unless there are some codes expected being instrumented, then the classes could be in test.org.apache.skywalking.apm.testcase.* package.\nJVM-container test project hierarchical structure\n[plugin-scenario] |- [bin] |- startup.sh |- [config] |- expectedData.yaml |- [src] |- [main] |- ... |- [resource] |- log4j2.xml |- pom.xml |- configuration.yaml |- support-version.list [] = directory Tomcat-container test project hierarchical structure\n[plugin-scenario] |- [config] |- expectedData.yaml |- [src] |- [main] |- ... |- [resource] |- log4j2.xml |- [webapp] |- [WEB-INF] |- web.xml |- pom.xml |- configuration.yaml |- support-version.list [] = directory Test case configuration files The following files are required in every test case.\n   File Name Descriptions     configuration.yml Declare the basic case inform, including, case name, entrance endpoints, mode, dependencies.   expectedData.yaml Describe the expected segmentItems.   support-version.list List the target versions for this case   startup.sh JVM-container only, don\u0026rsquo;t need this when useTomcat-container    * support-version.list format requires every line for a single version(Contains only the last version number of each minor version). Could use # to comment out this version.\nconfiguration.yml    Field description     type Image type, options, jvm or tomcat. Required.   entryService The entrance endpoint(URL) for test case access. Required. (HTTP Method: GET)   healthCheck The health check endpoint(URL) for test case access. Required. (HTTP Method: HEAD)   startScript Path of start up script. Required in type: jvm only.   framework Case name.   runningMode Running mode whether with the optional plugin, options, default(default), with_optional, with_bootstrap   withPlugins Plugin selector rule. eg:apm-spring-annotation-plugin-*.jar. Required when runningMode=with_optional or runningMode=with_bootstrap.   environment Same as docker-compose#environment.   depends_on Same as docker-compose#depends_on.   dependencies Same as docker-compose#services, image、links、hostname、environment、depends_on are supported.    Notice:, docker-compose active only when dependencies is only blank.\nrunningMode option description.\n   Option description     default Active all plugins in plugin folder like the official distribution agent.   with_optional Active default and plugins in optional-plugin by the give selector.   with_bootstrap Active default and plugins in bootstrap-plugin by the give selector.    with_optional/with_bootstrap supports multiple selectors, separated by ;.\nFile Format\ntype: entryService: healthCheck: startScript: framework: runningMode: withPlugins: environment: ... depends_on: ... dependencies: service1: image: hostname: expose: ... environment: ... depends_on: ... links: ... entrypoint: ... healthcheck: ...  dependencies supports docker compose healthcheck. But the format is a little difference. We need - as the start of every config item, and describe it as a string line.  Such as in official doc, the health check is\nhealthcheck: test: [\u0026#34;CMD\u0026#34;, \u0026#34;curl\u0026#34;, \u0026#34;-f\u0026#34;, \u0026#34;http://localhost\u0026#34;] interval: 1m30s timeout: 10s retries: 3 start_period: 40s The here, you should write as\nhealthcheck: - \u0026#39;test: [\u0026#34;CMD\u0026#34;, \u0026#34;curl\u0026#34;, \u0026#34;-f\u0026#34;, \u0026#34;http://localhost\u0026#34;]\u0026#39; - \u0026#34;interval: 1m30s\u0026#34; - \u0026#34;timeout: 10s\u0026#34; - \u0026#34;retries: 3\u0026#34; - \u0026#34;start_period: 40s\u0026#34; In some cases, the dependency service, mostly 3rd party server like SolrJ server, is required to keep the same version as client lib version, which defined as ${test.framework.version} in pom. Could use ${CASE_SERVER_IMAGE_VERSION} as the version number, it will be changed in the test for every version.\n Don\u0026rsquo;t support resource related configurations, such as volumes, ports and ulimits. Because in test scenarios, don\u0026rsquo;t need mapping any port to the host VM, or mount any folder.\n Take following test cases as examples\n dubbo-2.7.x with JVM-container jetty with JVM-container gateway with runningMode canal with docker-compose  expectedData.yaml Operator for number\n   Operator Description     nq Not equal   eq Equal(default)   ge Greater than or equal   gt Greater than    Operator for String\n   Operator Description     not null Not null   null Null or empty String   eq Equal(default)    Expected Data Format Of The Segment\nsegmentItems: - serviceName: SERVICE_NAME(string) segmentSize: SEGMENT_SIZE(int) segments: - segmentId: SEGMENT_ID(string) spans: ...    Field Description     serviceName Service Name.   segmentSize The number of segments is expected.   segmentId trace ID.   spans segment span list. Follow the next section to see how to describe every span.    Expected Data Format Of The Span\nNotice: The order of span list should follow the order of the span finish time.\noperationName: OPERATION_NAME(string) parentSpanId: PARENT_SPAN_ID(int) spanId: SPAN_ID(int) startTime: START_TIME(int) endTime: END_TIME(int) isError: IS_ERROR(string: true, false) spanLayer: SPAN_LAYER(string: DB, RPC_FRAMEWORK, HTTP, MQ, CACHE) spanType: SPAN_TYPE(string: Exit, Entry, Local) componentId: COMPONENT_ID(int) tags: - {key: TAG_KEY(string), value: TAG_VALUE(string)} ... logs: - {key: LOG_KEY(string), value: LOG_VALUE(string)} ... peer: PEER(string) refs: - { traceId: TRACE_ID(string), parentTraceSegmentId: PARENT_TRACE_SEGMENT_ID(string), parentSpanId: PARENT_SPAN_ID(int), parentService: PARENT_SERVICE(string), parentServiceInstance: PARENT_SERVICE_INSTANCE(string), parentEndpoint: PARENT_ENDPOINT_NAME(string), networkAddress: NETWORK_ADDRESS(string), refType: REF_TYPE(string: CrossProcess, CrossThread) } ...    Field Description     operationName Span Operation Name.   parentSpanId Parent span id. Notice: The parent span id of the first span should be -1.   spanId Span Id. Notice, start from 0.   startTime Span start time. It is impossible to get the accurate time, not 0 should be enough.   endTime Span finish time. It is impossible to get the accurate time, not 0 should be enough.   isError Span status, true or false.   componentId Component id for your plugin.   tags Span tag list. Notice, Keep in the same order as the plugin coded.   logs Span log list. Notice, Keep in the same order as the plugin coded.   SpanLayer Options, DB, RPC_FRAMEWORK, HTTP, MQ, CACHE.   SpanType Span type, options, Exit, Entry or Local.   peer Remote network address, IP + port mostly. For exit span, this should be required.    The verify description for SegmentRef\n   Field Description     traceId    parentTraceSegmentId Parent SegmentId, pointing to the segment id in the parent segment.   parentSpanId Parent SpanID, pointing to the span id in the parent segment.   parentService The service of parent/downstream service name.   parentServiceInstance The instance of parent/downstream service instance name.   parentEndpoint The endpoint of parent/downstream service.   networkAddress The peer value of parent exit span.   refType Ref type, options, CrossProcess or CrossThread.    Expected Data Format Of The Meter Items\nmeterItems: - serviceName: SERVICE_NAME(string) meterSize: METER_SIZE(int) meters: - ...    Field Description     serviceName Service Name.   meterSize The number of meters is expected.   meters meter list. Follow the next section to see how to describe every meter.    Expected Data Format Of The Meter\nmeterId: name: NAME(string) tags: - {name: TAG_NAME(string), value: TAG_VALUE(string)} singleValue: SINGLE_VALUE(double) histogramBuckets: - HISTOGRAM_BUCKET(double) ... The verify description for MeterId\n   Field Description     name meter name.   tags meter tags.   tags.name tag name.   tags.value tag value.   singleValue counter or gauge value. Using condition operate of the number to validate, such as gt, ge. If current meter is histogram, don\u0026rsquo;t need to write this field.   histogramBuckets histogram bucket. The bucket list must be ordered. The tool assert at least one bucket of the histogram having nonzero count. If current meter is counter or gauge, don\u0026rsquo;t need to write this field.    startup.sh This script provide a start point to JVM based service, most of them starts by a java -jar, with some variables. The following system environment variables are available in the shell.\n   Variable Description     agent_opts Agent plugin opts, check the detail in plugin doc or the same opt added in this PR.   SCENARIO_NAME Service name. Default same as the case folder name   SCENARIO_VERSION Version   SCENARIO_ENTRY_SERVICE Entrance URL to access this service   SCENARIO_HEALTH_CHECK_URL Health check URL     ${agent_opts} is required to add into your java -jar command, which including the parameter injected by test framework, and make agent installed. All other parameters should be added after ${agent_opts}.\n The test framework will set the service name as the test case folder name by default, but in some cases, there are more than one test projects are required to run in different service codes, could set it explicitly like the following example.\nExample\nhome=\u0026#34;$(cd \u0026#34;$(dirname $0)\u0026#34;; pwd)\u0026#34; java -jar ${agent_opts} \u0026#34;-Dskywalking.agent.service_name=jettyserver-scenario\u0026#34; ${home}/../libs/jettyserver-scenario.jar \u0026amp; sleep 1 java -jar ${agent_opts} \u0026#34;-Dskywalking.agent.service_name=jettyclient-scenario\u0026#34; ${home}/../libs/jettyclient-scenario.jar \u0026amp;  Only set this or use other skywalking options when it is really necessary.\n Take the following test cases as examples\n undertow webflux  Best Practices How To Use The Archetype To Create A Test Case Project We provided archetypes and a script to make creating a project easier. It creates a completed project of a test case. So that we only need to focus on cases. First, we can use followed command to get usage about the script.\nbash ${SKYWALKING_HOME}/test/plugin/generator.sh\nThen, runs and generates a project, named by scenario_name, in ./scenarios.\nRecommendations for pom \u0026lt;properties\u0026gt; \u0026lt;!-- Provide and use this property in the pom. --\u0026gt; \u0026lt;!-- This version should match the library version, --\u0026gt; \u0026lt;!-- in this case, http components lib version 4.3. --\u0026gt; \u0026lt;test.framework.version\u0026gt;4.3\u0026lt;/test.framework.version\u0026gt; \u0026lt;/properties\u0026gt; \u0026lt;dependencies\u0026gt; \u0026lt;dependency\u0026gt; \u0026lt;groupId\u0026gt;org.apache.httpcomponents\u0026lt;/groupId\u0026gt; \u0026lt;artifactId\u0026gt;httpclient\u0026lt;/artifactId\u0026gt; \u0026lt;version\u0026gt;${test.framework.version}\u0026lt;/version\u0026gt; \u0026lt;/dependency\u0026gt; ... \u0026lt;/dependencies\u0026gt; \u0026lt;build\u0026gt; \u0026lt;!-- Set the package final name as same as the test case folder case. --\u0026gt; \u0026lt;finalName\u0026gt;httpclient-4.3.x-scenario\u0026lt;/finalName\u0026gt; .... \u0026lt;/build\u0026gt; How To Implement Heartbeat Service Heartbeat service is designed for checking the service available status. This service is a simple HTTP service, returning 200 means the target service is ready. Then the traffic generator will access the entry service and verify the expected data. User should consider to use this service to detect such as whether the dependent services are ready, especially when dependent services are database or cluster.\nNotice, because heartbeat service could be traced fully or partially, so, segmentSize in expectedData.yaml should use ge as the operator, and don\u0026rsquo;t include the segments of heartbeat service in the expected segment data.\nThe example Process of Writing Tracing Expected Data Expected data file, expectedData.yaml, include SegmentItems part.\nWe are using the HttpClient plugin to show how to write the expected data.\nThere are two key points of testing\n Whether is HttpClient span created. Whether the ContextCarrier created correctly, and propagates across processes.  +-------------+ +------------------+ +-------------------------+ | Browser | | Case Servlet | | ContextPropagateServlet | | | | | | | +-----|-------+ +---------|--------+ +------------|------------+ | | | | | | | WebHttp +-+ | +------------------------\u0026gt; |-| HttpClient +-+ | |--------------------------------\u0026gt; |-| | |-| |-| | |-| |-| | |-| \u0026lt;--------------------------------| | |-| +-+ | \u0026lt;--------------------------| | | +-+ | | | | | | | | | | | | | + + + segmentItems By following the flow of HttpClient case, there should be two segments created.\n Segment represents the CaseServlet access. Let\u0026rsquo;s name it as SegmentA. Segment represents the ContextPropagateServlet access. Let\u0026rsquo;s name it as SegmentB.  segmentItems: - serviceName: httpclient-case segmentSize: ge 2 # Could have more than one health check segments, because, the dependency is not standby. Because Tomcat plugin is a default plugin of SkyWalking, so, in SegmentA, there are two spans\n Tomcat entry span HttpClient exit span  SegmentA span list should like following\n- segmentId: not null spans: - operationName: /httpclient-case/case/context-propagate parentSpanId: 0 spanId: 1 startTime: nq 0 endTime: nq 0 isError: false spanLayer: Http spanType: Exit componentId: eq 2 tags: - {key: url, value: \u0026#39;http://127.0.0.1:8080/httpclient-case/case/context-propagate\u0026#39;} - {key: http.method, value: GET} logs: [] peer: 127.0.0.1:8080 - operationName: /httpclient-case/case/httpclient parentSpanId: -1 spanId: 0 startTime: nq 0 endTime: nq 0 spanLayer: Http isError: false spanType: Entry componentId: 1 tags: - {key: url, value: \u0026#39;http://localhost:{SERVER_OUTPUT_PORT}/httpclient-case/case/httpclient\u0026#39;} - {key: http.method, value: GET} logs: [] peer: null SegmentB should only have one Tomcat entry span, but includes the Ref pointing to SegmentA.\nSegmentB span list should like following\n- segmentId: not null spans: - operationName: /httpclient-case/case/context-propagate parentSpanId: -1 spanId: 0 tags: - {key: url, value: \u0026#39;http://127.0.0.1:8080/httpclient-case/case/context-propagate\u0026#39;} - {key: http.method, value: GET} logs: [] startTime: nq 0 endTime: nq 0 spanLayer: Http isError: false spanType: Entry componentId: 1 peer: null refs: - {parentEndpoint: /httpclient-case/case/httpclient, networkAddress: \u0026#39;localhost:8080\u0026#39;, refType: CrossProcess, parentSpanId: 1, parentTraceSegmentId: not null, parentServiceInstance: not null, parentService: not null, traceId: not null} The example Process of Writing Meter Expected Data Expected data file, expectedData.yaml, include MeterItems part.\nWe are using the toolkit plugin to demonstrate how to write the expected data. When write the meter plugin, the expected data file keeps the same.\nThere is one key point of testing\n Build a meter and operate it.  Such as Counter:\nMeterFactory.counter(\u0026#34;test_counter\u0026#34;).tag(\u0026#34;ck1\u0026#34;, \u0026#34;cv1\u0026#34;).build().increment(1d); MeterFactory.histogram(\u0026#34;test_histogram\u0026#34;).tag(\u0026#34;hk1\u0026#34;, \u0026#34;hv1\u0026#34;).steps(1d, 5d, 10d).build().addValue(2d); +-------------+ +------------------+ | Plugin | | Agent core | | | | | +-----|-------+ +---------|--------+ | | | | | Build or operate +-+ +------------------------\u0026gt; |-| | |-] | |-| | |-| | |-| | |-| | \u0026lt;--------------------------| | +-+ | | | | | | | | + + meterItems By following the flow of the toolkit case, there should be two meters created.\n Meter test_counter created from MeterFactory#counter. Let\u0026rsquo;s name it as MeterA. Meter test_histogram created from MeterFactory#histogram. Let\u0026rsquo;s name it as MeterB.  meterItems: - serviceName: toolkit-case meterSize: 2 They\u0026rsquo;re showing two kinds of meter, MeterA has a single value, MeterB has a histogram value.\nMeterA should like following, counter and gauge use the same data format.\n- meterId: name: test_counter tags: - {name: ck1, value: cv1} singleValue: gt 0 MeterB should like following.\n- meterId: name: test_histogram tags: - {name: hk1, value: hv1} histogramBuckets: - 0.0 - 1.0 - 5.0 - 10.0 Local Test and Pull Request To The Upstream First of all, the test case project could be compiled successfully, with right project structure and be able to deploy. The developer should test the start script could run in Linux/MacOS, and entryService/health services are able to provide the response.\nYou could run test by using following commands\ncd ${SKYWALKING_HOME} bash ./test/plugin/run.sh -f ${scenario_name} Notice，if codes in ./apm-sniffer have been changed, no matter because your change or git update， please recompile the skywalking-agent. Because the test framework will use the existing skywalking-agent folder, rather than recompiling it every time.\nUse ${SKYWALKING_HOME}/test/plugin/run.sh -h to know more command options.\nIf the local test passed, then you could add it to .github/workflows/plugins-test.\u0026lt;n\u0026gt;.yaml file, which will drive the tests running on the GitHub Actions of official SkyWalking repository. Based on your plugin\u0026rsquo;s name, please add the test case into file .github/workflows/plugins-test.\u0026lt;n\u0026gt;.yaml, by alphabetical orders.\nEvery test case is a GitHub Actions Job. Please use the scenario directory name as the case name, mostly you\u0026rsquo;ll just need to decide which file (plugins-test.\u0026lt;n\u0026gt;.yaml) to add your test case, and simply put one line (as follows) in it, take the existed cases as examples. You can run python3 tools/select-group.py to see which file contains the least cases and add your cases into it, in order to balance the running time of each group.\nIf a test case required to run in JDK 14 environment, please add you test case into file plugins-jdk14-test.\u0026lt;n\u0026gt;.yaml.\njobs: PluginsTest: name: Plugin runs-on: ubuntu-18.04 timeout-minutes: 90 strategy: fail-fast: true matrix: case: # ... - \u0026lt;your scenario test directory name\u0026gt; # ... ","excerpt":"Plugin automatic test framework Plugin test framework is designed for verifying the plugins' …","ref":"/docs/main/v8.4.0/en/guides/plugin-test/","title":"Plugin automatic test framework"},{"body":"Plugin Development Guide This document describe how to understand, develop and contribute plugin.\nThere are 2 kinds of plugin\n Tracing plugin. Follow the distributed tracing concept to collect spans with tags and logs. Meter plugin. Collect numeric metrics in Counter, Guage, and Histogram formats.  We also provide the plugin test tool to verify the data collected and reported by the plugin. If you plan to contribute any plugin to our main repo, the data would be verified by this tool too.\nTracing plugin Concepts Span Span is an important and common concept in distributed tracing system. Learn Span from Google Dapper Paper and OpenTracing\nSkyWalking supports OpenTracing and OpenTracing-Java API from 2017. Our Span concepts are similar with the paper and OpenTracing. Also we extend the Span.\nThere are three types of Span\n1.1 EntrySpan EntrySpan represents a service provider, also the endpoint of server side. As an APM system, we are targeting the application servers. So almost all the services and MQ-consumer are EntrySpan(s).\n1.2 LocalSpan LocalSpan represents a normal Java method, which does not relate to remote service, neither a MQ producer/consumer nor a service(e.g. HTTP service) provider/consumer.\n1.3 ExitSpan ExitSpan represents a client of service or MQ-producer, as named as LeafSpan at early age of SkyWalking. e.g. accessing DB by JDBC, reading Redis/Memcached are cataloged an ExitSpan.\nContextCarrier In order to implement distributed tracing, the trace across process need to be bind, and the context should propagate across the process. That is ContextCarrier\u0026rsquo;s duty.\nHere are the steps about how to use ContextCarrier in a A-\u0026gt;B distributed call.\n Create a new and empty ContextCarrier at client side. Create an ExitSpan by ContextManager#createExitSpan or use ContextManager#inject to init the ContextCarrier. Put all items of ContextCarrier into heads(e.g. HTTP HEAD), attachments(e.g. Dubbo RPC framework) or messages(e.g. Kafka) The ContextCarrier propagates to server side by the service call. At server side, get all items from heads, attachments or messages. Create an EntrySpan by ContextManager#createEntrySpan or use ContextManager#extract to bind the client and server.  Let\u0026rsquo;s demonstrate the steps by Apache HTTPComponent client plugin and Tomcat 7 server plugin\n Client side steps by Apache HTTPComponent client plugin  span = ContextManager.createExitSpan(\u0026#34;/span/operation/name\u0026#34;, contextCarrier, \u0026#34;ip:port\u0026#34;); CarrierItem next = contextCarrier.items(); while (next.hasNext()) { next = next.next(); httpRequest.setHeader(next.getHeadKey(), next.getHeadValue()); } Server side steps by Tomcat 7 server plugin  ContextCarrier contextCarrier = new ContextCarrier(); CarrierItem next = contextCarrier.items(); while (next.hasNext()) { next = next.next(); next.setHeadValue(request.getHeader(next.getHeadKey())); } span = ContextManager.createEntrySpan(“/span/operation/name”, contextCarrier); ContextSnapshot Besides across process, across thread but in a process need to be supported, because async process(In-memory MQ) and batch process are common in Java. Across process and across thread are similar, because they are both about propagating context. The only difference is that, don\u0026rsquo;t need to serialize for across thread.\nHere are the three steps about across thread propagation:\n Use ContextManager#capture to get the ContextSnapshot object. Let the sub-thread access the ContextSnapshot by any way, through method arguments or carried by an existed arguments Use ContextManager#continued in sub-thread.  Core APIs ContextManager ContextManager provides all major and primary APIs.\n Create EntrySpan  public static AbstractSpan createEntrySpan(String endpointName, ContextCarrier carrier) Create EntrySpan by operation name(e.g. service name, uri) and ContextCarrier.\nCreate LocalSpan  public static AbstractSpan createLocalSpan(String endpointName) Create LocalSpan by operation name(e.g. full method signature)\nCreate ExitSpan  public static AbstractSpan createExitSpan(String endpointName, ContextCarrier carrier, String remotePeer) Create ExitSpan by operation name(e.g. service name, uri) and new ContextCarrier and peer address (e.g. ip+port, hostname+port)\nAbstractSpan /** * Set the component id, which defines in {@link ComponentsDefine} * * @param component * @return the span for chaining. */ AbstractSpan setComponent(Component component); AbstractSpan setLayer(SpanLayer layer); /** * Set a key:value tag on the Span. * * @return this Span instance, for chaining */ AbstractSpan tag(String key, String value); /** * Record an exception event of the current walltime timestamp. * * @param t any subclass of {@link Throwable}, which occurs in this span. * @return the Span, for chaining */ AbstractSpan log(Throwable t); AbstractSpan errorOccurred(); /** * Record an event at a specific timestamp. * * @param timestamp The explicit timestamp for the log record. * @param event the events * @return the Span, for chaining */ AbstractSpan log(long timestamp, Map\u0026lt;String, ?\u0026gt; event); /** * Sets the string name for the logical operation this span represents. * * @return this Span instance, for chaining */ AbstractSpan setOperationName(String endpointName); Besides setting operation name, tags and logs, two attributes should be set, which are component and layer, especially for EntrySpan and ExitSpan\nSpanLayer is the catalog of span. Here are 5 values:\n UNKNOWN (default) DB RPC_FRAMEWORK, for a RPC framework, not an ordinary HTTP HTTP MQ  Component IDs are defined and reserved by SkyWalking project. For component name/ID extension, please follow Component library definition and extension document.\nSpecial Span Tags All tags are available in the trace view, meanwhile, in the OAP backend analysis, some special tag or tag combination could provide other advanced features.\nTag key status_code The value should be an integer. The response code of OAL entities is according to this.\nTag key db.statement and db.type. The value of db.statement should be a String, representing the Database statement, such as SQL, or [No statement]/+span#operationName if value is empty. When exit span has this tag, OAP samples the slow statements based on agent-analyzer/default/maxSlowSQLLength. The threshold of slow statement is defined by following agent-analyzer/default/slowDBAccessThreshold\nExtension logic endpoint. Tag key x-le Logic endpoint is a concept, which doesn\u0026rsquo;t represent a real RPC call, but requires the statistic. The value of x-le should be JSON format, with two options.\n Define a separated logic endpoint. Provide its own endpoint name, latency and status. Suitable for entry and local span.  { \u0026#34;name\u0026#34;: \u0026#34;GraphQL-service\u0026#34;, \u0026#34;latency\u0026#34;: 100, \u0026#34;status\u0026#34;: true } Declare the current local span representing a logic endpoint.  { \u0026#34;logic-span\u0026#34;: true } Advanced APIs Async Span APIs There is a set of advanced APIs in Span, which work specific for async scenario. When tags, logs, attributes(including end time) of the span needs to set in another thread, you should use these APIs.\n/** * The span finish at current tracing context, but the current span is still alive, until {@link #asyncFinish} * called. * * This method must be called\u0026lt;br/\u0026gt; * 1. In original thread(tracing context). * 2. Current span is active span. * * During alive, tags, logs and attributes of the span could be changed, in any thread. * * The execution times of {@link #prepareForAsync} and {@link #asyncFinish()} must match. * * @return the current span */ AbstractSpan prepareForAsync(); /** * Notify the span, it could be finished. * * The execution times of {@link #prepareForAsync} and {@link #asyncFinish()} must match. * * @return the current span */ AbstractSpan asyncFinish();  Call #prepareForAsync in original context. Do ContextManager#stopSpan in original context when your job in current thread is done. Propagate the span to any other thread. After all set, call #asyncFinish in any thread. Tracing context will be finished and report to backend when all spans\u0026rsquo;s #prepareForAsync finished(Judged by count of API execution).  Develop a plugin Abstract The basic method to trace is intercepting a Java method, by using byte code manipulation tech and AOP concept. SkyWalking boxed the byte code manipulation tech and tracing context propagation, so you just need to define the intercept point(a.k.a. aspect pointcut in Spring)\nIntercept SkyWalking provide two common defines to intercept Contructor, instance method and class method.\n Extend ClassInstanceMethodsEnhancePluginDefine defines Contructor intercept points and instance method intercept points. Extend ClassStaticMethodsEnhancePluginDefine defines class method intercept points.  Of course, you can extend ClassEnhancePluginDefine to set all intercept points. But it is unusual.\nImplement plugin I will demonstrate about how to implement a plugin by extending ClassInstanceMethodsEnhancePluginDefine\n Define the target class name  protected abstract ClassMatch enhanceClass(); ClassMatch represents how to match the target classes, there are 4 ways:\n byName, through the full class name(package name + . + class name) byClassAnnotationMatch, through the class existed certain annotations. byMethodAnnotationMatch, through the class\u0026rsquo;s method existed certain annotations. byHierarchyMatch, through the class\u0026rsquo;s parent classes or interfaces  Attentions:\n Never use ThirdPartyClass.class in the instrumentation definitions, such as takesArguments(ThirdPartyClass.class), or byName(ThirdPartyClass.class.getName()), because of the fact that ThirdPartyClass dose not necessarily exist in the target application and this will break the agent; we have import checks to help on checking this in CI, but it doesn\u0026rsquo;t cover all scenarios of this limitation, so never try to work around this limitation by something like using full-qualified-class-name (FQCN), i.e. takesArguments(full.qualified.ThirdPartyClass.class) and byName(full.qualified.ThirdPartyClass.class.getName()) will pass the CI check, but are still invalid in the agent codes, Use Full Qualified Class Name String Literature Instead. Even you are perfectly sure that the class to be intercepted exists in the target application (such as JDK classes), still, don\u0026rsquo;t use *.class.getName() to get the class String name. Recommend you to use literal String. This is for avoiding ClassLoader issues. by*AnnotationMatch doesn\u0026rsquo;t support the inherited annotations. Don\u0026rsquo;t recommend to use byHierarchyMatch, unless it is really necessary. Because using it may trigger intercepting many unexcepted methods, which causes performance issues and concerns.  Example：\n@Override protected ClassMatch enhanceClassName() { return byName(\u0026#34;org.apache.catalina.core.StandardEngineValve\u0026#34;);\t}\tDefine an instance method intercept point  public InstanceMethodsInterceptPoint[] getInstanceMethodsInterceptPoints(); public interface InstanceMethodsInterceptPoint { /** * class instance methods matcher. * * @return methods matcher */ ElementMatcher\u0026lt;MethodDescription\u0026gt; getMethodsMatcher(); /** * @return represents a class name, the class instance must instanceof InstanceMethodsAroundInterceptor. */ String getMethodsInterceptor(); boolean isOverrideArgs(); } Also use Matcher to set the target methods. Return true in isOverrideArgs, if you want to change the argument ref in interceptor.\nThe following sections will tell you how to implement the interceptor.\nAdd plugin define into skywalking-plugin.def file  tomcat-7.x/8.x=TomcatInstrumentation  Set up witnessClasses and/or witnessMethods if the instrumentation should be activated in specific versions.\nExample:\n// The plugin is activated only when the foo.Bar class exists. @Override protected String[] witnessClasses() { return new String[] { \u0026#34;foo.Bar\u0026#34; }; } // The plugin is activated only when the foo.Bar#hello method exists. @Override protected List\u0026lt;WitnessMethod\u0026gt; witnessMethods() { List\u0026lt;WitnessMethod\u0026gt; witnessMethodList = new ArrayList\u0026lt;\u0026gt;(); WitnessMethod witnessMethod = new WitnessMethod(\u0026#34;foo.Bar\u0026#34;, ElementMatchers.named(\u0026#34;hello\u0026#34;)); witnessMethodList.add(witnessMethod); return witnessMethodList; } For more example, see WitnessTest.java\n  Implement an interceptor As an interceptor for an instance method, the interceptor implements org.apache.skywalking.apm.agent.core.plugin.interceptor.enhance.InstanceMethodsAroundInterceptor\n/** * A interceptor, which intercept method\u0026#39;s invocation. The target methods will be defined in {@link * ClassEnhancePluginDefine}\u0026#39;s subclass, most likely in {@link ClassInstanceMethodsEnhancePluginDefine} */ public interface InstanceMethodsAroundInterceptor { /** * called before target method invocation. * * @param result change this result, if you want to truncate the method. * @throws Throwable */ void beforeMethod(EnhancedInstance objInst, Method method, Object[] allArguments, Class\u0026lt;?\u0026gt;[] argumentsTypes, MethodInterceptResult result) throws Throwable; /** * called after target method invocation. Even method\u0026#39;s invocation triggers an exception. * * @param ret the method\u0026#39;s original return value. * @return the method\u0026#39;s actual return value. * @throws Throwable */ Object afterMethod(EnhancedInstance objInst, Method method, Object[] allArguments, Class\u0026lt;?\u0026gt;[] argumentsTypes, Object ret) throws Throwable; /** * called when occur exception. * * @param t the exception occur. */ void handleMethodException(EnhancedInstance objInst, Method method, Object[] allArguments, Class\u0026lt;?\u0026gt;[] argumentsTypes, Throwable t); } Use the core APIs in before, after and exception handle stages.\nDo bootstrap class instrumentation. SkyWalking has packaged the bootstrap instrumentation in the agent core. It is easy to open by declaring it in the Instrumentation definition.\nOverride the public boolean isBootstrapInstrumentation() and return true. Such as\npublic class URLInstrumentation extends ClassEnhancePluginDefine { private static String CLASS_NAME = \u0026#34;java.net.URL\u0026#34;; @Override protected ClassMatch enhanceClass() { return byName(CLASS_NAME); } @Override public ConstructorInterceptPoint[] getConstructorsInterceptPoints() { return new ConstructorInterceptPoint[] { new ConstructorInterceptPoint() { @Override public ElementMatcher\u0026lt;MethodDescription\u0026gt; getConstructorMatcher() { return any(); } @Override public String getConstructorInterceptor() { return \u0026#34;org.apache.skywalking.apm.plugin.jre.httpurlconnection.Interceptor2\u0026#34;; } } }; } @Override public InstanceMethodsInterceptPoint[] getInstanceMethodsInterceptPoints() { return new InstanceMethodsInterceptPoint[0]; } @Override public StaticMethodsInterceptPoint[] getStaticMethodsInterceptPoints() { return new StaticMethodsInterceptPoint[0]; } @Override public boolean isBootstrapInstrumentation() { return true; } } NOTICE, doing bootstrap instrumentation should only happen in necessary, but mostly it effect the JRE core(rt.jar), and could make very unexpected result or side effect.\nProvide Customization Config for the Plugin The config could provide different behaviours based on the configurations. SkyWalking plugin mechanism provides the configuration injection and initialization system in the agent core.\nEvery plugin could declare one or more classes to represent the config by using @PluginConfig annotation. The agent core could initialize this class' static field though System environments, System properties, and agent.config static file.\nThe #root() method in the @PluginConfig annotation requires to declare the root class for the initialization process. Typically, SkyWalking prefers to use nested inner static classes for the hierarchy of the configuration. Recommend using Plugin/plugin-name/config-key as the nested classes structure of the Config class.\nNOTE, because of the Java ClassLoader mechanism, the @PluginConfig annotation should be added on the real class used in the interceptor codes.\nSuch as, in the following example, @PluginConfig(root = SpringMVCPluginConfig.class) represents the initialization should start with using SpringMVCPluginConfig as the root. Then the config key of the attribute USE_QUALIFIED_NAME_AS_ENDPOINT_NAME, should be plugin.springmvc.use_qualified_name_as_endpoint_name.\npublic class SpringMVCPluginConfig { public static class Plugin { // NOTE, if move this annotation on the `Plugin` or `SpringMVCPluginConfig` class, it no longer has any effect.  @PluginConfig(root = SpringMVCPluginConfig.class) public static class SpringMVC { /** * If true, the fully qualified method name will be used as the endpoint name instead of the request URL, * default is false. */ public static boolean USE_QUALIFIED_NAME_AS_ENDPOINT_NAME = false; /** * This config item controls that whether the SpringMVC plugin should collect the parameters of the * request. */ public static boolean COLLECT_HTTP_PARAMS = false; } @PluginConfig(root = SpringMVCPluginConfig.class) public static class Http { /** * When either {@link Plugin.SpringMVC#COLLECT_HTTP_PARAMS} is enabled, how many characters to keep and send * to the OAP backend, use negative values to keep and send the complete parameters, NB. this config item is * added for the sake of performance */ public static int HTTP_PARAMS_LENGTH_THRESHOLD = 1024; } } } Meter Plugin Java agent plugin could use meter APIs to collect the metrics for backend analysis.\n Counter API represents a single monotonically increasing counter, automatic collect data and report to backend.  import org.apache.skywalking.apm.agent.core.meter.MeterFactory; Counter counter = MeterFactory.counter(meterName).tag(\u0026#34;tagKey\u0026#34;, \u0026#34;tagValue\u0026#34;).mode(Counter.Mode.INCREMENT).build(); counter.increment(1d);  MeterFactory.counter Create a new counter builder with the meter name. Counter.Builder.tag(String key, String value) Mark a tag key/value pair. Counter.Builder.mode(Counter.Mode mode) Change the counter mode, RATE mode means reporting rate to the backend. Counter.Builder.build() Build a new Counter which is collected and reported to the backend. Counter.increment(double count) Increment count to the Counter, It could be a positive value.   Gauge API represents a single numerical value.  import org.apache.skywalking.apm.agent.core.meter.MeterFactory; ThreadPoolExecutor threadPool = ...; Gauge gauge = MeterFactory.gauge(meterName, () -\u0026gt; threadPool.getActiveCount()).tag(\u0026#34;tagKey\u0026#34;, \u0026#34;tagValue\u0026#34;).build();  MeterFactory.gauge(String name, Supplier\u0026lt;Double\u0026gt; getter) Create a new gauge builder with the meter name and supplier function, this function need to return a double value. Gauge.Builder.tag(String key, String value) Mark a tag key/value pair. Gauge.Builder.build() Build a new Gauge which is collected and reported to the backend.   Histogram API represents a summary sample observations with customize buckets.  import org.apache.skywalking.apm.agent.core.meter.MeterFactory; Histogram histogram = MeterFactory.histogram(\u0026#34;test\u0026#34;).tag(\u0026#34;tagKey\u0026#34;, \u0026#34;tagValue\u0026#34;).steps(Arrays.asList(1, 5, 10)).minValue(0).build(); histogram.addValue(3);  MeterFactory.histogram(String name) Create a new histogram builder with the meter name. Histogram.Builder.tag(String key, String value) Mark a tag key/value pair. Histogram.Builder.steps(List\u0026lt;Double\u0026gt; steps) Set up the max values of every histogram buckets. Histogram.Builder.minValue(double value) Set up the minimal value of this histogram, default is 0. Histogram.Builder.build() Build a new Histogram which is collected and reported to the backend. Histogram.addValue(double value) Add value into the histogram, automatically analyze what bucket count needs to be increment. rule: count into [step1, step2).  Plugin Test Tool Apache SkyWalking Agent Test Tool Suite a tremendously useful test tools suite in a wide variety of languages of Agent. Includes mock collector and validator. The mock collector is a SkyWalking receiver, like OAP server.\nYou could learn how to use this tool to test the plugin in this doc. If you want to contribute plugins to SkyWalking official repo, this is required.\nContribute plugins into Apache SkyWalking repository We are welcome everyone to contribute plugins.\nPlease follow there steps:\n Submit an issue about which plugins you are going to contribute, including supported version. Create sub modules under apm-sniffer/apm-sdk-plugin or apm-sniffer/optional-plugins, and the name should include supported library name and versions Follow this guide to develop. Make sure comments and test cases are provided. Develop and test. Provide the automatic test cases. Learn how to write the plugin test case from this doc Send the pull request and ask for review. The plugin committers approve your plugins, plugin CI-with-IT, e2e and plugin tests passed. The plugin accepted by SkyWalking.  ","excerpt":"Plugin Development Guide This document describe how to understand, develop and contribute plugin. …","ref":"/docs/main/v8.4.0/en/guides/java-plugin-development-guide/","title":"Plugin Development Guide"},{"body":"Probe Introduction In SkyWalking, probe means an agent or SDK library integrated into target system, which take charge of collecting telemetry data including tracing and metrics. Based on the target system tech stack, probe could use very different ways to do so. But ultimately they are same, just collect and reformat data, then send to backend.\nIn high level, there are three typical groups in all SkyWalking probes.\n  Language based native agent. This kind of agents runs in target service user space, like a part of user codes. Such as SkyWalking Java agent, use -javaagent command line argument to manipulate codes in runtime, manipulate means change and inject user\u0026rsquo;s codes. Another kind of agents is using some hook or intercept mechanism provided by target libraries. So you can see, these kinds of agents based on languages and libraries.\n  Service Mesh probe. Service Mesh probe collects data from sidecar, control panel in service mesh or proxy. In old days, proxy is only used as ingress of the whole cluster, but with the Service Mesh and sidecar, now we can do observe based on that.\n  3rd-party instrument library. SkyWalking accepts other popular used instrument libraries data format. It analysis the data, transfer it to SkyWalking formats of trace, metrics or both. This feature starts with accepting Zipkin span data. See Receiver for other tracers to know more.\n  You don\u0026rsquo;t need to use Language based native agent and Service Mesh probe at the same time, because they both collect metrics data. As a result of that, your system suffers twice payloads, and the analytic numbers are doubled.\nThere are several recommend ways in using these probes:\n Use Language based native agent only. Use 3rd-party instrument library only, like Zipkin instrument ecosystem. Use Service Mesh probe only. Use Service Mesh probe with Language based native agent or 3rd-party instrument library in tracing status. (Advanced usage)  In addition, let\u0026rsquo;s example what is the meaning of in tracing status?\nIn default, Language based native agent and 3rd-party instrument library both send distributed traces to backend, which do analysis and aggregate on those traces. In tracing status means, backend considers these traces as something like logs, just save them, and build the links between traces and metrics, like which endpoint and service does the trace belong?.\nWhat is next?  Learn the SkyWalking supported probes in Service auto instrument agent, Manual instrument SDK, Service Mesh probe and Zipkin receiver. After understand the probe, read backend overview for understanding analysis and persistence.  ","excerpt":"Probe Introduction In SkyWalking, probe means an agent or SDK library integrated into target system, …","ref":"/docs/main/v8.4.0/en/concepts-and-designs/probe-introduction/","title":"Probe Introduction"},{"body":"Problem when you start your application with skywalking agent,if you find this exception in your agent log which mean EnhanceRequireObjectCache can not be casted to EnhanceRequireObjectCache.eg:\nERROR 2018-05-07 21:31:24 InstMethodsInter : class[class org.springframework.web.method.HandlerMethod] after method[getBean] intercept failure java.lang.ClassCastException: org.apache.skywalking.apm.plugin.spring.mvc.commons.EnhanceRequireObjectCache cannot be cast to org.apache.skywalking.apm.plugin.spring.mvc.commons.EnhanceRequireObjectCache at org.apache.skywalking.apm.plugin.spring.mvc.commons.interceptor.GetBeanInterceptor.afterMethod(GetBeanInterceptor.java:45) at org.apache.skywalking.apm.agent.core.plugin.interceptor.enhance.InstMethodsInter.intercept(InstMethodsInter.java:105) at org.springframework.web.method.HandlerMethod.getBean(HandlerMethod.java) at org.springframework.web.servlet.handler.AbstractHandlerMethodExceptionResolver.shouldApplyTo(AbstractHandlerMethodExceptionResolver.java:47) at org.springframework.web.servlet.handler.AbstractHandlerExceptionResolver.resolveException(AbstractHandlerExceptionResolver.java:131) at org.springframework.web.servlet.handler.HandlerExceptionResolverComposite.resolveException(HandlerExceptionResolverComposite.java:76) ... Reason this exception may caused by some hot deployment tools(spring-boot-devtool) or some else which may change the classloader in runtime.\nResolve  Production environment does not appear this error because developer tools are automatically disabled,look spring-boot-devtools If you want to debug in your development environment normally,you should remove such hot deployment package in your lib path temporarily.  ","excerpt":"Problem when you start your application with skywalking agent,if you find this exception in your …","ref":"/docs/main/v8.4.0/en/faq/enhancerequireobjectcache-cast-exception/","title":"Problem"},{"body":"Problem  Import skywalking project to Eclipse,Occur following errors:   Software being installed: Checkstyle configuration plugin for M2Eclipse 1.0.0.201705301746 (com.basistech.m2e.code.quality.checkstyle.feature.feature.group 1.0.0.201705301746) Missing requirement: Checkstyle configuration plugin for M2Eclipse 1.0.0.201705301746 (com.basistech.m2e.code.quality.checkstyle.feature.feature.group 1.0.0.201705301746) requires \u0026lsquo;net.sf.eclipsecs.core 5.2.0\u0026rsquo; but it could not be found\n Reason Haven\u0026rsquo;t installed Eclipse Checkstyle Plug-in\nResolve Download the plugin through the link:https://sourceforge.net/projects/eclipse-cs/?source=typ_redirect，Eclipse Checkstyle Plug-in version:8.7.0.201801131309 plugin required. plugin notification: The Eclipse Checkstyle plug-in integrates the Checkstyle Java code auditor into the Eclipse IDE. The plug-in provides real-time feedback to the user about violations of rules that check for coding style and possible error prone code constructs.\n","excerpt":"Problem  Import skywalking project to Eclipse,Occur following errors:   Software being installed: …","ref":"/docs/main/v8.4.0/en/faq/import-project-eclipse-requireitems-exception/","title":"Problem"},{"body":"Problem The trace doesn\u0026rsquo;t continue in kafka consumer side.\nReason The kafka client is responsible for pulling messages from the brokers, and after that the data will be processed by the user-defined codes. However, only the poll action can be traced by the pluign and the subsequent data processing work is inevitably outside the scope of the trace context. Thus, in order to complete the client-side trace, manual instrument has to be done, i.e. the poll action and the processing action should be wrapped manually.\nResolve With native kafka client, please use Application Toolkit libraries to do the manual instrumentation, with the help of @KafkaPollAndInvoke annotation in apm-toolkit-kafka or with OpenTracing API. And if you\u0026rsquo;re using spring-kafka 1.3.x, 2.2.x or above, you can track the Consumer side without effort.\n","excerpt":"Problem The trace doesn\u0026rsquo;t continue in kafka consumer side.\nReason The kafka client is …","ref":"/docs/main/v8.4.0/en/faq/kafka-plugin/","title":"Problem"},{"body":"Problem When using a thread pool, TraceSegment data in a thread cannot be reported and there are memory data that cannot be recycled (memory leaks)\nExample ThreadPoolTaskExecutor executor = new ThreadPoolTaskExecutor(); executor.setThreadFactory(r -\u0026gt; new Thread(RunnableWrapper.of(r))); Reason  Worker threads are enhanced, when using thread pool. According to the SkyWalking Java Agent design, when you want to trace cross thread, you need to enhance the task thread.  Resolve   When using Thread Schedule Framework Checked SkyWalking Thread Schedule Framework at SkyWalking Java agent supported list, such as Spring FrameWork @Async, which can implement tracing without any modification.\n  When using Custom Thread Pool Enhance the task thread with the following usage.\n  ExecutorService executorService = Executors.newFixedThreadPool(1); executorService.execute(RunnableWrapper.of(new Runnable() { @Override public void run() { //your code  } })); See across thread solution APIs for more usage\n","excerpt":"Problem When using a thread pool, TraceSegment data in a thread cannot be reported and there are …","ref":"/docs/main/v8.4.0/en/faq/memory-leak-enhance-worker-thread/","title":"Problem"},{"body":"Problem  In maven build, the protoc-plugin occurs error:  [ERROR] Failed to execute goal org.xolstice.maven.plugins:protobuf-maven-plugin:0.5.0:compile-custom (default) on project apm-network: Unable to copy the file to \\skywalking\\apm-network\\target\\protoc-plugins: \\skywalking\\apm-network\\target\\protoc-plugins\\protoc-3.3.0-linux-x86_64.exe (The process cannot access the file because it is being used by another process) -\u0026gt; [Help 1] Reason  Protobuf compiler is dependent on the glibc, but it is not-installed or installed old version in the system.  Resolve  Install or upgrade to the latest version of the glibc library. In container env, recommend using the latest glibc version of the alpine system. Please refer to http://www.gnu.org/software/libc/documentation.html  ","excerpt":"Problem  In maven build, the protoc-plugin occurs error:  [ERROR] Failed to execute goal …","ref":"/docs/main/v8.4.0/en/faq/protoc-plugin-fails-when-build/","title":"Problem"},{"body":"Problem The message with Field ID, 8888, must be revered.\nReason Because Thrift cannot carry metadata to transport Trace Header in the original API, we transport those by wrapping TProtocolFactory to do that.\nThrift allows us to append any additional field in the Message even if the receiver doesn\u0026rsquo;t deal with them. This data is going to be skipped while no one reads. Base on this, we take the 8888th field of Message to store Trace Header(or metadata) and to transport. That means the message with Field ID, 8888, must be revered.\nResolve Avoiding to use the Field(ID is 8888) in your application.\n","excerpt":"Problem The message with Field ID, 8888, must be revered.\nReason Because Thrift cannot carry …","ref":"/docs/main/v8.4.0/en/faq/thrift-plugin/","title":"Problem"},{"body":"Problem  There is no abnormal log in Agent log and Collector log, The traces show, but no other info in UI.  Reason The operating system where the monitored system is located is not set as the current time zone, causing statistics collection time points to deviate.\nResolve Make sure the time is sync in collector servers and monitored application servers.\n","excerpt":"Problem  There is no abnormal log in Agent log and Collector log, The traces show, but no other info …","ref":"/docs/main/v8.4.0/en/faq/why-have-traces-no-others/","title":"Problem"},{"body":"Problem： Maven compilation failure with error like Error: not found: python2 When you compile the project via maven, it failed at module apm-webapp and the following error occured.\nPay attention to key words such as node-sass and Error: not found: python2.\n[INFO] \u0026gt; node-sass@4.11.0 postinstall C:\\XXX\\skywalking\\skywalking-ui\\node_modules\\node-sass [INFO] \u0026gt; node scripts/build.js [ERROR] gyp verb check python checking for Python executable \u0026quot;python2\u0026quot; in the PATH [ERROR] gyp verb `which` failed Error: not found: python2 [ERROR] gyp verb `which` failed at getNotFoundError (C:\\XXX\\skywalking\\skywalking-ui\\node_modules\\which\\which.js:13:12) [ERROR] gyp verb `which` failed at F (C:\\XXX\\skywalking\\skywalking-ui\\node_modules\\which\\which.js:68:19) [ERROR] gyp verb `which` failed at E (C:\\XXX\\skywalking\\skywalking-ui\\node_modules\\which\\which.js:80:29) [ERROR] gyp verb `which` failed at C:\\XXX\\skywalking\\skywalking-ui\\node_modules\\which\\which.js:89:16 [ERROR] gyp verb `which` failed at C:\\XXX\\skywalking\\skywalking-ui\\node_modules\\isexe\\index.js:42:5 [ERROR] gyp verb `which` failed at C:\\XXX\\skywalking\\skywalking-ui\\node_modules\\isexe\\windows.js:36:5 [ERROR] gyp verb `which` failed at FSReqWrap.oncomplete (fs.js:152:21) [ERROR] gyp verb `which` failed code: 'ENOENT' } [ERROR] gyp verb check python checking for Python executable \u0026quot;python\u0026quot; in the PATH [ERROR] gyp verb `which` succeeded python C:\\Users\\XXX\\AppData\\Local\\Programs\\Python\\Python37\\python.EXE [ERROR] gyp ERR! configure error [ERROR] gyp ERR! stack Error: Command failed: C:\\Users\\XXX\\AppData\\Local\\Programs\\Python\\Python37\\python.EXE -c import sys; print \u0026quot;%s.%s.%s\u0026quot; % sys.version_info[:3]; [ERROR] gyp ERR! stack File \u0026quot;\u0026lt;string\u0026gt;\u0026quot;, line 1 [ERROR] gyp ERR! stack import sys; print \u0026quot;%s.%s.%s\u0026quot; % sys.version_info[:3]; [ERROR] gyp ERR! stack ^ [ERROR] gyp ERR! stack SyntaxError: invalid syntax [ERROR] gyp ERR! stack [ERROR] gyp ERR! stack at ChildProcess.exithandler (child_process.js:275:12) [ERROR] gyp ERR! stack at emitTwo (events.js:126:13) [ERROR] gyp ERR! stack at ChildProcess.emit (events.js:214:7) [ERROR] gyp ERR! stack at maybeClose (internal/child_process.js:925:16) [ERROR] gyp ERR! stack at Process.ChildProcess._handle.onexit (internal/child_process.js:209:5) [ERROR] gyp ERR! System Windows_NT 10.0.17134 ...... [INFO] server-starter-es7 ................................. SUCCESS [ 11.657 s] [INFO] apm-webapp ......................................... FAILURE [ 25.857 s] [INFO] apache-skywalking-apm .............................. SKIPPED [INFO] apache-skywalking-apm-es7 .......................... SKIPPED Reason It has nothing to do with SkyWalking.\nAccording to https://github.com/sass/node-sass/issues/1176, if you live in countries where requesting resources from GitHub and npmjs.org is very slowly, some precompiled binaries for dependency node-sass will fail to be downloaded during npm install, then npm will try to compile them itself. That\u0026rsquo;s why python2 is needed.\nResolve 1. Use mirror. Such as in China, please edit skywalking\\apm-webapp\\pom.xml Find\n\u0026lt;configuration\u0026gt; \u0026lt;arguments\u0026gt;install --registry=https://registry.npmjs.org/\u0026lt;/arguments\u0026gt; \u0026lt;/configuration\u0026gt; Replace it with\n\u0026lt;configuration\u0026gt; \u0026lt;arguments\u0026gt;install --registry=https://registry.npm.taobao.org/ --sass_binary_site=https://npm.taobao.org/mirrors/node-sass/\u0026lt;/arguments\u0026gt; \u0026lt;/configuration\u0026gt; 2. Get an enough powerful VPN ","excerpt":"Problem： Maven compilation failure with error like Error: not found: python2 When you compile the …","ref":"/docs/main/v8.4.0/en/faq/maven-compile-npm-failure/","title":"Problem： Maven compilation failure with error like `Error： not found： python2`"},{"body":"Protocols There are two types of protocols list here.\n  Probe Protocol. Include the descriptions and definitions about how agent send collected metrics data and traces, also the formats of each entities.\n  Query Protocol. The backend provide query capability to SkyWalking own UI and others. These queries are based on GraphQL.\n  Probe Protocols They also related to the probe group, for understand that, look Concepts and Designs document. These groups are Language based native agent protocol, Service Mesh protocol and 3rd-party instrument protocol.\nLanguage based native agent protocol There are two types of protocols to make language agents work in distributed environments.\n Cross Process Propagation Headers Protocol and Cross Process Correlation Headers Protocol are in wire data format, agent/SDK usually uses HTTP/MQ/HTTP2 headers to carry the data with rpc request. The remote agent will receive this in the request handler, and bind the context with this specific request. Trace Data Protocol is out of wire data, agent/SDK uses this to send traces and metrics to skywalking or other compatible backend.  Cross Process Propagation Headers Protocol v3 is the new protocol for in-wire context propagation, started in 8.0.0 release.\nCross Process Correlation Headers Protocol v1 is a new in-wire context propagation additional and optional protocols. Please read SkyWalking language agents documentations to see whether it is supported. This protocol defines the data format of transporting custom data with Cross Process Propagation Headers Protocol. SkyWalking javaagent begins to support this since 8.0.0.\nSkyWalking Trace Data Protocol v3 defines the communication way and format between agent and backend.\nSkyWalking Log Data Protocol defines the communication way and format between agent and backend.\nBrowser probe protocol The browser probe, such as skywalking-client-js could use this protocol to send to backend. This service provided by gRPC.\nSkyWalking Browser Protocol define the communication way and format between skywalking-client-js and backend.\nService Mesh probe protocol The probe in sidecar or proxy could use this protocol to send data to backendEnd. This service provided by gRPC, requires the following key info:\n Service Name or ID at both sides. Service Instance Name or ID at both sides. Endpoint. URI in HTTP, service method full signature in gRPC. Latency. In milliseconds. Response code in HTTP Status. Success or fail. Protocol. HTTP, gRPC DetectPoint. In Service Mesh sidecar, client or server. In normal L7 proxy, value is proxy.  3rd-party instrument protocol 3rd-party instrument protocols are not defined by SkyWalking. They are just protocols/formats, which SkyWalking is compatible and could receive from their existed libraries. SkyWalking starts with supporting Zipkin v1, v2 data formats.\nBackend is based on modularization principle, so very easy to extend a new receiver to support new protocol/format.\nQuery Protocol Query protocol follows GraphQL grammar, provides data query capabilities, which depends on your analysis metrics. Read query protocol doc for more details.\n","excerpt":"Protocols There are two types of protocols list here.\n  Probe Protocol. Include the descriptions and …","ref":"/docs/main/v8.4.0/en/protocols/readme/","title":"Protocols"},{"body":"Query Protocol Query Protocol defines a set of APIs in GraphQL grammar to provide data query and interactive capabilities with SkyWalking native visualization tool or 3rd party system, including Web UI, CLI or private system.\nQuery protocol official repository, https://github.com/apache/skywalking-query-protocol.\nMetadata Metadata includes the brief info of the whole under monitoring services and their instances, endpoints, etc. Use multiple ways to query this meta data.\nextend type Query { getGlobalBrief(duration: Duration!): ClusterBrief # Normal service related metainfo  getAllServices(duration: Duration!): [Service!]! searchServices(duration: Duration!, keyword: String!): [Service!]! searchService(serviceCode: String!): Service # Fetch all services of Browser type getAllBrowserServices(duration: Duration!): [Service!]! # Service intance query getServiceInstances(duration: Duration!, serviceId: ID!): [ServiceInstance!]! # Endpoint query # Consider there are huge numbers of endpoint, # must use endpoint owner\u0026#39;s service id, keyword and limit filter to do query. searchEndpoint(keyword: String!, serviceId: ID!, limit: Int!): [Endpoint!]! getEndpointInfo(endpointId: ID!): EndpointInfo # Database related meta info. getAllDatabases(duration: Duration!): [Database!]! getTimeInfo: TimeInfo } Topology Show the topology and dependency graph of services or endpoints. Including direct relationship or global map.\nextend type Query { # Query the global topology getGlobalTopology(duration: Duration!): Topology # Query the topology, based on the given service getServiceTopology(serviceId: ID!, duration: Duration!): Topology # Query the instance topology, based on the given clientServiceId and serverServiceId getServiceInstanceTopology(clientServiceId: ID!, serverServiceId: ID!, duration: Duration!): ServiceInstanceTopology # Query the topology, based on the given endpoint getEndpointTopology(endpointId: ID!, duration: Duration!): Topology } Metrics Metrics query targets all the objects defined in OAL script. You could get the metrics data in linear or thermodynamic matrix formats based on the aggregation functions in script.\n3 types of metrics could be query\n Single value. The type of most default metrics is single value, consider this as default. getValues and getLinearIntValues are suitable for this. Multiple value. One metrics defined in OAL include multiple value calculations. Use getMultipleLinearIntValues to get all values. percentile is a typical multiple value func in OAL. Heatmap value. Read Heatmap in WIKI for detail. thermodynamic is the only OAL func. Use getThermodynamic to get the values.  extend type Query { getValues(metric: BatchMetricConditions!, duration: Duration!): IntValues getLinearIntValues(metric: MetricCondition!, duration: Duration!): IntValues # Query the type of metrics including multiple values, and format them as multiple linears. # The seq of these multiple lines base on the calculation func in OAL # Such as, should us this to query the result of func percentile(50,75,90,95,99) in OAL, # then five lines will be responsed, p50 is the first element of return value. getMultipleLinearIntValues(metric: MetricCondition!, numOfLinear: Int!, duration: Duration!): [IntValues!]! getThermodynamic(metric: MetricCondition!, duration: Duration!): Thermodynamic } Metrics are defined in the config/oal/*.oal files.\nAggregation Aggregation query means the metrics data need a secondary aggregation in query stage, which makes the query interfaces have some different arguments. Such as, TopN list of services is a very typical aggregation query, metrics stream aggregation just calculates the metrics values of each service, but the expected list needs ordering metrics data by the values.\nAggregation query is for single value metrics only.\n# The aggregation query is different with the metric query. # All aggregation queries require backend or/and storage do aggregation in query time. extend type Query { # TopN is an aggregation query. getServiceTopN(name: String!, topN: Int!, duration: Duration!, order: Order!): [TopNEntity!]! getAllServiceInstanceTopN(name: String!, topN: Int!, duration: Duration!, order: Order!): [TopNEntity!]! getServiceInstanceTopN(serviceId: ID!, name: String!, topN: Int!, duration: Duration!, order: Order!): [TopNEntity!]! getAllEndpointTopN(name: String!, topN: Int!, duration: Duration!, order: Order!): [TopNEntity!]! getEndpointTopN(serviceId: ID!, name: String!, topN: Int!, duration: Duration!, order: Order!): [TopNEntity!]! } Others The following query(s) are for specific features, including trace, alarm or profile.\n Trace. Query distributed traces by this. Alarm. Through alarm query, you can have alarm trend and details.  The actual query GraphQL scrips could be found inside query-protocol folder in here.\nCondition Duration Duration is a widely used parameter type as the APM data is time related. The explanations are as following. Step is related the precision.\n# The Duration defines the start and end time for each query operation. # Fields: `start` and `end` # represents the time span. And each of them matches the step. # ref https://www.ietf.org/rfc/rfc3339.txt # The time formats are # `SECOND` step: yyyy-MM-dd HHmmss # `MINUTE` step: yyyy-MM-dd HHmm # `HOUR` step: yyyy-MM-dd HH # `DAY` step: yyyy-MM-dd # `MONTH` step: yyyy-MM # Field: `step` # represents the accurate time point. # e.g. # if step==HOUR , start=2017-11-08 09, end=2017-11-08 19 # then # metrics from the following time points expected # 2017-11-08 9:00 -\u0026gt; 2017-11-08 19:00 # there are 11 time points (hours) in the time span. input Duration { start: String! end: String! step: Step! } enum Step { MONTH DAY HOUR MINUTE SECOND } ","excerpt":"Query Protocol Query Protocol defines a set of APIs in GraphQL grammar to provide data query and …","ref":"/docs/main/v8.4.0/en/protocols/query-protocol/","title":"Query Protocol"},{"body":"Scopes and Fields By using Aggregation Function, the requests will group by time and Group Key(s) in each scope.\nSCOPE All    Name Remarks Group Key Type     name Represent the service name of each request.  string   serviceInstanceName Represent the name of the service instance id referred.  string   endpoint Represent the endpoint path of each request.  string   latency Represent how much time of each request.  int(in ms)   status Represent whether success or fail of the request.  bool(true for success)   responseCode Represent the response code of HTTP response, if this request is the HTTP call. e.g. 200, 404, 302  int   type Represent the type of each request. Such as: Database, HTTP, RPC, gRPC.  enum   tags Represent the labels of each request and each value is made up with the TagKey:TagValue in the segment.  List\u0026lt;String\u0026gt;    SCOPE Service Calculate the metrics data from each request of the service.\n   Name Remarks Group Key Type     name Represent the name of the service  string   nodeType Represent which kind of node of Service or Network address represents to, Such as: Normal, Database, MQ, Cache.  enum   serviceInstanceName Represent the name of the service instance id referred  string   endpointName Represent the name of the endpoint, such a full path of HTTP URI  string   latency Represent how much time of each request.  int   status Represent whether success or fail of the request.  bool(true for success)   responseCode Represent the response code of HTTP response, if this request is the HTTP call  int   type Represent the type of each request. Such as: Database, HTTP, RPC, gRPC.  enum   tags Represent the labels of each request and each value is made up with the TagKey:TagValue in the segment.  List\u0026lt;String\u0026gt;   sideCar.internalErrorCode Represent the sidecar/gateway proxy internal error code, the value bases on the implementation.  string    SCOPE ServiceInstance Calculate the metrics data from each request of the service instance.\n   Name Remarks Group Key Type     name Represent the name of the service instance. Such as ip:port@Service Name. Notice: current native agent uses uuid@ipv4 as instance name, which is useless when you want to setup a filter in aggregation.  string   serviceName Represent the name of the service.  string   nodeType Represent which kind of node of Service or Network address represents to, Such as: Normal, Database, MQ, Cache.  enum   endpointName Represent the name of the endpoint, such a full path of HTTP URI.  string   latency Represent how much time of each request.  int   status Represent whether success or fail of the request.  bool(true for success)   responseCode Represent the response code of HTTP response, if this request is the HTTP call.  int   type Represent the type of each request. Such as: Database, HTTP, RPC, gRPC.  enum   tags Represent the labels of each request and each value is made up with the TagKey:TagValue in the segment.  List\u0026lt;String\u0026gt;   sideCar.internalErrorCode Represent the sidecar/gateway proxy internal error code, the value bases on the implementation.  string    Secondary scopes of ServiceInstance Calculate the metrics data if the service instance is a JVM and collected by javaagent.\n SCOPE ServiceInstanceJVMCPU     Name Remarks Group Key Type     name Represent the name of the service instance. Such as ip:port@Service Name. Notice: current native agent uses uuid@ipv4 as instance name, which is useless when you want to setup a filter in aggregation.  string   serviceName Represent the name of the service.  string   usePercent Represent how much percent of cpu time cost  double    SCOPE ServiceInstanceJVMMemory     Name Remarks Group Key Type     name Represent the name of the service instance. Such as ip:port@Service Name. Notice: current native agent uses uuid@ipv4 as instance name, which is useless when you want to setup a filter in aggregation.  string   serviceName Represent the name of the service.  string   heapStatus Represent this value the memory metrics values are heap or not  bool   init See JVM document  long   max See JVM document  long   used See JVM document  long   committed See JVM document  long    SCOPE ServiceInstanceJVMMemoryPool     Name Remarks Group Key Type     name Represent the name of the service instance. Such as ip:port@Service Name. Notice: current native agent uses uuid@ipv4 as instance name, which is useless when you want to setup a filter in aggregation.  string   serviceName Represent the name of the service.  string   poolType Include CODE_CACHE_USAGE, NEWGEN_USAGE, OLDGEN_USAGE, SURVIVOR_USAGE, PERMGEN_USAGE, METASPACE_USAGE based on different version of JVM.  enum   init See JVM document  long   max See JVM document  long   used See JVM document  long   committed See JVM document  long    SCOPE ServiceInstanceJVMGC     Name Remarks Group Key Type     name Represent the name of the service instance. Such as ip:port@Service Name. Notice: current native agent uses uuid@ipv4 as instance name, which is useless when you want to setup a filter in aggregation.  string   serviceName Represent the name of the service.  string   phrase Include NEW and OLD  Enum   time GC time cost  long   count Count of GC op  long    SCOPE ServiceInstanceJVMThread     Name Remarks Group Key Type     name Represent the name of the service instance. Such as ip:port@Service Name. Notice: current native agent uses uuid@ipv4 as instance name, which is useless when you want to setup a filter in aggregation.  string   serviceName Represent the name of the service.  string   liveCount Represent Current number of live threads  int   daemonCount Represent Current number of daemon threads  int   peakCount Represent Current number of peak threads  int    SCOPE Endpoint Calculate the metrics data from each request of the endpoint in the service.\n   Name Remarks Group Key Type     name Represent the name of the endpoint, such a full path of HTTP URI.  string   serviceName Represent the name of the service.  string   serviceNodeType Represent which kind of node of Service or Network address represents to, Such as: Normal, Database, MQ, Cache.  enum   serviceInstanceName Represent the name of the service instance id referred.  string   latency Represent how much time of each request.  int   status Represent whether success or fail of the request.  bool(true for success)   responseCode Represent the response code of HTTP response, if this request is the HTTP call.  int   type Represent the type of each request. Such as: Database, HTTP, RPC, gRPC.  enum   tags Represent the labels of each request and each value is made up with the TagKey:TagValue in the segment.  List\u0026lt;String\u0026gt;   sideCar.internalErrorCode Represent the sidecar/gateway proxy internal error code, the value bases on the implementation.  string    SCOPE ServiceRelation Calculate the metrics data from each request between one service and the other service\n   Name Remarks Group Key Type     sourceServiceName Represent the name of the source service.  string   sourceServiceNodeType Represent which kind of node of Service or Network address represents to, Such as: Normal, Database, MQ, Cache.  enum   sourceServiceInstanceName Represent the name of the source service instance.  string   destServiceName Represent the name of the destination service.  string   destServiceNodeType Represent which kind of node of Service or Network address represents to.  enum   destServiceInstanceName Represent the name of the destination service instance.  string   endpoint Represent the endpoint used in this call.  string   componentId Represent the id of component used in this call. yes string   latency Represent how much time of each request.  int   status Represent whether success or fail of the request.  bool(true for success)   responseCode Represent the response code of HTTP response, if this request is the HTTP call.  int   type Represent the type of each request. Such as: Database, HTTP, RPC, gRPC.  enum   detectPoint Represent where is the relation detected. Values: client, server, proxy. yes enum   tlsMode Represent TLS mode between source and destination services. For example service_relation_mtls_cpm = from(ServiceRelation.*).filter(tlsMode == \u0026quot;mTLS\u0026quot;).cpm()  string   sideCar.internalErrorCode Represent the sidecar/gateway proxy internal error code, the value bases on the implementation.  string    SCOPE ServiceInstanceRelation Calculate the metrics data from each request between one service instance and the other service instance\n   Name Remarks Group Key Type     sourceServiceName Represent the name of the source service.  string   sourceServiceNodeType Represent which kind of node of Service or Network address represents to, Such as: Normal, Database, MQ, Cache.  enum   sourceServiceInstanceName Represent the name of the source service instance.  string   destServiceName Represent the name of the destination service.     destServiceNodeType Represent which kind of node of Service or Network address represents to, Such as: Normal, Database, MQ, Cache.  string   destServiceInstanceName Represent the name of the destination service instance.  string   endpoint Represent the endpoint used in this call.  string   componentId Represent the id of component used in this call. yes string   latency Represent how much time of each request.  int   status Represent whether success or fail of the request.  bool(true for success)   responseCode Represent the response code of HTTP response, if this request is the HTTP call.  int   type Represent the type of each request. Such as: Database, HTTP, RPC, gRPC.  enum   detectPoint Represent where is the relation detected. Values: client, server, proxy. yes enum   tlsMode Represent TLS mode between source and destination service instances. For example, service_instance_relation_mtls_cpm = from(ServiceInstanceRelation.*).filter(tlsMode == \u0026quot;mTLS\u0026quot;).cpm()  string   sideCar.internalErrorCode Represent the sidecar/gateway proxy internal error code, the value bases on the implementation.  string    SCOPE EndpointRelation Calculate the metrics data of the dependency between one endpoint and the other endpoint. This relation is hard to detect, also depends on tracing lib to propagate the prev endpoint. So EndpointRelation scope aggregation effects only in service under tracing by SkyWalking native agents, including auto instrument agents(like Java, .NET), OpenCensus SkyWalking exporter implementation or others propagate tracing context in SkyWalking spec.\n   Name Remarks Group Key Type     endpoint Represent the endpoint as parent in the dependency.  string   serviceName Represent the name of the service.  string   serviceNodeType Represent which kind of node of Service or Network address represents to, Such as: Normal, Database, MQ, Cache.  enum   childEndpoint Represent the endpoint being used by the parent endpoint in row(1)  string   childServiceName Represent the endpoint being used by the parent service in row(1)  string   childServiceNodeType Represent which kind of node of Service or Network address represents to, Such as: Normal, Database, MQ, Cache.  string   childServiceInstanceName Represent the endpoint being used by the parent service instance in row(1)  string   rpcLatency Represent the latency of the RPC from some codes in the endpoint to the childEndpoint. Exclude the latency caused by the endpoint(1) itself.     componentId Represent the id of component used in this call. yes string   status Represent whether success or fail of the request.  bool(true for success)   responseCode Represent the response code of HTTP response, if this request is the HTTP call.  int   type Represent the type of each request. Such as: Database, HTTP, RPC, gRPC.  enum   detectPoint Represent where is the relation detected. Values: client, server, proxy. yes enum    SCOPE BrowserAppTraffic Calculate the metrics data form each request of the browser app (only browser).\n   Name Remarks Group Key Type     name Represent the browser app name of each request.  string   count Represents the number of request, fixed at 1.  int   trafficCategory Represents traffic category, Values: NORMAL, FIRST_ERROR, ERROR  enum   errorCategory Represents error category, Values: AJAX, RESOURCE, VUE, PROMISE, UNKNOWN  enum    SCOPE BrowserAppSingleVersionTraffic Calculate the metrics data form each request of the browser single version in the browser app (only browser).\n   Name Remarks Group Key Type     name Represent the single version name of each request.  string   serviceName Represent the name of the browser app.  string   count Represents the number of request, fixed at 1.  int   trafficCategory Represents traffic category, Values: NORMAL, FIRST_ERROR, ERROR  enum   errorCategory Represents error category, Values: AJAX, RESOURCE, VUE, PROMISE, UNKNOWN  enum    SCOPE BrowserAppPageTraffic Calculate the metrics data form each request of the page in the browser app (only browser).\n   Name Remarks Group Key Type     name Represent the page name of each request.  string   serviceName Represent the name of the browser app.  string   count Represents the number of request, fixed at 1.  int   trafficCategory Represents the traffic category, Values: NORMAL, FIRST_ERROR, ERROR  enum   errorCategory Represents the error category, Values: AJAX, RESOURCE, VUE, PROMISE, UNKNOWN  enum    SCOPE BrowserAppPagePerf Calculate the metrics data form each request of the page in the browser app (only browser).\n   Name Remarks Group Key Type     name Represent the page name of each request.  string   serviceName Represent the name of the browser app.  string   redirectTime Represents the time of redirection.  int(in ms)   dnsTime Represents the DNS query time.  int(in ms)   ttfbTime Time to first Byte.  int(in ms)   tcpTime TCP connection time.  int(in ms)   transTime Content transfer time.  int(in ms)   domAnalysisTime Dom parsing time.  int(in ms)   fptTime First paint time or blank screen time.  int(in ms)   domReadyTime Dom ready time.  int(in ms)   loadPageTime Page full load time.  int(in ms)   resTime Synchronous load resources in the page.  int(in ms)   sslTime Only valid for HTTPS.  int(in ms)   ttlTime Time to interact.  int(in ms)   firstPackTime First pack time.  int(in ms)   fmpTime First Meaningful Paint.  int(in ms)    ","excerpt":"Scopes and Fields By using Aggregation Function, the requests will group by time and Group Key(s) in …","ref":"/docs/main/v8.4.0/en/concepts-and-designs/scope-definitions/","title":"Scopes and Fields"},{"body":"Send Envoy metrics to SkyWalking with / without Istio Envoy defines a gRPC service to emit the metrics, whatever implements this protocol can be used to receive the metrics. SkyWalking has a built-in receiver that implements this protocol so that you can configure Envoy to emit its metrics to SkyWalking.\nAs an APM system, SkyWalking does not only receive and store the metrics emitted by Envoy, it also analyzes the topology of services and service instances.\nAttention: There are two versions of Envoy metrics service protocol up to date, v2 and v3, SkyWalking (8.3.0+) supports both of them.\nConfigure Envoy to send metrics to SkyWalking without Istio Envoy can be used with / without Istio\u0026rsquo;s control. This section introduces how to configure the standalone Envoy to send the metrics to SkyWalking.\nIn order to let Envoy send metrics to SkyWalking, we need to feed Envoy with a configuration which contains stats_sinks that includes envoy.metrics_service. This envoy.metrics_service should be configured as a config.grpc_service entry.\nThe interesting parts of the config is shown in the config below:\nstats_sinks: - name: envoy.metrics_service config: grpc_service: # Note: we can use google_grpc implementation as well. envoy_grpc: cluster_name: service_skywalking static_resources: ... clusters: - name: service_skywalking connect_timeout: 5s type: LOGICAL_DNS http2_protocol_options: {} dns_lookup_family: V4_ONLY lb_policy: ROUND_ROBIN load_assignment: cluster_name: service_skywalking endpoints: - lb_endpoints: - endpoint: address: socket_address: address: skywalking # This is the port where SkyWalking serving the Envoy Metrics Service gRPC stream. port_value: 11800 A more complete static configuration, can be observed here.\nNote that Envoy can also be configured dynamically through xDS Protocol.\nAs mentioned above, SkyWalking also builds the topology of services from the metrics, this is because Envoy also carries the service metadata along with the metrics, to feed the Envoy such metadata, another configuration part is as follows:\nnode: # ... other configs metadata: LABELS: app: test-app NAME: service-instance-name Configure Envoy to send metrics to SkyWalking with Istio Typically, Envoy can be also used under Istio\u0026rsquo;s control, where the configurations are much more simple because Istio composes the configurations for you and sends them to Envoy via xDS Protocol. Istio also automatically injects the metadata such as service name and instance name into the bootstrap configurations.\nUnder this circumstance, emitting the metrics to SyWalking is as simple as adding the option --set meshConfig.defaultConfig.envoyMetricsService.address=\u0026lt;skywalking.address.port.11800\u0026gt; to Istio install command, for example:\nistioctl install -y \\  --set profile=demo `# replace the profile as per your need` \\ --set meshConfig.defaultConfig.envoyMetricsService.address=\u0026lt;skywalking.address.port.11800\u0026gt; # replace \u0026lt;skywalking.address.port.11800\u0026gt; with your actual SkyWalking OAP address If you already have Istio installed, you can use the following command to apply the config without re-installing Istio:\nistioctl manifest install -y \\  --set profile=demo `# replace the profile as per your need` \\ --set meshConfig.defaultConfig.envoyMetricsService.address=\u0026lt;skywalking.address.port.11800\u0026gt; # replace \u0026lt;skywalking.address.port.11800\u0026gt; with your actual SkyWalking OAP address Metrics data Some Envoy statistics are listed in this list. A sample data that contains identifier can be found here, while the metrics only can be observed here.\n","excerpt":"Send Envoy metrics to SkyWalking with / without Istio Envoy defines a gRPC service to emit the …","ref":"/docs/main/v8.4.0/en/setup/envoy/metrics_service_setting/","title":"Send Envoy metrics to SkyWalking with / without Istio"},{"body":"Sending Envoy Metrics to SkyWalking OAP Server Example This is an example of sending Envoy Stats to SkyWalking OAP server through Metric Service.\nRunning the example The example requires docker and docker-compose to be installed in your local. It fetches images from Docker Hub.\nNote that in ths setup, we override the log4j2.xml config to set the org.apache.skywalking.oap.server.receiver.envoy logger level to DEBUG. This enables us to see the messages sent by Envoy to SkyWalking OAP server.\n$ make up $ docker-compose logs -f skywalking $ # Please wait for a moment until SkyWalking is ready and Envoy starts sending the stats. You will see similar messages like the following: skywalking_1 | 2019-08-31 23:57:40,672 - org.apache.skywalking.oap.server.receiver.envoy.MetricServiceGRPCHandler -26870 [grpc-default-executor-0] DEBUG [] - Received msg identifier { skywalking_1 | node { skywalking_1 | id: \u0026quot;ingress\u0026quot; skywalking_1 | cluster: \u0026quot;envoy-proxy\u0026quot; skywalking_1 | metadata { skywalking_1 | fields { skywalking_1 | key: \u0026quot;skywalking\u0026quot; skywalking_1 | value { skywalking_1 | string_value: \u0026quot;iscool\u0026quot; skywalking_1 | } skywalking_1 | } skywalking_1 | fields { skywalking_1 | key: \u0026quot;envoy\u0026quot; skywalking_1 | value { skywalking_1 | string_value: \u0026quot;isawesome\u0026quot; skywalking_1 | } skywalking_1 | } skywalking_1 | } skywalking_1 | locality { skywalking_1 | region: \u0026quot;ap-southeast-1\u0026quot; skywalking_1 | zone: \u0026quot;zone1\u0026quot; skywalking_1 | sub_zone: \u0026quot;subzone1\u0026quot; skywalking_1 | } skywalking_1 | build_version: \u0026quot;e349fb6139e4b7a59a9a359be0ea45dd61e589c5/1.11.1/Clean/RELEASE/BoringSSL\u0026quot; skywalking_1 | } skywalking_1 | } skywalking_1 | envoy_metrics { skywalking_1 | name: \u0026quot;cluster.service_skywalking.update_success\u0026quot; skywalking_1 | type: COUNTER skywalking_1 | metric { skywalking_1 | counter { skywalking_1 | value: 2.0 skywalking_1 | } skywalking_1 | timestamp_ms: 1567295859556 skywalking_1 | } skywalking_1 | } ... $ # To tear down: $ make down ","excerpt":"Sending Envoy Metrics to SkyWalking OAP Server Example This is an example of sending Envoy Stats to …","ref":"/docs/main/v8.4.0/en/setup/envoy/examples/metrics/readme/","title":"Sending Envoy Metrics to SkyWalking OAP Server Example"},{"body":"Service Auto Grouping SkyWalking supports various default and customized dashboard templates. Each template provides the reasonable layout for the services in the particular field. Such as, services with a language agent installed could have different metrics with service detected by the service mesh observability solution, and different with SkyWalking\u0026rsquo;s self-observability metrics dashboard.\nTherefore, since 8.3.0, SkyWalking OAP would generate the group based on this simple naming format.\n${service name} = [${group name}::]${logic name} Once the service name includes double colons(::), the literal string before the colons would be considered as the group name. In the latest GraphQL query, the group name has been provided as an option parameter.\n getAllServices(duration: Duration!, group: String): [Service!]!\n RocketBot UI dashboards(Standard type) support the group name for default and custom configurations.\n","excerpt":"Service Auto Grouping SkyWalking supports various default and customized dashboard templates. Each …","ref":"/docs/main/v8.4.0/en/setup/backend/service-auto-grouping/","title":"Service Auto Grouping"},{"body":"Service Auto Instrument Agent Service auto instrument agent is a subset of Language based native agents. In this kind of agent, it is based on some language specific features, usually a VM based languages.\nWhat does Auto Instrument mean? Many users know these agents from hearing They say don't need to change any single line of codes, SkyWalking used to put these words in our readme page too. But actually, it is right and wrong. For end user, YES, they don\u0026rsquo;t need to change codes, at least for most cases. But also NO, the codes are still changed by agent, usually called manipulate codes at runtime. Underlying, it is just auto instrument agent including codes about how to change codes through VM interface, such as change class in Java through javaagent premain.\nAlso, we said that the most auto instrument agents are VM based, but actually, you can build a tool at compiling time, rather than runtime.\nWhat are limits? Auto instrument is so cool, also you can create those in compiling time, that you don\u0026rsquo;t depend on VM features, then is there any limit?\nThe answer definitely YES. And they are:\n  In process propagation possible in most cases. In many high level languages, they are used to build business system, such as Java and .NET. Most codes of business logic are running in the same thread for per request, which make the propagation could be based on thread Id, and stack module to make sure the context is safe.\n  Just effect frameworks or libraries. Because of the changing codes by agents, it also means the codes are already known by agent plugin developers. So, there is always a supported list in this kind of probes. Like SkyWalking Java agent supported list.\n  Across thread can\u0026rsquo;t be supported all the time. Like we said about in process propagation, most codes run in a single thread per request, especially business codes. But in some other scenarios, they do things in different threads, such as job assignment, task pool or batch process. Or some languages provide coroutine or similar thing like Goroutine, then developer could run async process with low payload, even been encouraged. In those cases, auto instrument will face problems.\n  So, no mystery for auto instrument, in short words, agent developers write an activation to make instrument codes work you. That is all.\nWhat is next? If you want to learn about manual instrument libs in SkyWalking, see Manual instrument SDK section.\n","excerpt":"Service Auto Instrument Agent Service auto instrument agent is a subset of Language based native …","ref":"/docs/main/v8.4.0/en/concepts-and-designs/service-agent/","title":"Service Auto Instrument Agent"},{"body":"Service Mesh Probe Service Mesh probes use the extendable mechanism provided in Service Mesh implementor, like Istio.\nWhat is Service Mesh? The following explanation came from Istio documents.\n The term service mesh is often used to describe the network of microservices that make up such applications and the interactions between them. As a service mesh grows in size and complexity, it can become harder to understand and manage. Its requirements can include discovery, load balancing, failure recovery, metrics, and monitoring, and often more complex operational requirements such as A/B testing, canary releases, rate limiting, access control, and end-to-end authentication.\n Where does the probe collect data from? Istio is a very typical Service Mesh design and implementor. It defines Control Panel and Data Panel, which are wide used. Here is Istio Architecture:\nService Mesh probe can choose to collect data from Data Panel. In Istio, it means collecting telemetry data from Envoy sidecar(Data Panel). The probe collects two telemetry entities from client side and server side per request.\nHow does Service Mesh make backend work? From the probe, you can see there must have no trace related in this kind of probe, so why SkyWalking platform still works?\nService Mesh probes collects telemetry data from each request, so it knows the source, destination, endpoint, latency and status. By those, backend can tell the whole topology map by combining these call as lines, and also the metrics of each nodes through their incoming request. Backend asked for the same metrics data from parsing tracing data. So, the right expression is: Service Mesh metrics are exact the metrics, what the traces parsers generate. They are same.\nWhat is Next?  If you want to use the service mesh probe, read set SkyWalking on Service Mesh document.  ","excerpt":"Service Mesh Probe Service Mesh probes use the extendable mechanism provided in Service Mesh …","ref":"/docs/main/v8.4.0/en/concepts-and-designs/service-mesh-probe/","title":"Service Mesh Probe"},{"body":"Setting Override SkyWalking backend supports setting overrides by system properties and system environment variables. You could override the settings in application.yml\nSystem properties key rule ModuleName.ProviderName.SettingKey.\n  Example\nOverride restHost in this setting segment\n  core: default: restHost: ${SW_CORE_REST_HOST:0.0.0.0} restPort: ${SW_CORE_REST_PORT:12800} restContextPath: ${SW_CORE_REST_CONTEXT_PATH:/} gRPCHost: ${SW_CORE_GRPC_HOST:0.0.0.0} gRPCPort: ${SW_CORE_GRPC_PORT:11800} Use command arg\n-Dcore.default.restHost=172.0.4.12 System environment variables   Example\nOverride restHost in this setting segment through environment variables\n  core: default: restHost: ${REST_HOST:0.0.0.0} restPort: ${SW_CORE_REST_PORT:12800} restContextPath: ${SW_CORE_REST_CONTEXT_PATH:/} gRPCHost: ${SW_CORE_GRPC_HOST:0.0.0.0} gRPCPort: ${SW_CORE_GRPC_PORT:11800} If the REST_HOST  environment variable exists in your operating system and its value is 172.0.4.12, then the value of restHost here will be overwritten to 172.0.4.12, otherwise, it will be set to 0.0.0.0.\nBy the way, Placeholder nesting is also supported, like ${REST_HOST:${ANOTHER_REST_HOST:127.0.0.1}}. In this case, if the REST_HOST  environment variable not exists, but the REST_ANOTHER_REST_HOSTHOST environment variable exists and its value is 172.0.4.12, then the value of restHost here will be overwritten to 172.0.4.12, otherwise, it will be set to 127.0.0.1.\n","excerpt":"Setting Override SkyWalking backend supports setting overrides by system properties and system …","ref":"/docs/main/v8.4.0/en/setup/backend/backend-setting-override/","title":"Setting Override"},{"body":"Setting Override In default, SkyWalking provide agent.config for agent.\nSetting override means end user can override the settings in these config file, through using system properties or agent options.\nSystem properties Use skywalking. + key in config file as system properties key, to override the value.\n  Why need this prefix?\nThe agent system properties and env share with target application, this prefix can avoid variable conflict.\n  Example\nOverride agent.application_code by this.\n  -Dskywalking.agent.application_code=31200 Agent options Add the properties after the agent path in JVM arguments.\n-javaagent:/path/to/skywalking-agent.jar=[option1]=[value1],[option2]=[value2]   Example\nOverride agent.application_code and logging.level by this.\n  -javaagent:/path/to/skywalking-agent.jar=agent.application_code=31200,logging.level=debug   Special characters\nIf a separator(, or =) in the option or value, it should be wrapped in quotes.\n  -javaagent:/path/to/skywalking-agent.jar=agent.ignore_suffix='.jpg,.jpeg' System environment variables   Example\nOverride agent.application_code and logging.level by this.\n  # The service name in UI agent.service_name=${SW_AGENT_NAME:Your_ApplicationName} # Logging level logging.level=${SW_LOGGING_LEVEL:INFO} If the SW_AGENT_NAME  environment variable exists in your operating system and its value is skywalking-agent-demo, then the value of agent.service_name here will be overwritten to skywalking-agent-demo, otherwise, it will be set to Your_ApplicationName.\nBy the way, Placeholder nesting is also supported, like ${SW_AGENT_NAME:${ANOTHER_AGENT_NAME:Your_ApplicationName}}. In this case, if the SW_AGENT_NAME  environment variable not exists, but the ANOTHER_AGENT_NAME environment variable exists and its value is skywalking-agent-demo, then the value of agent.service_name here will be overwritten to skywalking-agent-demo,otherwise, it will be set to Your_ApplicationName.\nOverride priority Agent Options \u0026gt; System.Properties(-D) \u0026gt; System environment variables \u0026gt; Config file\n","excerpt":"Setting Override In default, SkyWalking provide agent.config for agent.\nSetting override means end …","ref":"/docs/main/v8.4.0/en/setup/service-agent/java-agent/setting-override/","title":"Setting Override"},{"body":"Setup The document explains how to install Skywalking based on the kind of probes you are going to use. If you don\u0026rsquo;t understand, please read Concepts and Designs first.\nImportant: Don\u0026rsquo;t forget to configure the timezone on your UI, and you also need to be sure your OAP backend servers are also using the same timezone.\nIf you have any issues, please check that your issue is not already described in the FAQ.\nDownload official releases  Backend, UI and Java agent are Apache official release, you can find them on the Apache SkyWalking download page.  Language agents in Service   Java agent. Introduces how to install java agent to your service, without any impact in your code.\n  LUA agent. Introduce how to install the lua agent in Nginx + LUA module or OpenResty.\n  Python Agent. Introduce how to install the Python Agent in a Python service.\n  Node.js agent. Introduce how to install the NodeJS Agent in a NodeJS service.\n  The following agents and SDKs are compatible with the SkyWalking\u0026rsquo;s data formats and network protocols, but are maintained by 3rd-parties. You can go to their project repositories for additional info about guides and releases.\n  SkyAPM .NET Core agent. See .NET Core agent project document for more details.\n  SkyAPM Node.js agent. See Node.js server side agent project document for more details.\n  SkyAPM PHP agent. See PHP agent project document for more details.\n  SkyAPM Go SDK. See go2sky project document for more details.\n  SkyAPM C++ SDK. See cpp2sky project document for more details.\n  Browser Monitoring Apache SkyWalking Client JS. Support collecting metrics and error logs for the Browser or JavaScript based mobile app.\nNote, make sure the receiver-browser has been opened, default is ON since 8.2.0.\nService Mesh  Istio  SkyWalking on Istio. Introduces how to analyze Istio metrics.   Envoy  Use ALS (access log service) to observe service mesh, without Mixer. Follow document for guides.    Proxy  Envoy Proxy  Sending metrics to Skywalking from Envoy. How to send metrics from Envoy to SkyWalking using Metrics service.    Setup backend Follow backend and UI setup document to understand how the backend and UI configuration works. Different scenarios and advanced features are also explained.\nChanges log Backend, UI and Java agent changes are available here.\n","excerpt":"Setup The document explains how to install Skywalking based on the kind of probes you are going to …","ref":"/docs/main/v8.4.0/en/setup/readme/","title":"Setup"},{"body":"Setup java agent  Agent is available for JDK 8 - 14 in 7.x releases. JDK 1.6 - JDK 12 are supported in all 6.x releases NOTICE¹ Find agent folder in SkyWalking release package Set agent.service_name in config/agent.config. Could be any String in English. Set collector.backend_service in config/agent.config. Default point to 127.0.0.1:11800, only works for local backend. Add -javaagent:/path/to/skywalking-package/agent/skywalking-agent.jar to JVM argument. And make sure to add it before the -jar argument.  The agent release dist is included in Apache official release. New agent package looks like this.\n+-- agent +-- activations apm-toolkit-log4j-1.x-activation.jar apm-toolkit-log4j-2.x-activation.jar apm-toolkit-logback-1.x-activation.jar ... +-- config agent.config +-- plugins apm-dubbo-plugin.jar apm-feign-default-http-9.x.jar apm-httpClient-4.x-plugin.jar ..... +-- optional-plugins apm-gson-2.x-plugin.jar ..... +-- bootstrap-plugins jdk-http-plugin.jar ..... +-- logs skywalking-agent.jar  Start your application.  Supported middleware, framework and library SkyWalking agent has supported various middlewares, frameworks and libraries. Read supported list to get them and supported version. If the plugin is in Optional² catalog, go to optional plugins section to learn how to active it.\nAdvanced features  All plugins are in /plugins folder. The plugin jar is active when it is in there. Remove the plugin jar, it disabled. The default logging output folder is /logs.  Install javaagent FAQs  Linux Tomcat 7, Tomcat 8, Tomcat 9\nChange the first line of tomcat/bin/catalina.sh.  CATALINA_OPTS=\u0026#34;$CATALINA_OPTS-javaagent:/path/to/skywalking-agent/skywalking-agent.jar\u0026#34;; export CATALINA_OPTS  Windows Tomcat 7, Tomcat 8, Tomcat 9\nChange the first line of tomcat/bin/catalina.bat.  set \u0026#34;CATALINA_OPTS=-javaagent:/path/to/skywalking-agent/skywalking-agent.jar\u0026#34;  JAR file\nAdd -javaagent argument to command line in which you start your app. eg:  java -javaagent:/path/to/skywalking-agent/skywalking-agent.jar -jar yourApp.jar  Jetty\nModify jetty.sh, add -javaagent argument to command line in which you start your app. eg:  export JAVA_OPTIONS=\u0026#34;${JAVA_OPTIONS}-javaagent:/path/to/skywalking-agent/skywalking-agent.jar\u0026#34; Table of Agent Configuration Properties This is the properties list supported in agent/config/agent.config.\n   property key Description Default     agent.namespace Namespace isolates headers in cross process propagation. The HEADER name will be HeaderName:Namespace. Not set   agent.service_name The service name to represent a logic group providing the same capabilities/logic. Suggestion: set a unique name for every logic service group, service instance nodes share the same code, Max length is 50(UTF-8 char). Optional, once service_name follows \u0026lt;group name\u0026gt;::\u0026lt;logic name\u0026gt; format, OAP server assigns the group name to the service metadata. Your_ApplicationName   agent.sample_n_per_3_secs Negative or zero means off, by default.SAMPLE_N_PER_3_SECS means sampling N TraceSegment in 3 seconds tops. Not set   agent.authentication Authentication active is based on backend setting, see application.yml for more details.For most scenarios, this needs backend extensions, only basic match auth provided in default implementation. Not set   agent.span_limit_per_segment The max number of spans in a single segment. Through this config item, SkyWalking keep your application memory cost estimated. 300   agent.ignore_suffix If the operation name of the first span is included in this set, this segment should be ignored. Not set   agent.is_open_debugging_class If true, skywalking agent will save all instrumented classes files in /debugging folder. SkyWalking team may ask for these files in order to resolve compatible problem. Not set   agent.is_cache_enhanced_class If true, SkyWalking agent will cache all instrumented classes files to memory or disk files (decided by class cache mode), allow another java agent to enhance those classes that enhanced by SkyWalking agent. To use some Java diagnostic tools (such as BTrace, Arthas) to diagnose applications or add a custom java agent to enhance classes, you need to enable this feature. Read this FAQ for more details false   agent.class_cache_mode The instrumented classes cache mode: MEMORY or FILE. MEMORY: cache class bytes to memory, if instrumented classes is too many or too large, it may take up more memory. FILE: cache class bytes in /class-cache folder, automatically clean up cached class files when the application exits. MEMORY   agent.instance_name Instance name is the identity of an instance, should be unique in the service. If empty, SkyWalking agent will generate an 32-bit uuid. Default, use UUID@hostname as the instance name. Max length is 50(UTF-8 char) \u0026quot;\u0026quot;   agent.instance_properties[key]=value Add service instance custom properties. Not set   agent.cause_exception_depth How depth the agent goes, when log all cause exceptions. 5   agent.force_reconnection_period  Force reconnection period of grpc, based on grpc_channel_check_interval. 1   agent.operation_name_threshold  The operationName max length, setting this value \u0026gt; 190 is not recommended. 150   agent.keep_tracing Keep tracing even the backend is not available if this value is true. false   agent.force_tls Force open TLS for gRPC channel if this value is true. false   osinfo.ipv4_list_size Limit the length of the ipv4 list size. 10   collector.grpc_channel_check_interval grpc channel status check interval. 30   collector.heartbeat_period agent heartbeat report period. Unit, second. 30   collector.properties_report_period_factor The agent sends the instance properties to the backend every collector.heartbeat_period * collector.properties_report_period_factor seconds 10   collector.backend_service Collector SkyWalking trace receiver service addresses. 127.0.0.1:11800   collector.grpc_upstream_timeout How long grpc client will timeout in sending data to upstream. Unit is second. 30 seconds   collector.get_profile_task_interval Sniffer get profile task list interval. 20   collector.get_agent_dynamic_config_interval Sniffer get agent dynamic config interval 20   collector.dns_period_resolve_active If true, skywalking agent will enable periodically resolving DNS to update receiver service addresses. false   logging.level Log level: TRACE, DEBUG, INFO, WARN, ERROR, OFF. Default is info. INFO   logging.file_name Log file name. skywalking-api.log   logging.output Log output. Default is FILE. Use CONSOLE means output to stdout. FILE   logging.dir Log files directory. Default is blank string, means, use \u0026ldquo;{theSkywalkingAgentJarDir}/logs \u0026quot; to output logs. {theSkywalkingAgentJarDir} is the directory where the skywalking agent jar file is located \u0026quot;\u0026quot;   logging.resolver Logger resolver: PATTERN or JSON. The default is PATTERN, which uses logging.pattern to print traditional text logs. JSON resolver prints logs in JSON format. PATTERN   logging.pattern  Logging format. There are all conversion specifiers: * %level means log level. * %timestamp means now of time with format yyyy-MM-dd HH:mm:ss:SSS.\n* %thread means name of current thread.\n* %msg means some message which user logged. * %class means SimpleName of TargetClass. * %throwable means a throwable which user called. * %agent_name means agent.service_name. Only apply to the PatternLogger. %level %timestamp %thread %class : %msg %throwable   logging.max_file_size The max size of log file. If the size is bigger than this, archive the current file, and write into a new file. 300 * 1024 * 1024   logging.max_history_files The max history log files. When rollover happened, if log files exceed this number,then the oldest file will be delete. Negative or zero means off, by default. -1   statuscheck.ignored_exceptions Listed exceptions would not be treated as an error. Because in some codes, the exception is being used as a way of controlling business flow. \u0026quot;\u0026quot;   statuscheck.max_recursive_depth The max recursive depth when checking the exception traced by the agent. Typically, we don\u0026rsquo;t recommend setting this more than 10, which could cause a performance issue. Negative value and 0 would be ignored, which means all exceptions would make the span tagged in error status. 1   correlation.element_max_number Max element count in the correlation context. 3   correlation.value_max_length Max value length of each element. 128   correlation.auto_tag_keys Tag the span by the key/value in the correlation context, when the keys listed here exist. \u0026quot;\u0026quot;   jvm.buffer_size The buffer size of collected JVM info. 60 * 10   buffer.channel_size The buffer channel size. 5   buffer.buffer_size The buffer size. 300   profile.active If true, skywalking agent will enable profile when user create a new profile task. Otherwise disable profile. true   profile.max_parallel Parallel monitor segment count 5   profile.duration Max monitor segment time(minutes), if current segment monitor time out of limit, then stop it. 10   profile.dump_max_stack_depth Max dump thread stack depth 500   profile.snapshot_transport_buffer_size Snapshot transport to backend buffer size 50   meter.active If true, the agent collects and reports metrics to the backend. true   meter.report_interval Report meters interval. The unit is second 20   meter.max_meter_size Max size of the meter pool 500   plugin.mount Mount the specific folders of the plugins. Plugins in mounted folders would work. plugins,activations   plugin.peer_max_length  Peer maximum description limit. 200   plugin.exclude_plugins  Exclude some plugins define in plugins dir.Plugin names is defined in Agent plugin list \u0026quot;\u0026quot;   plugin.mongodb.trace_param If true, trace all the parameters in MongoDB access, default is false. Only trace the operation, not include parameters. false   plugin.mongodb.filter_length_limit If set to positive number, the WriteRequest.params would be truncated to this length, otherwise it would be completely saved, which may cause performance problem. 256   plugin.elasticsearch.trace_dsl If true, trace all the DSL(Domain Specific Language) in ElasticSearch access, default is false. false   plugin.springmvc.use_qualified_name_as_endpoint_name If true, the fully qualified method name will be used as the endpoint name instead of the request URL, default is false. false   plugin.toolit.use_qualified_name_as_operation_name If true, the fully qualified method name will be used as the operation name instead of the given operation name, default is false. false   plugin.jdbc.trace_sql_parameters If set to true, the parameters of the sql (typically java.sql.PreparedStatement) would be collected. false   plugin.jdbc.sql_parameters_max_length If set to positive number, the db.sql.parameters would be truncated to this length, otherwise it would be completely saved, which may cause performance problem. 512   plugin.jdbc.sql_body_max_length If set to positive number, the db.statement would be truncated to this length, otherwise it would be completely saved, which may cause performance problem. 2048   plugin.solrj.trace_statement If true, trace all the query parameters(include deleteByIds and deleteByQuery) in Solr query request, default is false. false   plugin.solrj.trace_ops_params If true, trace all the operation parameters in Solr request, default is false. false   plugin.light4j.trace_handler_chain If true, trace all middleware/business handlers that are part of the Light4J handler chain for a request. false   plugin.opgroup.* Support operation name customize group rules in different plugins. Read Group rule supported plugins Not set   plugin.springtransaction.simplify_transaction_definition_name If true, the transaction definition name will be simplified. false   plugin.jdkthreading.threading_class_prefixes Threading classes (java.lang.Runnable and java.util.concurrent.Callable) and their subclasses, including anonymous inner classes whose name match any one of the THREADING_CLASS_PREFIXES (splitted by ,) will be instrumented, make sure to only specify as narrow prefixes as what you\u0026rsquo;re expecting to instrument, (java. and javax. will be ignored due to safety issues) Not set   plugin.tomcat.collect_http_params This config item controls that whether the Tomcat plugin should collect the parameters of the request. Also, activate implicitly in the profiled trace. false   plugin.springmvc.collect_http_params This config item controls that whether the SpringMVC plugin should collect the parameters of the request, when your Spring application is based on Tomcat, consider only setting either plugin.tomcat.collect_http_params or plugin.springmvc.collect_http_params. Also, activate implicitly in the profiled trace. false   plugin.httpclient.collect_http_params This config item controls that whether the HttpClient plugin should collect the parameters of the request false   plugin.http.http_params_length_threshold When COLLECT_HTTP_PARAMS is enabled, how many characters to keep and send to the OAP backend, use negative values to keep and send the complete parameters, NB. this config item is added for the sake of performance. 1024   plugin.http.http_headers_length_threshold When include_http_headers declares header names, this threshold controls the length limitation of all header values. use negative values to keep and send the complete headers. Note. this config item is added for the sake of performance. 2048   plugin.http.include_http_headers Set the header names, which should be collected by the plugin. Header name must follow javax.servlet.http definition. Multiple names should be split by comma. ``(No header would be collected) |   plugin.feign.collect_request_body This config item controls that whether the Feign plugin should collect the http body of the request. false   plugin.feign.filter_length_limit When COLLECT_REQUEST_BODY is enabled, how many characters to keep and send to the OAP backend, use negative values to keep and send the complete body. 1024   plugin.feign.supported_content_types_prefix When COLLECT_REQUEST_BODY is enabled and content-type start with SUPPORTED_CONTENT_TYPES_PREFIX, collect the body of the request , multiple paths should be separated by , application/json,text/   plugin.influxdb.trace_influxql If true, trace all the influxql(query and write) in InfluxDB access, default is true. true   plugin.dubbo.collect_consumer_arguments Apache Dubbo consumer collect arguments in RPC call, use Object#toString to collect arguments. false   plugin.dubbo.consumer_arguments_length_threshold When plugin.dubbo.collect_consumer_arguments is true, Arguments of length from the front will to the OAP backend 256   plugin.dubbo.collect_provider_arguments Apache Dubbo provider collect arguments in RPC call, use Object#toString to collect arguments. false   plugin.dubbo.consumer_provider_length_threshold When plugin.dubbo.provider_consumer_arguments is true, Arguments of length from the front will to the OAP backend 256   plugin.kafka.bootstrap_servers A list of host/port pairs to use for establishing the initial connection to the Kafka cluster. localhost:9092   plugin.kafka.get_topic_timeout Timeout period of reading topics from the Kafka server, the unit is second. 10   plugin.kafka.consumer_config Kafka producer configuration.    plugin.kafka.producer_config Kafka producer configuration. Read producer configure to get more details. Check Kafka report doc for more details and examples.    plugin.kafka.topic_meter Specify which Kafka topic name for Meter System data to report to. skywalking_meters   plugin.kafka.topic_metrics Specify which Kafka topic name for JVM metrics data to report to. skywalking_metrics   plugin.kafka.topic_segment Specify which Kafka topic name for traces data to report to. skywalking_segments   plugin.kafka.topic_profilings Specify which Kafka topic name for Thread Profiling snapshot to report to. skywalking_profilings   plugin.kafka.topic_management Specify which Kafka topic name for the register or heartbeat data of Service Instance to report to. skywalking_managements   plugin.springannotation.classname_match_regex Match spring beans with regular expression for the class name. Multiple expressions could be separated by a comma. This only works when Spring annotation plugin has been activated. All the spring beans tagged with @Bean,@Service,@Dao, or @Repository.   plugin.toolkit.log.transmit_formatted Whether or not to transmit logged data as formatted or un-formatted. true   plugin.toolkit.log.grpc.reporter.server_host Specify which grpc server\u0026rsquo;s host for log data to report to. 127.0.0.1   plugin.toolkit.log.grpc.reporter.server_port Specify which grpc server\u0026rsquo;s port for log data to report to. 11800   plugin.toolkit.log.grpc.reporter.max_message_size Specify the maximum size of log data for grpc client to report to. 10485760   plugin.toolkit.log.grpc.reporter.upstream_timeout How long grpc client will timeout in sending data to upstream. Unit is second. 30 seconds    Dynamic Configurations All configurations above are static, if you need to change some agent settings at runtime, please read CDS - Configuration Discovery Service document for more details.\nOptional Plugins Java agent plugins are all pluggable. Optional plugins could be provided in optional-plugins folder under agent or 3rd party repositories. For using these plugins, you need to put the target plugin jar file into /plugins.\nNow, we have the following known optional plugins.\n Plugin of tracing Spring annotation beans Plugin of tracing Oracle and Resin Filter traces through specified endpoint name patterns Plugin of Gson serialization lib in optional plugin folder. Plugin of Zookeeper 3.4.x in optional plugin folder. The reason of being optional plugin is, many business irrelevant traces are generated, which cause extra payload to agents and backends. At the same time, those traces may be just heartbeat(s). Customize enhance Trace methods based on description files, rather than write plugin or change source codes. Plugin of Spring Cloud Gateway 2.1.x in optional plugin folder. Please only active this plugin when you install agent in Spring Gateway. spring-cloud-gateway-2.x-plugin and spring-webflux-5.x-plugin are both required. Plugin of Spring Transaction in optional plugin folder. The reason of being optional plugin is, many local span are generated, which also spend more CPU, memory and network. Plugin of Kotlin coroutine provides the tracing across coroutines automatically. As it will add local spans to all across routines scenarios, Please assess the performance impact. Plugin of quartz-scheduler-2.x in the optional plugin folder. The reason for being an optional plugin is, many task scheduling systems are based on quartz-scheduler, this will cause duplicate tracing and link different sub-tasks as they share the same quartz level trigger, such as ElasticJob. Plugin of spring-webflux-5.x in the optional plugin folder. Please only activate this plugin when you use webflux alone as a web container. If you are using SpringMVC 5 or Spring Gateway, you don\u0026rsquo;t need this plugin.  Bootstrap class plugins All bootstrap plugins are optional, due to unexpected risk. Bootstrap plugins are provided in bootstrap-plugins folder. For using these plugins, you need to put the target plugin jar file into /plugins.\nNow, we have the following known bootstrap plugins.\n Plugin of JDK HttpURLConnection. Agent is compatible with JDK 1.6+ Plugin of JDK Callable and Runnable. Agent is compatible with JDK 1.6+  The Logic Endpoint In default, all the RPC server-side names as entry spans, such as RESTFul API path and gRPC service name, would be endpoints with metrics. At the same time, SkyWalking introduces the logic endpoint concept, which allows plugins and users to add new endpoints without adding new spans. The following logic endpoints are added automatically by plugins.\n GraphQL Query and Mutation are logic endpoints by using the names of them. Spring\u0026rsquo;s ScheduledMethodRunnable jobs are logic endpoints. The name format is SpringScheduled/${className}/${methodName}. Apache ShardingSphere ElasticJob\u0026rsquo;s jobs are logic endpoints. The name format is ElasticJob/${jobName}. XXLJob\u0026rsquo;s jobs are logic endpoints. The name formats include xxl-job/MethodJob/${className}.${methodName}, xxl-job/ScriptJob/${GlueType}/id/${jobId}, and xxl-job/SimpleJob/${className}. Quartz(optional plugin)\u0026rsquo;s jobs are logic endpoints. the name format is quartz-scheduler/${className}.  User could use the SkyWalking\u0026rsquo;s application toolkits to add the tag into the local span to label the span as a logic endpoint in the analysis stage. The tag is, key=x-le and value = {\u0026quot;logic-span\u0026quot;:true}.\nAdvanced Features  Set the settings through system properties for config file override. Read setting override. Use gRPC TLS to link backend. See open TLS Monitor a big cluster by different SkyWalking services. Use Namespace to isolate the context propagation. Set client token if backend open token authentication. Application Toolkit, are a collection of libraries, provided by SkyWalking APM. Using them, you have a bridge between your application and SkyWalking APM agent.  If you want your codes to interact with SkyWalking agent, including getting trace id, setting tags, propagating custom data etc.. Try SkyWalking manual APIs. If you require customized metrics, try SkyWalking Meter System Toolkit. If you want to print trace context(e.g. traceId) in your logs, or collect logs, choose the log frameworks, log4j, log4j2, logback If you want to continue traces across thread manually, use across thread solution APIs. If you want to forward MicroMeter/Spring Sleuth metrics to Meter System, use SkyWalking MicroMeter Register. If you want to use OpenTracing Java APIs, try SkyWalking OpenTracing compatible tracer. More details you could find at http://opentracing.io If you want to tolerate some exceptions, read tolerate custom exception doc.   If you want to specify the path of your agent.config file. Read set config file through system properties  Advanced Reporters The advanced report provides an alternative way to submit the agent collected data to the backend. All of them are in the optional-reporter-plugins folder, move the one you needed into the reporter-plugins folder for the activation. Notice, don\u0026rsquo;t try to activate multiple reporters, that could cause unexpected fatal errors.\n Use Kafka to transport the traces, JVM metrics, instance properties, and profiled snapshots to the backend. Read the How to enable Kafka Reporter for more details.  Plugin Development Guide SkyWalking java agent supports plugin to extend the supported list. Please follow our Plugin Development Guide.\nTest If you are interested in plugin compatible tests or agent performance, see the following reports.\n Plugin Test in every Pull Request Java Agent Performance Test  Notice ¹ Due to gRPC didn\u0026rsquo;t support JDK 1.6 since 2018, SkyWalking abandoned the JDK 6/7 supports in all 7.x releases. But, with gRPC back forward compatibility(at least for now), all SkyWalking 6.x agents could work with 7.x, including the agent and backend.\n","excerpt":"Setup java agent  Agent is available for JDK 8 - 14 in 7.x releases. JDK 1.6 - JDK 12 are supported …","ref":"/docs/main/v8.4.0/en/setup/service-agent/java-agent/readme/","title":"Setup java agent"},{"body":"Skywalking Agent List  activemq-5.x armeria-063-084 armeria-085 armeria-086 armeria-098 async-http-client-2.x avro-1.x brpc-java canal-1.x cassandra-java-driver-3.x dbcp-2.x dubbo ehcache-2.x elastic-job-2.x elastic-job-3.x elasticsearch-5.x elasticsearch-6.x feign-default-http-9.x feign-pathvar-9.x finagle graphql grpc-1.x gson-2.8.x h2-1.x hbase-1.x httpasyncclient-4.x httpclient-3.x httpclient-4.x hystrix-1.x influxdb-2.x jdk-http-plugin jdk-threading-plugin jedis-2.x jetty-client-9.0 jetty-client-9.x jetty-server-9.x kafka-0.11.x/1.x/2.x kotlin-coroutine lettuce-5.x light4j mariadb-2.x memcache-2.x mongodb-2.x mongodb-3.x mongodb-4.x motan-0.x mysql-5.x mysql-6.x mysql-8.x netty-socketio nutz-http-1.x nutz-mvc-annotation-1.x okhttp-3.x play-2.x postgresql-8.x pulsar quasar quartz-scheduler-2.x rabbitmq-5.x redisson-3.x resteasy-server-3.x rocketMQ-3.x rocketMQ-4.x servicecomb-0.x servicecomb-1.x sharding-jdbc-1.5.x sharding-sphere-3.x sharding-sphere-4.0.0 sharding-sphere-4.1.0 sharding-sphere-4.x sharding-sphere-4.x-rc3 sofarpc solrj-7.x spring-annotation spring-async-annotation-5.x spring-cloud-feign-1.x spring-cloud-feign-2.x spring-cloud-gateway-2.0.x spring-cloud-gateway-2.1.x spring-concurrent-util-4.x spring-core-patch spring-kafka-1.x spring-kafka-2.x spring-mvc-annotation spring-mvc-annotation-3.x spring-mvc-annotation-4.x spring-mvc-annotation-5.x spring-resttemplate-4.x spring-scheduled-annotation spring-tx spring-webflux-5.x spring-webflux-5.x-webclient spymemcached-2.x struts2-2.x thrift tomcat-7.x/8.x toolkit-counter toolkit-gauge toolkit-histogram toolkit-kafka toolkit-log4j toolkit-log4j2 toolkit-logback toolkit-opentracing toolkit-tag toolkit-trace toolkit-exception undertow-2.x-plugin vertx-core-3.x xxl-job-2.x zookeeper-3.4.x mssql-jtds-1.x mssql-jdbc apache-cxf-3.x  ","excerpt":"Skywalking Agent List  activemq-5.x armeria-063-084 armeria-085 armeria-086 armeria-098 …","ref":"/docs/main/v8.4.0/en/setup/service-agent/java-agent/plugin-list/","title":"Skywalking Agent List"},{"body":"SkyWalking Cross Process Correlation Headers Protocol  Version 1.0  The Cross Process Correlation Headers Protocol is used to transport custom data by leveraging the capability of Cross Process Propagation Headers Protocol.\nThis is an optional and additional protocol for language tracer implementation. All tracer implementation could consider to implement this. Cross Process Correlation Header key is sw8-correlation. The value is the encoded(key):encoded(value) list with elements splitted by , such as base64(string key):base64(string value),base64(string key2):base64(string value2).\nRecommendations of language APIs Recommended implementation in different language API.\n TraceContext#putCorrelation and TraceContext#getCorrelation are recommended to write and read the correlation context, with key/value string. The key should be added if it is absent. The later writes should override the previous value. The total number of all keys should be less than 3, and the length of each value should be less than 128 bytes. The context should be propagated as well when tracing context is propagated across threads and processes.  ","excerpt":"SkyWalking Cross Process Correlation Headers Protocol  Version 1.0  The Cross Process Correlation …","ref":"/docs/main/v8.4.0/en/protocols/skywalking-cross-process-correlation-headers-protocol-v1/","title":"SkyWalking Cross Process Correlation Headers Protocol"},{"body":"SkyWalking Cross Process Propagation Headers Protocol  Version 3.0  SkyWalking is more likely an APM system, rather than the common distributed tracing system. The Headers are much more complex than them in order to improving analysis performance of OAP. You can find many similar mechanism in other commercial APM systems. (Some are even much more complex than our\u0026rsquo;s)\nAbstract SkyWalking Cross Process Propagation Headers Protocol v3 is also named as sw8 protocol, which is for context propagation.\nStandard Header Item The standard header should be the minimal requirement for the context propagation.\n Header Name: sw8. Header Value: 8 fields split by -. The length of header value should be less than 2k(default).  Value format example, XXXXX-XXXXX-XXXX-XXXX\nValues Values include the following segments, all String type values are in BASE64 encoding.\n Required(s)   Sample. 0 or 1. 0 means context exists, but could(most likely will) ignore. 1 means this trace need to be sampled and send to backend. Trace Id. String(BASE64 encoded). Literal String and unique globally. Parent trace segment Id. String(BASE64 encoded). Literal String and unique globally. Parent span Id. Integer. Begin with 0. This span id points to the parent span in parent trace segment. Parent service. String(BASE64 encoded). The length should not be less or equal than 50 UTF-8 characters. Parent service instance. String(BASE64 encoded). The length should be less or equal than 50 UTF-8 characters. Parent endpoint. String(BASE64 encoded). Operation Name of the first entry span in the parent segment. The length should be less than 150 UTF-8 characters. Target address used at client side of this request. String(BASE64 encoded). The network address(not must be IP + port) used at client side to access this target service.   Sample values, 1-TRACEID-SEGMENTID-3-PARENT_SERVICE-PARENT_INSTANCE-PARENT_ENDPOINT-IPPORT  Extension Header Item Extension header item is designed for the advanced features. It provides the interaction capabilities between the agents deployed in upstream and downstream services.\n Header Name: sw8-x Header Value: Split by -. The fields are extendable.  Values The current value includes fields.\n Tracing Mode. empty, 0 or 1. empty or 0 is default. 1 represents all spans generated in this context should skip analysis, spanObject#skipAnalysis=true. This context should be propagated to upstream in the default, unless it is changed in the tracing process. The timestamp of sending at the client-side. This is used in async RPC such as MQ. Once it is set, the consumer side would calculate the latency between sending and receiving, and tag the latency in the span by using key transmission.latency automatically.  ","excerpt":"SkyWalking Cross Process Propagation Headers Protocol  Version 3.0  SkyWalking is more likely an APM …","ref":"/docs/main/v8.4.0/en/protocols/skywalking-cross-process-propagation-headers-protocol-v3/","title":"SkyWalking Cross Process Propagation Headers Protocol"},{"body":"Apache SkyWalking release guide This document guides every committer to release SkyWalking in Apache Way, and also help committers to check the release for vote.\nSetup your development environment Follow Apache maven deployment environment document to set gpg tool and encrypt passwords\nUse the following block as a template and place it in ~/.m2/settings.xml\n\u0026lt;settings\u0026gt; ... \u0026lt;servers\u0026gt; \u0026lt;!-- To publish a snapshot of some part of Maven --\u0026gt; \u0026lt;server\u0026gt; \u0026lt;id\u0026gt;apache.snapshots.https\u0026lt;/id\u0026gt; \u0026lt;username\u0026gt; \u0026lt;!-- YOUR APACHE LDAP USERNAME --\u0026gt; \u0026lt;/username\u0026gt; \u0026lt;password\u0026gt; \u0026lt;!-- YOUR APACHE LDAP PASSWORD (encrypted) --\u0026gt; \u0026lt;/password\u0026gt; \u0026lt;/server\u0026gt; \u0026lt;!-- To stage a release of some part of Maven --\u0026gt; \u0026lt;server\u0026gt; \u0026lt;id\u0026gt;apache.releases.https\u0026lt;/id\u0026gt; \u0026lt;username\u0026gt; \u0026lt;!-- YOUR APACHE LDAP USERNAME --\u0026gt; \u0026lt;/username\u0026gt; \u0026lt;password\u0026gt; \u0026lt;!-- YOUR APACHE LDAP PASSWORD (encrypted) --\u0026gt; \u0026lt;/password\u0026gt; \u0026lt;/server\u0026gt; ... \u0026lt;/servers\u0026gt; \u0026lt;/settings\u0026gt; Add your GPG public key  Add your GPG public key into SkyWalking GPG KEYS file, only if you are a committer, use your Apache id and password login this svn, and update file. Don\u0026rsquo;t override the existing file. Upload your GPG public key to public GPG site. Such as MIT\u0026rsquo;s site. This site should be in Apache maven staging repository check list.  Test your settings This step is only for test, if your env is set right, don\u0026rsquo;t need to check every time.\n./mvnw clean install -Pall (this will build artifacts, sources and sign) Prepare the release ./mvnw release:clean ./mvnw release:prepare -DautoVersionSubmodules=true -Pall  Set version number as x.y.z, and tag as vx.y.z (version tag must start with v, you will find the purpose in next step.)  You could do a GPG sign before doing release, if you need input the password to sign, and the maven don\u0026rsquo;t give the chance, but just failure. Run gpg --sign xxx to any file could remember the password for enough time to do release.\nStage the release ./mvnw release:perform -DskipTests -Pall  The release will automatically be inserted into a temporary staging repository for you.  Build and sign the source code package export RELEASE_VERSION=x.y.z (example: RELEASE_VERSION=5.0.0-alpha) cd tools/releasing bash create_source_release.sh This scripts should do following things\n Use v + RELEASE_VERSION as tag to clone the codes. Make git submodule init/update done. Exclude all unnecessary files in the target source tar, such as .git, .github, .gitmodules. See the script for the details. Do gpg and shasum 512.  The apache-skywalking-apm-x.y.z-src.tgz should be found in tools/releasing folder, with .asc, .sha512.\nFind and download distribution in Apache Nexus Staging repositories  Use ApacheId to login https://repository.apache.org/ Go to https://repository.apache.org/#stagingRepositories Search skywalking and find your staging repository Close the repository and wait for all checks pass. In this step, your GPG KEYS will be checked. See set PGP document, if you haven\u0026rsquo;t done it before. Go to {REPO_URL}/org/apache/skywalking/apache-skywalking-apm/x.y.z Download .tar.gz and .zip with .asc and .sha1  Upload to Apache svn  Use ApacheId to login https://dist.apache.org/repos/dist/dev/skywalking/ Create folder, named by release version and round, such as: x.y.z Upload Source code package to the folder with .asc, .sha512  Package name: apache-skywalking-x.y.z-src.tar.gz See Section \u0026ldquo;Build and sign the source code package\u0026rdquo; for more details   Upload distribution package to the folder with .asc, .sha512  Package name: apache-skywalking-bin-x.y.z.tar.gz, apache-skywalking-bin-x.y.z.zip See Section \u0026ldquo;Find and download distribution in Apache Nexus Staging repositories\u0026rdquo; for more details Create .sha512 package: shasum -a 512 file \u0026gt; file.sha512    Make the internal announcements Send an announcement mail in dev mail list.\nMail title: [ANNOUNCE] SkyWalking x.y.z test build available Mail content: The test build of x.y.z is available. We welcome any comments you may have, and will take all feedback into account if a quality vote is called for this build. Release notes: * https://github.com/apache/skywalking/blob/master/changes/changes-x.y.z.md Release Candidate: * https://dist.apache.org/repos/dist/dev/skywalking/xxxx * sha512 checksums - sha512xxxxyyyzzz apache-skywalking-apm-x.x.x-src.tgz - sha512xxxxyyyzzz apache-skywalking-apm-bin-x.x.x.tar.gz - sha512xxxxyyyzzz apache-skywalking-apm-bin-x.x.x.zip Maven 2 staging repository: * https://repository.apache.org/content/repositories/xxxx/org/apache/skywalking/ Release Tag : * (Git Tag) x.y.z Release CommitID : * https://github.com/apache/skywalking/tree/(Git Commit ID) * Git submodule * skywalking-ui: https://github.com/apache/skywalking-rocketbot-ui/tree/(Git Commit ID) * apm-protocol/apm-network/src/main/proto: https://github.com/apache/skywalking-data-collect-protocol/tree/(Git Commit ID) * oap-server/server-query-plugin/query-graphql-plugin/src/main/resources/query-protocol https://github.com/apache/skywalking-query-protocol/tree/(Git Commit ID) Keys to verify the Release Candidate : * https://dist.apache.org/repos/dist/release/skywalking/KEYS Guide to build the release from source : * https://github.com/apache/skywalking/blob/x.y.z/docs/en/guides/How-to-build.md A vote regarding the quality of this test build will be initiated within the next couple of days. Wait at least 48 hours for test responses Any PMC, committer or contributor can test features for releasing, and feedback. Based on that, PMC will decide whether start a vote.\nCall a vote in dev Call a vote in dev@skywalking.apache.org\nMail title: [VOTE] Release Apache SkyWalking version x.y.z Mail content: Hi All, This is a call for vote to release Apache SkyWalking version x.y.z. Release notes: * https://github.com/apache/skywalking/blob/master/changes/changes-x.y.z.md Release Candidate: * https://dist.apache.org/repos/dist/dev/skywalking/xxxx * sha512 checksums - sha512xxxxyyyzzz apache-skywalking-apm-x.x.x-src.tgz - sha512xxxxyyyzzz apache-skywalking-apm-bin-x.x.x.tar.gz - sha512xxxxyyyzzz apache-skywalking-apm-bin-x.x.x.zip Maven 2 staging repository: * https://repository.apache.org/content/repositories/xxxx/org/apache/skywalking/ Release Tag : * (Git Tag) x.y.z Release CommitID : * https://github.com/apache/skywalking/tree/(Git Commit ID) * Git submodule * skywalking-ui: https://github.com/apache/skywalking-rocketbot-ui/tree/(Git Commit ID) * apm-protocol/apm-network/src/main/proto: https://github.com/apache/skywalking-data-collect-protocol/tree/(Git Commit ID) * oap-server/server-query-plugin/query-graphql-plugin/src/main/resources/query-protocol https://github.com/apache/skywalking-query-protocol/tree/(Git Commit ID) Keys to verify the Release Candidate : * https://dist.apache.org/repos/dist/release/skywalking/KEYS Guide to build the release from source : * https://github.com/apache/skywalking/blob/x.y.z/docs/en/guides/How-to-build.md Voting will start now (xxxx date) and will remain open for at least 72 hours, Request all PMC members to give their vote. [ ] +1 Release this package. [ ] +0 No opinion. [ ] -1 Do not release this package because.... Vote Check All PMC members and committers should check these before vote +1.\n Features test. All artifacts in staging repository are published with .asc, .md5, *sha1 files Source code and distribution package (apache-skywalking-x.y.z-src.tar.gz, apache-skywalking-bin-x.y.z.tar.gz, apache-skywalking-bin-x.y.z.zip) are in https://dist.apache.org/repos/dist/dev/skywalking/x.y.z with .asc, .sha512 LICENSE and NOTICE are in Source code and distribution package. Check shasum -c apache-skywalking-apm-x.y.z-src.tgz.sha512 Check gpg --verify apache-skywalking-apm-x.y.z-src.tgz.asc apache-skywalking-apm-x.y.z-src.tgz Build distribution from source code package (apache-skywalking-x.y.z-src.tar.gz) by following this doc. Check Apache License Header. Run docker run --rm -v $(pwd):/github/workspace apache/skywalking-eyes header check. (No binary in source codes)  Vote result should follow these.\n PMC vote is +1 binding, all others is +1 no binding. In 72 hours, you get at least 3 (+1 binding), and have more +1 than -1. Vote pass.  Publish release  Move source codes tar balls and distributions to https://dist.apache.org/repos/dist/release/skywalking/.  \u0026gt; export SVN_EDITOR=vim \u0026gt; svn mv https://dist.apache.org/repos/dist/dev/skywalking/x.y.z https://dist.apache.org/repos/dist/release/skywalking .... enter your apache password .... Do release in nexus staging repo. Public download source and distribution tar/zip locate in http://www.apache.org/dyn/closer.cgi/skywalking/x.y.z/xxx. We only publish Apache mirror path as release info. Public asc and sha512 locate in https://www.apache.org/dist/skywalking/x.y.z/xxx Public KEYS pointing to https://www.apache.org/dist/skywalking/KEYS Update website download page. http://skywalking.apache.org/downloads/ . Include new download source, distribution, sha512, asc and document links. Links could be found by following above rules(3)-(6). Add a release event on website homepage and event page. Announce the public release with changelog or key features. Send ANNOUNCE email to dev@skywalking.apache.org, announce@apache.org, the sender should use Apache email account.  Mail title: [ANNOUNCE] Apache SkyWalking x.y.z released Mail content: Hi all, Apache SkyWalking Team is glad to announce the first release of Apache SkyWalking x.y.z. SkyWalking: APM (application performance monitor) tool for distributed systems, especially designed for microservices, cloud native and container-based (Docker, Kubernetes, Mesos) architectures. This release contains a number of new features, bug fixes and improvements compared to version a.b.c(last release). The notable changes since x.y.z include: (Highlight key changes) 1. ... 2. ... 3. ... Please refer to the change log for the complete list of changes: https://github.com/apache/skywalking/blob/master/changes/changes-x.y.z.md Apache SkyWalking website: http://skywalking.apache.org/ Downloads: http://skywalking.apache.org/downloads/ Twitter: https://twitter.com/ASFSkyWalking SkyWalking Resources: - GitHub: https://github.com/apache/skywalking - Issue: https://github.com/apache/skywalking/issues - Mailing list: dev@skywalkiing.apache.org - Apache SkyWalking Team ","excerpt":"Apache SkyWalking release guide This document guides every committer to release SkyWalking in Apache …","ref":"/docs/main/v8.4.0/en/guides/how-to-release/","title":"SkyWalking release guide"},{"body":"Skywalking with Kotlin coroutine This Plugin provides an auto instrument support plugin for Kotlin coroutine based on context snapshot.\nDescription SkyWalking provide tracing context propagation inside thread. In order to support Kotlin Coroutine, we provide this additional plugin.\nImplementation principle As we know, Kotlin coroutine switches the execution thread by CoroutineDispatcher.\n Create a snapshot of the current context before dispatch the continuation. Then create a coroutine span after thread switched, mark the span continued with the snapshot. Every new span which created in the new thread will be a child of this coroutine span. So we can link those span together in a tracing. After the original runnable executed, we need to stop the coroutine span for cleaning thread state.  Some screenshots Run without the plugin We run a Kotlin coroutine based gRPC server without this coroutine plugin.\nYou can find, the one call (client -\u0026gt; server1 -\u0026gt; server2) has been split two tracing paths.\n Server1 without exit span and server2 tracing path.  Server2 tracing path.   Run with the plugin Without changing codes manually, just install the plugin. We can find the spans be connected together. We can get all info of one client call.\n","excerpt":"Skywalking with Kotlin coroutine This Plugin provides an auto instrument support plugin for Kotlin …","ref":"/docs/main/v8.4.0/en/setup/service-agent/java-agent/agent-optional-plugins/kotlin-coroutine-plugin/","title":"Skywalking with Kotlin coroutine"},{"body":"Slow Database Statement Slow Database statements are significant important to find out the bottleneck of the system, which relied on Database.\nSlow DB statements are based on sampling, right now, the core samples top 50 slowest in every 10 minutes. But duration of those statements must be slower than threshold.\nThe setting format is following, unit is millisecond.\n database-type:thresholdValue,database-type2:thresholdValue2\n Default setting is default:200,mongodb:100. Reserved DB type is default, which be as default threshold for all database types, except set explicitly.\nNotice, the threshold should not be too small, like 1ms. Functionally, it works, but would cost OAP performance issue, if your system statement access time are mostly more than 1ms.\n","excerpt":"Slow Database Statement Slow Database statements are significant important to find out the …","ref":"/docs/main/v8.4.0/en/setup/backend/slow-db-statement/","title":"Slow Database Statement"},{"body":"Source and Scope extension for new metrics From OAL scope introduction, you should already have understood what the scope is. At here, as you want to do more extension, you need understand deeper, which is the Source.\nSource and Scope are binding concepts. Scope declare the id(int) and name, Source declare the attributes. Please follow these steps to create a new Source and Scope.\n In the OAP core module, it provide SourceReceiver internal service.  public interface SourceReceiver extends Service { void receive(Source source); } All analysis data must be a org.apache.skywalking.oap.server.core.source.Source sub class, tagged by @SourceType annotation, and in org.apache.skywalking package. Then it could be supported by OAL script and OAP core.  Such as existed source, Service.\n@ScopeDeclaration(id = SERVICE_INSTANCE, name = \u0026#34;ServiceInstance\u0026#34;, catalog = SERVICE_INSTANCE_CATALOG_NAME) @ScopeDefaultColumn.VirtualColumnDefinition(fieldName = \u0026#34;entityId\u0026#34;, columnName = \u0026#34;entity_id\u0026#34;, isID = true, type = String.class) public class ServiceInstance extends Source { @Override public int scope() { return DefaultScopeDefine.SERVICE_INSTANCE; } @Override public String getEntityId() { return String.valueOf(id); } @Getter @Setter private int id; @Getter @Setter @ScopeDefaultColumn.DefinedByField(columnName = \u0026#34;service_id\u0026#34;) private int serviceId; @Getter @Setter private String name; @Getter @Setter private String serviceName; @Getter @Setter private String endpointName; @Getter @Setter private int latency; @Getter @Setter private boolean status; @Getter @Setter private int responseCode; @Getter @Setter private RequestType type; }  The scope() method in Source, returns an ID, which is not a random number. This ID need to be declared through @ScopeDeclaration annotation too. The ID in @ScopeDeclaration and ID in scope() method should be same for this Source.\n  The String getEntityId() method in Source, requests the return value representing unique entity which the scope related. Such as, in this Service scope, the id is service id, representing a particular service, like Order service. This value is used in OAL group mechanism.\n  @ScopeDefaultColumn.VirtualColumnDefinition and @ScopeDefaultColumn.DefinedByField are required, all declared fields(virtual/byField) are going to be pushed into persistent entity, mapping to such as ElasticSearch index and Database table column. Such as, include entity id mostly, and service id for endpoint and service instance level scope. Take a reference to all existing scopes. All these fields are detected by OAL Runtime, and required in query stage.\n  Add scope name as keyword to oal grammar definition file, OALLexer.g4, which is at antlr4 folder of generate-tool-grammar module.\n  Add scope name keyword as source in parser definition file, OALParser.g4, which is at same fold of OALLexer.g4.\n   After you done all of these, you could build a receiver, which do\n Get the original data of the metrics, Build the source, send into SourceReceiver. Write your whole OAL scripts. Repackage the project.  ","excerpt":"Source and Scope extension for new metrics From OAL scope introduction, you should already have …","ref":"/docs/main/v8.4.0/en/guides/source-extension/","title":"Source and Scope extension for new metrics"},{"body":"Spring annotation plugin This plugin allows to trace all methods of beans in Spring context, which are annotated with @Bean, @Service, @Component and @Repository.\n Why does this plugin optional?  Tracing all methods in Spring context all creates a lot of spans, which also spend more CPU, memory and network. Of course you want to have spans as many as possible, but please make sure your system payload can support these.\n","excerpt":"Spring annotation plugin This plugin allows to trace all methods of beans in Spring context, which …","ref":"/docs/main/v8.4.0/en/setup/service-agent/java-agent/agent-optional-plugins/spring-annotation-plugin/","title":"Spring annotation plugin"},{"body":"Spring sleuth setup Spring Sleuth provides Spring Boot auto-configuration for distributed tracing. Skywalking integrates it\u0026rsquo;s micrometer part, and it can send metrics to the Skywalking Meter System.\nSet up agent  Add the Micrometer and Skywalking meter registry dependency into project pom.xml file. Also you could found more detail at Toolkit micrometer.  \u0026lt;dependency\u0026gt; \u0026lt;groupId\u0026gt;org.springframework.boot\u0026lt;/groupId\u0026gt; \u0026lt;artifactId\u0026gt;spring-boot-starter-actuator\u0026lt;/artifactId\u0026gt; \u0026lt;/dependency\u0026gt; \u0026lt;dependency\u0026gt; \u0026lt;groupId\u0026gt;org.apache.skywalking\u0026lt;/groupId\u0026gt; \u0026lt;artifactId\u0026gt;apm-toolkit-micrometer-registry\u0026lt;/artifactId\u0026gt; \u0026lt;version\u0026gt;${skywalking.version}\u0026lt;/version\u0026gt; \u0026lt;/dependency\u0026gt; Create the Skywalking meter resgitry into spring bean management.  @Bean SkywalkingMeterRegistry skywalkingMeterRegistry() { // Add rate configs If you need, otherwise using none args construct  SkywalkingConfig config = new SkywalkingConfig(Arrays.asList(\u0026#34;\u0026#34;)); return new SkywalkingMeterRegistry(config); } Set up backend receiver  Make sure enable meter receiver in the applicaiton.yml.  receiver-meter: selector: ${SW_RECEIVER_METER:default} default: Configure the meter config file, It already has the spring sleuth meter config. If you also has some customized meter at the agent side, please read meter document to configure meter.  Add UI dashboard   Open the dashboard view, click edit button to edit the templates.\n  Create a new template. Template type: Standard -\u0026gt; Template Configuration: Spring -\u0026gt; Input the Template Name.\n  Click view button, Finally get the spring sleuth dashboard.\n  Supported meter Supported 3 types information: Application, System, JVM.\n Application: HTTP request count and duration, JDBC max/idle/active connection count, Tomcat session active/reject count. System: CPU system/process usage, OS System load, OS Process file count. JVM: GC pause count and duration, Memory max/used/committed size, Thread peak/live/daemon count, Classes loaded/unloaded count.  ","excerpt":"Spring sleuth setup Spring Sleuth provides Spring Boot auto-configuration for distributed tracing. …","ref":"/docs/main/v8.4.0/en/setup/backend/spring-sleuth-setup/","title":"Spring sleuth setup"},{"body":"Start up mode In different deployment tool, such as k8s, you may need different startup mode. We provide another two optional startup modes.\nDefault mode Default mode. Do initialization works if necessary, start listen and provide service.\nRun /bin/oapService.sh(.bat) to start in this mode. Also when use startup.sh(.bat) to start.\nInit mode In this mode, oap server starts up to do initialization works, then exit. You could use this mode to init your storage, such as ElasticSearch indexes, MySQL and TiDB tables, and init data.\nRun /bin/oapServiceInit.sh(.bat) to start in this mode.\nNo-init mode In this mode, oap server starts up without initialization works, but it waits for ElasticSearch indexes, MySQL and TiDB tables existed, start listen and provide service. Meaning, this oap server expect another oap server to do the initialization.\nRun /bin/oapServiceNoInit.sh(.bat) to start in this mode.\n","excerpt":"Start up mode In different deployment tool, such as k8s, you may need different startup mode. We …","ref":"/docs/main/v8.4.0/en/setup/backend/backend-start-up-mode/","title":"Start up mode"},{"body":"Support custom enhance Here is an optional plugin apm-customize-enhance-plugin\nIntroduce SkyWalking has provided Java agent plugin development guide to help developers to build new plugin.\nThis plugin is not designed for replacement but for user convenience. The behaviour is very similar with @Trace toolkit, but without code change requirement, and more powerful, such as provide tag and log.\nHow to configure Implementing enhancements to custom classes requires two steps.\n Active the plugin, move the optional-plugins/apm-customize-enhance-plugin.jar to plugin/apm-customize-enhance-plugin.jar. Set plugin.customize.enhance_file in agent.config, which targets to rule file, such as /absolute/path/to/customize_enhance.xml. Set enhancement rules in customize_enhance.xml. \u0026lt;?xml version=\u0026#34;1.0\u0026#34; encoding=\u0026#34;UTF-8\u0026#34;?\u0026gt; \u0026lt;enhanced\u0026gt; \u0026lt;class class_name=\u0026#34;test.apache.skywalking.testcase.customize.service.TestService1\u0026#34;\u0026gt; \u0026lt;method method=\u0026#34;staticMethod()\u0026#34; operation_name=\u0026#34;/is_static_method\u0026#34; static=\u0026#34;true\u0026#34;/\u0026gt; \u0026lt;method method=\u0026#34;staticMethod(java.lang.String,int.class,java.util.Map,java.util.List,[Ljava.lang.Object;)\u0026#34; operation_name=\u0026#34;/is_static_method_args\u0026#34; static=\u0026#34;true\u0026#34;\u0026gt; \u0026lt;operation_name_suffix\u0026gt;arg[0]\u0026lt;/operation_name_suffix\u0026gt; \u0026lt;operation_name_suffix\u0026gt;arg[1]\u0026lt;/operation_name_suffix\u0026gt; \u0026lt;operation_name_suffix\u0026gt;arg[3].[0]\u0026lt;/operation_name_suffix\u0026gt; \u0026lt;tag key=\u0026#34;tag_1\u0026#34;\u0026gt;arg[2].[\u0026#39;k1\u0026#39;]\u0026lt;/tag\u0026gt; \u0026lt;tag key=\u0026#34;tag_2\u0026#34;\u0026gt;arg[4].[1]\u0026lt;/tag\u0026gt; \u0026lt;log key=\u0026#34;log_1\u0026#34;\u0026gt;arg[4].[2]\u0026lt;/log\u0026gt; \u0026lt;/method\u0026gt; \u0026lt;method method=\u0026#34;method()\u0026#34; static=\u0026#34;false\u0026#34;/\u0026gt; \u0026lt;method method=\u0026#34;method(java.lang.String,int.class)\u0026#34; operation_name=\u0026#34;/method_2\u0026#34; static=\u0026#34;false\u0026#34;\u0026gt; \u0026lt;operation_name_suffix\u0026gt;arg[0]\u0026lt;/operation_name_suffix\u0026gt; \u0026lt;tag key=\u0026#34;tag_1\u0026#34;\u0026gt;arg[0]\u0026lt;/tag\u0026gt; \u0026lt;log key=\u0026#34;log_1\u0026#34;\u0026gt;arg[1]\u0026lt;/log\u0026gt; \u0026lt;/method\u0026gt; \u0026lt;method method=\u0026#34;method(test.apache.skywalking.testcase.customize.model.Model0,java.lang.String,int.class)\u0026#34; operation_name=\u0026#34;/method_3\u0026#34; static=\u0026#34;false\u0026#34;\u0026gt; \u0026lt;operation_name_suffix\u0026gt;arg[0].id\u0026lt;/operation_name_suffix\u0026gt; \u0026lt;operation_name_suffix\u0026gt;arg[0].model1.name\u0026lt;/operation_name_suffix\u0026gt; \u0026lt;operation_name_suffix\u0026gt;arg[0].model1.getId()\u0026lt;/operation_name_suffix\u0026gt; \u0026lt;tag key=\u0026#34;tag_os\u0026#34;\u0026gt;arg[0].os.[1]\u0026lt;/tag\u0026gt; \u0026lt;log key=\u0026#34;log_map\u0026#34;\u0026gt;arg[0].getM().[\u0026#39;k1\u0026#39;]\u0026lt;/log\u0026gt; \u0026lt;/method\u0026gt; \u0026lt;/class\u0026gt; \u0026lt;class class_name=\u0026#34;test.apache.skywalking.testcase.customize.service.TestService2\u0026#34;\u0026gt; \u0026lt;method method=\u0026#34;staticMethod(java.lang.String,int.class)\u0026#34; operation_name=\u0026#34;/is_2_static_method\u0026#34; static=\u0026#34;true\u0026#34;\u0026gt; \u0026lt;tag key=\u0026#34;tag_2_1\u0026#34;\u0026gt;arg[0]\u0026lt;/tag\u0026gt; \u0026lt;log key=\u0026#34;log_1_1\u0026#34;\u0026gt;arg[1]\u0026lt;/log\u0026gt; \u0026lt;/method\u0026gt; \u0026lt;method method=\u0026#34;method([Ljava.lang.Object;)\u0026#34; operation_name=\u0026#34;/method_4\u0026#34; static=\u0026#34;false\u0026#34;\u0026gt; \u0026lt;tag key=\u0026#34;tag_4_1\u0026#34;\u0026gt;arg[0].[0]\u0026lt;/tag\u0026gt; \u0026lt;/method\u0026gt; \u0026lt;method method=\u0026#34;method(java.util.List,int.class)\u0026#34; operation_name=\u0026#34;/method_5\u0026#34; static=\u0026#34;false\u0026#34;\u0026gt; \u0026lt;tag key=\u0026#34;tag_5_1\u0026#34;\u0026gt;arg[0].[0]\u0026lt;/tag\u0026gt; \u0026lt;log key=\u0026#34;log_5_1\u0026#34;\u0026gt;arg[1]\u0026lt;/log\u0026gt; \u0026lt;/method\u0026gt; \u0026lt;/class\u0026gt; \u0026lt;/enhanced\u0026gt; ``\n   Explanation of the configuration in the file    configuration explanation     class_name The enhanced class   method The interceptor method of the class   operation_name If fill it out, will use it instead of the default operation_name.   operation_name_suffix What it means adding dynamic data after the operation_name.   static Is this method static.   tag Will add a tag in local span. The value of key needs to be represented on the XML node.   log Will add a log in local span. The value of key needs to be represented on the XML node.   arg[x] What it means is to get the input arguments. such as arg[0] is means get first arguments.   .[x] When the parsing object is Array or List, you can use it to get the object at the specified index.   .[\u0026lsquo;key\u0026rsquo;] When the parsing object is Map, you can get the map \u0026lsquo;key\u0026rsquo; through it.      ","excerpt":"Support custom enhance Here is an optional plugin apm-customize-enhance-plugin\nIntroduce SkyWalking …","ref":"/docs/main/v8.4.0/en/setup/service-agent/java-agent/customize-enhance-trace/","title":"Support custom enhance"},{"body":"Support custom trace ignore Here is an optional plugin apm-trace-ignore-plugin\nNotice: Sampling still works when the trace ignores plug-in activation.\nIntroduce  The purpose of this plugin is to filter endpoint which are expected to be ignored by the tracing system. You can setup multiple URL path patterns, The endpoints match these patterns wouldn\u0026rsquo;t be traced. The current matching rules follow Ant Path match style , like /path/*, /path/**, /path/?. Copy apm-trace-ignore-plugin-x.jar to agent/plugins, restarting the agent can effect the plugin.  How to configure There are two ways to configure ignore patterns. Settings through system env has higher priority.\n Set through the system environment variable,you need to add skywalking.trace.ignore_path to the system variables, the value is the path that you need to ignore, multiple paths should be separated by , Copy/agent/optional-plugins/apm-trace-ignore-plugin/apm-trace-ignore-plugin.config to /agent/config/ dir, and add rules to filter traces  trace.ignore_path=/your/path/1/**,/your/path/2/** ","excerpt":"Support custom trace ignore Here is an optional plugin apm-trace-ignore-plugin\nNotice: Sampling …","ref":"/docs/main/v8.4.0/en/setup/service-agent/java-agent/agent-optional-plugins/trace-ignore-plugin/","title":"Support custom trace ignore"},{"body":"Support gRPC SSL transportation for OAP server For OAP communication we are currently using gRPC, a multi-platform RPC framework that uses protocol buffers for message serialization. The nice part about gRPC is that it promotes the use of SSL/TLS to authenticate and encrypt exchanges. Now OAP supports to enable SSL transportation for gRPC receivers.\nYou can follow below steps to enable this feature\nCreating SSL/TLS Certificates It seems like step one is to generate certificates and key files for encrypting communication. I thought this would be fairly straightforward using openssl from the command line.\nUse this script if you are not familiar with how to generate key files.\nWe need below files:\n server.pem a private RSA key to sign and authenticate the public key. It\u0026rsquo;s either a PKCS#8(PEM) or PKCS#1(DER). server.crt self-signed X.509 public keys for distribution. ca.crt a certificate authority public key for a client to validate the server\u0026rsquo;s certificate.  Config OAP server You can enable gRPC SSL by add following lines to application.yml/core/default.\ngRPCSslEnabled: true gRPCSslKeyPath: /path/to/server.pem gRPCSslCertChainPath: /path/to/server.crt gRPCSslTrustedCAPath: /path/to/ca.crt gRPCSslKeyPath and gRPCSslCertChainPath are loaded by OAP server to encrypt the communication. gRPCSslTrustedCAPath helps gRPC client to verify server certificates in cluster mode.\nWhen new files are in place, they can be load dynamically instead of restarting OAP instance.\nIf you enable sharding-server to ingest data from external, add following lines to application.yml/receiver-sharing-server/default:\ngRPCSslEnabled: true gRPCSslKeyPath: /path/to/server.pem gRPCSslCertChainPath: /path/to/server.crt Because sharding-server only receives data from external, so it doesn\u0026rsquo;t need CA at all.\nIf you port to java agent, refer to TLS.md to config java agent to enable TLS.\n","excerpt":"Support gRPC SSL transportation for OAP server For OAP communication we are currently using gRPC, a …","ref":"/docs/main/v8.4.0/en/setup/backend/grpc-ssl/","title":"Support gRPC SSL transportation for OAP server"},{"body":"Support Transport Layer Security (TLS) Transport Layer Security (TLS) is a very common security way when transport data through Internet. In some use cases, end users report the background:\n Target(under monitoring) applications are in a region, which also named VPC, at the same time, the SkyWalking backend is in another region (VPC).\nBecause of that, security requirement is very obvious.\n Authentication Mode Only support no mutual auth.\n Use this script if you are not familiar with how to generate key files. Find ca.crt, and use it at client side Find server.crt ,server.pem and ca.crt. Use them at server side. Please refer to gRPC SSL for more details.  Open and config TLS Agent config   Place ca.crt into /ca folder in agent package. Notice, /ca is not created in distribution, please create it by yourself.\n  Agent open TLS automatically after the /ca/ca.crt file detected.\n  TLS with no CA mode could be activated by this setting.\n  agent.force_tls=${SW_AGENT_FORCE_TLS:false} ","excerpt":"Support Transport Layer Security (TLS) Transport Layer Security (TLS) is a very common security way …","ref":"/docs/main/v8.4.0/en/setup/service-agent/java-agent/tls/","title":"Support Transport Layer Security (TLS)"},{"body":"Telemetry for backend By default, the telemetry is disabled by setting selector to none, like this\ntelemetry: selector: ${SW_TELEMETRY:none} none: prometheus: host: ${SW_TELEMETRY_PROMETHEUS_HOST:0.0.0.0} port: ${SW_TELEMETRY_PROMETHEUS_PORT:1234} sslEnabled: ${SW_TELEMETRY_PROMETHEUS_SSL_ENABLED:false} sslKeyPath: ${SW_TELEMETRY_PROMETHEUS_SSL_KEY_PATH:\u0026#34;\u0026#34;} sslCertChainPath: ${SW_TELEMETRY_PROMETHEUS_SSL_CERT_CHAIN_PATH:\u0026#34;\u0026#34;} but you can set one of prometheus to enable them, for more information, refer to the details below.\nPrometheus Prometheus is supported as telemetry implementor. By using this, prometheus collects metrics from SkyWalking backend.\nSet prometheus to provider. The endpoint open at http://0.0.0.0:1234/ and http://0.0.0.0:1234/metrics.\ntelemetry: selector: ${SW_TELEMETRY:prometheus} prometheus: Set host and port if needed.\ntelemetry: selector: ${SW_TELEMETRY:prometheus} prometheus: host: 127.0.0.1 port: 1543 Set SSL relevant settings to expose a secure endpoint. Notice private key file and cert chain file could be uploaded once changes are applied to them.\ntelemetry: selector: ${SW_TELEMETRY:prometheus} prometheus: host: 127.0.0.1 port: 1543 sslEnabled: true sslKeyPath: /etc/ssl/key.pem sslCertChainPath: /etc/ssl/cert-chain.pem Grafana Visualization Provide the grafana dashboard settings. Check SkyWalking Telemetry dashboard config.\nSelf Observability SkyWalking supports to collect telemetry data into OAP backend directly. Users could check them out through UI or GraphQL API then.\nAdding following configuration to enable self-observability related modules.\n Setting up prometheus telemetry.  telemetry: selector: ${SW_TELEMETRY:prometheus} prometheus: host: 127.0.0.1 port: 1543 Setting up prometheus fetcher  prometheus-fetcher: selector: ${SW_PROMETHEUS_FETCHER:default} default: enabledRules: ${SW_PROMETHEUS_FETCHER_ENABLED_RULES:\u0026#34;self\u0026#34;} Make sure config/fetcher-prom-rules/self.yaml exists.  Once you deploy an oap-server cluster, the target host should be replaced with a dedicated IP or hostname. For instances, there are three oap server in your cluster, their host is service1, service2 and service3 respectively. You should update each self.yaml to twist target host.\nservice1:\nfetcherInterval: PT15S fetcherTimeout: PT10S metricsPath: /metrics staticConfig: # targets will be labeled as \u0026#34;instance\u0026#34; targets: - service1:1234 labels: service: oap-server ... service2:\nfetcherInterval: PT15S fetcherTimeout: PT10S metricsPath: /metrics staticConfig: # targets will be labeled as \u0026#34;instance\u0026#34; targets: - service2:1234 labels: service: oap-server ... service3:\nfetcherInterval: PT15S fetcherTimeout: PT10S metricsPath: /metrics staticConfig: # targets will be labeled as \u0026#34;instance\u0026#34; targets: - service3:1234 labels: service: oap-server ... ","excerpt":"Telemetry for backend By default, the telemetry is disabled by setting selector to none, like this …","ref":"/docs/main/v8.4.0/en/setup/backend/backend-telemetry/","title":"Telemetry for backend"},{"body":"Thread dump merging mechanism The performance profile is an enhancement feature in the APM system. We are using the thread dump to estimate the method execution time, rather than adding many local spans. In this way, the resource cost would be much less than using distributed tracing to locate slow method. This feature is suitable in the production environment. This document introduces how thread dumps are merged into the final report as a stack tree(s).\nThread analyst Read data and transform Read data from the database and convert it to a data structure in gRPC.\nst=\u0026gt;start: Start e=\u0026gt;end: End op1=\u0026gt;operation: Load data using paging op2=\u0026gt;operation: Transform data using parallel st(right)-\u0026gt;op1(right)-\u0026gt;op2 op2(right)-\u0026gt;e Copy code and paste it into this link to generate flow chart.\n Use the stream to read data by page (50 records per page). Convert data into gRPC data structures in the form of parallel streams. Merge into a list of data.  Data analyze Use the group by and collector modes in the Java parallel stream to group according to the first stack element in the database records, and use the collector to perform data aggregation. Generate a multi-root tree.\nst=\u0026gt;start: Start e=\u0026gt;end: End op1=\u0026gt;operation: Group by first stack element sup=\u0026gt;operation: Generate empty stack tree acc=\u0026gt;operation: Accumulator data to stack tree com=\u0026gt;operation: Combine stack trees fin=\u0026gt;operation: Calculate durations and build result st(right)-\u0026gt;op1-\u0026gt;sup(right)-\u0026gt;acc acc(right)-\u0026gt;com(right)-\u0026gt;fin-\u0026gt;e Copy code and paste it into this link to generate flow chart.\n Group by first stack element: Use the first level element in each stack to group, ensuring that the stacks have the same root node. Generate empty stack tree: Generate multiple top-level empty trees for preparation of the following steps, The reason for generating multiple top-level trees is that original data can be add in parallel without generating locks. Accumulator data to stack tree: Add every thread dump into the generated trees.  Iterate through each element in the thread dump to find if there is any child element with the same code signature and same stack depth in the parent element. If not, then add this element. Keep the dump sequences and timestamps in each nodes from the source.   Combine stack trees: Combine all trees structures into one by using the rules as same as Accumulator.  Use LDR to traversal tree node. Use the Stack data structure to avoid recursive calls, each stack element represents the node that needs to be merged. The task of merging two nodes is to merge the list of children nodes. If they have the same code signature and same parents, save the dump sequences and timestamps in this node. Otherwise, the node needs to be added into the target node as a new child.   Calculate durations and build result: Calculate relevant statistics and generate response.  Use the same traversal node logic as in the Combine stack trees step. Convert to a GraphQL data structure, and put all nodes into a list for subsequent duration calculations. Calculate each node\u0026rsquo;s duration in parallel. For each node, sort the sequences, if there are two continuous sequences, the duration should add the duration of these two seq\u0026rsquo;s timestamp. Calculate each node execution in parallel. For each node, the duration of the current node should minus the time consumed by all children.    Profile data debug Please follow the exporter tool to package profile data. Unzip the profile data and using analyzer main function to run it.\n","excerpt":"Thread dump merging mechanism The performance profile is an enhancement feature in the APM system. …","ref":"/docs/main/v8.4.0/en/guides/backend-profile/","title":"Thread dump merging mechanism"},{"body":"Token Authentication Supported version 7.0.0+\nWhy need token authentication after we have TLS? TLS is about transport security, which makes sure the network can be trusted. The token authentication is about monitoring application data can be trusted.\nToken In current version, Token is considered as a simple string.\nSet Token  Set token in agent.config file  # Authentication active is based on backend setting, see application.yml for more details. agent.authentication = ${SW_AGENT_AUTHENTICATION:xxxx} Set token in application.yml file  ······ receiver-sharing-server: default: authentication: ${SW_AUTHENTICATION:\u0026#34;\u0026#34;} ······ Authentication fails The Skywalking OAP verifies every request from agent, only allows requests whose token matches the one configured in application.yml.\nIf the token is not right, you will see the following log in agent\norg.apache.skywalking.apm.dependencies.io.grpc.StatusRuntimeException: PERMISSION_DENIED FAQ Can I use token authentication instead of TLS? No, you shouldn\u0026rsquo;t. In tech way, you can of course, but token and TLS are used for untrusted network env. In that circumstance, TLS has higher priority than this. Token can be trusted only under TLS protection.Token can be stolen easily if you send it through a non-TLS network.\nDo you support other authentication mechanisms? Such as ak/sk? For now, no. But we appreciate someone contributes this feature.\n","excerpt":"Token Authentication Supported version 7.0.0+\nWhy need token authentication after we have TLS? TLS …","ref":"/docs/main/v8.4.0/en/setup/backend/backend-token-auth/","title":"Token Authentication"},{"body":"Token Authentication Token In current version, Token is considered as a simple string.\nSet Token Set token in agent.config file\n# Authentication active is based on backend setting, see application.yml for more details. agent.authentication = xxxx Meanwhile, open the backend token authentication.\nAuthentication fails The Collector verifies every request from agent, allowed only the token match.\nIf the token is not right, you will see the following log in agent\norg.apache.skywalking.apm.dependencies.io.grpc.StatusRuntimeException: PERMISSION_DENIED FAQ Can I use token authentication instead of TLS? No, you shouldn\u0026rsquo;t. In tech way, you can of course, but token and TLS are used for untrusted network env. In that circumstance, TLS has higher priority than this. Token can be trusted only under TLS protection.Token can be stolen easily if you send it through a non-TLS network.\nDo you support other authentication mechanisms? Such as ak/sk? For now, no. But we appreciate someone contributes this feature.\n","excerpt":"Token Authentication Token In current version, Token is considered as a simple string.\nSet Token Set …","ref":"/docs/main/v8.4.0/en/setup/service-agent/java-agent/token-auth/","title":"Token Authentication"},{"body":"trace cross thread  Dependency the toolkit, such as using maven or gradle  \u0026lt;dependency\u0026gt; \u0026lt;groupId\u0026gt;org.apache.skywalking\u0026lt;/groupId\u0026gt; \u0026lt;artifactId\u0026gt;apm-toolkit-trace\u0026lt;/artifactId\u0026gt; \u0026lt;version\u0026gt;${skywalking.version}\u0026lt;/version\u0026gt; \u0026lt;/dependency\u0026gt;  usage 1.  @TraceCrossThread public static class MyCallable\u0026lt;String\u0026gt; implements Callable\u0026lt;String\u0026gt; { @Override public String call() throws Exception { return null; } } ... ExecutorService executorService = Executors.newFixedThreadPool(1); executorService.submit(new MyCallable());  usage 2.  ExecutorService executorService = Executors.newFixedThreadPool(1); executorService.submit(CallableWrapper.of(new Callable\u0026lt;String\u0026gt;() { @Override public String call() throws Exception { return null; } })); or\nExecutorService executorService = Executors.newFixedThreadPool(1); executorService.execute(RunnableWrapper.of(new Runnable() { @Override public void run() { //your code  } }));  usage 3.  @TraceCrossThread public class MySupplier\u0026lt;String\u0026gt; implements Supplier\u0026lt;String\u0026gt; { @Override public String get() { return null; } } ... CompletableFuture.supplyAsync(new MySupplier\u0026lt;String\u0026gt;()); or\nCompletableFuture.supplyAsync(SupplierWrapper.of(()-\u0026gt;{ return \u0026#34;SupplierWrapper\u0026#34;; })).thenAccept(System.out::println); Sample codes only\n","excerpt":"trace cross thread  Dependency the toolkit, such as using maven or gradle  \u0026lt;dependency\u0026gt; …","ref":"/docs/main/v8.4.0/en/setup/service-agent/java-agent/application-toolkit-trace-cross-thread/","title":"trace cross thread"},{"body":"Trace Data Protocol v3 Trace Data Protocol describes the data format between SkyWalking agent/sniffer and backend.\nOverview Trace data protocol is defined and provided in gRPC format, also implemented in HTTP 1.1\nReport service instance status   Service Instance Properties Service instance has more information than a name, once the agent wants to report this, use ManagementService#reportInstanceProperties service providing a string-key/string-value pair list as the parameter. language of target instance is expected at least.\n  Service Ping Service instance should keep alive with the backend. The agent should set a scheduler using ManagementService#keepAlive service in every minute.\n  Send trace and metrics After you have service id and service instance id, you could send traces and metrics. Now we have\n TraceSegmentReportService#collect for skywalking native trace format JVMMetricReportService#collect for skywalking native jvm format  For trace format, there are some notices\n Segment is a concept in SkyWalking, it should include all span for per request in a single OS process, usually single thread based on language. Span has 3 different groups.    EntrySpan EntrySpan represents a service provider, also the endpoint of server side. As an APM system, we are targeting the application servers. So almost all the services and MQ-consumer are EntrySpan(s).\n  LocalSpan LocalSpan represents a normal Java method, which don\u0026rsquo;t relate with remote service, neither a MQ producer/consumer nor a service(e.g. HTTP service) provider/consumer.\n  ExitSpan ExitSpan represents a client of service or MQ-producer, as named as LeafSpan at early age of SkyWalking. e.g. accessing DB by JDBC, reading Redis/Memcached are cataloged an ExitSpan.\n   Span across thread or process parent info is called Reference. Reference carries trace id, segment id, span id, service name, service instance name, endpoint name and target address used at client side(not required in across thread) of this request in the parent. Follow Cross Process Propagation Headers Protocol v3 to get more details.\n  Span#skipAnalysis could be TRUE, if this span doesn\u0026rsquo;t require backend analysis.\n  ","excerpt":"Trace Data Protocol v3 Trace Data Protocol describes the data format between SkyWalking …","ref":"/docs/main/v8.4.0/en/protocols/trace-data-protocol-v3/","title":"Trace Data Protocol v3"},{"body":"Trace Sampling at server side When we run a distributed tracing system, the trace bring us detailed info, but cost a lot at storage. Open server side trace sampling mechanism, the metrics of service, service instance, endpoint and topology are all accurate as before, but only don\u0026rsquo;t save all the traces into storage.\nOf course, even you open sampling, the traces will be kept as consistent as possible. Consistent means, once the trace segments have been collected and reported by agents, the backend would do their best to don\u0026rsquo;t break the trace. See Recommendation to understand why we called it as consistent as possible and do their best to don't break the trace.\nSet the sample rate In agent-analyzer module, you will find sampleRate setting.\nagent-analyzer: default: ... sampleRate: ${SW_TRACE_SAMPLE_RATE:10000} # The sample rate precision is 1/10000. 10000 means 100% sample in default. forceSampleErrorSegment: ${SW_FORCE_SAMPLE_ERROR_SEGMENT:true} # When sampling mechanism activated, this config would make the error status segment sampled, ignoring the sampling rate. slowTraceSegmentThreshold: ${SW_SLOW_TRACE_SEGMENT_THRESHOLD:-1} # Setting this threshold about the latency would make the slow trace segments sampled if they cost more time, even the sampling mechanism activated. The default value is `-1`, which means would not sample slow traces. Unit, millisecond. sampleRate is for you to set sample rate to this backend. The sample rate precision is 1/10000. 10000 means 100% sample in default.\nforceSampleErrorSegment is for you to save all error segments when sampling mechanism actived. When sampling mechanism activated, this config would make the error status segment sampled, ignoring the sampling rate.\nslowTraceSegmentThreshold is for you to save all slow trace segments when sampling mechanism actived. Setting this threshold about the latency would make the slow trace segments sampled if they cost more time, even the sampling mechanism activated. The default value is -1, which means would not sample slow traces. Unit, millisecond.\nRecommendation You could set different backend instances with different sampleRate values, but we recommend you to set the same.\nWhen you set the rate different, let\u0026rsquo;s say\n Backend-InstanceA.sampleRate = 35 Backend-InstanceB.sampleRate = 55  And we assume the agents reported all trace segments to backend, Then the 35% traces in the global will be collected and saved in storage consistent/complete, with all spans. 20% trace segments, which reported to Backend-InstanceB, will saved in storage, maybe miss some trace segments, because they are reported to Backend-InstanceA and ignored.\nNote When you open sampling, the actual sample rate could be over sampleRate. Because currently, all error/slow segments will be saved, meanwhile, the upstream and downstream may not be sampled. This feature is going to make sure you could have the error/slow stacks and segments, but don\u0026rsquo;t guarantee you would have the whole trace.\nAlso, the side effect would be, if most of the accesses are fail/slow, the sampling rate would be closing to 100%, which could crash the backend or storage clusters.\n","excerpt":"Trace Sampling at server side When we run a distributed tracing system, the trace bring us detailed …","ref":"/docs/main/v8.4.0/en/setup/backend/trace-sampling/","title":"Trace Sampling at server side"},{"body":"Tracing and Tracing based Metrics Analyze Plugins The following plugins provide the distributed tracing capability, and the OAP backend would analyze the topology and metrics based on the tracing data.\n HTTP Server  Tomcat 7 Tomcat 8 Tomcat 9 Spring Boot Web 4.x Spring MVC 3.x, 4.x 5.x with servlet 3.x Nutz Web Framework 1.x Struts2 MVC 2.3.x -\u0026gt; 2.5.x Resin 3 (Optional¹) Resin 4 (Optional¹) Jetty Server 9 Spring WebFlux 5.x (Optional¹) Undertow 1.3.0.Final -\u0026gt; 2.0.27.Final RESTEasy 3.1.0.Final -\u0026gt; 3.7.0.Final Play Framework 2.6.x -\u0026gt; 2.8.x Light4J Microservices Framework 1.6.x -\u0026gt; 2.x Netty SocketIO 1.x   HTTP Client  Feign 9.x Netflix Spring Cloud Feign 1.1.x -\u0026gt; 2.x Okhttp 3.x Apache httpcomponent HttpClient 2.0 -\u0026gt; 3.1, 4.2, 4.3 Spring RestTemplete 4.x Jetty Client 9 Apache httpcomponent AsyncClient 4.x AsyncHttpClient 2.x   HTTP Gateway  Spring Cloud Gateway 2.0.2.RELEASE -\u0026gt; 2.2.x.RELEASE (Optional²)   JDBC  Mysql Driver 5.x, 6.x, 8.x Oracle Driver (Optional¹) H2 Driver 1.3.x -\u0026gt; 1.4.x Sharding-JDBC 1.5.x ShardingSphere 3.0.0, 4.0.0-RC1, 4.0.0, 4.0.1, 4.1.0, 4.1.1 PostgreSQL Driver 8.x, 9.x, 42.x Mariadb Driver 2.x, 1.8 InfluxDB 2.5 -\u0026gt; 2.17 Mssql-Jtds 1.x Mssql-jdbc 6.x -\u0026gt; 8.x   RPC Frameworks  Dubbo 2.5.4 -\u0026gt; 2.6.0 Dubbox 2.8.4 Apache Dubbo 2.7.0 Motan 0.2.x -\u0026gt; 1.1.0 gRPC 1.x Apache ServiceComb Java Chassis 0.1 -\u0026gt; 0.5,1.x SOFARPC 5.4.0 Armeria 0.63.0 -\u0026gt; 0.98.0 Apache Avro 1.7.0 - 1.8.x Finagle 6.44.0 -\u0026gt; 20.1.0 (6.25.0 -\u0026gt; 6.44.0 not tested) Brpc-Java 2.3.7 -\u0026gt; 2.5.3 Thrift 0.10.0 -\u0026gt; 0.12.0 Apache CXF 3.x   MQ  RocketMQ 4.x Kafka 0.11.0.0 -\u0026gt; 2.6.1 Spring-Kafka Spring Kafka Consumer 1.3.x -\u0026gt; 2.3.x (2.0.x and 2.1.x not tested and not recommended by the official document) ActiveMQ 5.10.0 -\u0026gt; 5.15.4 RabbitMQ 5.x Pulsar 2.2.x -\u0026gt; 2.4.x Aliyun ONS 1.x (Optional¹)   NoSQL  Redis  Jedis 2.x Redisson Easy Java Redis client 3.5.2+ Lettuce 5.x   MongoDB Java Driver 2.13-2.14, 3.4.0-3.12.7, 4.0.0-4.1.0 Memcached Client  Spymemcached 2.x Xmemcached 2.x   Elasticsearch  transport-client 5.2.x-5.6.x transport-client 6.7.1-6.8.4 rest-high-level-client 6.7.1-6.8.4 rest-high-level-client 7.0.0-7.5.2   Solr  SolrJ 7.x   Cassandra 3.x  cassandra-java-driver 3.7.0-3.7.2   HBase  hbase-client HTable 1.x     Service Discovery  Netflix Eureka   Distributed Coordination  Zookeeper 3.4.x (Optional² \u0026amp; Except 3.4.4)   Spring Ecosystem  Spring Bean annotations(@Bean, @Service, @Component, @Repository) 3.x and 4.x (Optional²) Spring Core Async SuccessCallback/FailureCallback/ListenableFutureCallback 4.x Spring Transaction 4.x and 5.x (Optional²)   Hystrix: Latency and Fault Tolerance for Distributed Systems 1.4.20 -\u0026gt; 1.5.18 Scheduler  Elastic Job 2.x Apache ShardingSphere-Elasticjob 3.0.0-alpha Spring @Scheduled 3.1+ Quartz Scheduler 2.x (Optional²) XXL Job 2.x   OpenTracing community supported Canal: Alibaba mysql database binlog incremental subscription \u0026amp; consumer components 1.0.25 -\u0026gt; 1.1.2 JSON  GSON 2.8.x (Optional²)   Vert.x Ecosystem  Vert.x Eventbus 3.2+ Vert.x Web 3.x   Thread Schedule Framework  Spring @Async 4.x and 5.x Quasar 0.7.x   Cache  Ehcache 2.x   Kotlin  Coroutine 1.0.1 -\u0026gt; 1.3.x (Optional²)   GraphQL  Graphql 8.0 -\u0026gt; 15.x   Pool  Apache Commons DBCP 2.x   Logging Framework  log4j 2.x log4j2 1.2.x logback 1.2.x    Meter Plugins The meter plugin provides the advanced metrics collections, which are not a part of tracing.\n ¹Due to license incompatibilities/restrictions these plugins are hosted and released in 3rd part repository, go to SkyAPM java plugin extension repository to get these.\n²These plugins affect the performance or must be used under some conditions, from experiences. So only released in /optional-plugins, copy to /plugins in order to make them work.\n","excerpt":"Tracing and Tracing based Metrics Analyze Plugins The following plugins provide the distributed …","ref":"/docs/main/v8.4.0/en/setup/service-agent/java-agent/supported-list/","title":"Tracing and Tracing based Metrics Analyze Plugins"},{"body":"TTL In SkyWalking, there are two types of observability data, besides metadata.\n Record, including trace and alarm. Maybe log in the future. Metric, including such as percentile, heat map, success rate, cpm(rpm) etc.  You have following settings for different types.\n# Set a timeout on metrics data. After the timeout has expired, the metrics data will automatically be deleted. recordDataTTL: ${SW_CORE_RECORD_DATA_TTL:3} # Unit is day metricsDataTTL: ${SW_CORE_METRICS_DATA_TTL:7} # Unit is day  recordDataTTL affects Record data, including tracing and alarm. metricsDataTTL affects all metrics, including service, instance, endpoint metrics and topology map metrics.  ","excerpt":"TTL In SkyWalking, there are two types of observability data, besides metadata.\n Record, including …","ref":"/docs/main/v8.4.0/en/setup/backend/ttl/","title":"TTL"},{"body":"UI SkyWalking UI distribution is already included in our Apache official release.\nStartup Startup script is also in /bin/webappService.sh(.bat). UI runs as an OS Java process, powered-by Zuul.\nSettings Setting file of UI is webapp/webapp.yml in distribution package. It is constituted by three parts.\n Listening port. Backend connect info.  server: port: 8080 collector: path: /graphql ribbon: ReadTimeout: 10000 # Point to all backend\u0026#39;s restHost:restPort, split by ,  listOfServers: 10.2.34.1:12800,10.2.34.2:12800 ","excerpt":"UI SkyWalking UI distribution is already included in our Apache official release.\nStartup Startup …","ref":"/docs/main/v8.4.0/en/setup/backend/ui-setup/","title":"UI"},{"body":"UI Introduction SkyWalking official UI provides the default and powerful visualization capabilities for SkyWalking observing distributed cluster.\nThe latest introduction video could be found on the Youtube\n\nSkyWalking dashboard includes the following part.\n Feature Tab Selector Zone. The key features are list there. The more details will be introduced below. Reload Zone. Control the reload mechanism, including reload periodically or manually. Time Selector Zone. Control the timezone and time range. And a Chinese/English switch button here, default, the UI uses the browser language setting. We also welcome to contribute more languages.  Dashboard Dashboard provide metrics of service, service instance and endpoint. There are a few metrics terms you need to understand\n Throughput CPM , represents calls per minute. Apdex score, Read Apdex in WIKI Response Time Percentile, including p99, p95, p90, p75, p50. Read percentile in WIKI SLA, represents the successful rate. For HTTP, it means the rate of 200 response code.  Service, Instance and Dashboard selector could reload manually rather than reload the whole page. NOTICE, the Reload Zone wouldn\u0026rsquo;t reload these selectors.\nTwo default dashboards are provided to visualize the metrics of service and database.\nUser could click the lock button left aside the Service/Instance/Endpoint Reload button to custom your own dashboard.\nCustom Dashboard Users could customize the dashboard. The default dashboards are provided through the default templates located in /ui-initialized-templates folders.\nThe template file follows this format.\ntemplates: - name: template name # The unique name # The type includes DASHBOARD, TOPOLOGY_INSTANCE, TOPOLOGY_ENDPOINT. # DASHBOARD type templates could have multiple definitions, by using different names. # TOPOLOGY_INSTANCE, TOPOLOGY_ENDPOINT type templates should be defined once,  # as they are used in the topology page only. type: \u0026#34;DASHBOARD\u0026#34; # Custom the dashboard or create a new one on the UI, set the metrics as you like in the edit mode. # Then, you could export this configuration through the page and add it here. configuration: |-[ { \u0026#34;name\u0026#34;:\u0026#34;Spring Sleuth\u0026#34;, \u0026#34;type\u0026#34;:\u0026#34;service\u0026#34;, \u0026#34;children\u0026#34;:[ { \u0026#34;name\u0026#34;:\u0026#34;Sleuth\u0026#34;, \u0026#34;children\u0026#34;: [{ \u0026#34;width\u0026#34;: \u0026#34;3\u0026#34;, \u0026#34;title\u0026#34;: \u0026#34;HTTP Request\u0026#34;, \u0026#34;height\u0026#34;: \u0026#34;200\u0026#34;, \u0026#34;entityType\u0026#34;: \u0026#34;ServiceInstance\u0026#34;, \u0026#34;independentSelector\u0026#34;: false, \u0026#34;metricType\u0026#34;: \u0026#34;REGULAR_VALUE\u0026#34;, \u0026#34;metricName\u0026#34;: \u0026#34;meter_http_server_requests_count\u0026#34;, \u0026#34;queryMetricType\u0026#34;: \u0026#34;readMetricsValues\u0026#34;, \u0026#34;chartType\u0026#34;: \u0026#34;ChartLine\u0026#34;, \u0026#34;unit\u0026#34;: \u0026#34;Count\u0026#34; } ... ] } ] } ] # Activated means this templates added into the UI page automatically. # False means providing a basic template, user needs to add it manually on the page. activated: false # True means wouldn\u0026#39;t show up on the dashboard. Only keeps the definition in the storage. disabled: false NOTE, UI initialized templates would only be initialized if there is no template in the storage has the same name. Check the entity named as ui_template in your storage.\nTopology Topology map shows the relationship among the services and instances with metrics.\n Topology shows the default global topology including all services. Service Selector provides 2 level selectors, service group list and service name list. The group name is separated from the service name if it follows \u0026lt;group name\u0026gt;::\u0026lt;logic name\u0026gt; format. Topology map is available for single group, single service, or global(include all services). Custom Group provides the any sub topology capability of service group. Service Deep Dive opens when you click any service. The honeycomb could do metrics, trace and alarm query of the selected service. Service Relationship Metrics gives the metrics of service RPC interactions and instances of these two services.  Trace Query Trace query is a typical feature as SkyWalking provided distributed agents.\n Trace Segment List is not the trace list. Every trace has several segments belonging to different services. If\nquery by all services or by trace id, different segments with same trace id could be list there. Span is clickable, the detail of each span will pop up on the left side. Trace Views provides 3 typical and different usage views to visualize the trace.  Profile Profile is an interaction feature. It provides the method level performance diagnosis.\nTo start the profile analysis, user need to create the profile task\n Select the specific service. Set the endpoint name. This endpoint name typically is the operation name of the first span. Find this on the trace segment list view. Monitor time could start right now or from any given future time. Monitor duration defines the observation time window to find the suitable request to do performance analysis. Even the profile add a very limited performance impact to the target system, but it is still an additional load. This duration make the impact controllable. Min duration threshold provides a filter mechanism, if a request of the given endpoint response quickly, it wouldn\u0026rsquo;t be profiled. This could make sure, the profiled data is the expected one. Max sampling count gives the max dataset of agent will collect. It helps to reduce the memory and network load. One implicit condition, in any moment, SkyWalking only accept one profile task for each service. Agent could have different settings to control or limit this feature, read document setup for more details. Not all SkyWalking ecosystem agent supports this feature, java agent from 7.0.0 supports this in default.  Once the profile done, the profiled trace segments would show up. And you could request for analysis for any span. Typically, we analysis spans having long self duration, if the span and its children both have long duration, you could choose include children or exclude childrend to set the analysis boundaries.\nAfter choose the right span, and click the analysis button, you will see the stack based analysis result. The slowest methods have been highlighted.\nAdvanced features  Since 7.1.0, the profiled trace collects the HTTP request parameters for Tomcat and SpringMVC Controller automatically.  Log Since 8.3.0, SkyWalking provides log query for the browser monitoring. Use Apache SkyWalking Client JS agent would collect metrics and error logs.\nAlarm Alarm page lists all triggered alarm. Read the backend setup documentation to know how to set up the alarm rule or integrate with 3rd party system.\n","excerpt":"UI Introduction SkyWalking official UI provides the default and powerful visualization capabilities …","ref":"/docs/main/v8.4.0/en/ui/readme/","title":"UI Introduction"},{"body":"Uninstrumented Gateways/Proxies The uninstrumented gateways are not instrumented by SkyWalking agent plugin when they are started, but they can be configured in gateways.yml file or via Dynamic Configuration. The reason why they can\u0026rsquo;t register to backend automatically is that there\u0026rsquo;re no suitable agent plugins, for example, there is no agent plugins for Nginx, haproxy, etc. So in order to visualize the real topology, we provide a way to configure the gateways/proxies manually.\nConfiguration Format The configuration content includes the gateways' names and their instances:\ngateways: - name: proxy0 # the name is not used for now instances: - host: 127.0.0.1 # the host/ip of this gateway instance port: 9099 # the port of this gateway instance, defaults to 80 Note that the host of the instance must be the one that is actually used in client side, for example, if the instance proxyA has 2 IPs, say 192.168.1.110 and 192.168.1.111, both of which delegates the target service, and the client connects to 192.168.1.110, then configuring 192.168.1.111 as the host won\u0026rsquo;t work properly.\n","excerpt":"Uninstrumented Gateways/Proxies The uninstrumented gateways are not instrumented by SkyWalking agent …","ref":"/docs/main/v8.4.0/en/setup/backend/uninstrumented-gateways/","title":"Uninstrumented Gateways/Proxies"},{"body":"Using E2E local remote debugging The E2E remote debugging port of service containers is 5005. If the developer wants to use remote debugging, he needs to add remote debugging parameters to the start service command, and then expose the port 5005.\nFor example, this is the configuration of a container in the skywalking/test/e2e/e2e-test/docker/base-compose.yml. JAVA_OPTS is a preset variable for passing additional parameters in the AOP service startup command, so we only need to add the JAVA remote debugging parameters agentlib:jdwp=transport=dt_socket,server=y,suspend=n,address=5005 to the configuration and exposes the port 5005.\noap: image: skywalking/oap:latest expose: ... - 5005 ... environment: ... JAVA_OPTS: \u0026gt;-... -agentlib:jdwp=transport=dt_socket,server=y,suspend=n,address=5005 ... At last, if the E2E test failed and is retrying, the developer can get the ports mapping in the file skywalking/test/e2e/e2e-test/remote_real_port and selects the host port of the corresponding service for remote debugging. For example,\n#remote_real_port #The remote debugging port on the host is 32783 oap-localhost:32783 #The remote debugging port on the host is 32782 provider-localhost:32782 ","excerpt":"Using E2E local remote debugging The E2E remote debugging port of service containers is 5005. If the …","ref":"/docs/main/v8.4.0/en/guides/e2e-local-remote-debug/","title":"Using E2E local remote debugging"},{"body":"V6 upgrade SkyWalking v6 is widely used in many production environments. Users may wants to upgrade to an old release to new. This is a guidance to tell users how to do that.\nNOTICE, the following ways are not the only ways to do upgrade.\nUse Canary Release Like all applications, SkyWalking could use canary release method to upgrade by following these steps\n Deploy a new cluster by using the latest(or new) version of SkyWalking OAP cluster with new database cluster. Once the target(being monitored) service has chance to upgrade the agent.jar(or just simply reboot), change the collector.backend_service pointing to the new OAP backend, and use/add a new namespace(agent.namespace in Table of Agent Configuration Properties). The namespace will avoid the conflict between different versions. When all target services have been rebooted, the old OAP clusters could be discarded.  Canary Release methods works for any version upgrade.\nOnline Hot Reboot Upgrade The reason we required Canary Release is, SkyWalking agent has cache mechanisms, switching to a new cluster makes the cache unavailable for new OAP cluster. In the 6.5.0+(especially for agent version), we have Agent hot reboot trigger mechanism. By using that, we could do upgrade an easier way, deploy a new cluster by using the latest(or new) version of SkyWalking OAP cluster with new database cluster, and shift the traffic to the new cluster once for all. Based on the mechanism, all agents will go into cool_down mode, then back online. More detail, read the backend setup document.\nNOTICE, as a known bug in 6.4.0, its agent could have re-connection issue, so, even this bot reboot mechanism included in 6.4.0, it may not work in some network scenarios, especially in k8s.\nAgent Compatibility All versions of SkyWalking 6.x(even 7.x) are compatible with each others, so users could only upgrade the OAP servers first. The agent is also enhanced from version to version, so from SkyWalking team\u0026rsquo;s recommendations, upgrade the agent once you have the chance.\n","excerpt":"V6 upgrade SkyWalking v6 is widely used in many production environments. Users may wants to upgrade …","ref":"/docs/main/v8.4.0/en/faq/v6-version-upgrade/","title":"V6 upgrade"},{"body":"V8 upgrade SkyWalking v8 begins to use v3 protocol, so, it is incompatible with previous releases. Users who intend to upgrade in v8 series releases could follow this guidance.\nRegister in v6 and v7 has been removed in v8 for better scaling out performance, please upgrade in the following ways.\n Use a different storage or a new namespace. Also, could consider erasing the whole storage index/table(s) related to SkyWalking. Deploy the whole SkyWalking cluster, and expose in a new network address. If you are using the language agents, upgrade the new agents too, meanwhile, make sure the agent has supported the different language. And set up the backend address to the new SkyWalking OAP cluster.  ","excerpt":"V8 upgrade SkyWalking v8 begins to use v3 protocol, so, it is incompatible with previous releases. …","ref":"/docs/main/v8.4.0/en/faq/v8-version-upgrade/","title":"V8 upgrade"},{"body":"Version 3.x -\u0026gt; 5.0.0-alpha Upgrade FAQs Collector Problem There is no information showing in the UI.\nCause In upgrate from 3.2.6 to 5.0.0, Elasticsearch indexes aren\u0026rsquo;t recreated, because not indexes exist, but aren\u0026rsquo;t compatible with 5.0.0-alpha. When service name registered, the es will create this column by default type string, which is wrong.\nSolution Clean the data folder in ElasticSearch and restart ElasticSearch, collector and your under monitoring application.\n","excerpt":"Version 3.x -\u0026gt; 5.0.0-alpha Upgrade FAQs Collector Problem There is no information showing in the …","ref":"/docs/main/v8.4.0/en/faq/v3-version-upgrade/","title":"Version 3.x -\u003e 5.0.0-alpha Upgrade FAQs"},{"body":"Visualization SkyWalking native UI provides the default visualization solution. It provides observability related graphs about overview, service, service instance, endpoint, trace and alarm, including topology, dependency graph, heatmap, etc.\nAlso, we have already known, many of our users have integrated SkyWalking into their products. If you want to do that too, please use SkyWalking query protocol.\n","excerpt":"Visualization SkyWalking native UI provides the default visualization solution. It provides …","ref":"/docs/main/v8.4.0/en/concepts-and-designs/ui-overview/","title":"Visualization"},{"body":"Welcome Here are SkyWalking 8 official documentation. You\u0026rsquo;re welcome to join us.\nFrom here you can learn all about SkyWalking’s architecture, how to deploy and use SkyWalking, and develop based on SkyWalking contributions guidelines.\nNOTICE, SkyWalking 8 uses brand new tracing APIs, it is incompatible with all previous releases.\n  Concepts and Designs. You\u0026rsquo;ll find the the most important core ideas about SkyWalking. You can learn from here if you want to understand what is going on under our cool features and visualization.\n  Setup. Guides for installing SkyWalking in different scenarios. As a platform, it provides several ways of the observability.\n  UI Introduction. Introduce the UI usage and features.\n  Contributing Guides. Guides are for PMC member, committer or new contributor. Here, you can find how to start contributing.\n  Protocols. Protocols show the communication ways between agents/probes and backend. Anyone interested in uplink telemetry data should definitely read this.\n  FAQs. A manifest of already known setup problems, secondary developments experiments. When you are facing a problem, check here first.\n  In addition, you might find these links interesting:\n  The latest and old releases are all available at Apache SkyWalking release page. The change logs are here.\n  SkyWalking WIKI hosts the context of some changes and events.\n  You can find the speaking schedules at Conf, online videos and articles about SkyWalking in Community resource catalog.\n  We\u0026rsquo;re always looking for help improving our documentation and codes, so please don’t hesitate to file an issue if you see any problem. Or better yet, submit your own contributions through pull request to help make them better.\n","excerpt":"Welcome Here are SkyWalking 8 official documentation. You\u0026rsquo;re welcome to join us.\nFrom here you …","ref":"/docs/main/v8.4.0/readme/","title":"Welcome"},{"body":"What is VNode? In the trace page, sometimes, people could find there are nodes named VNode as the span name, and there is no attribute for this span.\nVNode is created by the UI itself, rather than reported from the agent or tracing SDK. It represents there are some span(s) missed from the trace data in this query.\nHow does the UI detect the missing span(s)? The UI real check the parent spans and reference segments of all spans, if a parent id(segment id + span id) can\u0026rsquo;t be found, then it creates a VNode automatically.\nHow does this happen? The VNode was introduced, because there are some cases which could cause the trace data are not always completed.\n The agent fail-safe mechanism activated. The SkyWalking agent has the capability to abandon the trace data, if there is agent-\u0026gt;OAP network issue(unconnected, slow network speed), or the performance of the OAP cluster is not enough to process all traces. Some plugins could have bugs, then some segments in the trace never stop correctly, it is hold in the memory.  In these cases, the trace would not exist in the query. Then VNode shows up.\n","excerpt":"What is VNode? In the trace page, sometimes, people could find there are nodes named VNode as the …","ref":"/docs/main/v8.4.0/en/faq/vnode/","title":"What is VNode?"},{"body":"Why can\u0026rsquo;t I see any data in the UI? There are three main reasons no data can be shown by the UI:\n No traces have been sent to the collector. Traces have been sent, but the timezone of your containers is incorrect. Traces are in the collector, but you\u0026rsquo;re not watching the correct timeframe in the UI.  No traces Be sure to check the logs of your agents to see if they are connected to the collector and traces are being sent.\nIncorrect timezone in containers Be sure to check the time in your containers.\nThe UI isn\u0026rsquo;t showing any data Be sure to configure the timeframe shown by the UI.\n","excerpt":"Why can\u0026rsquo;t I see any data in the UI? There are three main reasons no data can be shown by the …","ref":"/docs/main/v8.4.0/en/faq/time-and-timezone/","title":"Why can't I see any data in the UI?"},{"body":"Why doesn\u0026rsquo;t SkyWalking involve MQ in the architecture? People usually ask about these questions when they know SkyWalking at the first time. They think MQ should be better in the performance and supporting high throughput, like the following\nHere are the reasons the SkyWalking\u0026rsquo;s opinions.\nIs MQ a good or right way to communicate with OAP backend? This question comes out when people think about what happens when the OAP cluster is not powerful enough or offline. But I want to ask the questions before answer this.\n Why do you think OAP should be not powerful enough? As it is not, the speed of data analysis wouldn\u0026rsquo;t catch up with producers(agents). Then what is the point of adding new deployment requirement? Maybe you will argue says, the payload is sometimes higher than usual as there is hot business time. But, my question is how much higher? If less than 40%, how many resources will you use for the new MQ cluster? How about moving them to new OAP and ES nodes? If higher than 40%, such as 70%-2x times? Then, I could say, your MQ wastes more resources than it saves. Your MQ would support 2x-3x payload, and with 10%-20% cost in general time. Furthermore, in this case, if the payload/throughput are so high, how long the OAP cluster could catch up. I would say never before it catches up, the next hot time event is coming.  Besides all this analysis, why do you want the traces still 100%, as you are costing so many resources? Better than this, we could consider adding a better dynamic trace sampling mechanism at the backend, when throughput goes over the threshold, active the sampling rate to 100%-\u0026gt;10% step by step, which means you could get the OAP and ES 3 times more powerful than usual, just ignore the traces at hot time.\nIs MQ transport acceptable even there are several side effects? Even MQ transport is not recommended from the production perspective, SkyWalking still has optional plugins named kafka-reporter and kafka-fetcher for this feature since 8.1.0.\nHow about MQ metrics data exporter? I would say, it is already available there. Exporter module with gRPC default mechanism is there. It is easy to provide a new implementor of that module.\n","excerpt":"Why doesn\u0026rsquo;t SkyWalking involve MQ in the architecture? People usually ask about these …","ref":"/docs/main/v8.4.0/en/faq/why_mq_not_involved/","title":"Why doesn't SkyWalking involve MQ in the architecture?"},{"body":"Why metrics indexes in Hour and Day precisions stop update after upgrade to 7.x? This is an expected case when 6.x-\u0026gt;7.x upgrade. Read Downsampling Data Packing feature of the ElasticSearch storage.\nThe users could simply delete all expired *-day_xxxxx and *-hour_xxxxx(xxxxx is a timestamp) indexes. SkyWalking is using metrics name-xxxxx and metrics name-month_xxxxx indexes only.\n","excerpt":"Why metrics indexes in Hour and Day precisions stop update after upgrade to 7.x? This is an expected …","ref":"/docs/main/v8.4.0/en/faq/hour-day-metrics-stopping/","title":"Why metrics indexes in Hour and Day precisions stop update after upgrade to 7.x?"},{"body":"Work with Istio Instructions for transport Istio\u0026rsquo;s metrics to the SkyWalking OAP server.\nPrerequisites Istio should be installed in the Kubernetes cluster. Follow Istio getting start to finish it.\nDeploy SkyWalking backend Follow the deploying backend in Kubernetes to install the OAP server in the kubernetes cluster. Refer to OpenTelemetry receiver to ingest metrics. otel-receiver defaults to be inactive. Set env var SW_OTEL_RECEIVER to default to enable it.\nDeploy OpenTelemetry collector OpenTelemetry collector is the location Istio telemetry sends metrics, then processing and sending them to SkyWalking backend.\nFollowing the Getting Started to deploy this collector. There are several components available in the collector, and they could be combined for different scenarios. For the sake of brevity, we use the Prometheus receiver to retrieve metrics from Istio control and data plane, then send them to SkyWalking by OpenCensus exporter.\nPrometheus receiver Refer to Prometheus Receiver to set up this receiver. you could find more configuration details in Prometheus Integration of Istio to figure out how to direct Prometheus receiver to query Istio metrics.\nSkyWalking supports receiving multi-cluster metrics in a single OAP cluster. A cluster label should be appended to every metric fetched by this receiver even there\u0026rsquo;s only a single cluster needed to be collected. You could leverage relabel to add it like below:\nrelabel_configs: - source_labels: [] target_label: cluster replacement: \u0026lt;cluster name\u0026gt; or opt to Resource Processor:\nprocessors: resource: attributes: - key: cluster value: \u0026quot;\u0026lt;cluster name\u0026gt;\u0026quot; action: upsert Notice, if you try the sample of istio Prometheus Kubernetes configuration, the issues described here might block you. Try to use the solution indicated in this issue if it\u0026rsquo;s not fixed.\nOpenCensus exporter Follow OpenCensus exporter configuration to set up a connection between OpenTelemetry collector and OAP cluster. endpoint is the address of OAP gRPC service.\nObserve Istio Open Istio Dashboard in SkyWaling UI by clicking Dashboard -\u0026gt; Istio, then you\u0026rsquo;re able to view charts and diagrams generated by Istio metrics. You also could view them by swctl and set up alarm rules based on them.\nNOTICE, if you want metrics of Istio managed services, including topology among them, we recommend you to consider our ALS solution\n","excerpt":"Work with Istio Instructions for transport Istio\u0026rsquo;s metrics to the SkyWalking OAP server. …","ref":"/docs/main/v8.4.0/en/setup/istio/readme/","title":"Work with Istio"},{"body":"Advanced deployment OAP servers inter communicate with each other in a cluster environment. In the cluster mode, you could run in different roles.\n Mixed(default) Receiver Aggregator  In some time, users want to deploy cluster nodes with explicit role. Then could use this.\nMixed Default role, the OAP should take responsibilities of\n Receive agent traces or metrics. Do L1 aggregation Internal communication(send/receive) Do L2 aggregation Persistence Alarm  Receiver The OAP should take responsibilities of\n Receive agent traces or metrics. Do L1 aggregation Internal communication(send)  Aggregator The OAP should take responsibilities of\n Internal communication(receive) Do L2 aggregation Persistence Alarm   These roles are designed for complex deployment requirements based on security and network policy.\nKubernetes If you are using our native Kubernetes coordinator, the labelSelector setting is used for Aggregator choose rules. Choose the right OAP deployment based on your requirements.\n","excerpt":"Advanced deployment OAP servers inter communicate with each other in a cluster environment. In the …","ref":"/docs/main/v8.3.0/en/setup/backend/advanced-deployment/","title":"Advanced deployment"},{"body":"Alarm Alarm core is driven by a collection of rules, which are defined in config/alarm-settings.yml. There are three parts in alarm rule definition.\n Alarm rules. They define how metrics alarm should be triggered, what conditions should be considered. Webhooks. The list of web service endpoint, which should be called after the alarm is triggered. gRPCHook. The host and port of remote gRPC method, which should be called after the alarm is triggered.  Entity name Define the relation between scope and entity name.\n Service: Service name Instance: {Instance name} of {Service name} Endpoint: {Endpoint name} in {Service name} Database: Database service name Service Relation: {Source service name} to {Dest service name} Instance Relation: {Source instance name} of {Source service name} to {Dest instance name} of {Dest service name} Endpoint Relation: {Source endpoint name} in {Source Service name} to {Dest endpoint name} in {Dest service name}  Rules There are two types of rules, individual rule and composite rule, composite rule is the combination of individual rules\nIndividual rules Alarm rule is constituted by following keys\n Rule name. Unique name, show in alarm message. Must end with _rule. Metrics name. A.K.A. metrics name in oal script. Only long, double, int types are supported. See List of all potential metrics name. Include names. The following entity names are included in this rule. Please follow Entity name define. Exclude names. The following entity names are excluded in this rule. Please follow Entity name define. Include names regex. Provide a regex to include the entity names. If both setting the include name list and include name regex, both rules will take effect. Exclude names regex. Provide a regex to exclude the exclude names. If both setting the exclude name list and exclude name regex, both rules will take effect. Include labels. The following labels of the metric are included in this rule. Exclude labels. The following labels of the metric are excluded in this rule. Include labels regex. Provide a regex to include labels. If both setting the include label list and include label regex, both rules will take effect. Exclude labels regex. Provide a regex to exclude labels. If both setting the exclude label list and exclude label regex, both rules will take effect.  The settings of labels is required by meter-system which intends to store metrics from label-system platform, just like Prometheus, Micrometer, etc. The function supports the above four settings should implement LabeledValueHolder.\n Threshold. The target value. For multiple values metrics, such as percentile, the threshold is an array. Described like value1, value2, value3, value4, value5. Each value could the threshold for each value of the metrics. Set the value to - if don\u0026rsquo;t want to trigger alarm by this or some of the values.\nSuch as in percentile, value1 is threshold of P50, and -, -, value3, value4, value5 means, there is no threshold for P50 and P75 in percentile alarm rule. OP. Operator, support \u0026gt;, \u0026gt;=, \u0026lt;, \u0026lt;=, =. Welcome to contribute all OPs. Period. How long should the alarm rule should be checked. This is a time window, which goes with the backend deployment env time. Count. In the period window, if the number of values over threshold(by OP), reaches count, alarm should send. Only as condition. Specify if the rule can send notification or just as an condition of composite rule. Silence period. After alarm is triggered in Time-N, then keep silence in the TN -\u0026gt; TN + period. By default, it is as same as Period, which means in a period, same alarm(same ID in same metrics name) will be trigger once.  Composite rules NOTE. Composite rules only work for alarm rules targeting the same entity level, such as alarm rules of the service level. For example, service_percent_rule \u0026amp;\u0026amp; service_resp_time_percentile_rule. You shouldn\u0026rsquo;t compose alarm rules of different entity levels. such as one alarm rule of the service metrics with another rule of the endpoint metrics.\nComposite rule is constituted by the following keys\n Rule name. Unique name, show in alarm message. Must end with _rule. Expression. Specify how to compose rules, support \u0026amp;\u0026amp;, ||, (). Message. Specify the notification message when rule triggered.  rules: # Rule unique name, must be ended with `_rule`. endpoint_percent_rule: # Metrics value need to be long, double or int metrics-name: endpoint_percent threshold: 75 op: \u0026lt; # The length of time to evaluate the metrics period: 10 # How many times after the metrics match the condition, will trigger alarm count: 3 # How many times of checks, the alarm keeps silence after alarm triggered, default as same as period. silence-period: 10 # Specify if the rule can send notification or just as an condition of composite rule only-as-condition: false service_percent_rule: metrics-name: service_percent # [Optional] Default, match all services in this metrics include-names: - service_a - service_b exclude-names: - service_c # Single value metrics threshold. threshold: 85 op: \u0026lt; period: 10 count: 4 only-as-condition: false service_resp_time_percentile_rule: # Metrics value need to be long, double or int metrics-name: service_percentile op: \u0026#34;\u0026gt;\u0026#34; # Multiple value metrics threshold. Thresholds for P50, P75, P90, P95, P99. threshold: 1000,1000,1000,1000,1000 period: 10 count: 3 silence-period: 5 message: Percentile response time of service {name} alarm in 3 minutes of last 10 minutes, due to more than one condition of p50 \u0026gt; 1000, p75 \u0026gt; 1000, p90 \u0026gt; 1000, p95 \u0026gt; 1000, p99 \u0026gt; 1000 only-as-condition: false meter_service_status_code_rule: metrics-name: meter_status_code exclude-labels: - \u0026#34;200\u0026#34; op: \u0026#34;\u0026gt;\u0026#34; threshold: 10 period: 10 count: 3 silence-period: 5 message: The request number of entity {name} non-200 status is more than expected. only-as-condition: false composite-rules: comp_rule: # Must satisfied percent rule and resp time rule  expression: service_percent_rule \u0026amp;\u0026amp; service_resp_time_percentile_rule message: Service {name} successful rate is less than 80% and P50 of response time is over 1000ms Default alarm rules We provided a default alarm-setting.yml in our distribution only for convenience, which including following rules\n Service average response time over 1s in last 3 minutes. Service success rate lower than 80% in last 2 minutes. Percentile of service response time is over 1s in last 3 minutes Service Instance average response time over 1s in last 2 minutes, and the instance name matches the regex. Endpoint average response time over 1s in last 2 minutes. Database access average response time over 1s in last 2 minutes. Endpoint relation average response time over 1s in last 2 minutes.  List of all potential metrics name The metrics names are defined in official OAL scripts, right now metrics from Service, Service Instance, Endpoint, Service Relation, Service Instance Relation, Endpoint Relation scopes could be used in Alarm, and the Database access same with Service scope.\nSubmit issue or pull request if you want to support any other scope in alarm.\nWebhook Webhook requires the peer is a web container. The alarm message will send through HTTP post by application/json content type. The JSON format is based on List\u0026lt;org.apache.skywalking.oap.server.core.alarm.AlarmMessage\u0026gt; with following key information.\n scopeId, scope. All scopes are defined in org.apache.skywalking.oap.server.core.source.DefaultScopeDefine. name. Target scope entity name. Please follow Entity name define. id0. The ID of the scope entity matched the name. When using relation scope, it is the source entity ID. id1. When using relation scope, it will be the dest entity ID. Otherwise, it is empty. ruleName. The rule name you configured in alarm-settings.yml. alarmMessage. Alarm text message. startTime. Alarm time measured in milliseconds, between the current time and midnight, January 1, 1970 UTC.  Example as following\n[{ \u0026#34;scopeId\u0026#34;: 1, \u0026#34;scope\u0026#34;: \u0026#34;SERVICE\u0026#34;, \u0026#34;name\u0026#34;: \u0026#34;serviceA\u0026#34;, \u0026#34;id0\u0026#34;: \u0026#34;12\u0026#34;, \u0026#34;id1\u0026#34;: \u0026#34;\u0026#34;, \u0026#34;ruleName\u0026#34;: \u0026#34;service_resp_time_rule\u0026#34;, \u0026#34;alarmMessage\u0026#34;: \u0026#34;alarmMessage xxxx\u0026#34;, \u0026#34;startTime\u0026#34;: 1560524171000 }, { \u0026#34;scopeId\u0026#34;: 1, \u0026#34;scope\u0026#34;: \u0026#34;SERVICE\u0026#34;, \u0026#34;name\u0026#34;: \u0026#34;serviceB\u0026#34;, \u0026#34;id0\u0026#34;: \u0026#34;23\u0026#34;, \u0026#34;id1\u0026#34;: \u0026#34;\u0026#34;, \u0026#34;ruleName\u0026#34;: \u0026#34;service_resp_time_rule\u0026#34;, \u0026#34;alarmMessage\u0026#34;: \u0026#34;alarmMessage yyy\u0026#34;, \u0026#34;startTime\u0026#34;: 1560524171000 }] gRPCHook The alarm message will send through remote gRPC method by Protobuf content type. The message format with following key information which are defined in oap-server/server-alarm-plugin/src/main/proto/alarm-hook.proto.\nPart of protocol looks as following:\nmessage AlarmMessage { int64 scopeId = 1; string scope = 2; string name = 3; string id0 = 4; string id1 = 5; string ruleName = 6; string alarmMessage = 7; int64 startTime = 8;}Slack Chat Hook To do this you need to follow the Getting Started with Incoming Webhooks guide and create new Webhooks.\nThe alarm message will send through HTTP post by application/json content type if you configured Slack Incoming Webhooks as following:\nslackHooks: textTemplate: |-{ \u0026#34;type\u0026#34;: \u0026#34;section\u0026#34;, \u0026#34;text\u0026#34;: { \u0026#34;type\u0026#34;: \u0026#34;mrkdwn\u0026#34;, \u0026#34;text\u0026#34;: \u0026#34;:alarm_clock: *Apache Skywalking Alarm* \\n **%s**.\u0026#34; } } webhooks: - https://hooks.slack.com/services/x/y/z WeChat Hook Note, only WeCom(WeChat Company Edition) supports webhook. To use the WeChat webhook you need to follow the Wechat Webhooks guide. The alarm message would send through HTTP post by application/json content type after you set up Wechat Webhooks as following:\nwechatHooks: textTemplate: |-{ \u0026#34;msgtype\u0026#34;: \u0026#34;text\u0026#34;, \u0026#34;text\u0026#34;: { \u0026#34;content\u0026#34;: \u0026#34;Apache SkyWalking Alarm: \\n %s.\u0026#34; } } webhooks: - https://qyapi.weixin.qq.com/cgi-bin/webhook/send?key=dummy_key Dingtalk Hook To do this you need to follow the Dingtalk Webhooks guide and create new Webhooks. For security issue, you can config optional secret for individual webhook url. The alarm message will send through HTTP post by application/json content type if you configured Dingtalk Webhooks as following:\ndingtalkHooks: textTemplate: |-{ \u0026#34;msgtype\u0026#34;: \u0026#34;text\u0026#34;, \u0026#34;text\u0026#34;: { \u0026#34;content\u0026#34;: \u0026#34;Apache SkyWalking Alarm: \\n %s.\u0026#34; } } webhooks: - url: https://oapi.dingtalk.com/robot/send?access_token=dummy_token secret: dummysecret Update the settings dynamically Since 6.5.0, the alarm settings can be updated dynamically at runtime by Dynamic Configuration, which will override the settings in alarm-settings.yml.\nIn order to determine that whether an alarm rule is triggered or not, SkyWalking needs to cache the metrics of a time window for each alarm rule, if any attribute (metrics-name, op, threshold, period, count, etc.) of a rule is changed, the sliding window will be destroyed and re-created, causing the alarm of this specific rule to restart again.\n","excerpt":"Alarm Alarm core is driven by a collection of rules, which are defined in config/alarm-settings.yml. …","ref":"/docs/main/v8.3.0/en/setup/backend/backend-alarm/","title":"Alarm"},{"body":"Apache SkyWalking committer SkyWalking Project Management Committee(PMC) takes the responsibilities to assess the contributions of candidates.\nIn the SkyWalking, like many Apache projects, we treat contributions including, but not limited to, code contributions. Such as writing blog, guiding new users, give public speak, prompting project in various ways, are all treated as significant contributions.\nCommitter New committer nomination In the SkyWalking, new committer nomination could only be started by existing PMC members officially. The new contributor could contact any existing PMC member if he/she feels he/she is qualified. Talk with the PMC member, if some members agree, they could start the process.\nThe following steps are recommended, and could only be started by existing PMC member.\n Send the [DISCUSS] Promote xxx as new committer mail to private@skywalking.a.o. List the important contributions of the candidates, in order to help the PMC members supporting your proposal. Keep discussion open in more than 3 days, but not more than 1 week, unless there is any explicit objection or concern. Send the [VOTE] Promote xxx as new committer mail to private@skywalking.a.o, when the PMC seems to agree the proposal. Keep vote more than 3 days, but not more than 1 week. Consider the result as Consensus Approval if there 3 +1 votes and +1 votes \u0026gt; -1 votes Send the [RESULT][VOTE] Promote xxx as new committer mail to private@skywalking.a.o, and list the vote detail including the voters.  Invite new committer The PMC member, who start the promotion, takes the responsibilities to send the invitation to new committer and guide him/her to set up the ASF env.\nYou should send the mail like the following template to new committer\nTo: JoeBloggs@foo.net Cc: private@skywalking.apache.org Subject: Invitation to become SkyWalking committer: Joe Bloggs Hello [invitee name], The SkyWalking Project Management Committee] (PMC) hereby offers you committer privileges to the project . These privileges are offered on the understanding that you'll use them reasonably and with common sense. We like to work on trust rather than unnecessary constraints. Being a committer enables you to more easily make changes without needing to go through the patch submission process. Being a committer does not require you to participate any more than you already do. It does tend to make one even more committed. You will probably find that you spend more time here. Of course, you can decline and instead remain as a contributor, participating as you do now. A. This personal invitation is a chance for you to accept or decline in private. Either way, please let us know in reply to the [private@skywalking.apache.org] address only. B. If you accept, the next step is to register an iCLA: 1. Details of the iCLA and the forms are found through this link: http://www.apache.org/licenses/#clas 2. Instructions for its completion and return to the Secretary of the ASF are found at http://www.apache.org/licenses/#submitting 3. When you transmit the completed iCLA, request to notify the Apache SkyWalking and choose a unique Apache id. Look to see if your preferred id is already taken at http://people.apache.org/committer-index.html This will allow the Secretary to notify the PMC when your iCLA has been recorded. When recording of your iCLA is noticed, you will receive a follow-up message with the next steps for establishing you as a committer. Invitation acceptance process And the new committer should reply the mail to private@skywalking.apache.org(Choose reply all), and express the will to accept the invitation explicitly. Then this invitation will be treated as accepted by project PMC. Of course, the new committer could just say NO, and reject the invitation.\nIf they accepted, then they need to do the following things.\n Make sure they have subscribed the dev@skywalking.apache.org. Usually they already have. Choose a Apache ID that is not in the apache committers list page. Download the ICLA (If they are going to contribute to the project as day job, CCLA is expected). After filling the icla.pdf (or ccla.pdf) with information correctly, print, sign it manually (by hand), scan it as an pdf, and send it in mail as an attachment to the secretary@apache.org. (If they prefer to sign electronically, please follow the steps of this page) Then the PMC will wait the Apache secretary confirmed the ICLA (or CCLA) filed. The new committer and PMC will receive the mail like following  Dear XXX, This message acknowledges receipt of your ICLA, which has been filed in the Apache Software Foundation records. Your account has been requested for you and you should receive email with next steps within the next few days (can take up to a week). Please refer to https://www.apache.org/foundation/how-it-works.html#developers for more information about roles at Apache. If in some case, the account has not be requested(rarely to see), the PMC member should contact the project V.P.. The V.P. could request through the Apache Account Submission Helper Form.\nAfter several days, the new committer will receive the account created mail, as this title, Welcome to the Apache Software Foundation (ASF)!. At this point, congratulate! You have the official Apache ID.\nThe PMC member should add the new committer to official committer list through roster.\nSet up the Apache ID and dev env  Go to Apache Account Utility Platform, initial your password, set up your personal mailbox(Forwarding email address) and GitHub account(Your GitHub Username). An organisational invite will be sent to you via email shortly thereafter (within 2 hours). If you want to use xxx@apache.org to send mail, please refer to here. Gmail is recommended, because in other mailbox service settings, this forwarding mode is not easy to find. Following the authorized GitHub 2FA wiki to enable two-factors authorization (2FA) on github. When you set 2FA to \u0026ldquo;off\u0026rdquo;, it will be delisted by the corresponding Apache committer write permission group until you set it up again. (NOTE: Treat your recovery codes with the same level of attention as you would your password !) Use GitBox Account Linking Utility to obtain write permission of the SkyWalking project. Follow this doc to update the website.  If you want others could see you are in the Apache GitHub org, you need to go to Apache GitHub org people page, search for yourself, and choose Organization visibility to Public.\nCommitter rights, duties and responsibilities SkyWalking project doesn\u0026rsquo;t require the continue contributions after you become a committer, but we hope and truly want you could.\nBeing a committer, you could\n Review and merge the pull request to the master branch in the Apache repo. A pull request often contains multiple commits. Those commits must be squashed and merged into a single commit with explanatory comments. For new committer, we hope you could request some senior committer to recheck the pull request. Create and push codes to new branch in the Apache repo. Follow the Release process to process new release. Of course, you need to ask committer team to confirm it is the right time to release.  The PMC hope the new committer to take part in the release and release vote, even still be consider +1 no binding. But be familiar with the release is one of the key to be promoted as a PMC member.\nProject Management Committee Project Management Committee(PMC) member has no special rights in code contributions. They just cover and make sure the project following the Apache requirement, including\n Release binding vote and license check New committer and PMC member recognition Identify branding issue and do branding protection. Response the ASF board question, take necessary actions.  V.P. and chair of the PMC is the secretary, take responsibility of initializing the board report.\nIn the normal case, the new PMC member should be nominated from committer team. But becoming a PMC member directly is not forbidden, if the PMC could agree and be confidence that the candidate is ready, such as he/she has been a PMC member of another project, Apache member or Apache officer.\nThe process of new PMC vote should also follow the same [DISCUSS], [VOTE] and [RESULT][VOTE] in private mail list as new committer vote. One more step before sending the invitation, the PMC need to send NOTICE mail to Apache board.\nTo: board@apache.org Cc: private@skywalking.apache.org Subject: [NOTICE] Jane Doe for SkyWalking PMC SkyWalking proposes to invite Jane Doe (janedoe) to join the PMC. (include if a vote was held) The vote result is available here: https://lists.apache.org/... After 72 hours, if the board doesn\u0026rsquo;t object(usually it wouldn\u0026rsquo;t be), send the invitation.\nAfter the committer accepted the invitation, The PMC member should add the new committer to official PMC list through roster.\n","excerpt":"Apache SkyWalking committer SkyWalking Project Management Committee(PMC) takes the responsibilities …","ref":"/docs/main/v8.3.0/en/guides/asf/committer/","title":"Apache SkyWalking committer"},{"body":"Apdex threshold Apdex is a measure of response time based against a set threshold. It measures the ratio of satisfactory response times to unsatisfactory response times. The response time is measured from an asset request to completed delivery back to the requestor.\nA user defines a response time threshold T. All responses handled in T or less time satisfy the user.\nFor example, if T is 1.2 seconds and a response completes in 0.5 seconds, then the user is satisfied. All responses greater than 1.2 seconds dissatisfy the user. Responses greater than 4.8 seconds frustrate the user.\nThe apdex threshold T can be configured in service-apdex-threshold.yml file or via Dynamic Configuration. The default item will be apply to a service isn\u0026rsquo;t defined in this configuration as the default threshold.\nConfiguration Format The configuration content includes the service' names and their threshold:\n# default threshold is 500ms default: 500 # example: # the threshold of service \u0026#34;tomcat\u0026#34; is 1s # tomcat: 1000 # the threshold of service \u0026#34;springboot1\u0026#34; is 50ms # springboot1: 50 ","excerpt":"Apdex threshold Apdex is a measure of response time based against a set threshold. It measures the …","ref":"/docs/main/v8.3.0/en/setup/backend/apdex-threshold/","title":"Apdex threshold"},{"body":"Backend setup First and most important thing is, SkyWalking backend startup behaviours are driven by config/application.yml. Understood the setting file will help you to read this document.\nStartup script The default startup scripts are /bin/oapService.sh(.bat). Read start up mode document to know other options of starting backend.\napplication.yml The core concept behind this setting file is, SkyWalking collector is based on pure modularization design. End user can switch or assemble the collector features by their own requirements.\nSo, in application.yml, there are three levels.\n Level 1, module name. Meaning this module is active in running mode. Level 2, provider option list and provider selector. Available providers are listed here with a selector to indicate which one will actually take effect, if there is only one provider listed, the selector is optional and can be omitted. Level 3. settings of the provider.  Example:\nstorage: selector: mysql # the mysql storage will actually be activated, while the h2 storage takes no effect h2: driver: ${SW_STORAGE_H2_DRIVER:org.h2.jdbcx.JdbcDataSource} url: ${SW_STORAGE_H2_URL:jdbc:h2:mem:skywalking-oap-db} user: ${SW_STORAGE_H2_USER:sa} metadataQueryMaxSize: ${SW_STORAGE_H2_QUERY_MAX_SIZE:5000} mysql: properties: jdbcUrl: ${SW_JDBC_URL:\u0026#34;jdbc:mysql://localhost:3306/swtest\u0026#34;} dataSource.user: ${SW_DATA_SOURCE_USER:root} dataSource.password: ${SW_DATA_SOURCE_PASSWORD:root@1234} dataSource.cachePrepStmts: ${SW_DATA_SOURCE_CACHE_PREP_STMTS:true} dataSource.prepStmtCacheSize: ${SW_DATA_SOURCE_PREP_STMT_CACHE_SQL_SIZE:250} dataSource.prepStmtCacheSqlLimit: ${SW_DATA_SOURCE_PREP_STMT_CACHE_SQL_LIMIT:2048} dataSource.useServerPrepStmts: ${SW_DATA_SOURCE_USE_SERVER_PREP_STMTS:true} metadataQueryMaxSize: ${SW_STORAGE_MYSQL_QUERY_MAX_SIZE:5000} # other configurations  storage is the module. selector selects one out of the all providers listed below, the unselected ones take no effect as if they were deleted. default is the default implementor of core module. driver, url, \u0026hellip; metadataQueryMaxSize are all setting items of the implementor.  At the same time, modules includes required and optional, the required modules provide the skeleton of backend, even modularization supported pluggable, removing those modules are meaningless, for optional modules, some of them have a provider implementation called none, meaning it only provides a shell with no actual logic, typically such as telemetry. Setting - to the selector means this whole module will be excluded at runtime. We highly recommend you don\u0026rsquo;t try to change APIs of those modules, unless you understand SkyWalking project and its codes very well.\nList the required modules here\n Core. Do basic and major skeleton of all data analysis and stream dispatch. Cluster. Manage multiple backend instances in a cluster, which could provide high throughputs process capabilities. Storage. Make the analysis result persistence. Query. Provide query interfaces to UI.  For Cluster and Storage have provided multiple implementors(providers), see Cluster management and Choose storage documents in the link list.\nAlso, several receiver modules are provided. Receiver is the module in charge of accepting incoming data requests to backend. Most(all) provide service by some network(RPC) protocol, such as gRPC, HTTPRestful.\nThe receivers have many different module names, you could read Set receivers document in the link list.\nConfiguration Vocabulary All available configurations in application.yml could be found in Configuration Vocabulary.\nAdvanced feature document link list After understand the setting file structure, you could choose your interesting feature document. We recommend you to read the feature documents in our following order.\n Overriding settings in application.yml is supported IP and port setting. Introduce how IP and port set and be used. Backend init mode startup. How to init the environment and exit graciously. Read this before you try to initial a new cluster. Cluster management. Guide you to set backend server in cluster mode. Deploy in kubernetes. Guide you to build and use SkyWalking image, and deploy in k8s. Choose storage. As we know, in default quick start, backend is running with H2 DB. But clearly, it doesn\u0026rsquo;t fit the product env. In here, you could find what other choices do you have. Choose the ones you like, we are also welcome anyone to contribute new storage implementor. Set receivers. You could choose receivers by your requirements, most receivers are harmless, at least our default receivers are. You would set and active all receivers provided. Open fetchers. You could open different fetchers to read metrics from the target applications. These ones works like receivers, but in pulling mode, typically like Prometheus. Token authentication. You could add token authentication mechanisms to avoid OAP receiving untrusted data. Do trace sampling at backend. This sample keep the metrics accurate, only don\u0026rsquo;t save some of traces in storage based on rate. Follow slow DB statement threshold config document to understand that, how to detect the Slow database statements(including SQL statements) in your system. Official OAL scripts. As you known from our OAL introduction, most of backend analysis capabilities based on the scripts. Here is the description of official scripts, which helps you to understand which metrics data are in process, also could be used in alarm. Alarm. Alarm provides a time-series based check mechanism. You could set alarm rules targeting the analysis oal metrics objects. Advanced deployment options. If you want to deploy backend in very large scale and support high payload, you may need this. Metrics exporter. Use metrics data exporter to forward metrics data to 3rd party system. Time To Live (TTL). Metrics and trace are time series data, TTL settings affect the expired time of them. Dynamic Configuration. Make configuration of OAP changed dynamic, from remote service or 3rd party configuration management system. Uninstrumented Gateways. Configure gateways/proxies that are not supported by SkyWalking agent plugins, to reflect the delegation in topology graph. Apdex threshold. Configure the thresholds for different services if Apdex calculation is activated in the OAL. Service Grouping. An automatic grouping mechanism for all services based on name. Group Parameterized Endpoints. Configure the grouping rules for parameterized endpoints, to improve the meaning of the metrics. Meter Analysis. Set up the backend analysis rules, when use SkyWalking Meter System Toolkit or meter plugins. Spring Sleuth Metrics Analysis. Configure the agent and backend to receiver metrics from micrometer.  Telemetry for backend OAP backend cluster itself underlying is a distributed streaming process system. For helping the Ops team, we provide the telemetry for OAP backend itself. Follow document to use it.\nAt the same time, we provide Health Check to get a score for the health status.\n 0 means healthy, more than 0 means unhealthy and less than 0 means oap doesn\u0026rsquo;t startup.\n FAQs When and why do we need to set Timezone? SkyWalking provides downsampling time series metrics features. Query and storage at each time dimension(minute, hour, day, month metrics indexes) related to timezone when doing time format.\nFor example, metrics time will be formatted like YYYYMMDDHHmm in minute dimension metrics, which format process is timezone related.\nIn default, SkyWalking OAP backend choose the OS default timezone. If you want to override it, please follow Java and OS documents to do so.\nHow to query the storage directly from 3rd party tool? SkyWalking provides browser UI, CLI and GraphQL ways to support extensions. But some users may have the idea to query data directly from the storage. Such as in ElasticSearch case, Kibana is a great tool to do this.\nIn default, due to reduce memory, network and storage space usages, SkyWalking saves based64-encoded id(s) only in the metrics entities. But these tools usually don\u0026rsquo;t support nested query, or don\u0026rsquo;t work conveniently. In this special case, SkyWalking provide a config to add all necessary name column(s) into the final metrics entities with ID as a trade-off.\nTake a look at core/default/activeExtraModelColumns config in the application.yaml, and set it as true to open this feature.\nThis feature wouldn\u0026rsquo;t provide any new feature to the native SkyWalking scenarios, just for the 3rd party integration.\n","excerpt":"Backend setup First and most important thing is, SkyWalking backend startup behaviours are driven by …","ref":"/docs/main/v8.3.0/en/setup/backend/backend-setup/","title":"Backend setup"},{"body":"Backend storage SkyWalking storage is pluggable, we have provided the following storage solutions, you could easily use one of them by specifying it as the selector in the application.yml：\nstorage: selector: ${SW_STORAGE:elasticsearch7} Native supported storage\n H2 ElasticSearch 6, 7 MySQL TiDB InfluxDB  Redistribution version with supported storage.\n ElasticSearch 5  H2 Active H2 as storage, set storage provider to H2 In-Memory Databases. Default in distribution package. Please read Database URL Overview in H2 official document, you could set the target to H2 in Embedded, Server and Mixed modes.\nSetting fragment example\nstorage: selector: ${SW_STORAGE:h2} h2: driver: org.h2.jdbcx.JdbcDataSource url: jdbc:h2:mem:skywalking-oap-db user: sa ElasticSearch  In order to activate ElasticSearch 6 as storage, set storage provider to elasticsearch In order to activate ElasticSearch 7 as storage, set storage provider to elasticsearch7  Required ElasticSearch 6.3.2 or higher. HTTP RestHighLevelClient is used to connect server.\n For ElasticSearch 6.3.2 ~ 7.0.0 (excluded), please download the apache-skywalking-bin.tar.gz or apache-skywalking-bin.zip, For ElasticSearch 7.0.0 ~ 8.0.0 (excluded), please download the apache-skywalking-bin-es7.tar.gz or apache-skywalking-bin-es7.zip.  For now, ElasticSearch 6 and ElasticSearch 7 share the same configurations, as follows:\nstorage: selector: ${SW_STORAGE:elasticsearch} elasticsearch: nameSpace: ${SW_NAMESPACE:\u0026#34;\u0026#34;} clusterNodes: ${SW_STORAGE_ES_CLUSTER_NODES:localhost:9200} protocol: ${SW_STORAGE_ES_HTTP_PROTOCOL:\u0026#34;http\u0026#34;} trustStorePath: ${SW_STORAGE_ES_SSL_JKS_PATH:\u0026#34;\u0026#34;} trustStorePass: ${SW_STORAGE_ES_SSL_JKS_PASS:\u0026#34;\u0026#34;} user: ${SW_ES_USER:\u0026#34;\u0026#34;} password: ${SW_ES_PASSWORD:\u0026#34;\u0026#34;} secretsManagementFile: ${SW_ES_SECRETS_MANAGEMENT_FILE:\u0026#34;\u0026#34;} # Secrets management file in the properties format includes the username, password, which are managed by 3rd party tool. dayStep: ${SW_STORAGE_DAY_STEP:1} # Represent the number of days in the one minute/hour/day index. indexShardsNumber: ${SW_STORAGE_ES_INDEX_SHARDS_NUMBER:1} # Shard number of new indexes indexReplicasNumber: ${SW_STORAGE_ES_INDEX_REPLICAS_NUMBER:1} # Replicas number of new indexes # Super data set has been defined in the codes, such as trace segments.The following 3 config would be improve es performance when storage super size data in es. superDatasetDayStep: ${SW_SUPERDATASET_STORAGE_DAY_STEP:-1} # Represent the number of days in the super size dataset record index, the default value is the same as dayStep when the value is less than 0 superDatasetIndexShardsFactor: ${SW_STORAGE_ES_SUPER_DATASET_INDEX_SHARDS_FACTOR:5} # This factor provides more shards for the super data set, shards number = indexShardsNumber * superDatasetIndexShardsFactor. Also, this factor effects Zipkin and Jaeger traces. superDatasetIndexReplicasNumber: ${SW_STORAGE_ES_SUPER_DATASET_INDEX_REPLICAS_NUMBER:0} # Represent the replicas number in the super size dataset record index, the default value is 0. bulkActions: ${SW_STORAGE_ES_BULK_ACTIONS:1000} # Execute the async bulk record data every ${SW_STORAGE_ES_BULK_ACTIONS} requests syncBulkActions: ${SW_STORAGE_ES_SYNC_BULK_ACTIONS:50000} # Execute the sync bulk metrics data every ${SW_STORAGE_ES_SYNC_BULK_ACTIONS} requests flushInterval: ${SW_STORAGE_ES_FLUSH_INTERVAL:10} # flush the bulk every 10 seconds whatever the number of requests concurrentRequests: ${SW_STORAGE_ES_CONCURRENT_REQUESTS:2} # the number of concurrent requests resultWindowMaxSize: ${SW_STORAGE_ES_QUERY_MAX_WINDOW_SIZE:10000} metadataQueryMaxSize: ${SW_STORAGE_ES_QUERY_MAX_SIZE:5000} segmentQueryMaxSize: ${SW_STORAGE_ES_QUERY_SEGMENT_SIZE:200} profileTaskQueryMaxSize: ${SW_STORAGE_ES_QUERY_PROFILE_TASK_SIZE:200} advanced: ${SW_STORAGE_ES_ADVANCED:\u0026#34;\u0026#34;} ElasticSearch 6 With Https SSL Encrypting communications. example:\nstorage: selector: ${SW_STORAGE:elasticsearch} elasticsearch: # nameSpace: ${SW_NAMESPACE:\u0026#34;\u0026#34;} user: ${SW_ES_USER:\u0026#34;\u0026#34;} # User needs to be set when Http Basic authentication is enabled password: ${SW_ES_PASSWORD:\u0026#34;\u0026#34;} # Password to be set when Http Basic authentication is enabled clusterNodes: ${SW_STORAGE_ES_CLUSTER_NODES:localhost:443} trustStorePath: ${SW_SW_STORAGE_ES_SSL_JKS_PATH:\u0026#34;../es_keystore.jks\u0026#34;} trustStorePass: ${SW_SW_STORAGE_ES_SSL_JKS_PASS:\u0026#34;\u0026#34;} protocol: ${SW_STORAGE_ES_HTTP_PROTOCOL:\u0026#34;https\u0026#34;} ...  File at trustStorePath is being monitored, once it is changed, the ElasticSearch client will do reconnecting. trustStorePass could be changed on the runtime through Secrets Management File Of ElasticSearch Authentication.  Daily Index Step Daily index step(storage/elasticsearch/dayStep, default 1) represents the index creation period. In this period, several days(dayStep value)' metrics are saved.\nMostly, users don\u0026rsquo;t need to change the value manually. As SkyWalking is designed to observe large scale distributed system. But in some specific cases, users want to set a long TTL value, such as more than 60 days, but their ElasticSearch cluster isn\u0026rsquo;t powerful due to the low traffic in the production environment. This value could be increased to 5(or more), if users could make sure single one index could support these days(5 in this case) metrics and traces.\nSuch as, if dayStep == 11,\n data in [2000-01-01, 2000-01-11] will be merged into the index-20000101. data in [2000-01-12, 2000-01-22] will be merged into the index-20000112.  storage/elasticsearch/superDatasetDayStep override the storage/elasticsearch/dayStep if the value is positive. This would affect the record related entities, such as the trace segment. In some cases, the size of metrics is much less than the record(trace), this would help the shards balance in the ElasticSearch cluster.\nNOTICE, TTL deletion would be affected by these. You should set an extra more dayStep in your TTL. Such as you want to TTL == 30 days and dayStep == 10, you actually need to set TTL = 40;\nSecrets Management File Of ElasticSearch Authentication The value of secretsManagementFile should point to the secrets management file absolute path. The file includes username, password and JKS password of ElasticSearch server in the properties format.\nuser=xxx password=yyy trustStorePass=zzz The major difference between using user, password, trustStorePass configs in the application.yaml file is, the Secrets Management File is being watched by the OAP server. Once it is changed manually or through 3rd party tool, such as Vault, the storage provider will use the new username, password and JKS password to establish the connection and close the old one. If the information exist in the file, the user/password will be overrided.\nAdvanced Configurations For Elasticsearch Index You can add advanced configurations in JSON format to set ElasticSearch index settings by following ElasticSearch doc\nFor example, set translog settings:\nstorage: elasticsearch: # ...... advanced: ${SW_STORAGE_ES_ADVANCED:\u0026#34;{\\\u0026#34;index.translog.durability\\\u0026#34;:\\\u0026#34;request\\\u0026#34;,\\\u0026#34;index.translog.sync_interval\\\u0026#34;:\\\u0026#34;5s\\\u0026#34;}\u0026#34;} Recommended ElasticSearch server-side configurations You could add following config to elasticsearch.yml, set the value based on your env.\n# In tracing scenario, consider to set more than this at least. thread_pool.index.queue_size: 1000 # Only suitable for ElasticSearch 6 thread_pool.write.queue_size: 1000 # Suitable for ElasticSearch 6 and 7 # When you face query error at trace page, remember to check this. index.max_result_window: 1000000 We strongly advice you to read more about these configurations from ElasticSearch official document. This effects the performance of ElasticSearch very much.\nElasticSearch 6 with Zipkin trace extension This implementation shares most of elasticsearch, just extend to support zipkin span storage. It has all same configs.\nstorage: selector: ${SW_STORAGE:zipkin-elasticsearch} zipkin-elasticsearch: nameSpace: ${SW_NAMESPACE:\u0026#34;\u0026#34;} clusterNodes: ${SW_STORAGE_ES_CLUSTER_NODES:localhost:9200} protocol: ${SW_STORAGE_ES_HTTP_PROTOCOL:\u0026#34;http\u0026#34;} user: ${SW_ES_USER:\u0026#34;\u0026#34;} password: ${SW_ES_PASSWORD:\u0026#34;\u0026#34;} indexShardsNumber: ${SW_STORAGE_ES_INDEX_SHARDS_NUMBER:2} indexReplicasNumber: ${SW_STORAGE_ES_INDEX_REPLICAS_NUMBER:0} # Batch process setting, refer to https://www.elastic.co/guide/en/elasticsearch/client/java-api/5.5/java-docs-bulk-processor.html bulkActions: ${SW_STORAGE_ES_BULK_ACTIONS:2000} # Execute the bulk every 2000 requests bulkSize: ${SW_STORAGE_ES_BULK_SIZE:20} # flush the bulk every 20mb flushInterval: ${SW_STORAGE_ES_FLUSH_INTERVAL:10} # flush the bulk every 10 seconds whatever the number of requests concurrentRequests: ${SW_STORAGE_ES_CONCURRENT_REQUESTS:2} # the number of concurrent requests ElasticSearch 6 with Jaeger trace extension This implementation shares most of elasticsearch, just extend to support jaeger span storage. It has all same configs.\nstorage: selector: ${SW_STORAGE:jaeger-elasticsearch} jaeger-elasticsearch: nameSpace: ${SW_NAMESPACE:\u0026#34;\u0026#34;} clusterNodes: ${SW_STORAGE_ES_CLUSTER_NODES:localhost:9200} protocol: ${SW_STORAGE_ES_HTTP_PROTOCOL:\u0026#34;http\u0026#34;} user: ${SW_ES_USER:\u0026#34;\u0026#34;} password: ${SW_ES_PASSWORD:\u0026#34;\u0026#34;} indexShardsNumber: ${SW_STORAGE_ES_INDEX_SHARDS_NUMBER:2} indexReplicasNumber: ${SW_STORAGE_ES_INDEX_REPLICAS_NUMBER:0} # Batch process setting, refer to https://www.elastic.co/guide/en/elasticsearch/client/java-api/5.5/java-docs-bulk-processor.html bulkActions: ${SW_STORAGE_ES_BULK_ACTIONS:2000} # Execute the bulk every 2000 requests bulkSize: ${SW_STORAGE_ES_BULK_SIZE:20} # flush the bulk every 20mb flushInterval: ${SW_STORAGE_ES_FLUSH_INTERVAL:10} # flush the bulk every 10 seconds whatever the number of requests concurrentRequests: ${SW_STORAGE_ES_CONCURRENT_REQUESTS:2} # the number of concurrent requests About Namespace When namespace is set, names of all indexes in ElasticSearch will use it as prefix.\nMySQL Active MySQL as storage, set storage provider to mysql.\nNOTICE: MySQL driver is NOT allowed in Apache official distribution and source codes. Please download MySQL driver by yourself. Copy the connection driver jar to oap-libs.\nstorage: selector: ${SW_STORAGE:mysql} mysql: properties: jdbcUrl: ${SW_JDBC_URL:\u0026#34;jdbc:mysql://localhost:3306/swtest\u0026#34;} dataSource.user: ${SW_DATA_SOURCE_USER:root} dataSource.password: ${SW_DATA_SOURCE_PASSWORD:root@1234} dataSource.cachePrepStmts: ${SW_DATA_SOURCE_CACHE_PREP_STMTS:true} dataSource.prepStmtCacheSize: ${SW_DATA_SOURCE_PREP_STMT_CACHE_SQL_SIZE:250} dataSource.prepStmtCacheSqlLimit: ${SW_DATA_SOURCE_PREP_STMT_CACHE_SQL_LIMIT:2048} dataSource.useServerPrepStmts: ${SW_DATA_SOURCE_USE_SERVER_PREP_STMTS:true} metadataQueryMaxSize: ${SW_STORAGE_MYSQL_QUERY_MAX_SIZE:5000} All connection related settings including link url, username and password are in application.yml. Here are some of the settings, please follow HikariCP connection pool document for all the settings.\nTiDB Tested TiDB Server 4.0.8 version and Mysql Client driver 8.0.13 version currently. Active TiDB as storage, set storage provider to tidb.\nstorage: selector: ${SW_STORAGE:tidb} tidb: properties: jdbcUrl: ${SW_JDBC_URL:\u0026#34;jdbc:mysql://localhost:4000/swtest\u0026#34;} dataSource.user: ${SW_DATA_SOURCE_USER:root} dataSource.password: ${SW_DATA_SOURCE_PASSWORD:\u0026#34;\u0026#34;} dataSource.cachePrepStmts: ${SW_DATA_SOURCE_CACHE_PREP_STMTS:true} dataSource.prepStmtCacheSize: ${SW_DATA_SOURCE_PREP_STMT_CACHE_SQL_SIZE:250} dataSource.prepStmtCacheSqlLimit: ${SW_DATA_SOURCE_PREP_STMT_CACHE_SQL_LIMIT:2048} dataSource.useServerPrepStmts: ${SW_DATA_SOURCE_USE_SERVER_PREP_STMTS:true} dataSource.useAffectedRows: ${SW_DATA_SOURCE_USE_AFFECTED_ROWS:true} metadataQueryMaxSize: ${SW_STORAGE_MYSQL_QUERY_MAX_SIZE:5000} maxSizeOfArrayColumn: ${SW_STORAGE_MAX_SIZE_OF_ARRAY_COLUMN:20} numOfSearchableValuesPerTag: ${SW_STORAGE_NUM_OF_SEARCHABLE_VALUES_PER_TAG:2} All connection related settings including link url, username and password are in application.yml. These settings can refer to the configuration of MySQL above.\nInfluxDB InfluxDB storage provides a time-series database as a new storage option.\nstorage: selector: ${SW_STORAGE:influxdb} influxdb: url: ${SW_STORAGE_INFLUXDB_URL:http://localhost:8086} user: ${SW_STORAGE_INFLUXDB_USER:root} password: ${SW_STORAGE_INFLUXDB_PASSWORD:} database: ${SW_STORAGE_INFLUXDB_DATABASE:skywalking} actions: ${SW_STORAGE_INFLUXDB_ACTIONS:1000} # the number of actions to collect duration: ${SW_STORAGE_INFLUXDB_DURATION:1000} # the time to wait at most (milliseconds) fetchTaskLogMaxSize: ${SW_STORAGE_INFLUXDB_FETCH_TASK_LOG_MAX_SIZE:5000} # the max number of fetch task log in a request All connection related settings including link url, username and password are in application.yml. The Metadata storage provider settings can refer to the configuration of H2/MySQL above.\nElasticSearch 5 ElasticSearch 5 is incompatible with ElasticSearch 6 Java client jar, so it could not be included in native distribution. OpenSkyWalking/SkyWalking-With-Es5x-Storage repo includes the distribution version.\nMore storage solution extension Follow Storage extension development guide in Project Extensions document in development guide.\n","excerpt":"Backend storage SkyWalking storage is pluggable, we have provided the following storage solutions, …","ref":"/docs/main/v8.3.0/en/setup/backend/backend-storage/","title":"Backend storage"},{"body":"Backend, UI, and CLI setup SkyWalking backend distribution package includes the following parts:\n  bin/cmd scripts, in /bin folder. Includes startup linux shell and Windows cmd scripts for Backend server and UI startup.\n  Backend config, in /config folder. Includes settings files of the backend, which are:\n application.yml log4j.xml alarm-settings.yml    Libraries of backend, in /oap-libs folder. All the dependencies of the backend are in it.\n  Webapp env, in webapp folder. UI frontend jar file is here, with its webapp.yml setting file.\n  Quick start Requirements and default settings Requirement: JDK8 to JDK12 are tested, other versions are not tested and may or may not work.\nBefore you start, you should know that the quickstart aims to get you a basic configuration mostly for previews/demo, performance and long-term running are not our goals.\nFor production/QA/tests environments, you should head to Backend and UI deployment documents.\nYou can use bin/startup.sh (or cmd) to startup the backend and UI with their default settings, which are:\n Backend storage uses H2 by default (for an easier start) Backend listens 0.0.0.0/11800 for gRPC APIs and 0.0.0.0/12800 for http rest APIs.  In Java, .NetCore, Node.js, Istio agents/probe, you should set the gRPC service address to ip/host:11800, with ip/host where your backend is.\n UI listens on 8080 port and request 127.0.0.1/12800 to do GraphQL query.  Deploy Backend and UI Before deploying Skywalking in your distributed environment, you should know how agents/probes, backend, UI communicates with each other:\n All native agents and probes, either language based or mesh probe, are using gRPC service (core/default/gRPC* in application.yml) to report data to the backend. Also, jetty service supported in JSON format. UI uses GraphQL (HTTP) query to access the backend also in Jetty service (core/default/rest* in application.yml).  Now, let\u0026rsquo;s continue with the backend, UI and CLI setting documents.\nBackend setup document UI setup document CLI set up document ","excerpt":"Backend, UI, and CLI setup SkyWalking backend distribution package includes the following parts: …","ref":"/docs/main/v8.3.0/en/setup/backend/backend-ui-setup/","title":"Backend, UI, and CLI setup"},{"body":"Browser Protocol Browser protocol describes the data format between skywalking-client-js and backend.\nOverview Browser protocol is defined and provided in gRPC format, also implemented in HTTP 1.1\nSend performance data and error log You can send performance data and error logs via the following services:\n BrowserPerfService#collectPerfData for performance data format. BrowserPerfService#collectErrorLogs for error log format.  For error log format, there are some notices\n BrowserErrorLog#uniqueId should be unique in the whole distributed environments.  ","excerpt":"Browser Protocol Browser protocol describes the data format between skywalking-client-js and …","ref":"/docs/main/v8.3.0/en/protocols/browser-protocol/","title":"Browser Protocol"},{"body":"Choose receiver Receiver is a concept in SkyWalking backend. All modules, which are responsible for receiving telemetry or tracing data from other being monitored system, are all being called Receiver. If you are looking for the pull mode, Take a look at fetcher document.\nWe have following receivers, and default implementors are provided in our Apache distribution.\n receiver-trace. gRPC and HTTPRestful services to accept SkyWalking format traces. receiver-register. gRPC and HTTPRestful services to provide service, service instance and endpoint register. service-mesh. gRPC services accept data from inbound mesh probes. receiver-jvm. gRPC services accept JVM metrics data. envoy-metric. Envoy metrics_service and ALS(access log service) supported by this receiver. OAL script support all GAUGE type metrics. receiver-profile. gRPC services accept profile task status and snapshot reporter. receiver_zipkin. See details. receiver_jaeger. See details. receiver-otel. See details. receiver-meter. See details. receiver-browser. gRPC services to accept browser performance data and error log.  The sample settings of these receivers should be already in default application.yml, and also list here\nreceiver-register: selector: ${SW_RECEIVER_REGISTER:default} default: receiver-trace: selector: ${SW_RECEIVER_TRACE:default} default: receiver-jvm: selector: ${SW_RECEIVER_JVM:default} default: service-mesh: selector: ${SW_SERVICE_MESH:default} default: envoy-metric: selector: ${SW_ENVOY_METRIC:default} default: acceptMetricsService: ${SW_ENVOY_METRIC_SERVICE:true} alsHTTPAnalysis: ${SW_ENVOY_METRIC_ALS_HTTP_ANALYSIS:\u0026#34;\u0026#34;} receiver_zipkin: selector: ${SW_RECEIVER_ZIPKIN:-} default: host: ${SW_RECEIVER_ZIPKIN_HOST:0.0.0.0} port: ${SW_RECEIVER_ZIPKIN_PORT:9411} contextPath: ${SW_RECEIVER_ZIPKIN_CONTEXT_PATH:/} jettyMinThreads: ${SW_RECEIVER_ZIPKIN_JETTY_MIN_THREADS:1} jettyMaxThreads: ${SW_RECEIVER_ZIPKIN_JETTY_MAX_THREADS:200} jettyIdleTimeOut: ${SW_RECEIVER_ZIPKIN_JETTY_IDLE_TIMEOUT:30000} jettyAcceptorPriorityDelta: ${SW_RECEIVER_ZIPKIN_JETTY_DELTA:0} jettyAcceptQueueSize: ${SW_RECEIVER_ZIPKIN_QUEUE_SIZE:0} receiver-profile: selector: ${SW_RECEIVER_PROFILE:default} default: receiver-browser: selector: ${SW_RECEIVER_BROWSER:default} default: sampleRate: ${SW_RECEIVER_BROWSER_SAMPLE_RATE:10000} gRPC/HTTP server for receiver In default, all gRPC/HTTP services should be served at core/gRPC and core/rest. But the receiver-sharing-server module provide a way to make all receivers serving at different ip:port, if you set them explicitly.\nreceiver-sharing-server: selector: ${SW_RECEIVER_SHARING_SERVER:default} default: host: ${SW_RECEIVER_JETTY_HOST:0.0.0.0} contextPath: ${SW_RECEIVER_JETTY_CONTEXT_PATH:/} authentication: ${SW_AUTHENTICATION:\u0026#34;\u0026#34;} jettyMinThreads: ${SW_RECEIVER_SHARING_JETTY_MIN_THREADS:1} jettyMaxThreads: ${SW_RECEIVER_SHARING_JETTY_MAX_THREADS:200} jettyIdleTimeOut: ${SW_RECEIVER_SHARING_JETTY_IDLE_TIMEOUT:30000} jettyAcceptorPriorityDelta: ${SW_RECEIVER_SHARING_JETTY_DELTA:0} jettyAcceptQueueSize: ${SW_RECEIVER_SHARING_JETTY_QUEUE_SIZE:0} Notice, if you add these settings, make sure they are not as same as core module, because gRPC/HTTP servers of core are still used for UI and OAP internal communications.\nZipkin receiver Zipkin receiver could work in two different mode.\n Tracing mode(default). Tracing mode is that, skywalking OAP acts like zipkin collector, fully supports Zipkin v1/v2 formats through HTTP service, also provide persistence and query in skywalking UI. But it wouldn\u0026rsquo;t analysis metrics from them. In most case, I suggest you could use this feature, when metrics come from service mesh. Notice, in this mode, Zipkin receiver requires zipkin-elasticsearch storage implementation active. Read this to know how to active.  Use following config to active.\nreceiver_zipkin: selector: ${SW_RECEIVER_ZIPKIN:-} default: host: ${SW_RECEIVER_ZIPKIN_HOST:0.0.0.0} port: ${SW_RECEIVER_ZIPKIN_PORT:9411} contextPath: ${SW_RECEIVER_ZIPKIN_CONTEXT_PATH:/} jettyMinThreads: ${SW_RECEIVER_ZIPKIN_JETTY_MIN_THREADS:1} jettyMaxThreads: ${SW_RECEIVER_ZIPKIN_JETTY_MAX_THREADS:200} jettyIdleTimeOut: ${SW_RECEIVER_ZIPKIN_JETTY_IDLE_TIMEOUT:30000} jettyAcceptorPriorityDelta: ${SW_RECEIVER_ZIPKIN_JETTY_DELTA:0} jettyAcceptQueueSize: ${SW_RECEIVER_ZIPKIN_QUEUE_SIZE:0} Analysis mode(Not production ready), receive Zipkin v1/v2 formats through HTTP service. Transform the trace to skywalking native format, and analysis like skywalking trace. This feature can\u0026rsquo;t work in production env right now, because of Zipkin tag/endpoint value unpredictable, we can\u0026rsquo;t make sure it fits production env requirements.  Active analysis mode, you should set needAnalysis config.\nreceiver_zipkin: selector: ${SW_RECEIVER_ZIPKIN:-} default: host: ${SW_RECEIVER_ZIPKIN_HOST:0.0.0.0} port: ${SW_RECEIVER_ZIPKIN_PORT:9411} contextPath: ${SW_RECEIVER_ZIPKIN_CONTEXT_PATH:/} jettyMinThreads: ${SW_RECEIVER_ZIPKIN_JETTY_MIN_THREADS:1} jettyMaxThreads: ${SW_RECEIVER_ZIPKIN_JETTY_MAX_THREADS:200} jettyIdleTimeOut: ${SW_RECEIVER_ZIPKIN_JETTY_IDLE_TIMEOUT:30000} jettyAcceptorPriorityDelta: ${SW_RECEIVER_ZIPKIN_JETTY_DELTA:0} jettyAcceptQueueSize: ${SW_RECEIVER_ZIPKIN_QUEUE_SIZE:0} needAnalysis: true NOTICE, Zipkin receiver is only provided in apache-skywalking-apm-x.y.z.tar.gz tar.\nJaeger receiver Jaeger receiver right now only works in Tracing Mode, and no analysis. Jaeger receiver provides extra gRPC host/port, if absent, sharing-server host/port will be used, then core gRPC host/port. Receiver requires jaeger-elasticsearch storage implementation active. Read this to know how to active.\nRight now, you need jaeger agent to batch send spans to SkyWalking oap server. Read Jaeger Architecture to get more details.\nActive the receiver.\nreceiver_jaeger: selector: ${SW_RECEIVER_JAEGER:-} default: gRPCHost: ${SW_RECEIVER_JAEGER_HOST:0.0.0.0} gRPCPort: ${SW_RECEIVER_JAEGER_PORT:14250} NOTICE, Jaeger receiver is only provided in apache-skywalking-apm-x.y.z.tar.gz tar.\nOpenTelemetry receiver OpenTelemetry receiver supports to ingest agent metrics by meter-system. OAP can load the configuration at bootstrap. If the new configuration is not well-formed, OAP fails to start up. The files are located at $CLASSPATH/otel-\u0026lt;handler\u0026gt;-rules. Eg, the oc handler loads fules from $CLASSPATH/otel-oc-rules,\nSupported handlers: * oc: OpenCensus gRPC service handler.\nThe rule file should be in YAML format, defined by the scheme described in prometheus-fetcher. Notice, receiver-otel only support group, defaultMetricLevel and metricsRules nodes of scheme due to the push mode it opts to.\nTo active the oc handler and istio relevant rules:\nreceiver-otel: selector: ${SW_OTEL_RECEIVER:default} default: enabledHandlers: ${SW_OTEL_RECEIVER_ENABLED_HANDLERS:\u0026#34;oc\u0026#34;} enabledOcRules: ${SW_OTEL_RECEIVER_ENABLED_OC_RULES:\u0026#34;istio-controlplane\u0026#34;} Meter receiver Meter receiver supports accept the metrics into the meter-system. OAP can load the configuration at bootstrap.\nThe file is written in YAML format, defined by the scheme described in backend-meter.\nTo active the default implementation:\nreceiver-meter: selector: ${SW_RECEIVER_METER:default} default: ","excerpt":"Choose receiver Receiver is a concept in SkyWalking backend. All modules, which are responsible for …","ref":"/docs/main/v8.3.0/en/setup/backend/backend-receivers/","title":"Choose receiver"},{"body":"Cluster Management In many product environments, backend needs to support high throughput and provides HA to keep robustness, so you should need cluster management always in product env.\nBackend provides several ways to do cluster management. Choose the one you need/want.\n Zookeeper coordinator. Use Zookeeper to let backend instance detects and communicates with each other. Kubernetes. When backend cluster are deployed inside kubernetes, you could choose this by using k8s native APIs to manage cluster. Consul. Use Consul as backend cluster management implementor, to coordinate backend instances. Etcd. Use Etcd to coordinate backend instances. Nacos. Use Nacos to coordinate backend instances. In the application.yml, there\u0026rsquo;re default configurations for the aforementioned coordinators under the section cluster, you can specify one of them in the selector property to enable it.  Zookeeper coordinator Zookeeper is a very common and wide used cluster coordinator. Set the cluster/selector to zookeeper in the yml to enable.\nRequired Zookeeper version, 3.4+\ncluster: selector: ${SW_CLUSTER:zookeeper} # other configurations  hostPort is the list of zookeeper servers. Format is IP1:PORT1,IP2:PORT2,...,IPn:PORTn enableACL enable Zookeeper ACL to control access to its znode. schema is Zookeeper ACL schemas. expression is a expression of ACL. The format of the expression is specific to the schema. hostPort, baseSleepTimeMs and maxRetries are settings of Zookeeper curator client.  Note:\n If Zookeeper ACL is enabled and /skywalking existed, must be sure SkyWalking has CREATE, READ and WRITE permissions. If /skywalking is not exists, it will be created by SkyWalking and grant all permissions to the specified user. Simultaneously, znode is granted READ to anyone. If set schema as digest, the password of expression is set in clear text.  In some cases, oap default gRPC host and port in core are not suitable for internal communication among the oap nodes. The following setting are provided to set the host and port manually, based on your own LAN env.\n internalComHost, the host registered and other oap node use this to communicate with current node. internalComPort, the port registered and other oap node use this to communicate with current node.  zookeeper: nameSpace: ${SW_NAMESPACE:\u0026#34;\u0026#34;} hostPort: ${SW_CLUSTER_ZK_HOST_PORT:localhost:2181} #Retry Policy baseSleepTimeMs: ${SW_CLUSTER_ZK_SLEEP_TIME:1000} # initial amount of time to wait between retries maxRetries: ${SW_CLUSTER_ZK_MAX_RETRIES:3} # max number of times to retry internalComHost: 172.10.4.10 internalComPort: 11800 # Enable ACL enableACL: ${SW_ZK_ENABLE_ACL:false} # disable ACL in default schema: ${SW_ZK_SCHEMA:digest} # only support digest schema expression: ${SW_ZK_EXPRESSION:skywalking:skywalking} Kubernetes Require backend cluster are deployed inside kubernetes, guides are in Deploy in kubernetes. Set the selector to kubernetes.\ncluster: selector: ${SW_CLUSTER:kubernetes} # other configurations Consul Now, consul is becoming a famous system, many of companies and developers using consul to be their service discovery solution. Set the cluster/selector to consul in the yml to enable.\ncluster: selector: ${SW_CLUSTER:consul} # other configurations Same as Zookeeper coordinator, in some cases, oap default gRPC host and port in core are not suitable for internal communication among the oap nodes. The following setting are provided to set the host and port manually, based on your own LAN env.\n internalComHost, the host registered and other oap node use this to communicate with current node. internalComPort, the port registered and other oap node use this to communicate with current node.  Etcd Set the cluster/selector to etcd in the yml to enable.\ncluster: selector: ${SW_CLUSTER:etcd} # other configurations Same as Zookeeper coordinator, in some cases, oap default gRPC host and port in core are not suitable for internal communication among the oap nodes. The following setting are provided to set the host and port manually, based on your own LAN env.\n internalComHost, the host registered and other oap node use this to communicate with current node. internalComPort, the port registered and other oap node use this to communicate with current node.  Nacos Set the cluster/selector to nacos in the yml to enable.\ncluster: selector: ${SW_CLUSTER:nacos} # other configurations Nacos support authenticate by username or accessKey, empty means no need auth. extra config is bellow:\nnacos: username: password: accessKey: secretKey: Same as Zookeeper coordinator, in some cases, oap default gRPC host and port in core are not suitable for internal communication among the oap nodes. The following setting are provided to set the host and port manually, based on your own LAN env.\n internalComHost, the host registered and other oap node use this to communicate with current node. internalComPort, the port registered and other oap node use this to communicate with current node.  ","excerpt":"Cluster Management In many product environments, backend needs to support high throughput and …","ref":"/docs/main/v8.3.0/en/setup/backend/backend-cluster/","title":"Cluster Management"},{"body":"Compatible with other javaagent bytecode processing Problem   When use skywalking agent, some other agent, such as Arthas, can\u0026rsquo;t work well https://github.com/apache/skywalking/pull/4858\n  Java agent retransforming class fails with Skywalking agent, such as in this demo\n  Reason SkyWalking agent uses ByteBuddy to transform classes when the Java application starts. ByteBuddy generates auxiliary classes with different random names every time.\nWhen another java agent retransforms the same class, it triggers the SkyWalking agent to enhance the class again. The bytecode regenerated by ByteBuddy is changed, the fields and imported class names are modified, the JVM verifications about class bytecode fail, causing the retransform fails.\nResolve 1.Enable the class cache feature\nAdd JVM parameters:\n-Dskywalking.agent.is_cache_enhanced_class=true -Dskywalking.agent.class_cache_mode=MEMORY\nOr uncomment options in agent.conf:\n# If true, SkyWalking agent will cache all instrumented classes files to memory or disk files (decided by class cache mode), # allow other javaagent to enhance those classes that enhanced by SkyWalking agent. agent.is_cache_enhanced_class = ${SW_AGENT_CACHE_CLASS:false} # The instrumented classes cache mode: MEMORY or FILE # MEMORY: cache class bytes to memory, if instrumented classes is too many or too large, it may take up more memory # FILE: cache class bytes to user temp folder starts with 'class-cache', automatically clean up cached class files when the application exits agent.class_cache_mode = ${SW_AGENT_CLASS_CACHE_MODE:MEMORY} If the class cache feature is enabled, save the instrumented class bytecode to memory or a temporary file. When other java agents retransform the same class, SkyWalking agent first attempts to load from the cache.\nIf the cached class is found, it will be used directly without regenerating a new random name auxiliary class, which will not affect the processing of the subsequent java agent.\n2.Class cache save mode\nIt is recommended to put the cache class in memory, meanwhile if it costs more memory resources. Another option is using the local file system. Set the class cache mode through the following options:\n-Dskywalking.agent.class_cache_mode=MEMORY : save cache classes to java memory. -Dskywalking.agent.class_cache_mode=FILE : save cache classes to SkyWalking agent path \u0026lsquo;/class-cache\u0026rsquo;.\nOr modify the option in agent.conf:\nagent.class_cache_mode = ${SW_AGENT_CLASS_CACHE_MODE:MEMORY} agent.class_cache_mode = ${SW_AGENT_CLASS_CACHE_MODE:FILE}\n","excerpt":"Compatible with other javaagent bytecode processing Problem   When use skywalking agent, some other …","ref":"/docs/main/v8.3.0/en/faq/compatible-with-other-javaagent-bytecode-processing/","title":"Compatible with other javaagent bytecode processing"},{"body":"Component library settings Component library settings are about your own or 3rd part libraries used in monitored application.\nIn agent or SDK, no matter library name collected as ID or String(literally, e.g. SpringMVC), collector formats data in ID for better performance and less storage requirements.\nAlso, collector conjectures the remote service based on the component library, such as: the component library is MySQL Driver library, then the remote service should be MySQL Server.\nFor those two reasons, collector require two parts of settings in this file:\n Component Library id, name and languages. Remote server mapping, based on local library.  All component names and IDs must be defined in this file.\nComponent Library id Define all component libraries' names and IDs, used in monitored application. This is a both-way mapping, agent or SDK could use the value(ID) to represent the component name in uplink data.\n Name: the component name used in agent and UI id: Unique ID. All IDs are reserved, once it is released. languages: Program languages may use this component. Multi languages should be separated by ,  ID rules  Java and multi languages shared: (0, 3000) .NET Platform reserved: [3000, 4000) Node.js Platform reserved: [4000, 5000) Go reserved: [5000, 6000) Lua reserved: [6000, 7000) Python reserved: [7000, 8000) PHP reserved: [8000, 9000) C++ reserved: [9000, 10000)  Example\nTomcat: id: 1 languages: Java HttpClient: id: 2 languages: Java,C#,Node.js Dubbo: id: 3 languages: Java H2: id: 4 languages: Java Remote server mapping Remote server will be conjectured by the local component. The mappings are based on Component library names.\n Key: client component library name Value: server component name  Component-Server-Mappings: Jedis: Redis StackExchange.Redis: Redis Redisson: Redis Lettuce: Redis Zookeeper: Zookeeper SqlClient: SqlServer Npgsql: PostgreSQL MySqlConnector: Mysql EntityFrameworkCore.InMemory: InMemoryDatabase ","excerpt":"Component library settings Component library settings are about your own or 3rd part libraries used …","ref":"/docs/main/v8.3.0/en/guides/component-library-settings/","title":"Component library settings"},{"body":"Concepts and Designs Concepts and Designs help you to learn and understand the SkyWalking and the landscape.\n What is SkyWalking?  Overview and Core concepts. Provides a high-level description and introduction, including the problems the project solves. Project Goals. Provides the goals, which SkyWalking is trying to focus and provide features about them.    After you read the above documents, you should understand the SkyWalking basic goals. Now, you can choose which following parts you are interested, then dive in.\n Probe  Introduction. Lead readers to understand what the probe is, how many different probes existed and why need them. Service auto instrument agent. Introduce what the auto instrument agents do and which languages does SkyWalking already support. Manual instrument SDK. Introduce the role of the manual instrument SDKs in SkyWalking ecosystem. Service Mesh probe. Introduce why and how SkyWalking receive telemetry data from Service mesh and proxy probe.   Backend  Overview. Provides a high level introduction about the OAP backend. Observability Analysis Language. Introduces the core languages, which is designed for aggregation behaviour definition. Query in OAP. A set of query protocol provided, based on the Observability Analysis Language metrics definition.   UI  Overview. A simple brief about SkyWalking UI.   CLI  SkyWalking CLI. A command line interface for SkyWalking.    ","excerpt":"Concepts and Designs Concepts and Designs help you to learn and understand the SkyWalking and the …","ref":"/docs/main/v8.3.0/en/concepts-and-designs/readme/","title":"Concepts and Designs"},{"body":"Configuration Vocabulary Configuration Vocabulary lists all available configurations provided by application.yml.\n   Module Provider Settings Value(s) and Explanation System Environment Variable¹ Default     core default role Option values, Mixed/Receiver/Aggregator. Receiver mode OAP open the service to the agents, analysis and aggregate the results and forward the results for distributed aggregation. Aggregator mode OAP receives data from Mixer and Receiver role OAP nodes, and do 2nd level aggregation. Mixer means being Receiver and Aggregator both. SW_CORE_ROLE Mixed   - - restHost Binding IP of restful service. Services include GraphQL query and HTTP data report SW_CORE_REST_HOST 0.0.0.0   - - restPort Binding port of restful service SW_CORE_REST_PORT 12800   - - restContextPath Web context path of restful service SW_CORE_REST_CONTEXT_PATH /   - - restMinThreads Min threads number of restful service SW_CORE_REST_JETTY_MIN_THREADS 1   - - restMaxThreads Max threads number of restful service SW_CORE_REST_JETTY_MAX_THREADS 200   - - restIdleTimeOut Connector idle timeout in milliseconds of restful service SW_CORE_REST_JETTY_IDLE_TIMEOUT 30000   - - restAcceptorPriorityDelta Thread priority delta to give to acceptor threads of restful service SW_CORE_REST_JETTY_DELTA 0   - - restAcceptQueueSize ServerSocketChannel backlog of restful service SW_CORE_REST_JETTY_QUEUE_SIZE 0   - - gRPCHost Binding IP of gRPC service. Services include gRPC data report and internal communication among OAP nodes SW_CORE_GRPC_HOST 0.0.0.0   - - gRPCPort Binding port of gRPC service SW_CORE_GRPC_PORT 11800   - - gRPCSslEnabled Activate SSL for gRPC service SW_CORE_GRPC_SSL_ENABLED false   - - gRPCSslKeyPath The file path of gRPC SSL key SW_CORE_GRPC_SSL_KEY_PATH -   - - gRPCSslCertChainPath The file path of gRPC SSL cert chain SW_CORE_GRPC_SSL_CERT_CHAIN_PATH -   - - gRPCSslTrustedCAPath The file path of gRPC trusted CA SW_CORE_GRPC_SSL_TRUSTED_CA_PATH -   - - downsampling The activated level of down sampling aggregation  Hour,Day   - - enableDataKeeperExecutor Controller of TTL scheduler. Once disabled, TTL wouldn\u0026rsquo;t work. SW_CORE_ENABLE_DATA_KEEPER_EXECUTOR true   - - dataKeeperExecutePeriod The execution period of TTL scheduler, unit is minute. Execution doesn\u0026rsquo;t mean deleting data. The storage provider could override this, such as ElasticSearch storage. SW_CORE_DATA_KEEPER_EXECUTE_PERIOD 5   - - recordDataTTL The lifecycle of record data. Record data includes traces, top n sampled records, and logs. Unit is day. Minimal value is 2. SW_CORE_RECORD_DATA_TTL 3   - - metricsDataTTL The lifecycle of metrics data, including the metadata. Unit is day. Recommend metricsDataTTL \u0026gt;= recordDataTTL. Minimal value is 2. SW_CORE_METRICS_DATA_TTL 7   - - enableDatabaseSession Cache metrics data for 1 minute to reduce database queries, and if the OAP cluster changes within that minute. SW_CORE_ENABLE_DATABASE_SESSION true   - - topNReportPeriod The execution period of top N sampler, which saves sampled data into the storage. Unit is minute SW_CORE_TOPN_REPORT_PERIOD 10   - - activeExtraModelColumns Append the names of entity, such as service name, into the metrics storage entities. SW_CORE_ACTIVE_EXTRA_MODEL_COLUMNS false   - - serviceNameMaxLength Max length limitation of service name. SW_SERVICE_NAME_MAX_LENGTH 70   - - instanceNameMaxLength Max length limitation of service instance name. The max length of service + instance names should be less than 200. SW_INSTANCE_NAME_MAX_LENGTH 70   - - endpointNameMaxLength Max length limitation of endpoint name. The max length of service + endpoint names should be less than 240. SW_ENDPOINT_NAME_MAX_LENGTH 150   - - searchableTracesTags Define the set of span tag keys, which should be searchable through the GraphQL. Multiple values should be separated through the comma. SW_SEARCHABLE_TAG_KEYS http.method,status_code,db.type,db.instance,mq.queue,mq.topic,mq.broker   - - gRPCThreadPoolSize Pool size of gRPC server SW_CORE_GRPC_THREAD_POOL_SIZE CPU core * 4   - - gRPCThreadPoolQueueSize The queue size of gRPC server SW_CORE_GRPC_POOL_QUEUE_SIZE 10000   - - maxConcurrentCallsPerConnection The maximum number of concurrent calls permitted for each incoming connection. Defaults to no limit. SW_CORE_GRPC_MAX_CONCURRENT_CALL -   - - maxMessageSize Sets the maximum message size allowed to be received on the server. Empty means 4 MiB SW_CORE_GRPC_MAX_MESSAGE_SIZE 4M(based on Netty)   - - remoteTimeout Timeout for cluster internal communication, in seconds. - 20   - - maxSizeOfNetworkAddressAlias Max size of network address detected in the be monitored system. - 1_000_000   - - maxPageSizeOfQueryProfileSnapshot The max size in every OAP query for snapshot analysis - 500   - - maxSizeOfAnalyzeProfileSnapshot The max number of snapshots analyzed by OAP - 12000   cluster standalone - standalone is not suitable for one node running, no available configuration. - -   - zookeeper nameSpace The namespace, represented by root path, isolates the configurations in the zookeeper. SW_NAMESPACE /, root path   - - hostPort hosts and ports of Zookeeper Cluster SW_CLUSTER_ZK_HOST_PORT localhost:2181   - - baseSleepTimeMs The period of Zookeeper client between two retries. Unit is ms. SW_CLUSTER_ZK_SLEEP_TIME 1000   - - maxRetries The max retry time of re-trying. SW_CLUSTER_ZK_MAX_RETRIES 3   - - enableACL Open ACL by using schema and expression SW_ZK_ENABLE_ACL false   - - schema schema for the authorization SW_ZK_SCHEMA digest   - - expression expression for the authorization SW_ZK_EXPRESSION skywalking:skywalking   - - internalComHost The hostname registered in the Zookeeper for the internal communication of OAP cluster. - -   - - internalComPort The port registered in the Zookeeper for the internal communication of OAP cluster. - -1   - kubernetes namespace Namespace SkyWalking deployed in the k8s SW_CLUSTER_K8S_NAMESPACE default   - - labelSelector Labels used for filtering the OAP deployment in the k8s SW_CLUSTER_K8S_LABEL app=collector,release=skywalking   - - uidEnvName Environment variable name for reading uid. SW_CLUSTER_K8S_UID SKYWALKING_COLLECTOR_UID   - consul serviceName Service name used for SkyWalking cluster. SW_SERVICE_NAME SkyWalking_OAP_Cluster   - - hostPort hosts and ports used of Consul cluster. SW_CLUSTER_CONSUL_HOST_PORT localhost:8500   - - aclToken ALC Token of Consul. Empty string means without ALC token. SW_CLUSTER_CONSUL_ACLTOKEN -   - - internalComHost The hostname registered in the Consul for the internal communication of OAP cluster. - -   - - internalComPort The port registered in the Consul for the internal communication of OAP cluster. - -1   - etcd serviceName Service name used for SkyWalking cluster. SW_SERVICE_NAME SkyWalking_OAP_Cluster   - - hostPort hosts and ports used of etcd cluster. SW_CLUSTER_ETCD_HOST_PORT localhost:2379   - - isSSL Open SSL for the connection between SkyWalking and etcd cluster. - -   - - internalComHost The hostname registered in the etcd for the internal communication of OAP cluster. - -   - - internalComPort The port registered in the etcd for the internal communication of OAP cluster. - -1   - Nacos serviceName Service name used for SkyWalking cluster. SW_SERVICE_NAME SkyWalking_OAP_Cluster   - - hostPort hosts and ports used of Nacos cluster. SW_CLUSTER_NACOS_HOST_PORT localhost:8848   - - namespace Namespace used by SkyWalking node coordination. SW_CLUSTER_NACOS_NAMESPACE public   - - internalComHost The hostname registered in the Nacos for the internal communication of OAP cluster. - -   - - internalComPort The port registered in the Nacos for the internal communication of OAP cluster. - -1   - - username Nacos Auth username SW_CLUSTER_NACOS_USERNAME -   - - password Nacos Auth password SW_CLUSTER_NACOS_PASSWORD -   - - accessKey Nacos Auth accessKey SW_CLUSTER_NACOS_ACCESSKEY -   - - secretKey Nacos Auth secretKey SW_CLUSTER_NACOS_SECRETKEY -   storage elasticsearch - ElasticSearch 6 storage implementation - -   - - nameSpace Prefix of indexes created and used by SkyWalking. SW_NAMESPACE -   - - clusterNodes ElasticSearch cluster nodes for client connection. SW_STORAGE_ES_CLUSTER_NODES localhost   - - protocol HTTP or HTTPs. SW_STORAGE_ES_HTTP_PROTOCOL HTTP   - - user User name of ElasticSearch cluster SW_ES_USER -   - - password Password of ElasticSearch cluster SW_ES_PASSWORD -   - - trustStorePath Trust JKS file path. Only work when user name and password opened SW_STORAGE_ES_SSL_JKS_PATH -   - - trustStorePass Trust JKS file password. Only work when user name and password opened SW_STORAGE_ES_SSL_JKS_PASS -   - - secretsManagementFile Secrets management file in the properties format includes the username, password, which are managed by 3rd party tool. Provide the capability to update them in the runtime. SW_ES_SECRETS_MANAGEMENT_FILE -   - - dayStep Represent the number of days in the one minute/hour/day index. SW_STORAGE_DAY_STEP 1   - - indexShardsNumber Shard number of new indexes SW_STORAGE_ES_INDEX_SHARDS_NUMBER 1   - - indexReplicasNumber Replicas number of new indexes SW_STORAGE_ES_INDEX_REPLICAS_NUMBER 0   - - superDatasetDayStep Represent the number of days in the super size dataset record index, the default value is the same as dayStep when the value is less than 0. SW_SUPERDATASET_STORAGE_DAY_STEP -1   - - superDatasetIndexShardsFactor Super data set has been defined in the codes, such as trace segments. This factor provides more shards for the super data set, shards number = indexShardsNumber * superDatasetIndexShardsFactor. Also, this factor effects Zipkin and Jaeger traces. SW_STORAGE_ES_SUPER_DATASET_INDEX_SHARDS_FACTOR 5   - - superDatasetIndexReplicasNumber Represent the replicas number in the super size dataset record index. SW_STORAGE_ES_SUPER_DATASET_INDEX_REPLICAS_NUMBER 0   - - bulkActions Async bulk size of the record data batch execution. SW_STORAGE_ES_BULK_ACTIONS 1000   - - syncBulkActions Sync bulk size of the metrics data batch execution. SW_STORAGE_ES_SYNC_BULK_ACTIONS 50000   - - flushInterval Period of flush, no matter bulkActions reached or not. Unit is second. SW_STORAGE_ES_FLUSH_INTERVAL 10   - - concurrentRequests The number of concurrent requests allowed to be executed. SW_STORAGE_ES_CONCURRENT_REQUESTS 2   - - resultWindowMaxSize The max size of dataset when OAP loading cache, such as network alias. SW_STORAGE_ES_QUERY_MAX_WINDOW_SIZE 10000   - - metadataQueryMaxSize The max size of metadata per query. SW_STORAGE_ES_QUERY_MAX_SIZE 5000   - - segmentQueryMaxSize The max size of trace segments per query. SW_STORAGE_ES_QUERY_SEGMENT_SIZE 200   - - profileTaskQueryMaxSize The max size of profile task per query. SW_STORAGE_ES_QUERY_PROFILE_TASK_SIZE 200   - - advanced All settings of ElasticSearch index creation. The value should be in JSON format SW_STORAGE_ES_ADVANCED -   - elasticsearch7 - ElasticSearch 7 storage implementation - -   - - nameSpace Prefix of indexes created and used by SkyWalking. SW_NAMESPACE -   - - clusterNodes ElasticSearch cluster nodes for client connection. SW_STORAGE_ES_CLUSTER_NODES localhost   - - protocol HTTP or HTTPs. SW_STORAGE_ES_HTTP_PROTOCOL HTTP   - - user User name of ElasticSearch cluster SW_ES_USER -   - - password Password of ElasticSearch cluster SW_ES_PASSWORD -   - - trustStorePath Trust JKS file path. Only work when user name and password opened SW_STORAGE_ES_SSL_JKS_PATH -   - - trustStorePass Trust JKS file password. Only work when user name and password opened SW_STORAGE_ES_SSL_JKS_PASS -   - - secretsManagementFile Secrets management file in the properties format includes the username, password, which are managed by 3rd party tool. Provide the capability to update them in the runtime. SW_ES_SECRETS_MANAGEMENT_FILE -   - - dayStep Represent the number of days in the one minute/hour/day index. SW_STORAGE_DAY_STEP 1   - - indexShardsNumber Shard number of new indexes SW_STORAGE_ES_INDEX_SHARDS_NUMBER 1   - - indexReplicasNumber Replicas number of new indexes SW_STORAGE_ES_INDEX_REPLICAS_NUMBER 0   - - superDatasetDayStep Represent the number of days in the super size dataset record index, the default value is the same as dayStep when the value is less than 0. SW_SUPERDATASET_STORAGE_DAY_STEP -1   - - superDatasetIndexShardsFactor Super data set has been defined in the codes, such as trace segments. This factor provides more shards for the super data set, shards number = indexShardsNumber * superDatasetIndexShardsFactor. Also, this factor effects Zipkin and Jaeger traces. SW_STORAGE_ES_SUPER_DATASET_INDEX_SHARDS_FACTOR 5   - - superDatasetIndexReplicasNumber Represent the replicas number in the super size dataset record index. SW_STORAGE_ES_SUPER_DATASET_INDEX_REPLICAS_NUMBER 0   - - bulkActions Async bulk size of the record data batch execution. SW_STORAGE_ES_BULK_ACTIONS 1000   - - syncBulkActions Sync bulk size of the metrics data batch execution. SW_STORAGE_ES_SYNC_BULK_ACTIONS 50000   - - flushInterval Period of flush, no matter bulkActions reached or not. Unit is second. SW_STORAGE_ES_FLUSH_INTERVAL 10   - - concurrentRequests The number of concurrent requests allowed to be executed. SW_STORAGE_ES_CONCURRENT_REQUESTS 2   - - resultWindowMaxSize The max size of dataset when OAP loading cache, such as network alias. SW_STORAGE_ES_QUERY_MAX_WINDOW_SIZE 10000   - - metadataQueryMaxSize The max size of metadata per query. SW_STORAGE_ES_QUERY_MAX_SIZE 5000   - - segmentQueryMaxSize The max size of trace segments per query. SW_STORAGE_ES_QUERY_SEGMENT_SIZE 200   - - profileTaskQueryMaxSize The max size of profile task per query. SW_STORAGE_ES_QUERY_PROFILE_TASK_SIZE 200   - - advanced All settings of ElasticSearch index creation. The value should be in JSON format SW_STORAGE_ES_ADVANCED -   - h2 - H2 storage is designed for demonstration and running in short term(1-2 hours) only - -   - - driver H2 JDBC driver. SW_STORAGE_H2_DRIVER org.h2.jdbcx.JdbcDataSource   - - url H2 connection URL. Default is H2 memory mode SW_STORAGE_H2_URL jdbc:h2:mem:skywalking-oap-db   - - user User name of H2 database. SW_STORAGE_H2_USER sa   - - password Password of H2 database. - -   - - metadataQueryMaxSize The max size of metadata per query. SW_STORAGE_H2_QUERY_MAX_SIZE 5000   - - maxSizeOfArrayColumn Some entities, such as trace segment, include the logic column with multiple values. In the H2, we use multiple physical columns to host the values, such as, Change column_a with values [1,2,3,4,5] to column_a_0 = 1, column_a_1 = 2, column_a_2 = 3 , column_a_3 = 4, column_a_4 = 5 SW_STORAGE_MAX_SIZE_OF_ARRAY_COLUMN 20   - - numOfSearchableValuesPerTag In a trace segment, it includes multiple spans with multiple tags. Different spans could have same tag keys, such as multiple HTTP exit spans all have their own http.method tag. This configuration set the limitation of max num of values for the same tag key. SW_STORAGE_NUM_OF_SEARCHABLE_VALUES_PER_TAG 2   - mysql - MySQL Storage. The MySQL JDBC Driver is not in the dist, please copy it into oap-lib folder manually - -   - - properties Hikari connection pool configurations - Listed in the application.yaml.   - - metadataQueryMaxSize The max size of metadata per query. SW_STORAGE_MYSQL_QUERY_MAX_SIZE 5000   - - maxSizeOfArrayColumn Some entities, such as trace segment, include the logic column with multiple values. In the MySQL, we use multiple physical columns to host the values, such as, Change column_a with values [1,2,3,4,5] to column_a_0 = 1, column_a_1 = 2, column_a_2 = 3 , column_a_3 = 4, column_a_4 = 5 SW_STORAGE_MAX_SIZE_OF_ARRAY_COLUMN 20   - - numOfSearchableValuesPerTag In a trace segment, it includes multiple spans with multiple tags. Different spans could have same tag keys, such as multiple HTTP exit spans all have their own http.method tag. This configuration set the limitation of max num of values for the same tag key. SW_STORAGE_NUM_OF_SEARCHABLE_VALUES_PER_TAG 2   - influxdb - InfluxDB storage. - -   - - url InfluxDB connection URL. SW_STORAGE_INFLUXDB_URL http://localhost:8086   - - user User name of InfluxDB. SW_STORAGE_INFLUXDB_USER root   - - password Password of InfluxDB. SW_STORAGE_INFLUXDB_PASSWORD -   - - database Database of InfluxDB. SW_STORAGE_INFLUXDB_DATABASE skywalking   - - actions The number of actions to collect. SW_STORAGE_INFLUXDB_ACTIONS 1000   - - duration The time to wait at most (milliseconds). SW_STORAGE_INFLUXDB_DURATION 1000   - - batchEnabled If true, write points with batch api. SW_STORAGE_INFLUXDB_BATCH_ENABLED true   - - fetchTaskLogMaxSize The max number of fetch task log in a request. SW_STORAGE_INFLUXDB_FETCH_TASK_LOG_MAX_SIZE 5000   agent-analyzer default Agent Analyzer. SW_AGENT_ANALYZER default    - - sampleRate Sampling rate for receiving trace. The precision is 1/10000. 10000 means 100% sample in default. SW_TRACE_SAMPLE_RATE 10000   - - slowDBAccessThreshold The slow database access thresholds. Unit ms. SW_SLOW_DB_THRESHOLD default:200,mongodb:100   - - forceSampleErrorSegment When sampling mechanism activated, this config would make the error status segment sampled, ignoring the sampling rate. SW_FORCE_SAMPLE_ERROR_SEGMENT true   - - segmentStatusAnalysisStrategy Determine the final segment status from the status of spans. Available values are FROM_SPAN_STATUS , FROM_ENTRY_SPAN and FROM_FIRST_SPAN. FROM_SPAN_STATUS represents the segment status would be error if any span is in error status. FROM_ENTRY_SPAN means the segment status would be determined by the status of entry spans only. FROM_FIRST_SPAN means the segment status would be determined by the status of the first span only. SW_SEGMENT_STATUS_ANALYSIS_STRATEGY FROM_SPAN_STATUS   - - noUpstreamRealAddressAgents Exit spans with the component in the list would not generate the client-side instance relation metrics. As some tracing plugins can\u0026rsquo;t collect the real peer ip address, such as Nginx-LUA and Envoy. SW_NO_UPSTREAM_REAL_ADDRESS 6000,9000   - - slowTraceSegmentThreshold Setting this threshold about the latency would make the slow trace segments sampled if they cost more time, even the sampling mechanism activated. The default value is -1, which means would not sample slow traces. Unit, millisecond. SW_SLOW_TRACE_SEGMENT_THRESHOLD -1   - - meterAnalyzerActiveFiles Which files could be meter analyzed, files split by \u0026ldquo;,\u0026rdquo; SW_METER_ANALYZER_ACTIVE_FILES    receiver-sharing-server default Sharing server provides new gRPC and restful servers for data collection. Ana make the servers in the core module working for internal communication only. - -    - - restHost Binding IP of restful service. Services include GraphQL query and HTTP data report SW_RECEIVER_SHARING_REST_HOST -   - - restPort Binding port of restful service SW_RECEIVER_SHARING_REST_PORT -   - - restContextPath Web context path of restful service SW_RECEIVER_SHARING_REST_CONTEXT_PATH -   - - restMinThreads Min threads number of restful service SW_RECEIVER_SHARING_JETTY_MIN_THREADS 1   - - restMaxThreads Max threads number of restful service SW_RECEIVER_SHARING_JETTY_MAX_THREADS 200   - - restIdleTimeOut Connector idle timeout in milliseconds of restful service SW_RECEIVER_SHARING_JETTY_IDLE_TIMEOUT 30000   - - restAcceptorPriorityDelta Thread priority delta to give to acceptor threads of restful service SW_RECEIVER_SHARING_JETTY_DELTA 0   - - restAcceptQueueSize ServerSocketChannel backlog of restful service SW_RECEIVER_SHARING_JETTY_QUEUE_SIZE 0   - - gRPCHost Binding IP of gRPC service. Services include gRPC data report and internal communication among OAP nodes SW_RECEIVER_GRPC_HOST 0.0.0.0. Not Activated   - - gRPCPort Binding port of gRPC service SW_RECEIVER_GRPC_PORT Not Activated   - - gRPCThreadPoolSize Pool size of gRPC server SW_RECEIVER_GRPC_THREAD_POOL_SIZE CPU core * 4   - - gRPCThreadPoolQueueSize The queue size of gRPC server SW_RECEIVER_GRPC_POOL_QUEUE_SIZE 10000   - - gRPCSslEnabled Activate SSL for gRPC service SW_RECEIVER_GRPC_SSL_ENABLED false   - - gRPCSslKeyPath The file path of gRPC SSL key SW_RECEIVER_GRPC_SSL_KEY_PATH -   - - gRPCSslCertChainPath The file path of gRPC SSL cert chain SW_RECEIVER_GRPC_SSL_CERT_CHAIN_PATH -   - - maxConcurrentCallsPerConnection The maximum number of concurrent calls permitted for each incoming connection. Defaults to no limit. SW_RECEIVER_GRPC_MAX_CONCURRENT_CALL -   - - authentication The token text for the authentication. Work for gRPC connection only. Once this is set, the client is required to use the same token. SW_AUTHENTICATION -   receiver-register default Read receiver doc for more details - -    receiver-trace default Read receiver doc for more details - -    receiver-jvm default Read receiver doc for more details - -    receiver-clr default Read receiver doc for more details - -    receiver-profile default Read receiver doc for more details - -    service-mesh default Read receiver doc for more details - -    envoy-metric default Read receiver doc for more details - -    - - acceptMetricsService Open Envoy Metrics Service analysis SW_ENVOY_METRIC_SERVICE true   - - alsHTTPAnalysis Open Envoy Access Log Service analysis. Value = k8s-mesh means open the analysis SW_ENVOY_METRIC_ALS_HTTP_ANALYSIS -   - - k8sServiceNameRule k8sServiceNameRule allows you to customize the service name in ALS via Kubernetes metadata, the available variables are pod, service, e.g., you can use ${service.metadata.name}-${pod.metadata.labels.version} to append the version number to the service name. Be careful, when using environment variables to pass this configuration, use single quotes('') to avoid it being evaluated by the shell. -    receiver-otel default Read receiver doc for more details - -    - - enabledHandlers Enabled handlers for otel SW_OTEL_RECEIVER_ENABLED_HANDLERS -   - - enabledOcRules Enabled metric rules for OC handler SW_OTEL_RECEIVER_ENABLED_OC_RULES -   receiver_zipkin default Read receiver doc - -    - - restHost Binding IP of restful service. SW_RECEIVER_ZIPKIN_HOST 0.0.0.0   - - restPort Binding port of restful service SW_RECEIVER_ZIPKIN_PORT 9411   - - restContextPath Web context path of restful service SW_RECEIVER_ZIPKIN_CONTEXT_PATH /   - - needAnalysis Analysis zipkin span to generate metrics - false   - - maxCacheSize Max cache size for span analysis - 1_000_000   - - expireTime The expire time of analysis cache, unit is second. - 20   receiver_jaeger default Read receiver doc - -    - - gRPCHost Binding IP of gRPC service. Services include gRPC data report and internal communication among OAP nodes SW_RECEIVER_JAEGER_HOST -   - - gRPCPort Binding port of gRPC service SW_RECEIVER_JAEGER_PORT -   - - gRPCThreadPoolSize Pool size of gRPC server - CPU core * 4   - - gRPCThreadPoolQueueSize The queue size of gRPC server - 10000   - - maxConcurrentCallsPerConnection The maximum number of concurrent calls permitted for each incoming connection. Defaults to no limit. - -   - - maxMessageSize Sets the maximum message size allowed to be received on the server. Empty means 4 MiB - 4M(based on Netty)   prometheus-fetcher default Read fetcher doc for more details - -    - - active Activate the Prometheus fetcher. SW_PROMETHEUS_FETCHER_ACTIVE false   kafka-fetcher default Read fetcher doc for more details - -    - - bootstrapServers A list of host/port pairs to use for establishing the initial connection to the Kafka cluster. SW_KAFKA_FETCHER_SERVERS localhost:9092   - - groupId A unique string that identifies the consumer group this consumer belongs to. - skywalking-consumer   - - consumePartitions Which PartitionId(s) of the topics assign to the OAP server. If more than one, is separated by commas. SW_KAFKA_FETCHER_CONSUME_PARTITIONS -   - - isSharding it was true when OAP Server in cluster. SW_KAFKA_FETCHER_IS_SHARDING false   - - createTopicIfNotExist If true, create the Kafka topic when it does not exist. - true   - - partitions The number of partitions for the topic being created. SW_KAFKA_FETCHER_PARTITIONS 3   - - enableMeterSystem To enable to fetch and handle Meter System data. SW_KAFKA_FETCHER_ENABLE_METER_SYSTEM false   - - replicationFactor The replication factor for each partition in the topic being created. SW_KAFKA_FETCHER_PARTITIONS_FACTOR 2   - - kafkaHandlerThreadPoolSize Pool size of kafka message handler executor. SW_KAFKA_HANDLER_THREAD_POOL_SIZE CPU core * 2   - - kafkaHandlerThreadPoolQueueSize The queue size of kafka message handler executor. SW_KAFKA_HANDLER_THREAD_POOL_QUEUE_SIZE 10000   - - topicNameOfMeters Specifying Kafka topic name for Meter system data. - skywalking-meters   - - topicNameOfMetrics Specifying Kafka topic name for JVM Metrics data. - skywalking-metrics   - - topicNameOfProfiling Specifying Kafka topic name for Profiling data. - skywalking-profilings   - - topicNameOfTracingSegments Specifying Kafka topic name for Tracing data. - skywalking-segments   - - topicNameOfManagements Specifying Kafka topic name for service instance reporting and registering. - skywalking-managements   receiver-browser default Read receiver doc for more details - - -   - - sampleRate Sampling rate for receiving trace. The precision is 1/10000. 10000 means 100% sample in default. SW_RECEIVER_BROWSER_SAMPLE_RATE 10000   query graphql - GraphQL query implementation -    - - path Root path of GraphQL query and mutation. SW_QUERY_GRAPHQL_PATH /graphql   alarm default - Read alarm doc for more details. -    telemetry - - Read telemetry doc for more details. -    - none - No op implementation -    - prometheus host Binding host for Prometheus server fetching data SW_TELEMETRY_PROMETHEUS_HOST 0.0.0.0   - - port Binding port for Prometheus server fetching data SW_TELEMETRY_PROMETHEUS_PORT 1234   configuration - - Read dynamic configuration doc for more details. -    - grpc host DCS server binding hostname SW_DCS_SERVER_HOST -   - - port DCS server binding port SW_DCS_SERVER_PORT 80   - - clusterName Cluster name when reading latest configuration from DSC server. SW_DCS_CLUSTER_NAME SkyWalking   - - period The period of OAP reading data from DSC server. Unit is second. SW_DCS_PERIOD 20   - apollo apolloMeta apollo.meta in Apollo SW_CONFIG_APOLLO http://106.12.25.204:8080   - - apolloCluster apollo.cluster in Apollo SW_CONFIG_APOLLO_CLUSTER default   - - apolloEnv env in Apollo SW_CONFIG_APOLLO_ENV -   - - appId app.id in Apollo SW_CONFIG_APOLLO_APP_ID skywalking   - - period The period of data sync. Unit is second. SW_CONFIG_APOLLO_PERIOD 60   - zookeeper nameSpace The namespace, represented by root path, isolates the configurations in the zookeeper. SW_CONFIG_ZK_NAMESPACE /, root path   - - hostPort hosts and ports of Zookeeper Cluster SW_CONFIG_ZK_HOST_PORT localhost:2181   - - baseSleepTimeMs The period of Zookeeper client between two retries. Unit is ms. SW_CONFIG_ZK_BASE_SLEEP_TIME_MS 1000   - - maxRetries The max retry time of re-trying. SW_CONFIG_ZK_MAX_RETRIES 3   - - period The period of data sync. Unit is second. SW_CONFIG_ZK_PERIOD 60   - etcd clusterName Service name used for SkyWalking cluster. SW_CONFIG_ETCD_CLUSTER_NAME default   - - serverAddr hosts and ports used of etcd cluster. SW_CONFIG_ETCD_SERVER_ADDR localhost:2379   - - group Additional prefix of the configuration key SW_CONFIG_ETCD_GROUP skywalking   - - period The period of data sync. Unit is second. SW_CONFIG_ZK_PERIOD 60   - consul hostPort hosts and ports used of Consul cluster. SW_CONFIG_CONSUL_HOST_AND_PORTS localhost:8500   - - aclToken ALC Token of Consul. Empty string means without ALC token. SW_CONFIG_CONSUL_ACL_TOKEN -   - - period The period of data sync. Unit is second. SW_CONFIG_CONSUL_PERIOD 60   - k8s-configmap namespace Deployment namespace of the config map. SW_CLUSTER_K8S_NAMESPACE default   - - labelSelector Labels used for locating configmap. SW_CLUSTER_K8S_LABEL app=collector,release=skywalking   - - period The period of data sync. Unit is second. SW_CONFIG_ZK_PERIOD 60   - nacos serverAddr Nacos Server Host SW_CONFIG_NACOS_SERVER_ADDR 127.0.0.1   - - port Nacos Server Port SW_CONFIG_NACOS_SERVER_PORT 8848   - - group Nacos Configuration namespace SW_CONFIG_NACOS_SERVER_NAMESPACE -   - - period The period of data sync. Unit is second. SW_CONFIG_CONFIG_NACOS_PERIOD 60   - - username Nacos Auth username SW_CONFIG_NACOS_USERNAME -   - - password Nacos Auth password SW_CONFIG_NACOS_PASSWORD -   - - accessKey Nacos Auth accessKey SW_CONFIG_NACOS_ACCESSKEY -   - - secretKey Nacos Auth secretKey SW_CONFIG_NACOS_SECRETKEY -   exporter grpc targetHost The host of target grpc server for receiving export data. SW_EXPORTER_GRPC_HOST 127.0.0.1   - - targetPort The port of target grpc server for receiving export data. SW_EXPORTER_GRPC_PORT 9870   health-checker default checkIntervalSeconds The period of check OAP internal health status. Unit is second. SW_HEALTH_CHECKER_INTERVAL_SECONDS 5    Notice ¹ System Environment Variable name could be declared and changed in the application.yml. The names listed here, are just provided in the default application.yml file.\n","excerpt":"Configuration Vocabulary Configuration Vocabulary lists all available configurations provided by …","ref":"/docs/main/v8.3.0/en/setup/backend/configuration-vocabulary/","title":"Configuration Vocabulary"},{"body":"Configuring Envoy to send metrics to SkyWalking In order to let Envoy to send metrics to SkyWalking, we need to feed Envoy with a configuration which contains stats_sinks that includes envoy.metrics_service. This envoy.metrics_service should be configured as a config.grpc_service entry.\nThe interesting parts of the config is shown in the config below:\nstats_sinks: - name: envoy.metrics_service config: grpc_service: # Note: we can use google_grpc implementation as well. envoy_grpc: cluster_name: service_skywalking static_resources: ... clusters: - name: service_skywalking connect_timeout: 5s type: LOGICAL_DNS http2_protocol_options: {} dns_lookup_family: V4_ONLY lb_policy: ROUND_ROBIN load_assignment: cluster_name: service_skywalking endpoints: - lb_endpoints: - endpoint: address: socket_address: address: skywalking # This is the port where SkyWalking serving the Envoy Metrics Service gRPC stream. port_value: 11800 A more complete static configuration, can be observed here.\nNote that Envoy can also be configured dynamically through xDS Protocol.\nMetrics data Some of the Envoy statistics are listed in this list. A sample data that contains identifier can be found here, while the metrics only can be observed here.\n","excerpt":"Configuring Envoy to send metrics to SkyWalking In order to let Envoy to send metrics to SkyWalking, …","ref":"/docs/main/v8.3.0/en/setup/envoy/metrics_service_setting/","title":"Configuring Envoy to send metrics to SkyWalking"},{"body":" Dependency the toolkit, such as using maven or gradle  \u0026lt;dependency\u0026gt; \u0026lt;groupId\u0026gt;org.apache.skywalking\u0026lt;/groupId\u0026gt; \u0026lt;artifactId\u0026gt;apm-toolkit-log4j-1.x\u0026lt;/artifactId\u0026gt; \u0026lt;version\u0026gt;{project.release.version}\u0026lt;/version\u0026gt; \u0026lt;/dependency\u0026gt;  Config a layout  log4j.appender.CONSOLE.layout=TraceIdPatternLayout  set %T in layout.ConversionPattern ( In 2.0-2016, you should use %x, Why change? )  log4j.appender.CONSOLE.layout.ConversionPattern=%d [%T] %-5p %c{1}:%L - %m%n  When you use -javaagent to active the sky-walking tracer, log4j will output traceId, if it existed. If the tracer is inactive, the output will be TID: N/A.  ","excerpt":"Dependency the toolkit, such as using maven or gradle  \u0026lt;dependency\u0026gt; …","ref":"/docs/main/v8.3.0/en/setup/service-agent/java-agent/application-toolkit-log4j-1.x/","title":"Dependency the toolkit, such as using maven or gradle"},{"body":" Dependency the toolkit, such as using maven or gradle  \u0026lt;dependency\u0026gt; \u0026lt;groupId\u0026gt;org.apache.skywalking\u0026lt;/groupId\u0026gt; \u0026lt;artifactId\u0026gt;apm-toolkit-log4j-2.x\u0026lt;/artifactId\u0026gt; \u0026lt;version\u0026gt;{project.release.version}\u0026lt;/version\u0026gt; \u0026lt;/dependency\u0026gt;  Config the [%traceId] pattern in your log4j2.xml  \u0026lt;Appenders\u0026gt; \u0026lt;Console name=\u0026#34;Console\u0026#34; target=\u0026#34;SYSTEM_OUT\u0026#34;\u0026gt; \u0026lt;PatternLayout pattern=\u0026#34;%d [%traceId] %-5p %c{1}:%L - %m%n\u0026#34;/\u0026gt; \u0026lt;/Console\u0026gt; \u0026lt;/Appenders\u0026gt;  Support log4j2 AsyncRoot , No additional configuration is required. Refer to the demo of log4j2.xml below. For details: Log4j2 Async Loggers  \u0026lt;Configuration\u0026gt; \u0026lt;Appenders\u0026gt; \u0026lt;Console name=\u0026#34;Console\u0026#34; target=\u0026#34;SYSTEM_OUT\u0026#34;\u0026gt; \u0026lt;PatternLayout pattern=\u0026#34;%d [%traceId] %-5p %c{1}:%L - %m%n\u0026#34;/\u0026gt; \u0026lt;/Console\u0026gt; \u0026lt;/Appenders\u0026gt; \u0026lt;Loggers\u0026gt; \u0026lt;AsyncRoot level=\u0026#34;INFO\u0026#34;\u0026gt; \u0026lt;AppenderRef ref=\u0026#34;Console\u0026#34;/\u0026gt; \u0026lt;/AsyncRoot\u0026gt; \u0026lt;/Loggers\u0026gt; \u0026lt;/Configuration\u0026gt;   Support log4j2 AsyncAppender , No additional configuration is required. Refer to the demo of log4j2.xml below.\nFor details: All Loggers Async\nLog4j-2.9 and higher require disruptor-3.3.4.jar or higher on the classpath. Prior to Log4j-2.9, disruptor-3.0.0.jar or higher was required. This is simplest to configure and gives the best performance. To make all loggers asynchronous, add the disruptor jar to the classpath and set the system property log4j2.contextSelector to org.apache.logging.log4j.core.async.AsyncLoggerContextSelector.\n\u0026lt;Configuration status=\u0026#34;WARN\u0026#34;\u0026gt; \u0026lt;Appenders\u0026gt; \u0026lt;!-- Async Loggers will auto-flush in batches, so switch off immediateFlush. --\u0026gt; \u0026lt;RandomAccessFile name=\u0026#34;RandomAccessFile\u0026#34; fileName=\u0026#34;async.log\u0026#34; immediateFlush=\u0026#34;false\u0026#34; append=\u0026#34;false\u0026#34;\u0026gt; \u0026lt;PatternLayout\u0026gt; \u0026lt;Pattern\u0026gt;%d %p %c{1.} [%t] [%traceId] %m %ex%n\u0026lt;/Pattern\u0026gt; \u0026lt;/PatternLayout\u0026gt; \u0026lt;/RandomAccessFile\u0026gt; \u0026lt;/Appenders\u0026gt; \u0026lt;Loggers\u0026gt; \u0026lt;Root level=\u0026#34;info\u0026#34; includeLocation=\u0026#34;false\u0026#34;\u0026gt; \u0026lt;AppenderRef ref=\u0026#34;RandomAccessFile\u0026#34;/\u0026gt; \u0026lt;/Root\u0026gt; \u0026lt;/Loggers\u0026gt; \u0026lt;/Configuration\u0026gt; For details: Mixed Sync \u0026amp; Async\nLog4j-2.9 and higher require disruptor-3.3.4.jar or higher on the classpath. Prior to Log4j-2.9, disruptor-3.0.0.jar or higher was required. There is no need to set system property Log4jContextSelector to any value.\n\u0026lt;Configuration status=\u0026#34;WARN\u0026#34;\u0026gt; \u0026lt;Appenders\u0026gt; \u0026lt;!-- Async Loggers will auto-flush in batches, so switch off immediateFlush. --\u0026gt; \u0026lt;RandomAccessFile name=\u0026#34;RandomAccessFile\u0026#34; fileName=\u0026#34;asyncWithLocation.log\u0026#34; immediateFlush=\u0026#34;false\u0026#34; append=\u0026#34;false\u0026#34;\u0026gt; \u0026lt;PatternLayout\u0026gt; \u0026lt;Pattern\u0026gt;%d %p %class{1.} [%t] [%traceId] %location %m %ex%n\u0026lt;/Pattern\u0026gt; \u0026lt;/PatternLayout\u0026gt; \u0026lt;/RandomAccessFile\u0026gt; \u0026lt;/Appenders\u0026gt; \u0026lt;Loggers\u0026gt; \u0026lt;!-- pattern layout actually uses location, so we need to include it --\u0026gt; \u0026lt;AsyncLogger name=\u0026#34;com.foo.Bar\u0026#34; level=\u0026#34;trace\u0026#34; includeLocation=\u0026#34;true\u0026#34;\u0026gt; \u0026lt;AppenderRef ref=\u0026#34;RandomAccessFile\u0026#34;/\u0026gt; \u0026lt;/AsyncLogger\u0026gt; \u0026lt;Root level=\u0026#34;info\u0026#34; includeLocation=\u0026#34;true\u0026#34;\u0026gt; \u0026lt;AppenderRef ref=\u0026#34;RandomAccessFile\u0026#34;/\u0026gt; \u0026lt;/Root\u0026gt; \u0026lt;/Loggers\u0026gt; \u0026lt;/Configuration\u0026gt;   Support log4j2 AsyncAppender, For details: Log4j2 AsyncAppender\n  \u0026lt;Configuration\u0026gt; \u0026lt;Appenders\u0026gt; \u0026lt;Console name=\u0026#34;Console\u0026#34; target=\u0026#34;SYSTEM_OUT\u0026#34;\u0026gt; \u0026lt;PatternLayout pattern=\u0026#34;%d [%traceId] %-5p %c{1}:%L - %m%n\u0026#34;/\u0026gt; \u0026lt;/Console\u0026gt; \u0026lt;Async name=\u0026#34;Async\u0026#34;\u0026gt; \u0026lt;AppenderRef ref=\u0026#34;Console\u0026#34;/\u0026gt; \u0026lt;/Async\u0026gt; \u0026lt;/Appenders\u0026gt; \u0026lt;Loggers\u0026gt; \u0026lt;Root level=\u0026#34;INFO\u0026#34;\u0026gt; \u0026lt;AppenderRef ref=\u0026#34;Async\u0026#34;/\u0026gt; \u0026lt;/Root\u0026gt; \u0026lt;/Loggers\u0026gt; \u0026lt;/Configuration\u0026gt;  When you use -javaagent to active the sky-walking tracer, log4j2 will output traceId, if it existed. If the tracer is inactive, the output will be TID: N/A.  ","excerpt":"Dependency the toolkit, such as using maven or gradle  \u0026lt;dependency\u0026gt; …","ref":"/docs/main/v8.3.0/en/setup/service-agent/java-agent/application-toolkit-log4j-2.x/","title":"Dependency the toolkit, such as using maven or gradle"},{"body":" Dependency the toolkit, such as using maven or gradle  \u0026lt;dependency\u0026gt; \u0026lt;groupId\u0026gt;org.apache.skywalking\u0026lt;/groupId\u0026gt; \u0026lt;artifactId\u0026gt;apm-toolkit-meter\u0026lt;/artifactId\u0026gt; \u0026lt;version\u0026gt;${skywalking.version}\u0026lt;/version\u0026gt; \u0026lt;/dependency\u0026gt; If you\u0026rsquo;re using Spring sleuth, you could use Spring Sleuth Setup.\n Counter API represents a single monotonically increasing counter, automatic collect data and report to backend.  import org.apache.skywalking.apm.toolkit.meter.MeterFactory; Counter counter = MeterFactory.counter(meterName).tag(\u0026#34;tagKey\u0026#34;, \u0026#34;tagValue\u0026#34;).mode(Counter.Mode.INCREMENT).build(); counter.increment(1d);  MeterFactory.counter Create a new counter builder with the meter name. Counter.Builder.tag(String key, String value) Mark a tag key/value pair. Counter.Builder.mode(Counter.Mode mode) Change the counter mode, RATE mode means reporting rate to the backend. Counter.Builder.build() Build a new Counter which is collected and reported to the backend. Counter.increment(double count) Increment count to the Counter, It could be a positive value.   Gauge API represents a single numerical value.  import org.apache.skywalking.apm.toolkit.meter.MeterFactory; ThreadPoolExecutor threadPool = ...; Gauge gauge = MeterFactory.gauge(meterName, () -\u0026gt; threadPool.getActiveCount()).tag(\u0026#34;tagKey\u0026#34;, \u0026#34;tagValue\u0026#34;).build();  MeterFactory.gauge(String name, Supplier\u0026lt;Double\u0026gt; getter) Create a new gauge builder with the meter name and supplier function, this function need to return a double value. Gauge.Builder.tag(String key, String value) Mark a tag key/value pair. Gauge.Builder.build() Build a new Gauge which is collected and reported to the backend.   Histogram API represents a summary sample observations with customize buckets.  import org.apache.skywalking.apm.toolkit.meter.MeterFactory; Histogram histogram = MeterFactory.histogram(\u0026#34;test\u0026#34;).tag(\u0026#34;tagKey\u0026#34;, \u0026#34;tagValue\u0026#34;).steps(Arrays.asList(1, 5, 10)).minValue(0).build(); histogram.addValue(3);  MeterFactory.histogram(String name) Create a new histogram builder with the meter name. Histogram.Builder.tag(String key, String value) Mark a tag key/value pair. Histogram.Builder.steps(List\u0026lt;Double\u0026gt; steps) Set up the max values of every histogram buckets. Histogram.Builder.minValue(double value) Set up the minimal value of this histogram, default is 0. Histogram.Builder.build() Build a new Histogram which is collected and reported to the backend. Histogram.addValue(double value) Add value into the histogram, automatically analyze what bucket count needs to be increment. rule: count into [step1, step2).  ","excerpt":"Dependency the toolkit, such as using maven or gradle  \u0026lt;dependency\u0026gt; …","ref":"/docs/main/v8.3.0/en/setup/service-agent/java-agent/application-toolkit-meter/","title":"Dependency the toolkit, such as using maven or gradle"},{"body":" Dependency the toolkit, such as using maven or gradle  \u0026lt;dependency\u0026gt; \u0026lt;groupId\u0026gt;org.apache.skywalking\u0026lt;/groupId\u0026gt; \u0026lt;artifactId\u0026gt;apm-toolkit-micrometer-registry\u0026lt;/artifactId\u0026gt; \u0026lt;version\u0026gt;${skywalking.version}\u0026lt;/version\u0026gt; \u0026lt;/dependency\u0026gt;  Using org.apache.skywalking.apm.meter.micrometer.SkywalkingMeterRegistry as the registry, it could forward the MicroMeter collected metrics to OAP server.  import org.apache.skywalking.apm.meter.micrometer.SkywalkingMeterRegistry; SkywalkingMeterRegistry registry = new SkywalkingMeterRegistry(); // If you has some counter want to rate by agent side SkywalkingConfig config = new SkywalkingConfig(Arrays.asList(\u0026#34;test_rate_counter\u0026#34;)); new SkywalkingMeterRegistry(config); // Also you could using composite registry to combine multiple meter registry, such as collect to Skywalking and prometheus CompositeMeterRegistry compositeRegistry = new CompositeMeterRegistry(); compositeRegistry.add(new PrometheusMeterRegistry(PrometheusConfig.DEFAULT)); compositeRegistry.add(new SkywalkingMeterRegistry());   Using snake case as the naming convention. Such as test.meter will be send to test_meter.\n  Using Millisecond as the time unit.\n  Adapt micrometer data convention.\n     Micrometer data type Transform to meter name Skywalking data type Description     Counter Counter name Counter Same with counter   Gauges Gauges name Gauges Same with gauges   Timer Timer name + \u0026ldquo;_count\u0026rdquo; Counter Execute finished count    Timer name + \u0026ldquo;_sum\u0026rdquo; Counter Total execute finished duration    Timer name + \u0026ldquo;_max\u0026rdquo; Gauges Max duration of execute finished time    Timer name + \u0026ldquo;_histogram\u0026rdquo; Histogram Histogram of execute finished duration   LongTaskTimer Timer name + \u0026ldquo;_active_count\u0026rdquo; Gauges Executing task count    Timer name + \u0026ldquo;_duration_sum\u0026rdquo; Counter All of executing task sum duration    Timer name + \u0026ldquo;_max\u0026rdquo; Counter Current longest running task execute duration   Function Timer Timer name + \u0026ldquo;_count\u0026rdquo; Gauges Execute finished timer count    Timer name + \u0026ldquo;_sum\u0026rdquo; Gauges Execute finished timer total duration   Function Counter Counter name Counter Custom counter value   Distribution summary Summary name + \u0026ldquo;_count\u0026rdquo; Counter Total record count    Summary name + \u0026ldquo;_sum\u0026rdquo; Counter Total record amount sum    Summary name + \u0026ldquo;_max\u0026rdquo; Gauges Max record amount    Summary name + \u0026ldquo;_histogram\u0026rdquo; Gauges Histogram of the amount     Not Adapt data convention.     Micrometer data type Data type     LongTaskTimer Histogram    ","excerpt":"Dependency the toolkit, such as using maven or gradle  \u0026lt;dependency\u0026gt; …","ref":"/docs/main/v8.3.0/en/setup/service-agent/java-agent/application-toolkit-micrometer/","title":"Dependency the toolkit, such as using maven or gradle"},{"body":" Dependency the toolkit, such as using maven or gradle  \u0026lt;dependency\u0026gt; \u0026lt;groupId\u0026gt;org.apache.skywalking\u0026lt;/groupId\u0026gt; \u0026lt;artifactId\u0026gt;apm-toolkit-trace\u0026lt;/artifactId\u0026gt; \u0026lt;version\u0026gt;${skywalking.version}\u0026lt;/version\u0026gt; \u0026lt;/dependency\u0026gt;  Use TraceContext.traceId() API to obtain traceId.  import TraceContext; ... modelAndView.addObject(\u0026#34;traceId\u0026#34;, TraceContext.traceId()); Sample codes only\n  Add @Trace to any method you want to trace. After that, you can see the span in the Stack.\n  Methods annotated with @Tag will try to tag the current active span with the given key (Tag#key()) and (Tag#value()), if there is no active span at all, this annotation takes no effect. @Tag can be repeated, and can be used in companion with @Trace, see examples below. The value of Tag is the same as what are supported in Customize Enhance Trace.\n  Add custom tag in the context of traced method, ActiveSpan.tag(\u0026quot;key\u0026quot;, \u0026quot;val\u0026quot;).\n  ActiveSpan.error() Mark the current span as error status.\n  ActiveSpan.error(String errorMsg) Mark the current span as error status with a message.\n  ActiveSpan.error(Throwable throwable) Mark the current span as error status with a Throwable.\n  ActiveSpan.debug(String debugMsg) Add a debug level log message in the current span.\n  ActiveSpan.info(String infoMsg) Add an info level log message in the current span.\n  ActiveSpan.setOperationName(String operationName) Customize an operation name.\n  ActiveSpan.tag(\u0026#34;my_tag\u0026#34;, \u0026#34;my_value\u0026#34;); ActiveSpan.error(); ActiveSpan.error(\u0026#34;Test-Error-Reason\u0026#34;); ActiveSpan.error(new RuntimeException(\u0026#34;Test-Error-Throwable\u0026#34;)); ActiveSpan.info(\u0026#34;Test-Info-Msg\u0026#34;); ActiveSpan.debug(\u0026#34;Test-debug-Msg\u0026#34;); /** * The codes below will generate a span, * and two types of tags, one type tag: keys are `tag1` and `tag2`, values are the passed-in parameters, respectively, the other type tag: keys are `username` and `age`, values are the return value in User, respectively */ @Trace @Tag(key = \u0026#34;tag1\u0026#34;, value = \u0026#34;arg[0]\u0026#34;) @Tag(key = \u0026#34;tag2\u0026#34;, value = \u0026#34;arg[1]\u0026#34;) @Tag(key = \u0026#34;username\u0026#34;, value = \u0026#34;returnedObj.username\u0026#34;) @Tag(key = \u0026#34;age\u0026#34;, value = \u0026#34;returnedObj.age\u0026#34;) public User methodYouWantToTrace(String param1, String param2) { // ActiveSpan.setOperationName(\u0026#34;Customize your own operation name, if this is an entry span, this would be an endpoint name\u0026#34;);  // ... }  Use TraceContext.putCorrelation() API to put custom data in tracing context.  Optional\u0026lt;String\u0026gt; previous = TraceContext.putCorrelation(\u0026#34;customKey\u0026#34;, \u0026#34;customValue\u0026#34;); CorrelationContext will remove the item when the value is null or empty.\n Use TraceContext.getCorrelation() API to get custom data.  Optional\u0026lt;String\u0026gt; value = TraceContext.getCorrelation(\u0026#34;customKey\u0026#34;); CorrelationContext configuration descriptions could be found in the agent configuration documentation, with correlation. as the prefix.\n","excerpt":"Dependency the toolkit, such as using maven or gradle  \u0026lt;dependency\u0026gt; …","ref":"/docs/main/v8.3.0/en/setup/service-agent/java-agent/application-toolkit-trace/","title":"Dependency the toolkit, such as using maven or gradle"},{"body":" Dependency the toolkit, such as using maven or gradle  \u0026lt;dependency\u0026gt; \u0026lt;groupId\u0026gt;org.apache.skywalking\u0026lt;/groupId\u0026gt; \u0026lt;artifactId\u0026gt;apm-toolkit-opentracing\u0026lt;/artifactId\u0026gt; \u0026lt;version\u0026gt;{project.release.version}\u0026lt;/version\u0026gt; \u0026lt;/dependency\u0026gt;  Use our OpenTracing tracer implementation  Tracer tracer = new SkywalkingTracer(); Tracer.SpanBuilder spanBuilder = tracer.buildSpan(\u0026#34;/yourApplication/yourService\u0026#34;); ","excerpt":" Dependency the toolkit, such as using maven or gradle  \u0026lt;dependency\u0026gt; …","ref":"/docs/main/v8.3.0/en/setup/service-agent/java-agent/opentracing/","title":"Dependency the toolkit, such as using maven or gradle"},{"body":"Deploy SkyWalking backend and UI in kubernetes Follow instructions in the deploying SkyWalking backend to Kubernetes cluster to deploy oap and ui to a kubernetes cluster.\nPlease read the Readme file.\n","excerpt":"Deploy SkyWalking backend and UI in kubernetes Follow instructions in the deploying SkyWalking …","ref":"/docs/main/v8.3.0/en/setup/backend/backend-k8s/","title":"Deploy SkyWalking backend and UI in kubernetes"},{"body":"Design Goals The document outlines the core design goals for SkyWalking project.\n  Keep Observability. No matter how does the target system deploy, SkyWalking could provide a solution or integration way to keep observability for it. Based on this, SkyWalking provides several runtime forms and probes.\n  Topology, Metrics and Trace Together. The first step to see and understand a distributed system should be from topology map. It visualizes the whole complex system as an easy map. Under that topology, OSS people requires more about metrics for service, instance, endpoint and calls. Trace exists as detail logs for making sense of those metrics. Such as when endpoint latency becomes long, you want to see the slowest the trace to find out why. So you can see, they are from big picture to details, they are all needed. SkyWalking integrates and provides a lot of features to make this possible and easy understand.\n  Light Weight. There two parts of light weight are needed. (1) In probe, we just depend on network communication framework, prefer gRPC. By that, the probe should be as small as possible, to avoid the library conflicts and the payload of VM, such as permsize requirement in JVM. (2) As an observability platform, it is secondary and third level system in your project environment. So we are using our own light weight framework to build the backend core. Then you don\u0026rsquo;t need to deploy big data tech platform and maintain them. SkyWalking should be simple in tech stack.\n  Pluggable. SkyWalking core team provides many default implementations, but definitely it is not enough, and also don\u0026rsquo;t fit every scenario. So, we provide a lot of features for being pluggable.\n  Portability. SkyWalking can run in multiple environments, including: (1) Use traditional register center like eureka. (2) Use RPC framework including service discovery, like Spring Cloud, Apache Dubbo. (3) Use Service Mesh in modern infrastructure. (4) Use cloud services. (5) Across cloud deployment. SkyWalking should run well in all these cases.\n  Interop. Observability is a big landscape, SkyWalking is impossible to support all, even by its community. As that, it supports to interop with other OSS system, mostly probes, such as Zipkin, Jaeger, OpenTracing, OpenCensus. To accept and understand their data formats makes sure SkyWalking more useful for end users. And don\u0026rsquo;t require the users to switch their libraries.\n  What is next?  See probe Introduction to know SkyWalking\u0026rsquo;s probe groups. From backend overview, you can understand what backend does after it received probe data. If you want to customize UI, start with UI overview document.  ","excerpt":"Design Goals The document outlines the core design goals for SkyWalking project.\n  Keep …","ref":"/docs/main/v8.3.0/en/concepts-and-designs/project-goals/","title":"Design Goals"},{"body":"Disable plugins Delete or remove the specific libraries / jars in skywalking-agent/plugins/*.jar\n+-- skywalking-agent +-- activations apm-toolkit-log4j-1.x-activation.jar apm-toolkit-log4j-2.x-activation.jar apm-toolkit-logback-1.x-activation.jar ... +-- config agent.config +-- plugins apm-dubbo-plugin.jar apm-feign-default-http-9.x.jar apm-httpClient-4.x-plugin.jar ..... skywalking-agent.jar ","excerpt":"Disable plugins Delete or remove the specific libraries / jars in skywalking-agent/plugins/*.jar\n+-- …","ref":"/docs/main/v8.3.0/en/setup/service-agent/java-agent/how-to-disable-plugin/","title":"Disable plugins"},{"body":"Dynamic Configuration SkyWalking Configurations mostly are set through application.yml and OS system environment variables. At the same time, some of them are supporting dynamic settings from upstream management system.\nRight now, SkyWalking supports following dynamic configurations.\n   Config Key Value Description Value Format Example     agent-analyzer.default.slowDBAccessThreshold Thresholds of slow Database statement, override receiver-trace/default/slowDBAccessThreshold of application.yml. default:200,mongodb:50   agent-analyzer.default.uninstrumentedGateways The uninstrumented gateways, override gateways.yml. same as gateways.yml   alarm.default.alarm-settings The alarm settings, will override alarm-settings.yml. same as alarm-settings.yml   core.default.apdexThreshold The apdex threshold settings, will override service-apdex-threshold.yml. same as service-apdex-threshold.yml   core.default.endpoint-name-grouping The endpoint name grouping setting, will override endpoint-name-grouping.yml. same as endpoint-name-grouping.yml   agent-analyzer.default.sampleRate Trace sampling , override receiver-trace/default/sampleRate of application.yml. 10000   agent-analyzer.default.slowTraceSegmentThreshold Setting this threshold about the latency would make the slow trace segments sampled if they cost more time, even the sampling mechanism activated. The default value is -1, which means would not sample slow traces. Unit, millisecond. override receiver-trace/default/slowTraceSegmentThreshold of application.yml. -1    This feature depends on upstream service, so it is DISABLED by default.\nconfiguration: selector: ${SW_CONFIGURATION:none} none: grpc: host: ${SW_DCS_SERVER_HOST:\u0026#34;\u0026#34;} port: ${SW_DCS_SERVER_PORT:80} clusterName: ${SW_DCS_CLUSTER_NAME:SkyWalking} period: ${SW_DCS_PERIOD:20} # ... other implementations Dynamic Configuration Service, DCS Dynamic Configuration Service is a gRPC service, which requires the upstream system implemented. The SkyWalking OAP fetches the configuration from the implementation(any system), after you open this implementation like this.\nconfiguration: selector: ${SW_CONFIGURATION:grpc} grpc: host: ${SW_DCS_SERVER_HOST:\u0026#34;\u0026#34;} port: ${SW_DCS_SERVER_PORT:80} clusterName: ${SW_DCS_CLUSTER_NAME:SkyWalking} period: ${SW_DCS_PERIOD:20} Dynamic Configuration Zookeeper Implementation Zookeeper is also supported as DCC(Dynamic Configuration Center), to use it, please configure as follows:\nconfiguration: selector: ${SW_CONFIGURATION:zookeeper} zookeeper: period: ${SW_CONFIG_ZK_PERIOD:60} # Unit seconds, sync period. Default fetch every 60 seconds. nameSpace: ${SW_CONFIG_ZK_NAMESPACE:/default} hostPort: ${SW_CONFIG_ZK_HOST_PORT:localhost:2181} # Retry Policy baseSleepTimeMs: ${SW_CONFIG_ZK_BASE_SLEEP_TIME_MS:1000} # initial amount of time to wait between retries maxRetries: ${SW_CONFIG_ZK_MAX_RETRIES:3} # max number of times to retry The nameSpace is the ZooKeeper path. The config key and value are the properties of the namespace folder.\nDynamic Configuration Etcd Implementation Etcd is also supported as DCC(Dynamic Configuration Center), to use it, please configure as follows:\nconfiguration: selector: ${SW_CONFIGURATION:etcd} etcd: period: ${SW_CONFIG_ETCD_PERIOD:60} # Unit seconds, sync period. Default fetch every 60 seconds. group: ${SW_CONFIG_ETCD_GROUP:skywalking} serverAddr: ${SW_CONFIG_ETCD_SERVER_ADDR:localhost:2379} clusterName: ${SW_CONFIG_ETCD_CLUSTER_NAME:default} Dynamic Configuration Consul Implementation Consul is also supported as DCC(Dynamic Configuration Center), to use it, please configure as follows:\nconfiguration: selector: ${SW_CONFIGURATION:consul} consul: # Consul host and ports, separated by comma, e.g. 1.2.3.4:8500,2.3.4.5:8500 hostAndPorts: ${SW_CONFIG_CONSUL_HOST_AND_PORTS:1.2.3.4:8500} # Sync period in seconds. Defaults to 60 seconds. period: ${SW_CONFIG_CONSUL_PERIOD:1} # Consul aclToken aclToken: ${SW_CONFIG_CONSUL_ACL_TOKEN:\u0026#34;\u0026#34;} Dynamic Configuration Apollo Implementation Apollo is also supported as DCC(Dynamic Configuration Center), to use it, just configured as follows:\nconfiguration: selector: ${SW_CONFIGURATION:apollo} apollo: apolloMeta: ${SW_CONFIG_APOLLO:http://106.12.25.204:8080} apolloCluster: ${SW_CONFIG_APOLLO_CLUSTER:default} apolloEnv: ${SW_CONFIG_APOLLO_ENV:\u0026#34;\u0026#34;} appId: ${SW_CONFIG_APOLLO_APP_ID:skywalking} period: ${SW_CONFIG_APOLLO_PERIOD:5} Dynamic Configuration Kuberbetes Configmap Implementation configmap is also supported as DCC(Dynamic Configuration Center), to use it, just configured as follows:\nconfiguration: selector: ${SW_CONFIGURATION:k8s-configmap} # [example] (../../../../oap-server/server-configuration/configuration-k8s-configmap/src/test/resources/skywalking-dynamic-configmap.example.yaml) k8s-configmap: # Sync period in seconds. Defaults to 60 seconds. period: ${SW_CONFIG_CONFIGMAP_PERIOD:60} # Which namespace is confiigmap deployed in. namespace: ${SW_CLUSTER_K8S_NAMESPACE:default} # Labelselector is used to locate specific configmap labelSelector: ${SW_CLUSTER_K8S_LABEL:app=collector,release=skywalking} Dynamic Configuration Nacos Implementation Nacos is also supported as DCC(Dynamic Configuration Center), to use it, please configure as follows:\nconfiguration: selector: ${SW_CONFIGURATION:nacos} nacos: # Nacos Server Host serverAddr: ${SW_CONFIG_NACOS_SERVER_ADDR:127.0.0.1} # Nacos Server Port port: ${SW_CONFIG_NACOS_SERVER_PORT:8848} # Nacos Configuration Group group: ${SW_CONFIG_NACOS_SERVER_GROUP:skywalking} # Nacos Configuration namespace namespace: ${SW_CONFIG_NACOS_SERVER_NAMESPACE:} # Unit seconds, sync period. Default fetch every 60 seconds. period: ${SW_CONFIG_NACOS_PERIOD:60} # the name of current cluster, set the name if you want to upstream system known. clusterName: ${SW_CONFIG_NACOS_CLUSTER_NAME:default} ","excerpt":"Dynamic Configuration SkyWalking Configurations mostly are set through application.yml and OS system …","ref":"/docs/main/v8.3.0/en/setup/backend/dynamic-config/","title":"Dynamic Configuration"},{"body":"ElasticSearch Some new users may face\n ElasticSearch performance is not as good as expected. Such as, can\u0026rsquo;t have latest data after a while.  Or\n ERROR CODE 429.   Suppressed: org.elasticsearch.client.ResponseException: method [POST], host [http://127.0.0.1:9200], URI [/service_instance_inventory/type/6_tcc-app-gateway-77b98ff6ff-crblx.cards_0_0/_update?refresh=true\u0026amp;timeout=1m], status line [HTTP/1.1 429 Too Many Requests] {\u0026quot;error\u0026quot;:{\u0026quot;root_cause\u0026quot;:[{\u0026quot;type\u0026quot;:\u0026quot;remote_transport_exception\u0026quot;,\u0026quot;reason\u0026quot;:\u0026quot;[elasticsearch-0][10.16.9.130:9300][indices:data/write/update[s]]\u0026quot;}],\u0026quot;type\u0026quot;:\u0026quot;es_rejected_execution_exception\u0026quot;,\u0026quot;reason\u0026quot;:\u0026quot;rejected execution of org.elasticsearch.transport.TransportService$7@19a5cf02 on EsThreadPoolExecutor[name = elasticsearch-0/write, queue capacity = 200, org.elasticsearch.common.util.concurrent.EsThreadPoolExecutor@389297ad[Running, pool size = 2, active threads = 2, queued tasks = 200, completed tasks = 147611]]\u0026quot;},\u0026quot;status\u0026quot;:429} at org.elasticsearch.client.RestClient$SyncResponseListener.get(RestClient.java:705) ~[elasticsearch-rest-client-6.3.2.jar:6.3.2] at org.elasticsearch.client.RestClient.performRequest(RestClient.java:235) ~[elasticsearch-rest-client-6.3.2.jar:6.3.2] at org.elasticsearch.client.RestClient.performRequest(RestClient.java:198) ~[elasticsearch-rest-client-6.3.2.jar:6.3.2] at org.elasticsearch.client.RestHighLevelClient.performRequest(RestHighLevelClient.java:522) ~[elasticsearch You could add following config to elasticsearch.yml, set the value based on your env.\n# In tracing scenario, consider to set more than this at least. thread_pool.index.queue_size: 1000 thread_pool.write.queue_size: 1000 # When you face query error at trace page, remember to check this. index.max_result_window: 1000000 Read ElasticSearch official documents to get more information.\n","excerpt":"ElasticSearch Some new users may face\n ElasticSearch performance is not as good as expected. Such …","ref":"/docs/main/v8.3.0/en/faq/es-server-faq/","title":"ElasticSearch"},{"body":"Exporter tool of profile raw data When the visualization doesn\u0026rsquo;t work well through the official UI, users could submit the issue to report. This tool helps the users to package the original profile data for helping the community to locate the issue in the user case. NOTICE, this report includes the class name, method name, line number, etc. Before submit this, please make sure this wouldn\u0026rsquo;t become your system vulnerability.\nExport command line Usage  Set the storage in tools/profile-exporter/application.yml file by following your use case. Prepare data  Profile task id: Profile task id Trace id: Wrong profiled trace id Export dir: Directory of the data will export   Enter the Skywalking root path Execute shell command bash tools/profile-exporter/profile_exporter.sh --taskid={profileTaskId} --traceid={traceId} {exportDir}  The file {traceId}.tar.gz will be generated after execution shell.  Exported data content  basic.yml: Contains the complete information of the profiled segments in the trace. snapshot.data: All monitored thread snapshot data in the current segment.  Report profile issue  Provide exported data generated from this tool. Provide span operation name, analyze mode(include/exclude children). Issue description. (If there have the UI screenshots, it\u0026rsquo;s better)  ","excerpt":"Exporter tool of profile raw data When the visualization doesn\u0026rsquo;t work well through the …","ref":"/docs/main/v8.3.0/en/guides/backend-profile-export/","title":"Exporter tool of profile raw data"},{"body":"Extend storage SkyWalking has already provided several storage solutions. In this document, you could learn how to implement a new storage easily.\nDefine your storage provider  Define a class extends org.apache.skywalking.oap.server.library.module.ModuleProvider. Set this provider targeting to Storage module.  @Override public Class\u0026lt;? extends ModuleDefine\u0026gt; module() { return StorageModule.class; } Implement all DAOs Here is the list of all DAO interfaces in storage\n IServiceInventoryCacheDAO IServiceInstanceInventoryCacheDAO IEndpointInventoryCacheDAO INetworkAddressInventoryCacheDAO IBatchDAO StorageDAO IRegisterLockDAO ITopologyQueryDAO IMetricsQueryDAO ITraceQueryDAO IMetadataQueryDAO IAggregationQueryDAO IAlarmQueryDAO IHistoryDeleteDAO IMetricsDAO IRecordDAO IRegisterDAO ILogQueryDAO ITopNRecordsQueryDAO IBrowserLogQueryDAO  Register all service implementations In public void prepare(), use this#registerServiceImplementation method to do register binding your implementation with the above interfaces.\nExample Take org.apache.skywalking.oap.server.storage.plugin.elasticsearch.StorageModuleElasticsearchProvider or org.apache.skywalking.oap.server.storage.plugin.jdbc.mysql.MySQLStorageProvider as a good example.\nRedistribution with new storage implementation. You don\u0026rsquo;t have to clone the main repo just for implementing the storage. You could just easy depend our Apache releases. Take a look at SkyAPM/SkyWalking-With-Es5x-Storage repo, SkyWalking v6 redistribution with ElasticSearch 5 TCP connection storage implementation.\n","excerpt":"Extend storage SkyWalking has already provided several storage solutions. In this document, you …","ref":"/docs/main/v8.3.0/en/guides/storage-extention/","title":"Extend storage"},{"body":"FAQs These are known and common FAQs. We welcome you to contribute yours.\nDesign  Why doesn\u0026rsquo;t SkyWalking involve MQ in the architecture?  Compiling  Protoc plugin fails in maven build Required items could not be found, when import project into Eclipse Maven compilation failure with python2 not found error  Runtime  8.x+ upgrade Why metrics indexes(ElasticSearch) in Hour and Day precisions stop update after upgrade to 7.x? 6.x version upgrade Why only traces in UI? The trace doesn\u0026rsquo;t continue in kafka consumer side Agent or collector version upgrade, 3.x -\u0026gt; 5.0.0-alpha EnhanceRequireObjectCache class cast exception ElasticSearch server performance FAQ, including ERROR CODE:429 IllegalStateException when install Java agent on WebSphere 7 \u0026ldquo;FORBIDDEN/12/index read-only / allow delete (api)\u0026rdquo; appears in the log No data shown and backend replies with \u0026ldquo;Variable \u0026lsquo;serviceId\u0026rsquo; has coerced Null value for NonNull type \u0026lsquo;ID!'\u0026quot; Unexpected endpoint register warning after 6.6.0 Use the profile exporter tool if the profile analysis is not right Compatible with other javaagent bytecode processing Java agent memory leak when enhance Worker thread at use Thread Pool Thrift plugin  UI  What is VNode? And why does SkyWalking have that?  ","excerpt":"FAQs These are known and common FAQs. We welcome you to contribute yours.\nDesign  Why doesn\u0026rsquo;t …","ref":"/docs/main/v8.3.0/en/faq/readme/","title":"FAQs"},{"body":"Group Parameterized Endpoints In most cases, the endpoint should be detected automatically through the language agents, service mesh observability solution, or configuration of meter system.\nThere are some special cases, especially when people use REST style URI, the application codes put the parameter in the endpoint name, such as putting order id in the URI, like /prod/ORDER123 and /prod/ORDER123. But logically, people expect they could have an endpoint name like prod/{order-id}. This is the feature of parameterized endpoint grouping designed for.\nCurrent, user could set up grouping rules through the static YAML file, named endpoint-name-grouping.yml, or use Dynamic Configuration to initial and update the endpoint grouping rule.\nConfiguration Format No matter in static local file or dynamic configuration value, they are sharing the same YAML format.\ngrouping: # Endpoint of the service would follow the following rules - service-name: serviceA rules: # Logic name when the regex expression matched. - endpoint-name: /prod/{id} regex: \\/prod\\/.+ ","excerpt":"Group Parameterized Endpoints In most cases, the endpoint should be detected automatically through …","ref":"/docs/main/v8.3.0/en/setup/backend/endpoint-grouping-rules/","title":"Group Parameterized Endpoints"},{"body":"Guides There are many ways that you can help the SkyWalking community.\n Go through our documents, point out or fix unclear things. Translate the documents to other languages. Download our releases, try to monitor your applications, and feedback to us about what you think. Read our source codes, Ask questions for details. Find some bugs, submit issue, and try to fix it. Find help wanted issues, which are good for you to start. Submit issue or start discussion through GitHub issue. See all mail list discussion through website list review. If you are a SkyWalking committer, could login and use the mail list in browser mode. Otherwise, follow the next step to subscribe. Issue report and discussion also could take place in dev@skywalking.apache.org. Mail to dev-subscribe@skywalking.apache.org, follow the reply to subscribe the mail list.  Contact Us All the following channels are open to the community, you could choose the way you like.\n Submit an issue Mail list: dev@skywalking.apache.org. Mail to dev-subscribe@skywalking.apache.org, follow the reply to subscribe the mail list. Gitter QQ Group: 392443393  Become official Apache SkyWalking Committer The PMC will assess the contributions of every contributor, including, but not limited to, code contributions, and follow the Apache guides to promote, vote and invite new committer and PMC member. Read Become official Apache SkyWalking Committer to get details.\nFor code developer For developers, first step, read Compiling Guide. It teaches developer how to build the project in local and set up the environment.\nIntegration Tests After setting up the environment and writing your codes, in order to make it more easily accepted by SkyWalking project, you\u0026rsquo;ll need to run the tests locally to verify that your codes don\u0026rsquo;t break any existed features, and write some unit test (UT) codes to verify that the new codes work well, preventing them being broke by future contributors. If the new codes involve other components or libraries, you\u0026rsquo;re also supposed to write integration tests (IT).\nSkyWalking leverages plugin maven-surefire-plugin to run the UTs while using maven-failsafe-plugin to run the ITs, maven-surefire-plugin will exclude ITs (whose class name starts with IT) and leave them for maven-failsafe-plugin to run, which is bound to the verify goal, CI-with-IT profile. Therefore, to run the UTs, try ./mvnw clean test, this will only run the UTs, not including ITs.\nIf you want to run the ITs please activate the CI-with-IT profile as well as the the profiles of the modules whose ITs you want to run. e.g. if you want to run the ITs in oap-server, try ./mvnw -Pbackend,CI-with-IT clean verify, and if you\u0026rsquo;d like to run all the ITs, simply run ./mvnw -Pall,CI-with-IT clean verify.\nPlease be advised that if you\u0026rsquo;re writing integration tests, name it with the pattern IT* to make them only run in CI-with-IT profile.\nEnd to End Tests (E2E for short) Since version 6.3.0, we have introduced more automatic tests to perform software quality assurance, E2E is one of the most important parts.\n End-to-end testing is a methodology used to test whether the flow of an application is performing as designed from start to finish. The purpose of carrying out end-to-end tests is to identify system dependencies and to ensure that the right information is passed between various system components and systems.\n The e2e test involves some/all of the OAP server, storage, coordinator, webapp, and the instrumented services, all of which are orchestrated by docker-compose, besides, there is a test controller(JUnit test) running outside of the container that sends traffics to the instrumented service, and then verifies the corresponding results after those requests, by GraphQL API of the SkyWalking Web App.\nWriting E2E Cases  Set up environment in IntelliJ IDEA  The e2e test is an separated project under the SkyWalking root directory and the IDEA cannot recognize it by default, right click on the file test/e2e/pom.xml and click Add as Maven Project, things should be ready now. But we recommend to open the directory skywalking/test/e2e in a separated IDE window for better experience because there may be shaded classes issues.\n Orchestrate the components  Our goal of E2E tests is to test the SkyWalking project in a whole, including the OAP server, storage, coordinator, webapp, and even the frontend UI(not now), in single node mode as well as cluster mode, therefore the first step is to determine what case we are going to verify and orchestrate the components.\nIn order to make it more easily to orchestrate, we\u0026rsquo;re using a docker-compose that provides a convenient file format (docker-compose.yml) to orchestrate the needed containers, and gives us possibilities to define the dependencies of the components.\nBasically you will need:\n Decide what (and how many) containers will be needed, e.g. for cluster testing, you\u0026rsquo;ll need \u0026gt; 2 OAP nodes, coordinators like zookeeper, storage like ElasticSearch, and instrumented services; Define the containers in docker-compose.yml, and carefully specify the dependencies, starting orders, and most importantly, link them together, e.g. set correct OAP address in the agent side, set correct coordinator address in OAP, etc. Write (or hopefully reuse) the test codes, to verify the results is correct.  As for the last step, we have a friendly framework to help you get started more quickly, which provides annotation @DockerCompose(\u0026quot;docker-compose.yml\u0026quot;) to load/parse and start up all the containers in a proper order, @ContainerHost/@ContainerPort to get the real host/port of the container, @ContainerHostAndPort to get both, @DockerContainer to get the running container.\n Write test controller  To put it simple, test controllers are basically tests that can be bound to the Maven integration-test/verify phase. They send designed requests to the instrumented service, and expect to get corresponding traces/metrics/metadata from the SkyWalking webapp GraphQL API.\nIn the test framework, we provide a TrafficController to periodically send traffic data to the instrumented services, you can simply enable it by giving a url and traffic data, refer to [../../../test/e2e/e2e-test/src/test/java/org/apache/skywalking/e2e/base/TrafficController.java].\n Troubleshooting  We expose all the logs from all containers to the stdout in non-CI (local) mode, but save/and upload them all to the GitHub server and you can download them (only when tests failed) in the right-up button \u0026ldquo;Artifacts/Download artifacts/logs\u0026rdquo; for debugging.\nNOTE: Please verify the newly-added E2E test case locally first, however, if you find it passed locally but failed in the PR check status, make sure all the updated/newly-added files (especially those in submodules) are committed and included in that PR, or reset the git HEAD to the remote and verify locally again.\nE2E local remote debugging When the E2E test is executed locally, if any test case fails, the E2E local remote debugging function can be used to quickly troubleshoot the bug.\nProject Extensions SkyWalking project supports many ways to extend existing features. If you are interesting in these ways, read the following guides.\n Java agent plugin development guide. This guide helps you to develop SkyWalking agent plugin to support more frameworks. Both open source plugin and private plugin developer should read this. If you want to build a new probe or plugin in any language, please read Component library definition and extension document. Storage extension development guide. Help potential contributors to build a new storage implementor besides the official. Customize analysis by oal script. OAL scripts locate in config/oal/*.oal. You could change it and reboot the OAP server. Read Observability Analysis Language Introduction if you need to learn about OAL script. Source and scope extension for new metrics. If you want to analysis a new metrics, which SkyWalking haven\u0026rsquo;t provide. You need to add a new receiver rather than choosing existed receiver. At that moment, you most likely need to add a new source and scope. This document will teach you how to do.  UI developer Our UI is constituted by static pages and web container.\n RocketBot UI is SkyWalking primary UI since 6.1 release. It is built with vue + typescript. You could know more at the rocketbot repository. Web container source codes are in apm-webapp module. This is a just an easy zuul proxy to host static resources and send GraphQL query requests to backend. Legacy UI repository is still there, but not included in SkyWalking release, after 6.0.0-GA.  OAP backend dependency management  This section is only applicable to the dependencies of the backend module\n Being one of the Top Level Projects of The Apache Software Foundation (ASF), SkyWalking is supposed to follow the ASF 3RD PARTY LICENSE POLICY, so if you\u0026rsquo;re adding new dependencies to the project, you\u0026rsquo;re responsible to check the newly-added dependencies won\u0026rsquo;t break the policy, and add their LICENSE\u0026rsquo;s and NOTICES\u0026rsquo;s to the project.\nWe have a simple script to help you make sure that you didn\u0026rsquo;t miss any newly-added dependency:\n Build a distribution package and unzip/untar it to folder dist. Run the script in the root directory, it will print out all newly-added dependencies. Check the LICENSE\u0026rsquo;s and NOTICE\u0026rsquo;s of those dependencies, if they can be included in an ASF project, add them in the apm-dist/release-docs/{LICENSE,NOTICE} file. Add those dependencies' names to the tools/dependencies/known-oap-backend-dependencies.txt file (alphabetical order), the next run of check-LICENSE.sh should pass.  Profile The performance profile is an enhancement feature in the APM system. We are using the thread dump to estimate the method execution time, rather than adding many local spans. In this way, the resource cost would be much less than using distributed tracing to locate slow method. This feature is suitable in the production environment. The following documents are important for developers to understand the key parts of this feature\n Profile data report procotol is provided like other trace, JVM data through gRPC. Thread dump merging mechanism introduces the merging mechanism, which helps the end users to understand the profile report. Exporter tool of profile raw data introduces when the visualization doesn\u0026rsquo;t work well through the official UI, how to package the original profile data, which helps the users report the issue.  For release Apache Release Guide introduces to the committer team about doing official Apache version release, to avoid breaking any Apache rule. Apache license allows everyone to redistribute if you keep our licenses and NOTICE in your redistribution.\n","excerpt":"Guides There are many ways that you can help the SkyWalking community.\n Go through our documents, …","ref":"/docs/main/v8.3.0/en/guides/readme/","title":"Guides"},{"body":"Health Check Health check intends to provide a unique approach to check the healthy status of OAP server. It includes the health status of modules, GraphQL and gRPC services readiness.\nHealth Checker Module. Health Checker module could solute how to observe the health status of modules. We can active it by below:\nhealth-checker: selector: ${SW_HEALTH_CHECKER:default} default: checkIntervalSeconds: ${SW_HEALTH_CHECKER_INTERVAL_SECONDS:5} Notice, we should enable telemetry module at the same time. That means the provider should not be - and none.\nAfter that, we can query OAP server health status by querying GraphQL:\nquery{ checkHealth{ score details } } If the OAP server is healthy, the response should be\n{ \u0026#34;data\u0026#34;: { \u0026#34;checkHealth\u0026#34;: { \u0026#34;score\u0026#34;: 0, \u0026#34;details\u0026#34;: \u0026#34;\u0026#34; } } } Once some modules are unhealthy, for instance, storage H2 is down. The result might be like below:\n{ \u0026#34;data\u0026#34;: { \u0026#34;checkHealth\u0026#34;: { \u0026#34;score\u0026#34;: 1, \u0026#34;details\u0026#34;: \u0026#34;storage_h2,\u0026#34; } } } You could refer to checkHealth query for more details.\nThe readiness of GraphQL and gRPC We could opt to above query to check the readiness of GraphQL.\nOAP has implemented gRPC Health Checking Protocol. We could use grpc-health-probe or any other tools to check the health of OAP gRPC services.\nCLI tool Please follow the CLI doc to get the health status score directly through the checkhealth command.\n","excerpt":"Health Check Health check intends to provide a unique approach to check the healthy status of OAP …","ref":"/docs/main/v8.3.0/en/setup/backend/backend-health-check/","title":"Health Check"},{"body":"How to build project This document helps people to compile and build the project in your maven and set your IDE.\nBuild Project Because we are using Git submodule, we recommend don\u0026rsquo;t use GitHub tag or release page to download source codes for compiling.\nMaven behind Proxy If you need to execute build behind the proxy, edit the .mvn/jvm.config and put the follow properties:\n-Dhttp.proxyHost=proxy_ip -Dhttp.proxyPort=proxy_port -Dhttps.proxyHost=proxy_ip -Dhttps.proxyPort=proxy_port -Dhttp.proxyUser=username -Dhttp.proxyPassword=password Build from GitHub   Prepare git, JDK8+ and Maven 3.6+\n  Clone project\nIf you want to build a release from source codes, provide a tag name by using git clone -b [tag_name] ... while cloning.\ngit clone --recurse-submodules https://github.com/apache/skywalking.git cd skywalking/ OR git clone https://github.com/apache/skywalking.git cd skywalking/ git submodule init git submodule update   Run ./mvnw clean package -DskipTests\n  All packages are in /dist (.tar.gz for Linux and .zip for Windows).\n  Build from Apache source code release  What is Apache source code release?  For each official Apache release, there is a complete and independent source code tar, which is including all source codes. You could download it from SkyWalking Apache download page. No git related stuff required when compiling this. Just follow these steps.\n Prepare JDK8+ and Maven 3.6+ Run ./mvnw clean package -DskipTests All packages are in /dist.(.tar.gz for Linux and .zip for Windows).  Advanced compile SkyWalking is a complex maven project, including many modules, which could cause long compiling time. If you just want to recompile part of the project, you have following options\n Compile agent and package   ./mvnw package -Pagent,dist\n or\n make build.agent\n If you intend to compile a single one plugin, such as in the dev stage, you could\n cd plugin_module_dir \u0026amp; mvn clean package\n  Compile backend and package   ./mvnw package -Pbackend,dist\n or\n make build.backend\n  Compile UI and package   ./mvnw package -Pui,dist\n or\n make build.ui\n Build docker images We can build docker images of backend and ui with Makefile located in root folder.\nRefer to Build docker image for more details.\nSetup your IntelliJ IDEA NOTICE: If you clone the codes from GitHub, please make sure that you had finished step 1 to 3 in section Build from GitHub, if you download the source codes from the official website of SkyWalking, please make sure that you had followed the steps in section Build from Apache source code release.\n Import the project as a maven project Run ./mvnw compile -Dmaven.test.skip=true to compile project and generate source codes. Because we use gRPC and protobuf. Set Generated Source Codes folders.  grpc-java and java folders in apm-protocol/apm-network/target/generated-sources/protobuf grpc-java and java folders in oap-server/server-core/target/generated-sources/protobuf grpc-java and java folders in oap-server/server-receiver-plugin/receiver-proto/target/generated-sources/protobuf grpc-java and java folders in oap-server/exporter/target/generated-sources/protobuf grpc-java and java folders in oap-server/server-configuration/grpc-configuration-sync/target/generated-sources/protobuf antlr4 folder in oap-server/oal-grammar/target/generated-sources    ","excerpt":"How to build project This document helps people to compile and build the project in your maven and …","ref":"/docs/main/v8.3.0/en/guides/how-to-build/","title":"How to build project"},{"body":"How to enable Kafka Reporter The Kafka reporter plugin support report traces, JVM metrics, Instance Properties, and profiled snapshots to Kafka cluster, which is disabled in default. Move the jar of the plugin, kafka-reporter-plugin-x.y.z.jar, from agent/optional-reporter-plugins to agent/plugins for activating.\nNotice, currently, the agent still needs to configure GRPC receiver for delivering the task of profiling. In other words, the following configure cannot be omitted.\n# Backend service addresses. collector.backend_service=${SW_AGENT_COLLECTOR_BACKEND_SERVICES:127.0.0.1:11800} # Kafka producer configuration plugin.kafka.bootstrap_servers=${SW_KAFKA_BOOTSTRAP_SERVERS:localhost:9092} plugin.kafka.producer_config[delivery.timeout.ms]=12000 plugin.kafka.get_topic_timeout=${SW_GET_TOPIC_TIMEOUT:10} Kafka reporter plugin support to customize all configurations of listed in here.\nBefore you activated the Kafka reporter, you have to make sure that Kafka fetcher has been opened in service.\n","excerpt":"How to enable Kafka Reporter The Kafka reporter plugin support report traces, JVM metrics, Instance …","ref":"/docs/main/v8.3.0/en/setup/service-agent/java-agent/how-to-enable-kafka-reporter/","title":"How to enable Kafka Reporter"},{"body":"How to tolerate custom exceptions In some codes, the exception is being used as a way of controlling business flow. Skywalking provides 2 ways to tolerate an exception which is traced in a span.\n Set the names of exception classes in the agent config Use our annotation in the codes.  Set the names of exception classes in the agent config The property named \u0026ldquo;statuscheck.ignored_exceptions\u0026rdquo; is used to set up class names in the agent configuration file. if the exception listed here are detected in the agent, the agent core would flag the related span as the error status.\nDemo   A custom exception.\n TestNamedMatchException  package org.apache.skywalking.apm.agent.core.context.status; public class TestNamedMatchException extends RuntimeException { public TestNamedMatchException() { } public TestNamedMatchException(final String message) { super(message); } ... }  TestHierarchyMatchException  package org.apache.skywalking.apm.agent.core.context.status; public class TestHierarchyMatchException extends TestNamedMatchException { public TestHierarchyMatchException() { } public TestHierarchyMatchException(final String message) { super(message); } ... }   When the above exceptions traced in some spans, the status is like the following.\n   The traced exception Final span status     org.apache.skywalking.apm.agent.core.context.status.TestNamedMatchException true   org.apache.skywalking.apm.agent.core.context.status.TestHierarchyMatchException true      After set these class names through \u0026ldquo;statuscheck.ignored_exceptions\u0026rdquo;, the status of spans would be changed.\nstatuscheck.ignored_exceptions=org.apache.skywalking.apm.agent.core.context.status.TestNamedMatchException    The traced exception Final span status     org.apache.skywalking.apm.agent.core.context.status.TestNamedMatchException false   org.apache.skywalking.apm.agent.core.context.status.TestHierarchyMatchException false      Use our annotation in the codes. If an exception has the @IgnoredException annotation, the exception wouldn\u0026rsquo;t be marked as error status when tracing. Because the annotation supports inheritance, also affects the subclasses.\nDependency  Dependency the toolkit, such as using maven or gradle. Since 8.2.0.  \u0026lt;dependency\u0026gt; \u0026lt;groupId\u0026gt;org.apache.skywalking\u0026lt;/groupId\u0026gt; \u0026lt;artifactId\u0026gt;apm-toolkit-log4j-1.x\u0026lt;/artifactId\u0026gt; \u0026lt;version\u0026gt;{project.release.version}\u0026lt;/version\u0026gt; \u0026lt;/dependency\u0026gt; Demo   A custom exception.\npackage org.apache.skywalking.apm.agent.core.context.status; public class TestAnnotatedException extends RuntimeException { public TestAnnotatedException() { } public TestAnnotatedException(final String message) { super(message); } ... }   When the above exception traced in some spans, the status is like the following.\n   The traced exception Final span status     org.apache.skywalking.apm.agent.core.context.status.TestAnnotatedException true      However, when the exception annotated with the annotation, the status would be changed.\npackage org.apache.skywalking.apm.agent.core.context.status; @IgnoredException public class TestAnnotatedException extends RuntimeException { public TestAnnotatedException() { } public TestAnnotatedException(final String message) { super(message); } ... }    The traced exception Final span status     org.apache.skywalking.apm.agent.core.context.status.TestAnnotatedException false      Recursive check Due to the wrapper nature of Java exceptions, sometimes users need recursive checking. Skywalking also supports it. Typically, we don\u0026rsquo;t recommend setting this more than 10, which could cause a performance issue. Negative value and 0 would be ignored, which means all exceptions would make the span tagged in error status.\n statuscheck.max_recursive_depth=${SW_STATUSCHECK_MAX_RECURSIVE_DEPTH:1} ","excerpt":"How to tolerate custom exceptions In some codes, the exception is being used as a way of controlling …","ref":"/docs/main/v8.3.0/en/setup/service-agent/java-agent/how-to-tolerate-exceptions/","title":"How to tolerate custom exceptions"},{"body":"HTTP API Protocol HTTP API Protocol defines the API data format, including api request and response data format. They use the HTTP1.1 wrapper of the official SkyWalking Browser Protocol. Read it for more details.\nPerformance Data Report Detail information about data format can be found in BrowserPerf.proto.\nPOST http://localhost:12800/browser/perfData Send a performance data object with JSON format.\nInput:\n{ \u0026#34;service\u0026#34;: \u0026#34;web\u0026#34;, \u0026#34;serviceVersion\u0026#34;: \u0026#34;v0.0.1\u0026#34;, \u0026#34;pagePath\u0026#34;: \u0026#34;/index.html\u0026#34;, \u0026#34;redirectTime\u0026#34;: 10, \u0026#34;dnsTime\u0026#34;: 10, \u0026#34;ttfbTime\u0026#34;: 10, \u0026#34;tcpTime\u0026#34;: 10, \u0026#34;transTime\u0026#34;: 10, \u0026#34;domAnalysisTime\u0026#34;: 10, \u0026#34;fptTime\u0026#34;: 10, \u0026#34;domReadyTime\u0026#34;: 10, \u0026#34;loadPageTime\u0026#34;: 10, \u0026#34;resTime\u0026#34;: 10, \u0026#34;sslTime\u0026#34;: 10, \u0026#34;ttlTime\u0026#34;: 10, \u0026#34;firstPackTime\u0026#34;: 10, \u0026#34;fmpTime\u0026#34;: 10 } OutPut:\nHttp Status: 204\nError Log Report Detail information about data format can be found in BrowserPerf.proto.\nPOST http://localhost:12800/browser/errorLogs Send an error log object list with JSON format.\nInput:\n[ { \u0026#34;uniqueId\u0026#34;: \u0026#34;55ec6178-3fb7-43ef-899c-a26944407b01\u0026#34;, \u0026#34;service\u0026#34;: \u0026#34;web\u0026#34;, \u0026#34;serviceVersion\u0026#34;: \u0026#34;v0.0.1\u0026#34;, \u0026#34;pagePath\u0026#34;: \u0026#34;/index.html\u0026#34;, \u0026#34;category\u0026#34;: \u0026#34;ajax\u0026#34;, \u0026#34;message\u0026#34;: \u0026#34;error\u0026#34;, \u0026#34;line\u0026#34;: 1, \u0026#34;col\u0026#34;: 1, \u0026#34;stack\u0026#34;: \u0026#34;error\u0026#34;, \u0026#34;errorUrl\u0026#34;: \u0026#34;/index.html\u0026#34; }, { \u0026#34;uniqueId\u0026#34;: \u0026#34;55ec6178-3fb7-43ef-899c-a26944407b02\u0026#34;, \u0026#34;service\u0026#34;: \u0026#34;web\u0026#34;, \u0026#34;serviceVersion\u0026#34;: \u0026#34;v0.0.1\u0026#34;, \u0026#34;pagePath\u0026#34;: \u0026#34;/index.html\u0026#34;, \u0026#34;category\u0026#34;: \u0026#34;ajax\u0026#34;, \u0026#34;message\u0026#34;: \u0026#34;error\u0026#34;, \u0026#34;line\u0026#34;: 1, \u0026#34;col\u0026#34;: 1, \u0026#34;stack\u0026#34;: \u0026#34;error\u0026#34;, \u0026#34;errorUrl\u0026#34;: \u0026#34;/index.html\u0026#34; } ] OutPut:\nHttp Status: 204\nPOST http://localhost:12800/browser/errorLog Send a single error log object with JSON format.\nInput:\n{ \u0026#34;uniqueId\u0026#34;: \u0026#34;55ec6178-3fb7-43ef-899c-a26944407b01\u0026#34;, \u0026#34;service\u0026#34;: \u0026#34;web\u0026#34;, \u0026#34;serviceVersion\u0026#34;: \u0026#34;v0.0.1\u0026#34;, \u0026#34;pagePath\u0026#34;: \u0026#34;/index.html\u0026#34;, \u0026#34;category\u0026#34;: \u0026#34;ajax\u0026#34;, \u0026#34;message\u0026#34;: \u0026#34;error\u0026#34;, \u0026#34;line\u0026#34;: 1, \u0026#34;col\u0026#34;: 1, \u0026#34;stack\u0026#34;: \u0026#34;error\u0026#34;, \u0026#34;errorUrl\u0026#34;: \u0026#34;/index.html\u0026#34; } OutPut:\nHttp Status: 204\n","excerpt":"HTTP API Protocol HTTP API Protocol defines the API data format, including api request and response …","ref":"/docs/main/v8.3.0/en/protocols/browser-http-api-protocol/","title":"HTTP API Protocol"},{"body":"HTTP API Protocol HTTP API Protocol defines the API data format, including api request and response data format. They use the HTTP1.1 wrapper of the official SkyWalking Trace Data Protocol v3. Read it for more details.\nInstance Management Detail information about data format can be found in Instance Management.\n Report service instance properties   POST http://localhost:12800/v3/management/reportProperties\n Input:\n{ \u0026#34;service\u0026#34;: \u0026#34;User Service Name\u0026#34;, \u0026#34;serviceInstance\u0026#34;: \u0026#34;User Service Instance Name\u0026#34;, \u0026#34;properties\u0026#34;: [{ \u0026#34;language\u0026#34;: \u0026#34;Lua\u0026#34; }] } Output JSON Array:\n{}  Service instance ping   POST http://localhost:12800/v3/management/keepAlive\n Input:\n{ \u0026#34;service\u0026#34;: \u0026#34;User Service Name\u0026#34;, \u0026#34;serviceInstance\u0026#34;: \u0026#34;User Service Instance Name\u0026#34; } OutPut:\n{} Trace Report Detail information about data format can be found in Instance Management. There are two ways to report segment data, one segment per request or segment array in the bulk mode.\nPOST http://localhost:12800/v3/segment Send a single segment object with JSON format.\nInput:\n{ \u0026#34;traceId\u0026#34;: \u0026#34;a12ff60b-5807-463b-a1f8-fb1c8608219e\u0026#34;, \u0026#34;serviceInstance\u0026#34;: \u0026#34;User_Service_Instance_Name\u0026#34;, \u0026#34;spans\u0026#34;: [{ \u0026#34;operationName\u0026#34;: \u0026#34;/ingress\u0026#34;, \u0026#34;startTime\u0026#34;: 1588664577013, \u0026#34;endTime\u0026#34;: 1588664577028, \u0026#34;spanType\u0026#34;: \u0026#34;Exit\u0026#34;, \u0026#34;spanId\u0026#34;: 1, \u0026#34;isError\u0026#34;: false, \u0026#34;parentSpanId\u0026#34;: 0, \u0026#34;componentId\u0026#34;: 6000, \u0026#34;peer\u0026#34;: \u0026#34;upstream service\u0026#34;, \u0026#34;spanLayer\u0026#34;: \u0026#34;Http\u0026#34; }, { \u0026#34;operationName\u0026#34;: \u0026#34;/ingress\u0026#34;, \u0026#34;startTime\u0026#34;: 1588664577013, \u0026#34;tags\u0026#34;: [{ \u0026#34;key\u0026#34;: \u0026#34;http.method\u0026#34;, \u0026#34;value\u0026#34;: \u0026#34;GET\u0026#34; }, { \u0026#34;key\u0026#34;: \u0026#34;http.params\u0026#34;, \u0026#34;value\u0026#34;: \u0026#34;http://localhost/ingress\u0026#34; }], \u0026#34;endTime\u0026#34;: 1588664577028, \u0026#34;spanType\u0026#34;: \u0026#34;Entry\u0026#34;, \u0026#34;spanId\u0026#34;: 0, \u0026#34;parentSpanId\u0026#34;: -1, \u0026#34;isError\u0026#34;: false, \u0026#34;spanLayer\u0026#34;: \u0026#34;Http\u0026#34;, \u0026#34;componentId\u0026#34;: 6000 }], \u0026#34;service\u0026#34;: \u0026#34;User_Service_Name\u0026#34;, \u0026#34;traceSegmentId\u0026#34;: \u0026#34;a12ff60b-5807-463b-a1f8-fb1c8608219e\u0026#34; } OutPut:\nPOST http://localhost:12800/v3/segments Send a segment object list with JSON format.\nInput:\n[{ \u0026#34;traceId\u0026#34;: \u0026#34;a12ff60b-5807-463b-a1f8-fb1c8608219e\u0026#34;, \u0026#34;serviceInstance\u0026#34;: \u0026#34;User_Service_Instance_Name\u0026#34;, \u0026#34;spans\u0026#34;: [{ \u0026#34;operationName\u0026#34;: \u0026#34;/ingress\u0026#34;, \u0026#34;startTime\u0026#34;: 1588664577013, \u0026#34;endTime\u0026#34;: 1588664577028, \u0026#34;spanType\u0026#34;: \u0026#34;Exit\u0026#34;, \u0026#34;spanId\u0026#34;: 1, \u0026#34;isError\u0026#34;: false, \u0026#34;parentSpanId\u0026#34;: 0, \u0026#34;componentId\u0026#34;: 6000, \u0026#34;peer\u0026#34;: \u0026#34;upstream service\u0026#34;, \u0026#34;spanLayer\u0026#34;: \u0026#34;Http\u0026#34; }, { \u0026#34;operationName\u0026#34;: \u0026#34;/ingress\u0026#34;, \u0026#34;startTime\u0026#34;: 1588664577013, \u0026#34;tags\u0026#34;: [{ \u0026#34;key\u0026#34;: \u0026#34;http.method\u0026#34;, \u0026#34;value\u0026#34;: \u0026#34;GET\u0026#34; }, { \u0026#34;key\u0026#34;: \u0026#34;http.params\u0026#34;, \u0026#34;value\u0026#34;: \u0026#34;http://localhost/ingress\u0026#34; }], \u0026#34;endTime\u0026#34;: 1588664577028, \u0026#34;spanType\u0026#34;: \u0026#34;Entry\u0026#34;, \u0026#34;spanId\u0026#34;: 0, \u0026#34;parentSpanId\u0026#34;: -1, \u0026#34;isError\u0026#34;: false, \u0026#34;spanLayer\u0026#34;: \u0026#34;Http\u0026#34;, \u0026#34;componentId\u0026#34;: 6000 }], \u0026#34;service\u0026#34;: \u0026#34;User_Service_Name\u0026#34;, \u0026#34;traceSegmentId\u0026#34;: \u0026#34;a12ff60b-5807-463b-a1f8-fb1c8608219e\u0026#34; }, { \u0026#34;traceId\u0026#34;: \u0026#34;f956699e-5106-4ea3-95e5-da748c55bac1\u0026#34;, \u0026#34;serviceInstance\u0026#34;: \u0026#34;User_Service_Instance_Name\u0026#34;, \u0026#34;spans\u0026#34;: [{ \u0026#34;operationName\u0026#34;: \u0026#34;/ingress\u0026#34;, \u0026#34;startTime\u0026#34;: 1588664577250, \u0026#34;endTime\u0026#34;: 1588664577250, \u0026#34;spanType\u0026#34;: \u0026#34;Exit\u0026#34;, \u0026#34;spanId\u0026#34;: 1, \u0026#34;isError\u0026#34;: false, \u0026#34;parentSpanId\u0026#34;: 0, \u0026#34;componentId\u0026#34;: 6000, \u0026#34;peer\u0026#34;: \u0026#34;upstream service\u0026#34;, \u0026#34;spanLayer\u0026#34;: \u0026#34;Http\u0026#34; }, { \u0026#34;operationName\u0026#34;: \u0026#34;/ingress\u0026#34;, \u0026#34;startTime\u0026#34;: 1588664577250, \u0026#34;tags\u0026#34;: [{ \u0026#34;key\u0026#34;: \u0026#34;http.method\u0026#34;, \u0026#34;value\u0026#34;: \u0026#34;GET\u0026#34; }, { \u0026#34;key\u0026#34;: \u0026#34;http.params\u0026#34;, \u0026#34;value\u0026#34;: \u0026#34;http://localhost/ingress\u0026#34; }], \u0026#34;endTime\u0026#34;: 1588664577250, \u0026#34;spanType\u0026#34;: \u0026#34;Entry\u0026#34;, \u0026#34;spanId\u0026#34;: 0, \u0026#34;parentSpanId\u0026#34;: -1, \u0026#34;isError\u0026#34;: false, \u0026#34;spanLayer\u0026#34;: \u0026#34;Http\u0026#34;, \u0026#34;componentId\u0026#34;: 6000 }], \u0026#34;service\u0026#34;: \u0026#34;User_Service_Name\u0026#34;, \u0026#34;traceSegmentId\u0026#34;: \u0026#34;f956699e-5106-4ea3-95e5-da748c55bac1\u0026#34; }] OutPut:\n","excerpt":"HTTP API Protocol HTTP API Protocol defines the API data format, including api request and response …","ref":"/docs/main/v8.3.0/en/protocols/http-api-protocol/","title":"HTTP API Protocol"},{"body":"IllegalStateException when install Java agent on WebSphere This FAQ came from community discussion and feedback. This user installed SkyWalking Java agent on WebSphere 7.0.0.11 and ibm jdk 1.8_20160719 and 1.7.0_20150407, and had following error logs\nWARN 2019-05-09 17:01:35:905 SkywalkingAgent-1-GRPCChannelManager-0 ProtectiveShieldMatcher : Byte-buddy occurs exception when match type. java.lang.IllegalStateException: Cannot resolve type description for java.security.PrivilegedAction at org.apache.skywalking.apm.dependencies.net.bytebuddy.pool.TypePool$Resolution$Illegal.resolve(TypePool.java:144) at org.apache.skywalking.apm.dependencies.net.bytebuddy.pool.TypePool$Default$WithLazyResolution$LazyTypeDescription.delegate(TypePool.java:1392) at org.apache.skywalking.apm.dependencies.net.bytebuddy.description.type.TypeDescription$AbstractBase$OfSimpleType$WithDelegation.getInterfaces(TypeDescription.java:8016) at org.apache.skywalking.apm.dependencies.net.bytebuddy.description.type.TypeDescription$Generic$OfNonGenericType.getInterfaces(TypeDescription.java:3621) at org.apache.skywalking.apm.dependencies.net.bytebuddy.matcher.HasSuperTypeMatcher.hasInterface(HasSuperTypeMatcher.java:53) at org.apache.skywalking.apm.dependencies.net.bytebuddy.matcher.HasSuperTypeMatcher.hasInterface(HasSuperTypeMatcher.java:54) at org.apache.skywalking.apm.dependencies.net.bytebuddy.matcher.HasSuperTypeMatcher.matches(HasSuperTypeMatcher.java:38) at org.apache.skywalking.apm.dependencies.net.bytebuddy.matcher.HasSuperTypeMatcher.matches(HasSuperTypeMatcher.java:15) at org.apache.skywalking.apm.dependencies.net.bytebuddy.matcher.ElementMatcher$Junction$Conjunction.matches(ElementMatcher.java:107) at org.apache.skywalking.apm.dependencies.net.bytebuddy.matcher.ElementMatcher$Junction$Disjunction.matches(ElementMatcher.java:147) at org.apache.skywalking.apm.dependencies.net.bytebuddy.matcher.ElementMatcher$Junction$Disjunction.matches(ElementMatcher.java:147) at org.apache.skywalking.apm.dependencies.net.bytebuddy.matcher.ElementMatcher$Junction$Disjunction.matches(ElementMatcher.java:147) at org.apache.skywalking.apm.dependencies.net.bytebuddy.matcher.ElementMatcher$Junction$Disjunction.matches(ElementMatcher.java:147) at org.apache.skywalking.apm.dependencies.net.bytebuddy.matcher.ElementMatcher$Junction$Disjunction.matches(ElementMatcher.java:147) ... The exception has been addressed as access grant required in WebSphere. You could follow these steps.\n Set the agent\u0026rsquo;s owner to the owner of WebSphere. Add \u0026ldquo;grant codeBase \u0026ldquo;file:${agent_dir}/-\u0026rdquo; { permission java.security.AllPermission; };\u0026rdquo; in the file of \u0026ldquo;server.policy\u0026rdquo;.  ","excerpt":"IllegalStateException when install Java agent on WebSphere This FAQ came from community discussion …","ref":"/docs/main/v8.3.0/en/faq/install_agent_on_websphere/","title":"IllegalStateException when install Java agent on WebSphere"},{"body":"Init mode SkyWalking backend supports multiple storage implementors. Most of them could initialize the storage, such as Elastic Search, Database automatically when the backend startup in first place.\nBut there are some unexpected happens based on the storage, such as When create Elastic Search indexes concurrently, because of several backend instances startup at the same time., there is a change, the APIs of Elastic Search would be blocked without any exception. And this has more chances happen in container management platform, like k8s.\nThat is where you need Init mode startup.\nSolution Only one single instance should run in Init mode before other instances start up. And this instance will exit graciously after all initialization steps are done.\nUse oapServiceInit.sh/oapServiceInit.bat to start up backend. You should see the following logs\n 2018-11-09 23:04:39,465 - org.apache.skywalking.oap.server.starter.OAPServerStartUp -2214 [main] INFO [] - OAP starts up in init mode successfully, exit now\u0026hellip;\n Kubernetes Initialization in this mode would be included in our Kubernetes scripts and Helm.\n","excerpt":"Init mode SkyWalking backend supports multiple storage implementors. Most of them could initialize …","ref":"/docs/main/v8.3.0/en/setup/backend/backend-init-mode/","title":"Init mode"},{"body":"IP and port setting Backend is using IP and port binding, in order to support the OS having multiple IPs. The binding/listening IP and port are specified by core module\ncore: default: restHost: 0.0.0.0 restPort: 12800 restContextPath: / gRPCHost: 0.0.0.0 gRPCPort: 11800 There are two IP/port pair for gRPC and HTTP rest services.\n Most agents and probes use gRPC service for better performance and code readability. Few agent use rest service, because gRPC may be not supported in that language. UI uses rest service, but data in GraphQL format, always.  Notice IP binding In case some users are not familiar with IP binding, you should know, after you did that, the client could only use this IP to access the service. For example, bind 172.09.13.28, even you are in this machine, must use 172.09.13.28 rather than 127.0.0.1 or localhost to access the service.\nModule provider specified IP and port The IP and port in core are only default provided by core. But some module provider may provide other IP and port settings, this is common. Such as many receiver modules provide this.\n","excerpt":"IP and port setting Backend is using IP and port binding, in order to support the OS having multiple …","ref":"/docs/main/v8.3.0/en/setup/backend/backend-ip-port/","title":"IP and port setting"},{"body":"JVM Metrics Service Abstract Uplink the JVM metrics, including PermSize, HeapSize, CPU, Memory, etc., every second.\ngRPC service define\n","excerpt":"JVM Metrics Service Abstract Uplink the JVM metrics, including PermSize, HeapSize, CPU, Memory, …","ref":"/docs/main/v8.3.0/en/protocols/jvm-protocol/","title":"JVM Metrics Service"},{"body":"Local span and Exit span should not be register Since 6.6.0, SkyWalking cancelled the local span and exit span register. If old java agent(before 6.6.0) is still running, and do register to 6.6.0+ backend, you will face the following warning message.\nclass=RegisterServiceHandler, message = Unexpected endpoint register, endpoint isn't detected from server side. This will not harm the backend or cause any issue. This is a reminder that, your agent or other client should follow the new protocol requirements.\nYou could simply use log4j2.xml to filter this warning message out.\n","excerpt":"Local span and Exit span should not be register Since 6.6.0, SkyWalking cancelled the local span and …","ref":"/docs/main/v8.3.0/en/faq/unexpected-endpoint-register/","title":"Local span and Exit span should not be register"},{"body":"Locate agent config file by system property Supported version 5.0.0-RC+\nWhat is Locate agent config file by system property ？ In Default. The agent will try to locate agent.config, which should be in the /config dictionary of agent package. If User sets the specified agent config file through system properties, The agent will try to load file from there. By the way, This function has no conflict with Setting Override\nOverride priority The specified agent config \u0026gt; The default agent config\nHow to use The content formats of the specified config must be same as the default config.\nUsing System.Properties(-D) to set the specified config path\n-Dskywalking_config=/path/to/agent.config /path/to/agent.config is the absolute path of the specified config file\n","excerpt":"Locate agent config file by system property Supported version 5.0.0-RC+\nWhat is Locate agent config …","ref":"/docs/main/v8.3.0/en/setup/service-agent/java-agent/specified-agent-config/","title":"Locate agent config file by system property"},{"body":"logback plugin  Dependency the toolkit, such as using maven or gradle  \u0026lt;dependency\u0026gt; \u0026lt;groupId\u0026gt;org.apache.skywalking\u0026lt;/groupId\u0026gt; \u0026lt;artifactId\u0026gt;apm-toolkit-logback-1.x\u0026lt;/artifactId\u0026gt; \u0026lt;version\u0026gt;{project.release.version}\u0026lt;/version\u0026gt; \u0026lt;/dependency\u0026gt;  set %tid in Pattern section of logback.xml  \u0026lt;appender name=\u0026#34;STDOUT\u0026#34; class=\u0026#34;ch.qos.logback.core.ConsoleAppender\u0026#34;\u0026gt; \u0026lt;encoder class=\u0026#34;ch.qos.logback.core.encoder.LayoutWrappingEncoder\u0026#34;\u0026gt; \u0026lt;layout class=\u0026#34;org.apache.skywalking.apm.toolkit.log.logback.v1.x.TraceIdPatternLogbackLayout\u0026#34;\u0026gt; \u0026lt;Pattern\u0026gt;%d{yyyy-MM-dd HH:mm:ss.SSS} [%tid] [%thread] %-5level %logger{36} -%msg%n\u0026lt;/Pattern\u0026gt; \u0026lt;/layout\u0026gt; \u0026lt;/encoder\u0026gt; \u0026lt;/appender\u0026gt;  with the MDC, set %X{tid} in Pattern section of logback.xml  \u0026lt;appender name=\u0026#34;STDOUT\u0026#34; class=\u0026#34;ch.qos.logback.core.ConsoleAppender\u0026#34;\u0026gt; \u0026lt;encoder class=\u0026#34;ch.qos.logback.core.encoder.LayoutWrappingEncoder\u0026#34;\u0026gt; \u0026lt;layout class=\u0026#34;org.apache.skywalking.apm.toolkit.log.logback.v1.x.mdc.TraceIdMDCPatternLogbackLayout\u0026#34;\u0026gt; \u0026lt;Pattern\u0026gt;%d{yyyy-MM-dd HH:mm:ss.SSS} [%X{tid}] [%thread] %-5level %logger{36} -%msg%n\u0026lt;/Pattern\u0026gt; \u0026lt;/layout\u0026gt; \u0026lt;/encoder\u0026gt; \u0026lt;/appender\u0026gt;  Support logback AsyncAppender(MDC also support), No additional configuration is required. Refer to the demo of logback.xml below. For details: Logback AsyncAppender  \u0026lt;configuration scan=\u0026#34;true\u0026#34; scanPeriod=\u0026#34; 5 seconds\u0026#34;\u0026gt; \u0026lt;appender name=\u0026#34;STDOUT\u0026#34; class=\u0026#34;ch.qos.logback.core.ConsoleAppender\u0026#34;\u0026gt; \u0026lt;encoder class=\u0026#34;ch.qos.logback.core.encoder.LayoutWrappingEncoder\u0026#34;\u0026gt; \u0026lt;layout class=\u0026#34;org.apache.skywalking.apm.toolkit.log.logback.v1.x.mdc.TraceIdMDCPatternLogbackLayout\u0026#34;\u0026gt; \u0026lt;Pattern\u0026gt;%d{yyyy-MM-dd HH:mm:ss.SSS} [%X{tid}] [%thread] %-5level %logger{36} -%msg%n\u0026lt;/Pattern\u0026gt; \u0026lt;/layout\u0026gt; \u0026lt;/encoder\u0026gt; \u0026lt;/appender\u0026gt; \u0026lt;appender name=\u0026#34;ASYNC\u0026#34; class=\u0026#34;ch.qos.logback.classic.AsyncAppender\u0026#34;\u0026gt; \u0026lt;discardingThreshold\u0026gt;0\u0026lt;/discardingThreshold\u0026gt; \u0026lt;queueSize\u0026gt;1024\u0026lt;/queueSize\u0026gt; \u0026lt;neverBlock\u0026gt;true\u0026lt;/neverBlock\u0026gt; \u0026lt;appender-ref ref=\u0026#34;STDOUT\u0026#34;/\u0026gt; \u0026lt;/appender\u0026gt; \u0026lt;root level=\u0026#34;INFO\u0026#34;\u0026gt; \u0026lt;appender-ref ref=\u0026#34;ASYNC\u0026#34;/\u0026gt; \u0026lt;/root\u0026gt; \u0026lt;/configuration\u0026gt;  When you use -javaagent to active the sky-walking tracer, logback will output traceId, if it existed. If the tracer is inactive, the output will be TID: N/A.  logstash logback plugin  Dependency the toolkit, such as using maven or gradle  \u0026lt;dependency\u0026gt; \u0026lt;groupId\u0026gt;org.apache.skywalking\u0026lt;/groupId\u0026gt; \u0026lt;artifactId\u0026gt;apm-toolkit-logback-1.x\u0026lt;/artifactId\u0026gt; \u0026lt;version\u0026gt;${skywalking.version}\u0026lt;/version\u0026gt; \u0026lt;/dependency\u0026gt;  set LogstashEncoder of logback.xml  \u0026lt;encoder charset=\u0026#34;UTF-8\u0026#34; class=\u0026#34;net.logstash.logback.encoder.LogstashEncoder\u0026#34;\u0026gt; \u0026lt;provider class=\u0026#34;org.apache.skywalking.apm.toolkit.log.logback.v1.x.logstash.TraceIdJsonProvider\u0026#34;\u0026gt; \u0026lt;/provider\u0026gt; \u0026lt;/encoder\u0026gt;  set LoggingEventCompositeJsonEncoder of logstash in logback-spring.xml for custom json format  1.add converter for %tid as child of  node\n\u0026lt;!--add converter for %tid --\u0026gt; \u0026lt;conversionRule conversionWord=\u0026#34;tid\u0026#34; converterClass=\u0026#34;org.apache.skywalking.apm.toolkit.log.logback.v1.x.LogbackPatternConverter\u0026#34;/\u0026gt; 2.add json encoder for custom json format\n\u0026lt;encoder class=\u0026#34;net.logstash.logback.encoder.LoggingEventCompositeJsonEncoder\u0026#34;\u0026gt; \u0026lt;providers\u0026gt; \u0026lt;timestamp\u0026gt; \u0026lt;timeZone\u0026gt;UTC\u0026lt;/timeZone\u0026gt; \u0026lt;/timestamp\u0026gt; \u0026lt;pattern\u0026gt; \u0026lt;pattern\u0026gt; { \u0026#34;level\u0026#34;: \u0026#34;%level\u0026#34;, \u0026#34;tid\u0026#34;: \u0026#34;%tid\u0026#34;, \u0026#34;thread\u0026#34;: \u0026#34;%thread\u0026#34;, \u0026#34;class\u0026#34;: \u0026#34;%logger{1.}:%L\u0026#34;, \u0026#34;message\u0026#34;: \u0026#34;%message\u0026#34;, \u0026#34;stackTrace\u0026#34;: \u0026#34;%exception{10}\u0026#34; } \u0026lt;/pattern\u0026gt; \u0026lt;/pattern\u0026gt; \u0026lt;/providers\u0026gt; \u0026lt;/encoder\u0026gt; ","excerpt":"logback plugin  Dependency the toolkit, such as using maven or gradle  \u0026lt;dependency\u0026gt; …","ref":"/docs/main/v8.3.0/en/setup/service-agent/java-agent/application-toolkit-logback-1.x/","title":"logback plugin"},{"body":"Manual instrument SDK We have manual instrument SDK contributed from the community.\n Go2Sky. Go SDK follows SkyWalking format.  Welcome to consider contributing in following languages:\n Python C++  What is SkyWalking formats and propagation protocols? See these protocols in protocols document.\nCan SkyWalking provide OpenCensus exporter in above languages? At the moment I am writing this document, NO. Because, OC(OpenCensus) don\u0026rsquo;t support context extendable mechanism, and no hook mechanism when manipulate spans. SkyWalking relied on those to propagate more things than trace id and span id.\nWe are already in the middle of discussion, see https://github.com/census-instrumentation/opencensus-specs/issues/70. After OC provides this officially, we can.\nHow about Zipkin instrument SDKs? See Zipkin receiver in backend Choose receiver section.\n","excerpt":"Manual instrument SDK We have manual instrument SDK contributed from the community.\n Go2Sky. Go SDK …","ref":"/docs/main/v8.3.0/en/concepts-and-designs/manual-sdk/","title":"Manual instrument SDK"},{"body":"Meter Analysis Language Meter system provides a functional analysis language called MAL(Meter Analysis Language) that lets the user analyze and aggregate meter data in OAP streaming system. The result of an expression can either be ingested by agent analyzer, or OC/Prometheus analyzer.\nLanguage data type In MAL, an expression or sub-expression can evaluate to one of two types:\n Sample family - a set of samples(metrics) containing a range of metrics whose name is identical. Scalar - a simple numeric value. it supports integer/long, floating/double,  Sample family A set of samples, which is as the basic unit in MAL. For example:\ninstance_trace_count The above sample family might contains following simples which are provided by external modules, for instance, agent analyzer:\ninstance_trace_count{region=\u0026quot;us-west\u0026quot;,az=\u0026quot;az-1\u0026quot;} 100 instance_trace_count{region=\u0026quot;us-east\u0026quot;,az=\u0026quot;az-3\u0026quot;} 20 instance_trace_count{region=\u0026quot;asia-north\u0026quot;,az=\u0026quot;az-1\u0026quot;} 33 Tag filter MAL support four type operations to filter samples in a sample family:\n tagEqual: Filter tags that are exactly equal to the provided string. tagNotEqual: Filter tags that are not equal to the provided string. tagMatch: Filter tags that regex-match the provided string. tagNotMatch: Filter labels that do not regex-match the provided string.  For example, this filters all instance_trace_count samples for us-west and asia-north region and az-1 az:\ninstance_trace_count.tagMatch(\u0026quot;region\u0026quot;, \u0026quot;us-west|asia-north\u0026quot;).tagEqual(\u0026quot;az\u0026quot;, \u0026quot;az-1\u0026quot;) Binary operators The following binary arithmetic operators are available in MAL:\n + (addition) - (subtraction) * (multiplication) / (division)  Binary operators are defined between scalar/scalar, sampleFamily/scalar and sampleFamily/sampleFamily value pairs.\nBetween two scalars: they evaluate to another scalar that is the result of the operator applied to both scalar operands:\n1 + 2 Between a sample family and a scalar, the operator is applied to the value of every sample in the smaple family. For example:\ninstance_trace_count + 2 or\n2 + instance_trace_count results in\ninstance_trace_count{region=\u0026quot;us-west\u0026quot;,az=\u0026quot;az-1\u0026quot;} 102 // 100 + 2 instance_trace_count{region=\u0026quot;us-east\u0026quot;,az=\u0026quot;az-3\u0026quot;} 22 // 20 + 2 instance_trace_count{region=\u0026quot;asia-north\u0026quot;,az=\u0026quot;az-1\u0026quot;} 35 // 33 + 2 Between two sample families, a binary operator is applied to each sample in the left-hand side sample family and its matching sample in the right-hand sample family. A new sample family with empty name will be generated. Only the matched tags will be reserved. Samples for which no matching sample in the right-hand sample family are not in the result.\nAnother sample family instance_trace_analysis_error_count is\ninstance_trace_analysis_error_count{region=\u0026quot;us-west\u0026quot;,az=\u0026quot;az-1\u0026quot;} 20 instance_trace_analysis_error_count{region=\u0026quot;asia-north\u0026quot;,az=\u0026quot;az-1\u0026quot;} 11 Example expression:\ninstance_trace_analysis_error_count / instance_trace_count This returns a result sample family containing the error rate of trace analysis. The samples with region us-west and az az-3 have no match and will not show up in the result:\n{region=\u0026quot;us-west\u0026quot;,az=\u0026quot;az-1\u0026quot;} 0.8 // 20 / 100 {region=\u0026quot;asia-north\u0026quot;,az=\u0026quot;az-1\u0026quot;} 0.3333 // 11 / 33 Aggregation Operation Sample family supports the following aggregation operations that can be used to aggregate the samples of a single sample family, resulting in a new sample family of fewer samples(even single one) with aggregated values:\n sum (calculate sum over dimensions) min (select minimum over dimensions) (TODO) max (select maximum over dimensions) (TODO) avg (calculate the average over dimensions) (TODO)  These operations can be used to aggregate over all label dimensions or preserve distinct dimensions by inputting by parameter.\n\u0026lt;aggr-op\u0026gt;(by: \u0026lt;tag1, tag2, ...\u0026gt;) Example expression:\ninstance_trace_count.sum(by: ['az']) will output a result:\ninstance_trace_count{az=\u0026quot;az-1\u0026quot;} 133 // 100 + 33 instance_trace_count{az=\u0026quot;az-3\u0026quot;} 20 Function Duraton is a textual representation of a time range. The formats accepted are based on the ISO-8601 duration format {@code PnDTnHnMn.nS} with days considered to be exactly 24 hours.\nExamples:\n \u0026ldquo;PT20.345S\u0026rdquo; \u0026ndash; parses as \u0026ldquo;20.345 seconds\u0026rdquo; \u0026ldquo;PT15M\u0026rdquo; \u0026ndash; parses as \u0026ldquo;15 minutes\u0026rdquo; (where a minute is 60 seconds) \u0026ldquo;PT10H\u0026rdquo; \u0026ndash; parses as \u0026ldquo;10 hours\u0026rdquo; (where an hour is 3600 seconds) \u0026ldquo;P2D\u0026rdquo; \u0026ndash; parses as \u0026ldquo;2 days\u0026rdquo; (where a day is 24 hours or 86400 seconds) \u0026ldquo;P2DT3H4M\u0026rdquo; \u0026ndash; parses as \u0026ldquo;2 days, 3 hours and 4 minutes\u0026rdquo; \u0026ldquo;P-6H3M\u0026rdquo; \u0026ndash; parses as \u0026ldquo;-6 hours and +3 minutes\u0026rdquo; \u0026ldquo;-P6H3M\u0026rdquo; \u0026ndash; parses as \u0026ldquo;-6 hours and -3 minutes\u0026rdquo; \u0026ldquo;-P-6H+3M\u0026rdquo; \u0026ndash; parses as \u0026ldquo;+6 hours and -3 minutes\u0026rdquo;  increase increase(Duration). Calculates the increase in the time range.\nrate rate(Duration). Calculates the per-second average rate of increase of the time range.\nirate irate(). Calculates the per-second instant rate of increase of the time range.\ntag tag({allTags -\u0026gt; }). Update tags of samples. User can add, drop, rename and update tags.\nhistogram histogram(le: '\u0026lt;the tag name of le\u0026gt;'). Transforms less based histogram buckets to meter system histogram buckets. le parameter hints the tag name of a bucket.\nhistogram_percentile histogram_percentile([\u0026lt;p scalar\u0026gt;]). Hints meter-system to calculates the p-percentile (0 ≤ p ≤ 100) from the buckets.\ntime time(). returns the number of seconds since January 1, 1970 UTC.\nDown Sampling Operation MAL should instruct meter-system how to do downsampling for metrics. It doesn\u0026rsquo;t only refer to aggregate raw samples to minute level, but also hints data from minute to higher levels, for instance, hour and day.\nDown sampling operations are as global function in MAL:\n avg latest (TODO) min (TODO) max (TODO) mean (TODO) sum (TODO) count (TODO)  The default one is avg if not specific an operation.\nIf user want get latest time from last_server_state_sync_time_in_seconds:\nlatest(last_server_state_sync_time_in_seconds.tagEqual('production', 'catalog')) or latest last_server_state_sync_time_in_seconds.tagEqual('production', 'catalog') Metric level function Metric has three level, service, instance and endpoint. They extract level relevant labels from metric labels, then hints meter-system which level this metrics should be.\n servcie([svc_label1, svc_label2...]) extracts service level labels from the array argument. instance([svc_label1, svc_label2...], [ins_label1, ins_label2...]) extracts service level labels from the first array argument, extracts instance level labels from the second array argument. endpoint([svc_label1, svc_label2...], [ep_label1, ep_label2...]) extracts service level labels from the first array argument, extracts endpoint level labels from the second array argument.  More Examples Please refer to OAP Self-Observability\n","excerpt":"Meter Analysis Language Meter system provides a functional analysis language called MAL(Meter …","ref":"/docs/main/v8.3.0/en/concepts-and-designs/mal/","title":"Meter Analysis Language"},{"body":"Meter Receiver Meter receiver is accepting the metrics of meter protocol format into the Meter System.\nModule define receiver-meter: selector: ${SW_RECEIVER_METER:default} default: In Kafka Fetcher, we need to follow the configuration to enable it.\nkafka-fetcher: selector: ${SW_KAFKA_FETCHER:default} default: bootstrapServers: ${SW_KAFKA_FETCHER_SERVERS:localhost:9092} enableMeterSystem: ${SW_KAFKA_FETCHER_ENABLE_METER_SYSTEM:true} Configuration file Meter receiver is configured via a configuration file. The configuration file defines everything related to receiving from agents, as well as which rule files to load.\nOAP can load the configuration at bootstrap. If the new configuration is not well-formed, OAP fails to start up. The files are located at $CLASSPATH/meter-analyzer-config.\nThe file is written in YAML format, defined by the scheme described below. Brackets indicate that a parameter is optional.\nA example can be found here. If you\u0026rsquo;re using Spring sleuth, you could use Spring Sleuth Setup.\nMeters configure # Meter config allow your to recompute meters: # Meter name which combines with a prefix \u0026#39;meter_\u0026#39; as the index/table name in storage. - name: \u0026lt;string\u0026gt; # The meter scope scope: # Scope type should be one of SERVICE, SERVICE_INSTANCE, ENDPOINT type: \u0026lt;string\u0026gt; # \u0026lt;Optional\u0026gt; Appoint the endpoint name if using ENDPOINT scope endpoint: \u0026lt;string\u0026gt; # The agent source of the transformation operation. meter: # The transformation operation from prometheus metrics to Skywalking ones.  operation: \u0026lt;string\u0026gt; # Meter value parse groovy script. value: \u0026lt;string\u0026gt; # Aggregate metrics group by dedicated labels groupBy: - \u0026lt;labelName\u0026gt; # \u0026lt;Optional\u0026gt; Appoint percentiles if using avgHistogramPercentile operation. percentile: - \u0026lt;rank\u0026gt; Meter transform operation The available operations are avg, avgLabeled, avgHistogram and avgHistogramPercentile. The avg and avgXXX mean to average the raw received metrics.\nWhen you specify avgHistogram and avgHistogramPercentile, the source should be the type of histogram.\nMeter value script The script is provide a easy way to custom build a complex value, and it also support combine multiple meter into one.\nMeter value grammar // Declare the meter value. meter[METER_NAME] [.tagFilter(TAG_KEY, TAG_VALUE)] .FUNCTION(VALUE | METER) Meter Name Use name to refer the metrics raw data from agent side.\nTag Filter Use the meter tag to filter the meter value.\n meter[\u0026ldquo;test_meter\u0026rdquo;].tagFilter(\u0026ldquo;k1\u0026rdquo;, \u0026ldquo;v1\u0026rdquo;)\n In this case, filter the tag key equals k1 and tag value equals v1 value from test_meter.\nAggregation Function Use multiple build-in methods to help operate the value.\nProvided functions\n add. Add value into meter. Support single value.   meter[\u0026ldquo;test_meter\u0026rdquo;].add(2)\n In this case, all of the meter values will add 2.\n meter[\u0026ldquo;test_meter1\u0026rdquo;].add(meter[\u0026ldquo;test_meter2\u0026rdquo;])\n In this case, all of the test_meter1 values will add value from test_meter2, ensure test_meter2 only has single value to operate, could use tagFilter.\n subtract. Subtract value into meter. Support single value.   meter[\u0026ldquo;test_meter\u0026rdquo;].subtract(2)\n In this case, all of the meter values will subtract 2.\n meter[\u0026ldquo;test_meter1\u0026rdquo;].subtract(meter[\u0026ldquo;test_meter2\u0026rdquo;])\n In this case, all of the test_meter1 values will subtract value from test_meter2, ensure test_meter2 only has single value to operate, could use tagFilter.\n multiply. Multiply value into meter. Support single value.   meter[\u0026ldquo;test_meter\u0026rdquo;].multiply(2)\n In this case, all of the meter values will multiply 2.\n meter[\u0026ldquo;test_meter1\u0026rdquo;].multiply(meter[\u0026ldquo;test_meter2\u0026rdquo;])\n In this case, all of the test_meter1 values will multiply value from test_meter2, ensure test_meter2 only has single value to operate, could use tagFilter.\n divide. Divide value into meter. Support single value.   meter[\u0026ldquo;test_meter\u0026rdquo;].divide(2)\n In this case, all of the meter values will divide 2.\n meter[\u0026ldquo;test_meter1\u0026rdquo;].divide(meter[\u0026ldquo;test_meter2\u0026rdquo;])\n In this case, all of the test_meter1 values will divide value from test_meter2, ensure test_meter2 only has single value to operate, could use tagFilter.\n scale. Scale value into meter. Support single value.   meter[\u0026ldquo;test_meter\u0026rdquo;].scale(2)\n In this case, all of the meter values will scale 2. For example, meter[\u0026quot;test_meter\u0026quot;] value is 1, then using scale(2), the result will be 100.\n rate.(Not Recommended) Rate value from the time range. Support single value and Histogram.   meter[\u0026ldquo;test_meter\u0026rdquo;].rate(\u0026ldquo;P15S\u0026rdquo;)\n In this case, all of the meter values will rate from 15s before.\n irate.(Not Recommended) IRate value from the time range. Support single value and Histogram.   meter[\u0026ldquo;test_meter\u0026rdquo;].irate(\u0026ldquo;P15S\u0026rdquo;)\n In this case, all of the meter values will irate from 15s before.\n increase.(Not Recommended) increase value from the time range. Support single value and Histogram.   meter[\u0026ldquo;test_meter\u0026rdquo;].increase(\u0026ldquo;P15S\u0026rdquo;)\n In this case, all of the meter values will increase from 15s before.\nEven we supported rate, irate, increase function in the backend, but we still recommend user to consider using client-side APIs to do these. Because\n The OAP has to set up caches to calculate the value. Once the agent reconnected to another OAP instance, the time windows of rate calculation will break. Then, the result would not be accurate.  ","excerpt":"Meter Receiver Meter receiver is accepting the metrics of meter protocol format into the Meter …","ref":"/docs/main/v8.3.0/en/setup/backend/backend-meter/","title":"Meter Receiver"},{"body":"Meter System Meter system is another streaming calculation mode, especially for metrics data. In the OAL, there are clear Scope Definitions, including native objects. Meter system is focusing on the data type itself, and provides more flexible to the end user to define the scope entity.\nThe meter system is open to different receivers and fetchers in the backend, follow the backend setup document for more details.\nEvery metrics is declared in the meter system should include following attribute\n Metrics Name. An unique name globally, should avoid overlap the OAL variable names. Function Name. The function used for this metrics, distributed aggregation, value calculation and down sampling calculation based on the function implementation. Also, the data structure is determined by the function too, such as function Avg is for Long. Scope Type. Unlike inside the OAL, there are plenty of logic scope definitions, in meter system, only type is required. Type values include service, instance, and endpoint, like we introduced in the Overview. The values of scope entity name, such as service name, are required when metrics data generated with the metrics data value.  NOTICE, the metrics must be declared in the bootstrap stage, no runtime changed.\nMeter System supports following binding functions\n avg. Calculate the avg value for every entity in the same metrics name. histogram. Aggregate the counts in the configurable buckets, buckets is configurable but must be assigned in the declaration stage. percentile. Read percentile in WIKI. Unlike in the OAL, we provide 50/75/90/95/99 in default, in the meter system function, percentile function accepts several ranks, which should be in the (0, 100) range.  ","excerpt":"Meter System Meter system is another streaming calculation mode, especially for metrics data. In the …","ref":"/docs/main/v8.3.0/en/concepts-and-designs/meter/","title":"Meter System"},{"body":"Metrics Exporter SkyWalking provides basic and most important metrics aggregation, alarm and analysis. In real world, people may want to forward the data to their 3rd party system, for deeper analysis or anything else. Metrics Exporter makes that possible.\nMetrics exporter is an independent module, you need manually active it.\nRight now, we provide the following exporters\n gRPC exporter  gRPC exporter gRPC exporter uses SkyWalking native exporter service definition. Here is proto definition.\nservice MetricExportService { rpc export (stream ExportMetricValue) returns (ExportResponse) { } rpc subscription (SubscriptionReq) returns (SubscriptionsResp) { }}message ExportMetricValue { string metricName = 1; string entityName = 2; string entityId = 3; ValueType type = 4; int64 timeBucket = 5; int64 longValue = 6; double doubleValue = 7; repeated int64 longValues = 8;}message SubscriptionsResp { repeated string metricNames = 1;}enum ValueType { LONG = 0; DOUBLE = 1; MULTI_LONG = 2;}message SubscriptionReq {}message ExportResponse {}To active the exporter, you should add this into your application.yml\nexporter: grpc: targetHost: 127.0.0.1 targetPort: 9870  targetHost:targetPort is the expected target service address. You could set any gRPC server to receive the data. Target gRPC service needs to be standby, otherwise, the OAP starts up failure.  For target exporter service subscription implementation Return the expected metrics name list, all the names must match the OAL script definition. Return empty list, if you want to export all metrics.\nexport implementation Stream service, all subscribed metrics will be sent to here, based on OAP core schedule. Also, if the OAP deployed as cluster, then this method will be called concurrently. For metrics value, you need follow #type to choose #longValue or #doubleValue.\n","excerpt":"Metrics Exporter SkyWalking provides basic and most important metrics aggregation, alarm and …","ref":"/docs/main/v8.3.0/en/setup/backend/metrics-exporter/","title":"Metrics Exporter"},{"body":"Namespace Background SkyWalking is a monitoring tool, which collects metrics from a distributed system. In the real world, a very large distributed system includes hundreds of services, thousands of service instances. In that case, most likely, more than one group, even more than one company are maintaining and monitoring the distributed system. Each one of them takes charge of different parts, don\u0026rsquo;t want or shouldn\u0026rsquo;t share there metrics.\nNamespace is the proposal from this.It is used for tracing and monitoring isolation.\nSet the namespace Set agent.namespace in agent config # The agent namespace # agent.namespace=default-namespace The default value of agent.namespace is empty.\nInfluence The default header key of SkyWalking is sw8, more in this document. After agent.namespace is set, the key changes to namespace-sw8.\nThe across process propagation chain breaks, when the two sides are using different namespace.\n","excerpt":"Namespace Background SkyWalking is a monitoring tool, which collects metrics from a distributed …","ref":"/docs/main/v8.3.0/en/setup/service-agent/java-agent/namespace/","title":"Namespace"},{"body":"Observability Analysis Language Provide OAL(Observability Analysis Language) to analysis incoming data in streaming mode.\nOAL focuses on metrics in Service, Service Instance and Endpoint. Because of that, the language is easy to learn and use.\nSince 6.3, the OAL engine is embedded in OAP server runtime, as oal-rt(OAL Runtime). OAL scripts now locate in /config folder, user could simply change and reboot the server to make it effective. But still, OAL script is compile language, OAL Runtime generates java codes dynamically.\nYou could open set SW_OAL_ENGINE_DEBUG=Y at system env, to see which classes generated.\nGrammar Scripts should be named as *.oal\n// Declare the metrics. METRICS_NAME = from(SCOPE.(* | [FIELD][,FIELD ...])) [.filter(FIELD OP [INT | STRING])] .FUNCTION([PARAM][, PARAM ...]) // Disable hard code disable(METRICS_NAME); Scope Primary SCOPEs are All, Service, ServiceInstance, Endpoint, ServiceRelation, ServiceInstanceRelation, EndpointRelation. Also there are some secondary scopes, which belongs to one primary scope.\nRead Scope Definitions, you can find all existing Scopes and Fields.\nFilter Use filter to build the conditions for the value of fields, by using field name and expression.\nThe expressions support to link by and, or and (...). The OPs support ==, !=, \u0026gt;, \u0026lt;, \u0026gt;=, \u0026lt;=, in [...] ,like %..., like ...% , like %...% , contain and not contain, with type detection based of field type. Trigger compile or code generation error if incompatible.\nAggregation Function The default functions are provided by SkyWalking OAP core, and could implement more.\nProvided functions\n longAvg. The avg of all input per scope entity. The input field must be a long.   instance_jvm_memory_max = from(ServiceInstanceJVMMemory.max).longAvg();\n In this case, input are request of each ServiceInstanceJVMMemory scope, avg is based on field max.\n doubleAvg. The avg of all input per scope entity. The input field must be a double.   instance_jvm_cpu = from(ServiceInstanceJVMCPU.usePercent).doubleAvg();\n In this case, input are request of each ServiceInstanceJVMCPU scope, avg is based on field usePercent.\n percent. The number or ratio expressed as a fraction of 100, for the condition matched input.   endpoint_percent = from(Endpoint.*).percent(status == true);\n In this case, all input are requests of each endpoint, condition is endpoint.status == true.\n rate. The rate expressed as a fraction of 100, for the condition matched input.   browser_app_error_rate = from(BrowserAppTraffic.*).rate(trafficCategory == BrowserAppTrafficCategory.FIRST_ERROR, trafficCategory == BrowserAppTrafficCategory.NORMAL);\n In this case, all input are requests of each browser app traffic, numerator condition is trafficCategory == BrowserAppTrafficCategory.FIRST_ERROR and denominator condition is trafficCategory == BrowserAppTrafficCategory.NORMAL. The parameter (1) is the numerator condition. The parameter (2) is the denominator condition.\n count. The sum calls per scope entity.   service_calls_sum = from(Service.*).count();\n In this case, calls of each service.\n histogram. Read Heatmap in WIKI   all_heatmap = from(All.latency).histogram(100, 20);\n In this case, thermodynamic heatmap of all incoming requests. The parameter (1) is the precision of latency calculation, such as in above case, 113ms and 193ms are considered same in the 101-200ms group. The parameter (2) is the group amount. In above case, 21(param value + 1) groups are 0-100ms, 101-200ms, \u0026hellip; 1901-2000ms, 2000+ms\n apdex. Read Apdex in WIKI   service_apdex = from(Service.latency).apdex(name, status);\n In this case, apdex score of each service. The parameter (1) is the service name, which effects the Apdex threshold value loaded from service-apdex-threshold.yml in the config folder. The parameter (2) is the status of this request. The status(success/failure) effects the Apdex calculation.\n p99, p95, p90, p75, p50. Read percentile in WIKI   all_percentile = from(All.latency).percentile(10);\n percentile is the first multiple value metrics, introduced since 7.0.0. As having multiple values, it could be query through getMultipleLinearIntValues GraphQL query. In this case, p99, p95, p90, p75, p50 of all incoming request. The parameter is the precision of p99 latency calculation, such as in above case, 120ms and 124 are considered same. Before 7.0.0, use p99, p95, p90, p75, p50 func(s) to calculate metrics separately. Still supported in 7.x, but don\u0026rsquo;t be recommended, and don\u0026rsquo;t be included in official OAL script.\n all_p99 = from(All.latency).p99(10);\n In this case, p99 value of all incoming requests. The parameter is the precision of p99 latency calculation, such as in above case, 120ms and 124 are considered same.\nMetrics name The metrics name for storage implementor, alarm and query modules. The type inference supported by core.\nGroup All metrics data will be grouped by Scope.ID and min-level TimeBucket.\n In Endpoint scope, the Scope.ID = Endpoint id (the unique id based on service and its Endpoint)  Disable Disable is an advanced statement in OAL, which is only used in certain case. Some of the aggregation and metrics are defined through core hard codes, this disable statement is designed for make them de-active, such as segment, top_n_database_statement. In default, no one is being disable.\nExamples // Calculate p99 of both Endpoint1 and Endpoint2 endpoint_p99 = from(Endpoint.latency).filter(name in (\u0026quot;Endpoint1\u0026quot;, \u0026quot;Endpoint2\u0026quot;)).summary(0.99) // Calculate p99 of Endpoint name started with `serv` serv_Endpoint_p99 = from(Endpoint.latency).filter(name like \u0026quot;serv%\u0026quot;).summary(0.99) // Calculate the avg response time of each Endpoint endpoint_avg = from(Endpoint.latency).avg() // Calculate the p50, p75, p90, p95 and p99 of each Endpoint by 50 ms steps. endpoint_percentile = from(Endpoint.latency).percentile(10) // Calculate the percent of response status is true, for each service. endpoint_success = from(Endpoint.*).filter(status == true).percent() // Calculate the sum of response code in [404, 500, 503], for each service. endpoint_abnormal = from(Endpoint.*).filter(responseCode in [404, 500, 503]).count() // Calculate the sum of request type in [RequestType.PRC, RequestType.gRPC], for each service. endpoint_rpc_calls_sum = from(Endpoint.*).filter(type in [RequestType.PRC, RequestType.gRPC]).count() // Calculate the sum of endpoint name in [\u0026quot;/v1\u0026quot;, \u0026quot;/v2\u0026quot;], for each service. endpoint_url_sum = from(Endpoint.*).filter(name in [\u0026quot;/v1\u0026quot;, \u0026quot;/v2\u0026quot;]).count() // Calculate the sum of calls for each service. endpoint_calls = from(Endpoint.*).sum() // Calculate the CPM with the GET method for each service.The value is made up with `tagKey:tagValue`. service_cpm_http_get = from(Service.*).filter(tags contain \u0026quot;http.method:GET\u0026quot;).cpm() // Calculate the CPM with the HTTP method except for the GET method for each service.The value is made up with `tagKey:tagValue`. service_cpm_http_other = from(Service.*).filter(tags not contain \u0026quot;http.method:GET\u0026quot;).cpm() disable(segment); disable(endpoint_relation_server_side); disable(top_n_database_statement); ","excerpt":"Observability Analysis Language Provide OAL(Observability Analysis Language) to analysis incoming …","ref":"/docs/main/v8.3.0/en/concepts-and-designs/oal/","title":"Observability Analysis Language"},{"body":"Observability Analysis Platform OAP(Observability Analysis Platform) is a new concept, which starts in SkyWalking 6.x. OAP replaces the old SkyWalking whole backend. The capabilities of the platform are following.\nOAP capabilities OAP accepts data from more sources, which belongs two groups: Tracing and Metrics.\n Tracing. Including, SkyWalking native data formats. Zipkin v1,v2 data formats and Jaeger data formats. Metrics. SkyWalking integrates with Service Mesh platforms, such as Istio, Envoy, Linkerd, to provide observability from data panel or control panel. Also, SkyWalking native agents can run in metrics mode, which highly improve the performance.  At the same time by using any integration solution provided, such as SkyWalking log plugin or toolkits, SkyWalking provides visualization integration for binding tracing and logging together by using the trace id and span id.\nAs usual, all services provided by gRPC and HTTP protocol to make integration easier for unsupported ecosystem.\nTracing in OAP Tracing in OAP has two ways to process.\n Traditional way in SkyWalking 5 series. Format tracing data in SkyWalking trace segment and span formats, even for Zipkin data format. The OAP analysis the segments to get metrics, and push the metrics data into the streaming aggregation. Consider tracing as some kinds of logging only. Just provide save and visualization capabilities for trace.  Also, SkyWalking accepts trace formats from other project, such as Zipkin, Jaeger, OpenCensus. These formats could be processed in the two ways too.\nMetrics in OAP Metrics in OAP is totally new feature in 6 series. Build observability for a distributed system based on metrics of connected nodes. No tracing data is required.\nMetrics data are aggregated inside OAP cluster in streaming mode. See about Observability Analysis Language, which provides the easy way to do aggregation and analysis in script style.\n","excerpt":"Observability Analysis Platform OAP(Observability Analysis Platform) is a new concept, which starts …","ref":"/docs/main/v8.3.0/en/concepts-and-designs/backend-overview/","title":"Observability Analysis Platform"},{"body":"Observe service mesh through ALS Envoy ALS(access log service) provides full logs about RPC routed, including HTTP and TCP.\nThe solution is initialized and firstly implemented by Sheng Wu, Hongtao Gao, Lizan Zhou, and Dhi Aurrahman at 17 May. 2019, and presented on KubeCon China 2019. Here is the recorded Video.\nSkyWalking is the first open source project introducing this ALS based solution to the world. This provides a new way with very low payload to service mesh, but the same observability.\nYou need three steps to open ALS.\n  Open envoyAccessLogService in istio by enabling envoyAccessLogService in ProxyConfig.\nUpper istio 1.6.0, if istio installed by demo profile, you can open ALS ues command:\nistioctl manifest apply --set profile=demo --set meshConfig.defaultConfig.envoyAccessLogService.address=skywalking-oap.skywalking.svc:11800 --set meshConfig.enableEnvoyAccessLogService=true Note: SkyWalking OAP service is at skywalking namespace, and the port of gRPC service is 11800\n  (Default is ACTIVATED) Activate SkyWalking envoy receiver.\n  Active ALS analyzer, there are two available analyzers, k8s-mesh and mx-mesh, k8s-mesh uses the metadata from Kubernetes cluster, hence in this analyzer OAP needs access roles to Pod, Service, and Endpoints; mx-mesh uses the Envoy metadata exchange mechanism to get the service name, etc., this analyzer requires Istio to enable the metadata exchange filter(you can enable it by --set telemetry.v2.enabled=true, or if you\u0026rsquo;re using Istio 1.7+ and installing it with profile demo/preview, it should be enabled then). Setting system env variable SW_ENVOY_METRIC_ALS_HTTP_ANALYSIS to activate the analyzer, such as SW_ENVOY_METRIC_ALS_HTTP_ANALYSIS=k8s-mesh.\n  envoy-metric: selector: ${SW_ENVOY_METRIC:default} default: acceptMetricsService: ${SW_ENVOY_METRIC_SERVICE:true} alsHTTPAnalysis: ${SW_ENVOY_METRIC_ALS_HTTP_ANALYSIS:\u0026#34;\u0026#34;} # Setting the system env variable would override this.  For multiple values，please use , symbol to concatenate.\nHere\u0026rsquo;s an example to deploy SkyWalking by Helm chart.\nistioctl install --set profile=demo --set meshConfig.defaultConfig.envoyAccessLogService.address=skywalking-oap.istio-system:11800 --set meshConfig.enableEnvoyAccessLogService=true git clone https://github.com/apache/skywalking-kubernetes.git cd skywalking-kubernetes/chart helm repo add elastic https://helm.elastic.co helm dep up skywalking helm install 8.1.0 skywalking -n istio-system --set oap.env.SW_ENVOY_METRIC_ALS_HTTP_ANALYSIS=k8s-mesh --set fullnameOverride=skywalking --set oap.envoy.als.enabled=true Notice, only use this when Envoy is under Istio\u0026rsquo;s control, and they are in k8s environment. The OAP requires the read right to k8s API server for all pods IPs.\nYou can use kubectl -n istio-system logs -l app=skywalking | grep \u0026quot;K8sALSServiceMeshHTTPAnalysis\u0026quot; to ensure OAP ALS k8s-mesh analysis has been active.\n","excerpt":"Observe service mesh through ALS Envoy ALS(access log service) provides full logs about RPC routed, …","ref":"/docs/main/v8.3.0/en/setup/envoy/als_setting/","title":"Observe service mesh through ALS"},{"body":"Official OAL script First, read OAL introduction.\nFind OAL script at the /config/oal/*.oal of SkyWalking dist, since 8.0.0. You could change it(such as adding filter condition, or add new metrics) and reboot the OAP server, then it will affect.\nAll metrics named in this script could be used in alarm and UI query.\nNotice,\nIf you try to add or remove some metrics, UI may break, we only recommend you to do this when you plan to build your own UI based on the customization analysis core.\n","excerpt":"Official OAL script First, read OAL introduction.\nFind OAL script at the /config/oal/*.oal of …","ref":"/docs/main/v8.3.0/en/guides/backend-oal-scripts/","title":"Official OAL script"},{"body":"Open Fetcher Fetcher is a concept in SkyWalking backend. It uses pulling mode rather than receiver, which read the data from the target systems. This mode is typically in some metrics SDKs, such as Prometheus.\nPrometheus Fetcher Suppose you want to enable some metric-custom.yaml files stored at fetcher-prom-rules, append its name to enabledRules of promethues-fetcher as below:\nprometheus-fetcher: selector: ${SW_PROMETHEUS_FETCHER:default} default: enabledRules: ${SW_PROMETHEUS_FETCHER_ENABLED_RULES:\u0026#34;self,metric-custom\u0026#34;} Configuration file Prometheus fetcher is configured via a configuration file. The configuration file defines everything related to fetching services and their instances, as well as which rule files to load.\nOAP can load the configuration at bootstrap. If the new configuration is not well-formed, OAP fails to start up. The files are located at $CLASSPATH/fetcher-prom-rules.\nThe file is written in YAML format, defined by the scheme described below. Brackets indicate that a parameter is optional.\nA full example can be found here\nGeneric placeholders are defined as follows:\n \u0026lt;duration\u0026gt;: a duration This will parse a textual representation of a duration. The formats accepted are based on the ISO-8601 duration format PnDTnHnMn.nS with days considered to be exactly 24 hours. \u0026lt;labelname\u0026gt;: a string matching the regular expression [a-zA-Z_][a-zA-Z0-9_]* \u0026lt;labelvalue\u0026gt;: a string of unicode characters \u0026lt;host\u0026gt;: a valid string consisting of a hostname or IP followed by an optional port number \u0026lt;path\u0026gt;: a valid URL path \u0026lt;string\u0026gt;: a regular string  # How frequently to fetch targets. fetcherInterval: \u0026lt;duration\u0026gt; # Per-fetch timeout when fetching this target. fetcherTimeout: \u0026lt;duration\u0026gt; # The HTTP resource path on which to fetch metrics from targets. metricsPath: \u0026lt;path\u0026gt; #Statically configured targets. staticConfig: # The targets specified by the static config. targets: [ - \u0026lt;target\u0026gt; ] # Labels assigned to all metrics fetched from the targets. labels: [ \u0026lt;labelname\u0026gt;: \u0026lt;labelvalue\u0026gt; ... ] # expSuffix is appended to all expression in this file. expSuffix: \u0026lt;string\u0026gt; # insert metricPrefix into metric name: \u0026lt;metricPrefix\u0026gt;_\u0026lt;raw_metric_name\u0026gt; metricPrefix: \u0026lt;string\u0026gt; # Metrics rule allow you to recompute queries. metricsRules: [ - \u0026lt;metric_rules\u0026gt; ]  # The url of target exporter. the format should be complied with \u0026#34;java.net.URI\u0026#34; url: \u0026lt;string\u0026gt; # The path of root CA file. sslCaFilePath: \u0026lt;string\u0026gt; \u0026lt;metric_rules\u0026gt; # The name of rule, which combinates with a prefix \u0026#39;meter_\u0026#39; as the index/table name in storage. name: \u0026lt;string\u0026gt; # MAL expression. exp: \u0026lt;string\u0026gt; More about MAL, please refer to mal.md\nKafka Fetcher Kafka Fetcher pulls messages from Kafka Broker(s) what is the Agent delivered. Check the agent documentation about the details. Typically Tracing Segments, Service/Instance properties, JVM Metrics, and Meter system data are supported. Kafka Fetcher can work with gRPC/HTTP Receivers at the same time for adopting different transport protocols.\nKafka Fetcher is disabled in default, and we configure as following to enable.\nkafka-fetcher: selector: ${SW_KAFKA_FETCHER:default} default: bootstrapServers: ${SW_KAFKA_FETCHER_SERVERS:localhost:9092} skywalking-segments, skywalking-metrics, skywalking-profile, skywalking-managements and skywalking-meters topics are required by kafka-fetcher. If they do not exist, Kafka Fetcher will create them in default. Also, you can create them by yourself before the OAP server started.\nWhen using the OAP server automatical creation mechanism, you could modify the number of partitions and replications of the topics through the following configurations:\nkafka-fetcher: selector: ${SW_KAFKA_FETCHER:default} default: bootstrapServers: ${SW_KAFKA_FETCHER_SERVERS:localhost:9092} partitions: ${SW_KAFKA_FETCHER_PARTITIONS:3} replicationFactor: ${SW_KAFKA_FETCHER_PARTITIONS_FACTOR:2} enableMeterSystem: ${SW_KAFKA_FETCHER_ENABLE_METER_SYSTEM:false} isSharding: ${SW_KAFKA_FETCHER_IS_SHARDING:false} consumePartitions: ${SW_KAFKA_FETCHER_CONSUME_PARTITIONS:\u0026#34;\u0026#34;} In cluster mode, all topics have the same number of partitions. Then we have to set \u0026quot;isSharding\u0026quot; to \u0026quot;true\u0026quot; and assign the partitions to consume for OAP server. The OAP server can use commas to separate multiple partitions.\nKafka Fetcher allows to configure all the Kafka producers listed here in property kafkaConsumerConfig. Such as:\nkafka-fetcher: selector: ${SW_KAFKA_FETCHER:default} default: bootstrapServers: ${SW_KAFKA_FETCHER_SERVERS:localhost:9092} partitions: ${SW_KAFKA_FETCHER_PARTITIONS:3} replicationFactor: ${SW_KAFKA_FETCHER_PARTITIONS_FACTOR:2} enableMeterSystem: ${SW_KAFKA_FETCHER_ENABLE_METER_SYSTEM:false} isSharding: ${SW_KAFKA_FETCHER_IS_SHARDING:true} consumePartitions: ${SW_KAFKA_FETCHER_CONSUME_PARTITIONS:1,3,5} kafkaConsumerConfig: enable.auto.commit: true ... ","excerpt":"Open Fetcher Fetcher is a concept in SkyWalking backend. It uses pulling mode rather than receiver, …","ref":"/docs/main/v8.3.0/en/setup/backend/backend-fetcher/","title":"Open Fetcher"},{"body":"Oracle and Resin plugins These plugins can\u0026rsquo;t be provided in Apache release because of Oracle and Resin Licenses. If you want to know details, please read Apache license legal document\nDue to license incompatibilities/restrictions these plugins are hosted and released in 3rd part repository, go to OpenSkywalking java plugin extension repository to get these.\n","excerpt":"Oracle and Resin plugins These plugins can\u0026rsquo;t be provided in Apache release because of Oracle …","ref":"/docs/main/v8.3.0/en/setup/service-agent/java-agent/agent-optional-plugins/oracle-resin-plugins/","title":"Oracle and Resin plugins"},{"body":"Overview SkyWalking: an open source observability platform used to collect, analyze, aggregate and visualize data from services and cloud native infrastructures. SkyWalking provides an easy way to maintain a clear view of your distributed systems, even across Clouds. It is a modern APM, specially designed for cloud native, container based distributed systems.\nWhy use SkyWalking? SkyWalking provides solutions for observing and monitoring distributed systems, in many different scenarios. First of all, like traditional approaches, SkyWalking provides auto instrument agents for services, such as Java, C#, Node.js, Go, PHP and Nginx LUA. (with calls out for Python and C++ SDK contributions). In multilanguage, continuously deployed environments, cloud native infrastructures grow more powerful but also more complex. SkyWalking\u0026rsquo;s service mesh receiver allows SkyWalking to receive telemetry data from service mesh frameworks such as Istio/Envoy and Linkerd, allowing users to understanding the entire distributed system.\nSkyWalking provides observability capabilities for service(s), service instance(s), endpoint(s). The terms Service, Instance and Endpoint are used everywhere today, so it is worth defining their specific meanings in the context of SkyWalking:\n Service. Represents a set/group of workloads which provide the same behaviours for incoming requests. You can define the service name when you are using instrument agents or SDKs. SkyWalking can also use the name you define in platforms such as Istio. Service Instance. Each individual workload in the Service group is known as an instance. Like pods in Kubernetes, it doesn\u0026rsquo;t need to be a single OS process, however, if you are using instrument agents, an instance is actually a real OS process. Endpoint. A path in a service for incoming requests, such as an HTTP URI path or a gRPC service class + method signature.  SkyWalking allows users to understand the topology relationship between Services and Endpoints, to view the metrics of every Service/Service Instance/Endpoint and to set alarm rules.\nIn addition, you can integrate\n Other distributed tracing using SkyWalking native agents and SDKs with Zipkin, Jaeger and OpenCensus. Other metrics systems, such as Prometheus, Sleuth(Micrometer).  Architecture SkyWalking is logically split into four parts: Probes, Platform backend, Storage and UI.\n Probes collect data and reformat them for SkyWalking requirements (different probes support different sources). Platform backend, supports data aggregation, analysis and drives process flow from probes to the UI. The analysis includes SkyWalking natives traces and metrics, 3rd party, including Istio and Envoy telemetry, Zipkin trace format, etc. You even can customize aggregation and analysis by using Observability Analysis Language for native metrics and Meter System for extension metrics. Storage houses SkyWalking data through an open/plugable interface. You can choose an existing implementation, such as ElasticSearch, H2 or a MySQL cluster managed by Sharding-Sphere, or implement your own. Patches for new storage implementors welcome! UI a highly customizable web based interface allowing SkyWalking end users to visualize and manage SkyWalking data.  What is next?  Learn SkyWalking\u0026rsquo;s Project Goals FAQ, Why doesn\u0026rsquo;t SkyWalking involve MQ in the architecture?  ","excerpt":"Overview SkyWalking: an open source observability platform used to collect, analyze, aggregate and …","ref":"/docs/main/v8.3.0/en/concepts-and-designs/overview/","title":"Overview"},{"body":"Plugin automatic test framework Plugin test framework is designed for verifying the plugins' function and compatible status. As there are dozens of plugins and hundreds of versions need to be verified, it is impossible to do manually. The test framework uses container based tech stack, requires a set of real services with agent installed, then the test mock OAP backend is running to check the segments data sent from agents.\nEvery plugin maintained in the main repo requires corresponding test cases, also matching the versions in the supported list doc.\nEnvironment Requirements  MacOS/Linux JDK 8+ Docker Docker Compose  Case Base Image Introduction The test framework provides JVM-container and Tomcat-container base images including JDK8, JDK14. You could choose the suitable one for your test case, if both are suitable, JVM-container is preferred.\nJVM-container Image Introduction JVM-container uses openjdk:8 as the base image. JVM-container has supported JDK14, which inherits openjdk:14. The test case project is required to be packaged as project-name.zip, including startup.sh and uber jar, by using mvn clean package.\nTake the following test projects as good examples\n sofarpc-scenario as a single project case. webflux-scenario as a case including multiple projects. jdk14-with-gson-scenario as a single project case with JDK14.  Tomcat-container Image Introduction Tomcat-container uses tomcat:8.5.57-jdk8-openjdk or tomcat:8.5.57-jdk14-openjdk as the base image. The test case project is required to be packaged as project-name.war by using mvn package.\nTake the following test project as a good example\n spring-4.3.x-scenario  Test project hierarchical structure The test case is an independent maven project, and it is required to be packaged as a war tar ball or zip file, depends on the chosen base image. Also, two external accessible endpoints, mostly two URLs, are required.\nAll test case codes should be in org.apache.skywalking.apm.testcase.* package, unless there are some codes expected being instrumented, then the classes could be in test.org.apache.skywalking.apm.testcase.* package.\nJVM-container test project hierarchical structure\n[plugin-scenario] |- [bin] |- startup.sh |- [config] |- expectedData.yaml |- [src] |- [main] |- ... |- [resource] |- log4j2.xml |- pom.xml |- configuration.yaml |- support-version.list [] = directory Tomcat-container test project hierarchical structure\n[plugin-scenario] |- [config] |- expectedData.yaml |- [src] |- [main] |- ... |- [resource] |- log4j2.xml |- [webapp] |- [WEB-INF] |- web.xml |- pom.xml |- configuration.yaml |- support-version.list [] = directory Test case configuration files The following files are required in every test case.\n   File Name Descriptions     configuration.yml Declare the basic case inform, including, case name, entrance endpoints, mode, dependencies.   expectedData.yaml Describe the expected segmentItems.   support-version.list List the target versions for this case   startup.sh JVM-container only, don\u0026rsquo;t need this when useTomcat-container    * support-version.list format requires every line for a single version(Contains only the last version number of each minor version). Could use # to comment out this version.\nconfiguration.yml    Field description     type Image type, options, jvm or tomcat. Required.   entryService The entrance endpoint(URL) for test case access. Required. (HTTP Method: GET)   healthCheck The health check endpoint(URL) for test case access. Required. (HTTP Method: HEAD)   startScript Path of start up script. Required in type: jvm only.   framework Case name.   runningMode Running mode whether with the optional plugin, options, default(default), with_optional, with_bootstrap   withPlugins Plugin selector rule. eg:apm-spring-annotation-plugin-*.jar. Required when runningMode=with_optional or runningMode=with_bootstrap.   environment Same as docker-compose#environment.   depends_on Same as docker-compose#depends_on.   dependencies Same as docker-compose#services, image、links、hostname、environment、depends_on are supported.    Notice:, docker-compose active only when dependencies is only blank.\nrunningMode option description.\n   Option description     default Active all plugins in plugin folder like the official distribution agent.   with_optional Active default and plugins in optional-plugin by the give selector.   with_bootstrap Active default and plugins in bootstrap-plugin by the give selector.    with_optional/with_bootstrap supports multiple selectors, separated by ;.\nFile Format\ntype: entryService: healthCheck: startScript: framework: runningMode: withPlugins: environment: ... depends_on: ... dependencies: service1: image: hostname: expose: ... environment: ... depends_on: ... links: ... entrypoint: ... healthcheck: ...  dependencies supports docker compose healthcheck. But the format is a little difference. We need - as the start of every config item, and describe it as a string line.  Such as in official doc, the health check is\nhealthcheck: test: [\u0026#34;CMD\u0026#34;, \u0026#34;curl\u0026#34;, \u0026#34;-f\u0026#34;, \u0026#34;http://localhost\u0026#34;] interval: 1m30s timeout: 10s retries: 3 start_period: 40s The here, you should write as\nhealthcheck: - \u0026#39;test: [\u0026#34;CMD\u0026#34;, \u0026#34;curl\u0026#34;, \u0026#34;-f\u0026#34;, \u0026#34;http://localhost\u0026#34;]\u0026#39; - \u0026#34;interval: 1m30s\u0026#34; - \u0026#34;timeout: 10s\u0026#34; - \u0026#34;retries: 3\u0026#34; - \u0026#34;start_period: 40s\u0026#34; In some cases, the dependency service, mostly 3rd party server like SolrJ server, is required to keep the same version as client lib version, which defined as ${test.framework.version} in pom. Could use ${CASE_SERVER_IMAGE_VERSION} as the version number, it will be changed in the test for every version.\n Don\u0026rsquo;t support resource related configurations, such as volumes, ports and ulimits. Because in test scenarios, don\u0026rsquo;t need mapping any port to the host VM, or mount any folder.\n Take following test cases as examples\n dubbo-2.7.x with JVM-container jetty with JVM-container gateway with runningMode canal with docker-compose  expectedData.yaml Operator for number\n   Operator Description     nq Not equal   eq Equal(default)   ge Greater than or equal   gt Greater than    Operator for String\n   Operator Description     not null Not null   null Null or empty String   eq Equal(default)    Expected Data Format Of The Segment\nsegmentItems: - serviceName: SERVICE_NAME(string) segmentSize: SEGMENT_SIZE(int) segments: - segmentId: SEGMENT_ID(string) spans: ...    Field Description     serviceName Service Name.   segmentSize The number of segments is expected.   segmentId trace ID.   spans segment span list. Follow the next section to see how to describe every span.    Expected Data Format Of The Span\nNotice: The order of span list should follow the order of the span finish time.\noperationName: OPERATION_NAME(string) parentSpanId: PARENT_SPAN_ID(int) spanId: SPAN_ID(int) startTime: START_TIME(int) endTime: END_TIME(int) isError: IS_ERROR(string: true, false) spanLayer: SPAN_LAYER(string: DB, RPC_FRAMEWORK, HTTP, MQ, CACHE) spanType: SPAN_TYPE(string: Exit, Entry, Local) componentId: COMPONENT_ID(int) tags: - {key: TAG_KEY(string), value: TAG_VALUE(string)} ... logs: - {key: LOG_KEY(string), value: LOG_VALUE(string)} ... peer: PEER(string) refs: - { traceId: TRACE_ID(string), parentTraceSegmentId: PARENT_TRACE_SEGMENT_ID(string), parentSpanId: PARENT_SPAN_ID(int), parentService: PARENT_SERVICE(string), parentServiceInstance: PARENT_SERVICE_INSTANCE(string), parentEndpoint: PARENT_ENDPOINT_NAME(string), networkAddress: NETWORK_ADDRESS(string), refType: REF_TYPE(string: CrossProcess, CrossThread) } ...    Field Description     operationName Span Operation Name.   parentSpanId Parent span id. Notice: The parent span id of the first span should be -1.   spanId Span Id. Notice, start from 0.   startTime Span start time. It is impossible to get the accurate time, not 0 should be enough.   endTime Span finish time. It is impossible to get the accurate time, not 0 should be enough.   isError Span status, true or false.   componentId Component id for your plugin.   tags Span tag list. Notice, Keep in the same order as the plugin coded.   logs Span log list. Notice, Keep in the same order as the plugin coded.   SpanLayer Options, DB, RPC_FRAMEWORK, HTTP, MQ, CACHE.   SpanType Span type, options, Exit, Entry or Local.   peer Remote network address, IP + port mostly. For exit span, this should be required.    The verify description for SegmentRef\n   Field Description     traceId    parentTraceSegmentId Parent SegmentId, pointing to the segment id in the parent segment.   parentSpanId Parent SpanID, pointing to the span id in the parent segment.   parentService The service of parent/downstream service name.   parentServiceInstance The instance of parent/downstream service instance name.   parentEndpoint The endpoint of parent/downstream service.   networkAddress The peer value of parent exit span.   refType Ref type, options, CrossProcess or CrossThread.    Expected Data Format Of The Meter Items\nmeterItems: - serviceName: SERVICE_NAME(string) meterSize: METER_SIZE(int) meters: - ...    Field Description     serviceName Service Name.   meterSize The number of meters is expected.   meters meter list. Follow the next section to see how to describe every meter.    Expected Data Format Of The Meter\nmeterId: name: NAME(string) tags: - {name: TAG_NAME(string), value: TAG_VALUE(string)} singleValue: SINGLE_VALUE(double) histogramBuckets: - HISTOGRAM_BUCKET(double) ... The verify description for MeterId\n   Field Description     name meter name.   tags meter tags.   tags.name tag name.   tags.value tag value.   singleValue counter or gauge value. Using condition operate of the number to validate, such as gt, ge. If current meter is histogram, don\u0026rsquo;t need to write this field.   histogramBuckets histogram bucket. The bucket list must be ordered. The tool assert at least one bucket of the histogram having nonzero count. If current meter is counter or gauge, don\u0026rsquo;t need to write this field.    startup.sh This script provide a start point to JVM based service, most of them starts by a java -jar, with some variables. The following system environment variables are available in the shell.\n   Variable Description     agent_opts Agent plugin opts, check the detail in plugin doc or the same opt added in this PR.   SCENARIO_NAME Service name. Default same as the case folder name   SCENARIO_VERSION Version   SCENARIO_ENTRY_SERVICE Entrance URL to access this service   SCENARIO_HEALTH_CHECK_URL Health check URL     ${agent_opts} is required to add into your java -jar command, which including the parameter injected by test framework, and make agent installed. All other parameters should be added after ${agent_opts}.\n The test framework will set the service name as the test case folder name by default, but in some cases, there are more than one test projects are required to run in different service codes, could set it explicitly like the following example.\nExample\nhome=\u0026#34;$(cd \u0026#34;$(dirname $0)\u0026#34;; pwd)\u0026#34; java -jar ${agent_opts} \u0026#34;-Dskywalking.agent.service_name=jettyserver-scenario\u0026#34; ${home}/../libs/jettyserver-scenario.jar \u0026amp; sleep 1 java -jar ${agent_opts} \u0026#34;-Dskywalking.agent.service_name=jettyclient-scenario\u0026#34; ${home}/../libs/jettyclient-scenario.jar \u0026amp;  Only set this or use other skywalking options when it is really necessary.\n Take the following test cases as examples\n undertow webflux  Best Practices How To Use The Archetype To Create A Test Case Project We provided archetypes and a script to make creating a project easier. It creates a completed project of a test case. So that we only need to focus on cases. First, we can use followed command to get usage about the script.\nbash ${SKYWALKING_HOME}/test/plugin/generator.sh\nThen, runs and generates a project, named by scenario_name, in ./scenarios.\nRecommendations for pom \u0026lt;properties\u0026gt; \u0026lt;!-- Provide and use this property in the pom. --\u0026gt; \u0026lt;!-- This version should match the library version, --\u0026gt; \u0026lt;!-- in this case, http components lib version 4.3. --\u0026gt; \u0026lt;test.framework.version\u0026gt;4.3\u0026lt;/test.framework.version\u0026gt; \u0026lt;/properties\u0026gt; \u0026lt;dependencies\u0026gt; \u0026lt;dependency\u0026gt; \u0026lt;groupId\u0026gt;org.apache.httpcomponents\u0026lt;/groupId\u0026gt; \u0026lt;artifactId\u0026gt;httpclient\u0026lt;/artifactId\u0026gt; \u0026lt;version\u0026gt;${test.framework.version}\u0026lt;/version\u0026gt; \u0026lt;/dependency\u0026gt; ... \u0026lt;/dependencies\u0026gt; \u0026lt;build\u0026gt; \u0026lt;!-- Set the package final name as same as the test case folder case. --\u0026gt; \u0026lt;finalName\u0026gt;httpclient-4.3.x-scenario\u0026lt;/finalName\u0026gt; .... \u0026lt;/build\u0026gt; How To Implement Heartbeat Service Heartbeat service is designed for checking the service available status. This service is a simple HTTP service, returning 200 means the target service is ready. Then the traffic generator will access the entry service and verify the expected data. User should consider to use this service to detect such as whether the dependent services are ready, especially when dependent services are database or cluster.\nNotice, because heartbeat service could be traced fully or partially, so, segmentSize in expectedData.yaml should use ge as the operator, and don\u0026rsquo;t include the segments of heartbeat service in the expected segment data.\nThe example Process of Writing Tracing Expected Data Expected data file, expectedData.yaml, include SegmentItems part.\nWe are using the HttpClient plugin to show how to write the expected data.\nThere are two key points of testing\n Whether is HttpClient span created. Whether the ContextCarrier created correctly, and propagates across processes.  +-------------+ +------------------+ +-------------------------+ | Browser | | Case Servlet | | ContextPropagateServlet | | | | | | | +-----|-------+ +---------|--------+ +------------|------------+ | | | | | | | WebHttp +-+ | +------------------------\u0026gt; |-| HttpClient +-+ | |--------------------------------\u0026gt; |-| | |-| |-| | |-| |-| | |-| \u0026lt;--------------------------------| | |-| +-+ | \u0026lt;--------------------------| | | +-+ | | | | | | | | | | | | | + + + segmentItems By following the flow of HttpClient case, there should be two segments created.\n Segment represents the CaseServlet access. Let\u0026rsquo;s name it as SegmentA. Segment represents the ContextPropagateServlet access. Let\u0026rsquo;s name it as SegmentB.  segmentItems: - serviceName: httpclient-case segmentSize: ge 2 # Could have more than one health check segments, because, the dependency is not standby. Because Tomcat plugin is a default plugin of SkyWalking, so, in SegmentA, there are two spans\n Tomcat entry span HttpClient exit span  SegmentA span list should like following\n- segmentId: not null spans: - operationName: /httpclient-case/case/context-propagate parentSpanId: 0 spanId: 1 startTime: nq 0 endTime: nq 0 isError: false spanLayer: Http spanType: Exit componentId: eq 2 tags: - {key: url, value: \u0026#39;http://127.0.0.1:8080/httpclient-case/case/context-propagate\u0026#39;} - {key: http.method, value: GET} logs: [] peer: 127.0.0.1:8080 - operationName: /httpclient-case/case/httpclient parentSpanId: -1 spanId: 0 startTime: nq 0 endTime: nq 0 spanLayer: Http isError: false spanType: Entry componentId: 1 tags: - {key: url, value: \u0026#39;http://localhost:{SERVER_OUTPUT_PORT}/httpclient-case/case/httpclient\u0026#39;} - {key: http.method, value: GET} logs: [] peer: null SegmentB should only have one Tomcat entry span, but includes the Ref pointing to SegmentA.\nSegmentB span list should like following\n- segmentId: not null spans: - operationName: /httpclient-case/case/context-propagate parentSpanId: -1 spanId: 0 tags: - {key: url, value: \u0026#39;http://127.0.0.1:8080/httpclient-case/case/context-propagate\u0026#39;} - {key: http.method, value: GET} logs: [] startTime: nq 0 endTime: nq 0 spanLayer: Http isError: false spanType: Entry componentId: 1 peer: null refs: - {parentEndpoint: /httpclient-case/case/httpclient, networkAddress: \u0026#39;localhost:8080\u0026#39;, refType: CrossProcess, parentSpanId: 1, parentTraceSegmentId: not null, parentServiceInstance: not null, parentService: not null, traceId: not null} The example Process of Writing Meter Expected Data Expected data file, expectedData.yaml, include MeterItems part.\nWe are using the toolkit plugin to demonstrate how to write the expected data. When write the meter plugin, the expected data file keeps the same.\nThere is one key point of testing\n Build a meter and operate it.  Such as Counter:\nMeterFactory.counter(\u0026#34;test_counter\u0026#34;).tag(\u0026#34;ck1\u0026#34;, \u0026#34;cv1\u0026#34;).build().increment(1d); MeterFactory.histogram(\u0026#34;test_histogram\u0026#34;).tag(\u0026#34;hk1\u0026#34;, \u0026#34;hv1\u0026#34;).steps(1d, 5d, 10d).build().addValue(2d); +-------------+ +------------------+ | Plugin | | Agent core | | | | | +-----|-------+ +---------|--------+ | | | | | Build or operate +-+ +------------------------\u0026gt; |-| | |-] | |-| | |-| | |-| | |-| | \u0026lt;--------------------------| | +-+ | | | | | | | | + + meterItems By following the flow of the toolkit case, there should be two meters created.\n Meter test_counter created from MeterFactory#counter. Let\u0026rsquo;s name it as MeterA. Meter test_histogram created from MeterFactory#histogram. Let\u0026rsquo;s name it as MeterB.  meterItems: - serviceName: toolkit-case meterSize: 2 They\u0026rsquo;re showing two kinds of meter, MeterA has a single value, MeterB has a histogram value.\nMeterA should like following, counter and gauge use the same data format.\n- meterId: name: test_counter tags: - {name: ck1, value: cv1} singleValue: gt 0 MeterB should like following.\n- meterId: name: test_histogram tags: - {name: hk1, value: hv1} histogramBuckets: - 0.0 - 1.0 - 5.0 - 10.0 Local Test and Pull Request To The Upstream First of all, the test case project could be compiled successfully, with right project structure and be able to deploy. The developer should test the start script could run in Linux/MacOS, and entryService/health services are able to provide the response.\nYou could run test by using following commands\ncd ${SKYWALKING_HOME} bash ./test/plugin/run.sh -f ${scenario_name} Notice，if codes in ./apm-sniffer have been changed, no matter because your change or git update， please recompile the skywalking-agent. Because the test framework will use the existing skywalking-agent folder, rather than recompiling it every time.\nUse ${SKYWALKING_HOME}/test/plugin/run.sh -h to know more command options.\nIf the local test passed, then you could add it to .github/workflows/plugins-test.\u0026lt;n\u0026gt;.yaml file, which will drive the tests running on the GitHub Actions of official SkyWalking repository. Based on your plugin\u0026rsquo;s name, please add the test case into file .github/workflows/plugins-test.\u0026lt;n\u0026gt;.yaml, by alphabetical orders.\nEvery test case is a GitHub Actions Job. Please use the scenario directory name as the case name, mostly you\u0026rsquo;ll just need to decide which file (plugins-test.\u0026lt;n\u0026gt;.yaml) to add your test case, and simply put one line (as follows) in it, take the existed cases as examples. You can run python3 tools/select-group.py to see which file contains the least cases and add your cases into it, in order to balance the running time of each group.\nIf a test case required to run in JDK 14 environment, please add you test case into file plugins-jdk14-test.\u0026lt;n\u0026gt;.yaml.\njobs: PluginsTest: name: Plugin runs-on: ubuntu-18.04 timeout-minutes: 90 strategy: fail-fast: true matrix: case: # ... - \u0026lt;your scenario test directory name\u0026gt; # ... ","excerpt":"Plugin automatic test framework Plugin test framework is designed for verifying the plugins' …","ref":"/docs/main/v8.3.0/en/guides/plugin-test/","title":"Plugin automatic test framework"},{"body":"Plugin Development Guide This document describe how to understand, develop and contribute plugin.\nThere are 2 kinds of plugin\n Tracing plugin. Follow the distributed tracing concept to collect spans with tags and logs. Meter plugin. Collect numeric metrics in Counter, Guage, and Histogram formats.  We also provide the plugin test tool to verify the data collected and reported by the plugin. If you plan to contribute any plugin to our main repo, the data would be verified by this tool too.\nTracing plugin Concepts Span Span is an important and common concept in distributed tracing system. Learn Span from Google Dapper Paper and OpenTracing\nSkyWalking supports OpenTracing and OpenTracing-Java API from 2017. Our Span concepts are similar with the paper and OpenTracing. Also we extend the Span.\nThere are three types of Span\n1.1 EntrySpan EntrySpan represents a service provider, also the endpoint of server side. As an APM system, we are targeting the application servers. So almost all the services and MQ-consumer are EntrySpan(s).\n1.2 LocalSpan LocalSpan represents a normal Java method, which does not relate to remote service, neither a MQ producer/consumer nor a service(e.g. HTTP service) provider/consumer.\n1.3 ExitSpan ExitSpan represents a client of service or MQ-producer, as named as LeafSpan at early age of SkyWalking. e.g. accessing DB by JDBC, reading Redis/Memcached are cataloged an ExitSpan.\nContextCarrier In order to implement distributed tracing, the trace across process need to be bind, and the context should propagate across the process. That is ContextCarrier\u0026rsquo;s duty.\nHere are the steps about how to use ContextCarrier in a A-\u0026gt;B distributed call.\n Create a new and empty ContextCarrier at client side. Create an ExitSpan by ContextManager#createExitSpan or use ContextManager#inject to init the ContextCarrier. Put all items of ContextCarrier into heads(e.g. HTTP HEAD), attachments(e.g. Dubbo RPC framework) or messages(e.g. Kafka) The ContextCarrier propagates to server side by the service call. At server side, get all items from heads, attachments or messages. Create an EntrySpan by ContextManager#createEntrySpan or use ContextManager#extract to bind the client and server.  Let\u0026rsquo;s demonstrate the steps by Apache HTTPComponent client plugin and Tomcat 7 server plugin\n Client side steps by Apache HTTPComponent client plugin  span = ContextManager.createExitSpan(\u0026#34;/span/operation/name\u0026#34;, contextCarrier, \u0026#34;ip:port\u0026#34;); CarrierItem next = contextCarrier.items(); while (next.hasNext()) { next = next.next(); httpRequest.setHeader(next.getHeadKey(), next.getHeadValue()); } Server side steps by Tomcat 7 server plugin  ContextCarrier contextCarrier = new ContextCarrier(); CarrierItem next = contextCarrier.items(); while (next.hasNext()) { next = next.next(); next.setHeadValue(request.getHeader(next.getHeadKey())); } span = ContextManager.createEntrySpan(“/span/operation/name”, contextCarrier); ContextSnapshot Besides across process, across thread but in a process need to be supported, because async process(In-memory MQ) and batch process are common in Java. Across process and across thread are similar, because they are both about propagating context. The only difference is that, don\u0026rsquo;t need to serialize for across thread.\nHere are the three steps about across thread propagation:\n Use ContextManager#capture to get the ContextSnapshot object. Let the sub-thread access the ContextSnapshot by any way, through method arguments or carried by an existed arguments Use ContextManager#continued in sub-thread.  Core APIs ContextManager ContextManager provides all major and primary APIs.\n Create EntrySpan  public static AbstractSpan createEntrySpan(String endpointName, ContextCarrier carrier) Create EntrySpan by operation name(e.g. service name, uri) and ContextCarrier.\nCreate LocalSpan  public static AbstractSpan createLocalSpan(String endpointName) Create LocalSpan by operation name(e.g. full method signature)\nCreate ExitSpan  public static AbstractSpan createExitSpan(String endpointName, ContextCarrier carrier, String remotePeer) Create ExitSpan by operation name(e.g. service name, uri) and new ContextCarrier and peer address (e.g. ip+port, hostname+port)\nAbstractSpan /** * Set the component id, which defines in {@link ComponentsDefine} * * @param component * @return the span for chaining. */ AbstractSpan setComponent(Component component); AbstractSpan setLayer(SpanLayer layer); /** * Set a key:value tag on the Span. * * @return this Span instance, for chaining */ AbstractSpan tag(String key, String value); /** * Record an exception event of the current walltime timestamp. * * @param t any subclass of {@link Throwable}, which occurs in this span. * @return the Span, for chaining */ AbstractSpan log(Throwable t); AbstractSpan errorOccurred(); /** * Record an event at a specific timestamp. * * @param timestamp The explicit timestamp for the log record. * @param event the events * @return the Span, for chaining */ AbstractSpan log(long timestamp, Map\u0026lt;String, ?\u0026gt; event); /** * Sets the string name for the logical operation this span represents. * * @return this Span instance, for chaining */ AbstractSpan setOperationName(String endpointName); Besides setting operation name, tags and logs, two attributes should be set, which are component and layer, especially for EntrySpan and ExitSpan\nSpanLayer is the catalog of span. Here are 5 values:\n UNKNOWN (default) DB RPC_FRAMEWORK, for a RPC framework, not an ordinary HTTP HTTP MQ  Component IDs are defined and reserved by SkyWalking project. For component name/ID extension, please follow Component library definition and extension document.\nSpecial Span Tags All tags are available in the trace view, meanwhile, in the OAP backend analysis, some special tag or tag combination could provide other advanced features.\nTag key status_code The value should be an integer. The response code of OAL entities is according to this.\nTag key db.statement and db.type. The value of db.statement should be a String, representing the Database statement, such as SQL, or [No statement]/+span#operationName if value is empty. When exit span has this tag, OAP samples the slow statements based on agent-analyzer/default/maxSlowSQLLength. The threshold of slow statement is defined by following agent-analyzer/default/slowDBAccessThreshold\nExtension logic endpoint. Tag key x-le Logic endpoint is a concept, which doesn\u0026rsquo;t represent a real RPC call, but requires the statistic. The value of x-le should be JSON format, with two options.\n Define a separated logic endpoint. Provide its own endpoint name, latency and status. Suitable for entry and local span.  { \u0026#34;name\u0026#34;: \u0026#34;GraphQL-service\u0026#34;, \u0026#34;latency\u0026#34;: 100, \u0026#34;status\u0026#34;: true } Declare the current local span representing a logic endpoint.  { \u0026#34;logic-span\u0026#34;: true } Advanced APIs Async Span APIs There is a set of advanced APIs in Span, which work specific for async scenario. When tags, logs, attributes(including end time) of the span needs to set in another thread, you should use these APIs.\n/** * The span finish at current tracing context, but the current span is still alive, until {@link #asyncFinish} * called. * * This method must be called\u0026lt;br/\u0026gt; * 1. In original thread(tracing context). * 2. Current span is active span. * * During alive, tags, logs and attributes of the span could be changed, in any thread. * * The execution times of {@link #prepareForAsync} and {@link #asyncFinish()} must match. * * @return the current span */ AbstractSpan prepareForAsync(); /** * Notify the span, it could be finished. * * The execution times of {@link #prepareForAsync} and {@link #asyncFinish()} must match. * * @return the current span */ AbstractSpan asyncFinish();  Call #prepareForAsync in original context. Do ContextManager#stopSpan in original context when your job in current thread is done. Propagate the span to any other thread. After all set, call #asyncFinish in any thread. Tracing context will be finished and report to backend when all spans\u0026rsquo;s #prepareForAsync finished(Judged by count of API execution).  Develop a plugin Abstract The basic method to trace is intercepting a Java method, by using byte code manipulation tech and AOP concept. SkyWalking boxed the byte code manipulation tech and tracing context propagation, so you just need to define the intercept point(a.k.a. aspect pointcut in Spring)\nIntercept SkyWalking provide two common defines to intercept Contructor, instance method and class method.\n Extend ClassInstanceMethodsEnhancePluginDefine defines Contructor intercept points and instance method intercept points. Extend ClassStaticMethodsEnhancePluginDefine defines class method intercept points.  Of course, you can extend ClassEnhancePluginDefine to set all intercept points. But it is unusual.\nImplement plugin I will demonstrate about how to implement a plugin by extending ClassInstanceMethodsEnhancePluginDefine\n Define the target class name  protected abstract ClassMatch enhanceClass(); ClassMatch represents how to match the target classes, there are 4 ways:\n byName, through the full class name(package name + . + class name) byClassAnnotationMatch, through the class existed certain annotations. byMethodAnnotationMatch, through the class\u0026rsquo;s method existed certain annotations. byHierarchyMatch, through the class\u0026rsquo;s parent classes or interfaces  Attentions:\n Never use ThirdPartyClass.class in the instrumentation definitions, such as takesArguments(ThirdPartyClass.class), or byName(ThirdPartyClass.class.getName()), because of the fact that ThirdPartyClass dose not necessarily exist in the target application and this will break the agent; we have import checks to help on checking this in CI, but it doesn\u0026rsquo;t cover all scenarios of this limitation, so never try to work around this limitation by something like using full-qualified-class-name (FQCN), i.e. takesArguments(full.qualified.ThirdPartyClass.class) and byName(full.qualified.ThirdPartyClass.class.getName()) will pass the CI check, but are still invalid in the agent codes, Use Full Qualified Class Name String Literature Instead. Even you are perfectly sure that the class to be intercepted exists in the target application (such as JDK classes), still, don\u0026rsquo;t use *.class.getName() to get the class String name. Recommend you to use literal String. This is for avoiding ClassLoader issues. by*AnnotationMatch doesn\u0026rsquo;t support the inherited annotations. Don\u0026rsquo;t recommend to use byHierarchyMatch, unless it is really necessary. Because using it may trigger intercepting many unexcepted methods, which causes performance issues and concerns.  Example：\n@Override protected ClassMatch enhanceClassName() { return byName(\u0026#34;org.apache.catalina.core.StandardEngineValve\u0026#34;);\t}\tDefine an instance method intercept point  public InstanceMethodsInterceptPoint[] getInstanceMethodsInterceptPoints(); public interface InstanceMethodsInterceptPoint { /** * class instance methods matcher. * * @return methods matcher */ ElementMatcher\u0026lt;MethodDescription\u0026gt; getMethodsMatcher(); /** * @return represents a class name, the class instance must instanceof InstanceMethodsAroundInterceptor. */ String getMethodsInterceptor(); boolean isOverrideArgs(); } Also use Matcher to set the target methods. Return true in isOverrideArgs, if you want to change the argument ref in interceptor.\nThe following sections will tell you how to implement the interceptor.\nAdd plugin define into skywalking-plugin.def file  tomcat-7.x/8.x=TomcatInstrumentation Implement an interceptor As an interceptor for an instance method, the interceptor implements org.apache.skywalking.apm.agent.core.plugin.interceptor.enhance.InstanceMethodsAroundInterceptor\n/** * A interceptor, which intercept method\u0026#39;s invocation. The target methods will be defined in {@link * ClassEnhancePluginDefine}\u0026#39;s subclass, most likely in {@link ClassInstanceMethodsEnhancePluginDefine} */ public interface InstanceMethodsAroundInterceptor { /** * called before target method invocation. * * @param result change this result, if you want to truncate the method. * @throws Throwable */ void beforeMethod(EnhancedInstance objInst, Method method, Object[] allArguments, Class\u0026lt;?\u0026gt;[] argumentsTypes, MethodInterceptResult result) throws Throwable; /** * called after target method invocation. Even method\u0026#39;s invocation triggers an exception. * * @param ret the method\u0026#39;s original return value. * @return the method\u0026#39;s actual return value. * @throws Throwable */ Object afterMethod(EnhancedInstance objInst, Method method, Object[] allArguments, Class\u0026lt;?\u0026gt;[] argumentsTypes, Object ret) throws Throwable; /** * called when occur exception. * * @param t the exception occur. */ void handleMethodException(EnhancedInstance objInst, Method method, Object[] allArguments, Class\u0026lt;?\u0026gt;[] argumentsTypes, Throwable t); } Use the core APIs in before, after and exception handle stages.\nDo bootstrap class instrumentation. SkyWalking has packaged the bootstrap instrumentation in the agent core. It is easy to open by declaring it in the Instrumentation definition.\nOverride the public boolean isBootstrapInstrumentation() and return true. Such as\npublic class URLInstrumentation extends ClassEnhancePluginDefine { private static String CLASS_NAME = \u0026#34;java.net.URL\u0026#34;; @Override protected ClassMatch enhanceClass() { return byName(CLASS_NAME); } @Override public ConstructorInterceptPoint[] getConstructorsInterceptPoints() { return new ConstructorInterceptPoint[] { new ConstructorInterceptPoint() { @Override public ElementMatcher\u0026lt;MethodDescription\u0026gt; getConstructorMatcher() { return any(); } @Override public String getConstructorInterceptor() { return \u0026#34;org.apache.skywalking.apm.plugin.jre.httpurlconnection.Interceptor2\u0026#34;; } } }; } @Override public InstanceMethodsInterceptPoint[] getInstanceMethodsInterceptPoints() { return new InstanceMethodsInterceptPoint[0]; } @Override public StaticMethodsInterceptPoint[] getStaticMethodsInterceptPoints() { return new StaticMethodsInterceptPoint[0]; } @Override public boolean isBootstrapInstrumentation() { return true; } } NOTICE, doing bootstrap instrumentation should only happen in necessary, but mostly it effect the JRE core(rt.jar), and could make very unexpected result or side effect.\nProvide Customization Config for the Plugin The config could provide different behaviours based on the configurations. SkyWalking plugin mechanism provides the configuration injection and initialization system in the agent core.\nEvery plugin could declare one or more classes to represent the config by using @PluginConfig annotation. The agent core could initialize this class' static field though System environments, System properties, and agent.config static file.\nThe #root() method in the @PluginConfig annotation requires to declare the root class for the initialization process. Typically, SkyWalking prefers to use nested inner static classes for the hierarchy of the configuration. Recommend using Plugin/plugin-name/config-key as the nested classes structure of the Config class.\nNOTE, because of the Java ClassLoader mechanism, the @PluginConfig annotation should be added on the real class used in the interceptor codes.\nSuch as, in the following example, @PluginConfig(root = SpringMVCPluginConfig.class) represents the initialization should start with using SpringMVCPluginConfig as the root. Then the config key of the attribute USE_QUALIFIED_NAME_AS_ENDPOINT_NAME, should be plugin.springmvc.use_qualified_name_as_endpoint_name.\npublic class SpringMVCPluginConfig { public static class Plugin { // NOTE, if move this annotation on the `Plugin` or `SpringMVCPluginConfig` class, it no longer has any effect.  @PluginConfig(root = SpringMVCPluginConfig.class) public static class SpringMVC { /** * If true, the fully qualified method name will be used as the endpoint name instead of the request URL, * default is false. */ public static boolean USE_QUALIFIED_NAME_AS_ENDPOINT_NAME = false; /** * This config item controls that whether the SpringMVC plugin should collect the parameters of the * request. */ public static boolean COLLECT_HTTP_PARAMS = false; } @PluginConfig(root = SpringMVCPluginConfig.class) public static class Http { /** * When either {@link Plugin.SpringMVC#COLLECT_HTTP_PARAMS} is enabled, how many characters to keep and send * to the OAP backend, use negative values to keep and send the complete parameters, NB. this config item is * added for the sake of performance */ public static int HTTP_PARAMS_LENGTH_THRESHOLD = 1024; } } } Meter Plugin Java agent plugin could use meter APIs to collect the metrics for backend analysis.\n Counter API represents a single monotonically increasing counter, automatic collect data and report to backend.  import org.apache.skywalking.apm.agent.core.meter.MeterFactory; Counter counter = MeterFactory.counter(meterName).tag(\u0026#34;tagKey\u0026#34;, \u0026#34;tagValue\u0026#34;).mode(Counter.Mode.INCREMENT).build(); counter.increment(1d);  MeterFactory.counter Create a new counter builder with the meter name. Counter.Builder.tag(String key, String value) Mark a tag key/value pair. Counter.Builder.mode(Counter.Mode mode) Change the counter mode, RATE mode means reporting rate to the backend. Counter.Builder.build() Build a new Counter which is collected and reported to the backend. Counter.increment(double count) Increment count to the Counter, It could be a positive value.   Gauge API represents a single numerical value.  import org.apache.skywalking.apm.agent.core.meter.MeterFactory; ThreadPoolExecutor threadPool = ...; Gauge gauge = MeterFactory.gauge(meterName, () -\u0026gt; threadPool.getActiveCount()).tag(\u0026#34;tagKey\u0026#34;, \u0026#34;tagValue\u0026#34;).build();  MeterFactory.gauge(String name, Supplier\u0026lt;Double\u0026gt; getter) Create a new gauge builder with the meter name and supplier function, this function need to return a double value. Gauge.Builder.tag(String key, String value) Mark a tag key/value pair. Gauge.Builder.build() Build a new Gauge which is collected and reported to the backend.   Histogram API represents a summary sample observations with customize buckets.  import org.apache.skywalking.apm.agent.core.meter.MeterFactory; Histogram histogram = MeterFactory.histogram(\u0026#34;test\u0026#34;).tag(\u0026#34;tagKey\u0026#34;, \u0026#34;tagValue\u0026#34;).steps(Arrays.asList(1, 5, 10)).minValue(0).build(); histogram.addValue(3);  MeterFactory.histogram(String name) Create a new histogram builder with the meter name. Histogram.Builder.tag(String key, String value) Mark a tag key/value pair. Histogram.Builder.steps(List\u0026lt;Double\u0026gt; steps) Set up the max values of every histogram buckets. Histogram.Builder.minValue(double value) Set up the minimal value of this histogram, default is 0. Histogram.Builder.build() Build a new Histogram which is collected and reported to the backend. Histogram.addValue(double value) Add value into the histogram, automatically analyze what bucket count needs to be increment. rule: count into [step1, step2).  Plugin Test Tool Apache SkyWalking Agent Test Tool Suite a tremendously useful test tools suite in a wide variety of languages of Agent. Includes mock collector and validator. The mock collector is a SkyWalking receiver, like OAP server.\nYou could learn how to use this tool to test the plugin in this doc. If you want to contribute plugins to SkyWalking official repo, this is required.\nContribute plugins into Apache SkyWalking repository We are welcome everyone to contribute plugins.\nPlease follow there steps:\n Submit an issue about which plugins you are going to contribute, including supported version. Create sub modules under apm-sniffer/apm-sdk-plugin or apm-sniffer/optional-plugins, and the name should include supported library name and versions Follow this guide to develop. Make sure comments and test cases are provided. Develop and test. Provide the automatic test cases. Learn how to write the plugin test case from this doc Send the pull request and ask for review. The plugin committers approve your plugins, plugin CI-with-IT, e2e and plugin tests passed. The plugin accepted by SkyWalking.  ","excerpt":"Plugin Development Guide This document describe how to understand, develop and contribute plugin. …","ref":"/docs/main/v8.3.0/en/guides/java-plugin-development-guide/","title":"Plugin Development Guide"},{"body":"Powered by Apache SkyWalking This page documents an alphabetical list of institutions that are using Apache SkyWalking for research and production, or providing commercial products including Apache SkyWalking.\n 100tal.cn 北京世纪好未来教育科技有限公司 http://www.100tal.com/ 17173.com https://www.17173.com/ 300.cn 中企动力科技股份有限公司 http://www.300.cn/ 360jinrong.net 360金融 https://www.360jinrong.net/ 4399.com 四三九九网络股份有限公司. http://www.4399.com/ 51mydao.com 买道传感科技（上海）有限公司 https://www.51mydao.com/ 58 Daojia Inc. 58到家 https://www.daojia.com 5i5j. 上海我爱我家房地产经纪有限公司 https://sh.5i5j.com/about/ Anheuser-Busch InBev 百威英博 Agricultural Bank of China 中国农业银行 Aihuishou.com 爱回收网 http://www.aihuishou.com/ Alibaba Cloud, 阿里云, http://aliyun.com Anxin Insurance. 安心财产保险有限责任公司 https://www.95303.com APM Star 北京天空漫步科技有限公司 http://www.apmstar.com AsiaInfo Inc. http://www.asiainfo.com.cn/ Autohome. 汽车之家. http://www.autohome.com.cn baidu 百度 https://www.baidu.com/ Baixing.com 百姓网 http://www.baixing.com/ bitauto 易车 http://bitauto.com hellobanma 斑马网络 https://www.hellobanma.com/ bestsign. 上上签. https://www.bestsign.cn/page/ Beike Finance 贝壳金服 https://www.bkjk.com/ Bizsaas.cn 北京商云科技发展有限公司. http://www.bizsaas.cn/ BoCloud 苏州博纳讯动软件有限公司. http://www.bocloud.com.cn/ Cdlhyj.com 六合远教（成都）科技有限公司 http://www.cdlhyj.com Chehejia Automotive. 北京车和家信息技术有限责任公司. https://www.chehejia.com/ China Eastern Airlines 中国东方航空 http://www.ceair.com/ China Express Airlines 华夏航空 http://www.chinaexpressair.com/ Chinadaas. 北京中数智汇科技股份有限公司. https://www.chinadaas.com/ Chinasoft International 中软国际 China Merchants Bank. 中国招商银行. http://www.cmbchina.com/ China National Software 中软 China Mobile 中国移动 China Unicom 中国联通 China Tower 中国铁塔 China Telecom 中国电信 Chinese Academy of Sciences Chtwm.com. 恒天财富投资管理股份有限公司. https://www.chtwm.com/ Cmft.com. 招商局金融科技. https://www.cmft.com/ CXIST.com 上海程析智能科技有限公司 https://www.cxist.com/ Dangdang.com. 当当网. http://www.dangdang.com/ DaoCloud. https://www.daocloud.io/ deepblueai.com 深兰科技上海有限公司 https://www.deepblueai.com/ Deppon Logistics Co Ltd 德邦物流 https://www.deppon.com/ Deyoushenghuo in WeChat app. 河南有态度信息科技有限公司，微信小程序：得有生活 Dianfubao.com 垫富宝 https://www.dianfubao.com/ DiDi 滴滴出行 dxy.cn 丁香园 http://www.dxy.cn/ Byte Dance 字节跳动 https://bytedance.com Echplus.com 北京易诚互动网络技术有限公司 http://www.echplus.com/ Enmonster 怪兽充电 http://www.enmonster.com/ Eqxiu.com. 北京中网易企秀科技有限公司 http://www.eqxiu.com/ essence.com.cn 安信证券股份有限公司 http://www.essence.com.cn/ fangdd.com 房多多 https://www.fangdd.com fullgoal.com.cn 富国基金管理有限公司 https://www.fullgoal.com.cn/ GTrace System. (No company provided) GSX Techedu Inc. 跟谁学 https://www.genshuixue.com Gdeng.cn 深圳谷登科技有限公司 http://www.gdeng.cn/ GOME 国美 https://www.gome.com.cn/ Guazi.com 瓜子二手车直卖网. https://www.guazi.com/ guohuaitech.com 北京国槐信息科技有限公司. http://www.guohuaitech.com/ GrowingIO 北京易数科技有限公司 https://www.growingio.com/ Haier. 海尔消费金融 https://www.haiercash.com/ Haoyunhu. 上海好运虎供应链管理有限公司 http://www.haoyunhu56.com/ helijia.com 河狸家 http://www.helijia.com/ Huawei Hundun YUNRONG Fintech. 杭州恒生云融网络科技有限公司 https://www.hsjry.com/ hunliji.com 婚礼纪 https://www.hunliji.com/ hydee.cn 海典软件 http://www.hydee.cn/ iBoxChain 盒子科技 https://www.iboxpay.com/ iFLYTEK. 科大讯飞股份有限公司-消费者BG http://www.iflytek.com/ Inspur 浪潮集团 iQIYI.COM. 爱奇艺 https://www.iqiyi.com/ juhaokan 聚好看科技股份有限公司 https://www.juhaokan.org/ Ke.com. 贝壳找房. https://www.ke.com Keking.cn 凯京集团. http://www.keking.cn KubeSphere https://kubesphere.io JoinTown. 九州通医药集团 http://www.jztey.com/ Lagou.com. 拉勾. https://www.lagou.com/ laocaibao. 上海证大爱特金融信息服务有限公司 https://www.laocaibao.com/ Lenovo 联想 liaofan168.com 了凡科技 http://www.liaofan168.com lianzhongyouche.com.cn 联众优车 https://www.lianzhongyouche.com.cn/ Lima 北京力码科技有限公司 https://www.zhongbaounion.com/ Lifesense.com 广东乐心医疗电子股份有限公司 http://www.lifesense.com/ lizhi.fm 荔枝FM https://www.lizhi.fm/ Lixiang.com 理想汽车 https://www.lixiang.com/ Madecare. 北京美德远健科技有限公司. http://www.madecare.com/ Maodou.com 毛豆新车网. https://www.maodou.com/ Mobanker.com 上海前隆信息科技有限公司 http://www.mobanker.com/ Mxnavi. 沈阳美行科技有限公司 http://www.mxnavi.com/ Moji 墨叽（深圳）科技有限公司 https://www.mojivip.com Minsheng FinTech / China Minsheng Bank 民生科技有限责任公司 http://www.mskj.com/ Migu Digital Media Co.Ltd. 咪咕数字传媒有限公司 http://www.migu.cn/ Mypharma.com 北京融贯电子商务有限公司 https://www.mypharma.com NetEase 网易 https://www.163.com/ Osacart in WeChat app 广州美克曼尼电子商务有限公司 Oriente. https://oriente.com/ Peking University 北京大学 Ping An Technology / Ping An Insurance 平安科技 Primeton.com 普元信息技术股份有限公司 http://www.primeton.com qiniu.com 七牛云 http://qiniu.com Qingyidai.com 轻易贷 https://www.qingyidai.com/ Qsdjf.com 浙江钱宝网络科技有限公司 https://www.qsdjf.com/index.html Qk365.com 上海青客电子商务有限公司 https://www.qk365.com Qudian 趣店 http://ir.qudian.com/ Renren Network 人人网 Rong Data. 荣数数据 http://www.rong-data.com/ Rongjinbao. 深圳融金宝互联网金融服务有限公司. http://www.rjb777.com Safedog. 安全狗. http://www.safedog.cn/ servingcloud.com 盈佳云创科技(深圳)有限公司 http://www.servingcloud.com/ SF Express 顺丰速运 https://www.sf-express.com/ Shouqi Limousine \u0026amp; chauffeur Group 首约科技(北京)有限公司. https://www.01zhuanche.com/ shuaibaoshop.com 宁波鲸灵网络科技有限公司 http://www.shuaibaoshop.com/ shuyun.com 杭州数云信息技术有限公司 http://www.shuyun.com/ Sijibao.com 司机宝 https://www.sijibao.com/ Sina 新浪 Sinolink Securities Co.,Ltd. 国金证券佣金宝 http://www.yongjinbao.com.cn/ Source++ https://sourceplusplus.com SPD Bank 浦发银行 StartDT 奇点云 https://www.startdt.com/ State Grid Corporation of China 国家电网有限公司 Successchannel 苏州渠成易销网络科技有限公司. http://www.successchannel.com SuperMap 北京超图软件 syswin.com 北京思源集团 http://www.syswin.com/ szhittech.com 深圳和而泰智能控制股份有限公司. http://www.szhittech.com/ Tencent Tetrate.io https://www.tetrate.io/ Thomas Cook 托迈酷客 https://www.thomascook.com.cn Three Squirrels 三只松鼠 Today36524.com Today便利店 Tongcheng. 同城金服. https://jr.ly.com/ Tools information technology co. LTD 杭州图尔兹信息技术有限公司 http://bintools.cn/ TravelSky 中国航信 http://www.travelsky.net/ Tsfinance.com 重庆宜迅联供应链科技有限公司 https://www.tsfinance.com.cn/ tuhu.cn 途虎养车 https://www.tuhu.cn Tuya. 涂鸦智能. https://www.tuya.com Tydic 天源迪科 https://www.tydic.com/ VBill Payment Co., LTD. 随行付. https://www.vbill.cn/ Wahaha Group 娃哈哈 http://www.wahaha.com.cn/ WeBank. 微众银行 http://www.webank.com Weier. 广州文尔软件科技有限公司. https://www.site0.cn Wochu. 我厨买菜. https://www.wochu.cn Xiaomi. 小米. https://www.mi.com/en/ xin.com 优信集团 http://www.xin.com Xinyebang.com 重庆欣业邦网络技术有限公司 http://www.xinyebang.com xueqiu.com 雪球财经. https://xueqiu.com/ yibainetwork.com 深圳易佰网络有限公司 http://www.yibainetwork.com/ Yiguo. 易果生鲜. http://www.yiguo.com/ Yinji(shenzhen)Network Technology Co.,Ltd. 印记. http://www.yinjiyun.cn/ Yonghui Superstores Co., Ltd. 永辉超市 http://www.yonghui.com.cn Yonyou 用友 Youzan.com 杭州有赞科技有限公司 http://www.youzan.com/ Yunda Express 韵达快运 http://www.yunda56.com/ Yunnan Airport Group Co.,Ltd. 云南机场集团 yxt 云学堂 http://www.yxt.com/ zbj.com 猪八戒 https://www.zbj.com/ zhaopin.com 智联招聘 https://www.zhaopin.com/ zjs.com.cn 北京宅急送快运股份有限公司 http://www.zjs.com.cn/  Use Cases Alibaba and Alibaba Cloud Alibaba products including Cloud DevOps product are under SkyWalking monitoring.\nAlibaba Cloud supports SkyWalking agents and formats in Tracing Analysis cloud service.\nChina Eastern Airlines Integrated in the microservices architecture support platform.\nChina Merchants Bank Use SkyWalking and SkyAPM .net agent in the CMBChina Mall project.\nChina Mobile China Mobile Suzhou Research Center, CMSS, integrated SkyWalking as the APM component in China Mobile PAAS.\nke.com Deploy SkyWalking in production environments.\n Three CentOs Machines(32 CPUs, 64G RAM, 1.3T Disk) for Collector Server Three ElasticSearch(Version 6.4.2, 40 CPUs, 96G RAM, 7T Disk) Nodes for Storage  Support 60+ Instances, Over 300k Calls Per Minute, Over 50k Spans Per Second\nguazi.com Guazi.com uses SkyWalking monitoring 270+ services, including topology + metrics analysis, and collecting 1.1+ billion traces per day with 100% sampling.\nPlan is 1k+ services and 5 billion traces per day.\nOscart Use multiple language agents from SkyWalking and its ecosystem, including SkyWalking Javaagent and SkyAPM nodejs agent. SkyWalking OAP platform acts as backend and visualization.\nPrimeton Integrated in Primeton EOS PLATFORM 8, which is a commercial micro-service platform.\nQiniu Cloud Provide a customized version SkyWalking agent. It could provide distributed tracing and integrated in its intelligence log management platform.\nSource++ An open-source observant programming assistant which aims to bridge APM tools with the developer\u0026rsquo;s IDE to enable tighter feedback loops. Source++ uses SkyWalking as the defacto APM for JVM-based applications.\nTetrate Tetrate provides enterprise level service mesh. SkyWalking acts as the core observability platform for hybrid enterprise service mesh environment.\nlagou.com Lagou.com use Skywalking for JVM-based applications, deployed in production. Custom and optimize muiti collector functions, such as alarm, sql metric, circle operation metric, thread monitor, detail mode. Support 200+ Instances, over 4500k Segments Per Minute.\nYonghui Superstores Yonghui Superstores Co., Ltd. use SkyWalking as primary APM system, to monitor 1k+ instances clusters, which supports 150k+ tps/qps payload. SkyWalking collect, analysis and save 10 billions trace segments(cost 3T disk) each day in 100% sampling strategy. SkyWalking backend cluster is built with 15 nodes OAP and 20 nodes ElasticSearch.\n","excerpt":"Powered by Apache SkyWalking This page documents an alphabetical list of institutions that are using …","ref":"/docs/main/v8.3.0/powered-by/","title":"Powered by Apache SkyWalking"},{"body":"Probe Introduction In SkyWalking, probe means an agent or SDK library integrated into target system, which take charge of collecting telemetry data including tracing and metrics. Based on the target system tech stack, probe could use very different ways to do so. But ultimately they are same, just collect and reformat data, then send to backend.\nIn high level, there are three typical groups in all SkyWalking probes.\n  Language based native agent. This kind of agents runs in target service user space, like a part of user codes. Such as SkyWalking Java agent, use -javaagent command line argument to manipulate codes in runtime, manipulate means change and inject user\u0026rsquo;s codes. Another kind of agents is using some hook or intercept mechanism provided by target libraries. So you can see, these kinds of agents based on languages and libraries.\n  Service Mesh probe. Service Mesh probe collects data from sidecar, control panel in service mesh or proxy. In old days, proxy is only used as ingress of the whole cluster, but with the Service Mesh and sidecar, now we can do observe based on that.\n  3rd-party instrument library. SkyWalking accepts other popular used instrument libraries data format. It analysis the data, transfer it to SkyWalking formats of trace, metrics or both. This feature starts with accepting Zipkin span data. See Receiver for other tracers to know more.\n  You don\u0026rsquo;t need to use Language based native agent and Service Mesh probe at the same time, because they both collect metrics data. As a result of that, your system suffers twice payloads, and the analytic numbers are doubled.\nThere are several recommend ways in using these probes:\n Use Language based native agent only. Use 3rd-party instrument library only, like Zipkin instrument ecosystem. Use Service Mesh probe only. Use Service Mesh probe with Language based native agent or 3rd-party instrument library in tracing status. (Advanced usage)  In addition, let\u0026rsquo;s example what is the meaning of in tracing status?\nIn default, Language based native agent and 3rd-party instrument library both send distributed traces to backend, which do analysis and aggregate on those traces. In tracing status means, backend considers these traces as something like logs, just save them, and build the links between traces and metrics, like which endpoint and service does the trace belong?.\nWhat is next?  Learn the SkyWalking supported probes in Service auto instrument agent, Manual instrument SDK, Service Mesh probe and Zipkin receiver. After understand the probe, read backend overview for understanding analysis and persistence.  ","excerpt":"Probe Introduction In SkyWalking, probe means an agent or SDK library integrated into target system, …","ref":"/docs/main/v8.3.0/en/concepts-and-designs/probe-introduction/","title":"Probe Introduction"},{"body":"Problem when you start your application with skywalking agent,if you find this exception in your agent log which mean EnhanceRequireObjectCache can not be casted to EnhanceRequireObjectCache.eg:\nERROR 2018-05-07 21:31:24 InstMethodsInter : class[class org.springframework.web.method.HandlerMethod] after method[getBean] intercept failure java.lang.ClassCastException: org.apache.skywalking.apm.plugin.spring.mvc.commons.EnhanceRequireObjectCache cannot be cast to org.apache.skywalking.apm.plugin.spring.mvc.commons.EnhanceRequireObjectCache at org.apache.skywalking.apm.plugin.spring.mvc.commons.interceptor.GetBeanInterceptor.afterMethod(GetBeanInterceptor.java:45) at org.apache.skywalking.apm.agent.core.plugin.interceptor.enhance.InstMethodsInter.intercept(InstMethodsInter.java:105) at org.springframework.web.method.HandlerMethod.getBean(HandlerMethod.java) at org.springframework.web.servlet.handler.AbstractHandlerMethodExceptionResolver.shouldApplyTo(AbstractHandlerMethodExceptionResolver.java:47) at org.springframework.web.servlet.handler.AbstractHandlerExceptionResolver.resolveException(AbstractHandlerExceptionResolver.java:131) at org.springframework.web.servlet.handler.HandlerExceptionResolverComposite.resolveException(HandlerExceptionResolverComposite.java:76) ... Reason this exception may caused by some hot deployment tools(spring-boot-devtool) or some else which may change the classloader in runtime.\nResolve  Production environment does not appear this error because developer tools are automatically disabled,look spring-boot-devtools If you want to debug in your development environment normally,you should remove such hot deployment package in your lib path temporarily.  ","excerpt":"Problem when you start your application with skywalking agent,if you find this exception in your …","ref":"/docs/main/v8.3.0/en/faq/enhancerequireobjectcache-cast-exception/","title":"Problem"},{"body":"Problem  Import skywalking project to Eclipse,Occur following errors:   Software being installed: Checkstyle configuration plugin for M2Eclipse 1.0.0.201705301746 (com.basistech.m2e.code.quality.checkstyle.feature.feature.group 1.0.0.201705301746) Missing requirement: Checkstyle configuration plugin for M2Eclipse 1.0.0.201705301746 (com.basistech.m2e.code.quality.checkstyle.feature.feature.group 1.0.0.201705301746) requires \u0026lsquo;net.sf.eclipsecs.core 5.2.0\u0026rsquo; but it could not be found\n Reason Haven\u0026rsquo;t installed Eclipse Checkstyle Plug-in\nResolve Download the plugin through the link:https://sourceforge.net/projects/eclipse-cs/?source=typ_redirect，Eclipse Checkstyle Plug-in version:8.7.0.201801131309 plugin required. plugin notification: The Eclipse Checkstyle plug-in integrates the Checkstyle Java code auditor into the Eclipse IDE. The plug-in provides real-time feedback to the user about violations of rules that check for coding style and possible error prone code constructs.\n","excerpt":"Problem  Import skywalking project to Eclipse,Occur following errors:   Software being installed: …","ref":"/docs/main/v8.3.0/en/faq/import-project-eclipse-requireitems-exception/","title":"Problem"},{"body":"Problem The trace doesn\u0026rsquo;t continue in kafka consumer side.\nReason The kafka client is responsible for pulling messages from the brokers, and after that the data will be processed by the user-defined codes. However, only the poll action can be traced by the pluign and the subsequent data processing work is inevitably outside the scope of the trace context. Thus, in order to complete the client-side trace, manual instrument has to be done, i.e. the poll action and the processing action should be wrapped manually.\nResolve With native kafka client, please use Application Toolkit libraries to do the manual instrumentation, with the help of @KafkaPollAndInvoke annotation in apm-toolkit-kafka or with OpenTracing API. And if you\u0026rsquo;re using spring-kafka 1.3.x, 2.2.x or above, you can track the Consumer side without effort.\n","excerpt":"Problem The trace doesn\u0026rsquo;t continue in kafka consumer side.\nReason The kafka client is …","ref":"/docs/main/v8.3.0/en/faq/kafka-plugin/","title":"Problem"},{"body":"Problem When using a thread pool, TraceSegment data in a thread cannot be reported and there are memory data that cannot be recycled (memory leaks)\nExample ThreadPoolTaskExecutor executor = new ThreadPoolTaskExecutor(); executor.setThreadFactory(r -\u0026gt; new Thread(RunnableWrapper.of(r))); Reason  Worker threads are enhanced, when using thread pool. According to the SkyWalking Java Agent design, when you want to trace cross thread, you need to enhance the task thread.  Resolve   When using Thread Schedule Framework Checked SkyWalking Thread Schedule Framework at SkyWalking Java agent supported list, such as Spring FrameWork @Async, which can implement tracing without any modification.\n  When using Custom Thread Pool Enhance the task thread with the following usage.\n  ExecutorService executorService = Executors.newFixedThreadPool(1); executorService.execute(RunnableWrapper.of(new Runnable() { @Override public void run() { //your code  } })); See across thread solution APIs for more usage\n","excerpt":"Problem When using a thread pool, TraceSegment data in a thread cannot be reported and there are …","ref":"/docs/main/v8.3.0/en/faq/memory-leak-enhance-worker-thread/","title":"Problem"},{"body":"Problem  In maven build, the protoc-plugin occurs error:  [ERROR] Failed to execute goal org.xolstice.maven.plugins:protobuf-maven-plugin:0.5.0:compile-custom (default) on project apm-network: Unable to copy the file to \\skywalking\\apm-network\\target\\protoc-plugins: \\skywalking\\apm-network\\target\\protoc-plugins\\protoc-3.3.0-linux-x86_64.exe (The process cannot access the file because it is being used by another process) -\u0026gt; [Help 1] Reason  Protobuf compiler is dependent on the glibc, but it is not-installed or installed old version in the system.  Resolve  Install or upgrade to the latest version of the glibc library. In container env, recommend using the latest glibc version of the alpine system. Please refer to http://www.gnu.org/software/libc/documentation.html  ","excerpt":"Problem  In maven build, the protoc-plugin occurs error:  [ERROR] Failed to execute goal …","ref":"/docs/main/v8.3.0/en/faq/protoc-plugin-fails-when-build/","title":"Problem"},{"body":"Problem The message with Field ID, 8888, must be revered.\nReason Because Thrift cannot carry metadata to transport Trace Header in the original API, we transport those by wrapping TProtocolFactory to do that.\nThrift allows us to append any additional field in the Message even if the receiver doesn\u0026rsquo;t deal with them. This data is going to be skipped while no one reads. Base on this, we take the 8888th field of Message to store Trace Header(or metadata) and to transport. That means the message with Field ID, 8888, must be revered.\nResolve Avoiding to use the Field(ID is 8888) in your application.\n","excerpt":"Problem The message with Field ID, 8888, must be revered.\nReason Because Thrift cannot carry …","ref":"/docs/main/v8.3.0/en/faq/thrift-plugin/","title":"Problem"},{"body":"Problem  There is no abnormal log in Agent log and Collector log, The traces show, but no other info in UI.  Reason The operating system where the monitored system is located is not set as the current time zone, causing statistics collection time points to deviate.\nResolve Make sure the time is sync in collector servers and monitored application servers.\n","excerpt":"Problem  There is no abnormal log in Agent log and Collector log, The traces show, but no other info …","ref":"/docs/main/v8.3.0/en/faq/why-have-traces-no-others/","title":"Problem"},{"body":"Problem： Maven compilation failure with error like Error: not found: python2 When you compile the project via maven, it failed at module apm-webapp and the following error occured.\nPay attention to key words such as node-sass and Error: not found: python2.\n[INFO] \u0026gt; node-sass@4.11.0 postinstall C:\\XXX\\skywalking\\skywalking-ui\\node_modules\\node-sass [INFO] \u0026gt; node scripts/build.js [ERROR] gyp verb check python checking for Python executable \u0026quot;python2\u0026quot; in the PATH [ERROR] gyp verb `which` failed Error: not found: python2 [ERROR] gyp verb `which` failed at getNotFoundError (C:\\XXX\\skywalking\\skywalking-ui\\node_modules\\which\\which.js:13:12) [ERROR] gyp verb `which` failed at F (C:\\XXX\\skywalking\\skywalking-ui\\node_modules\\which\\which.js:68:19) [ERROR] gyp verb `which` failed at E (C:\\XXX\\skywalking\\skywalking-ui\\node_modules\\which\\which.js:80:29) [ERROR] gyp verb `which` failed at C:\\XXX\\skywalking\\skywalking-ui\\node_modules\\which\\which.js:89:16 [ERROR] gyp verb `which` failed at C:\\XXX\\skywalking\\skywalking-ui\\node_modules\\isexe\\index.js:42:5 [ERROR] gyp verb `which` failed at C:\\XXX\\skywalking\\skywalking-ui\\node_modules\\isexe\\windows.js:36:5 [ERROR] gyp verb `which` failed at FSReqWrap.oncomplete (fs.js:152:21) [ERROR] gyp verb `which` failed code: 'ENOENT' } [ERROR] gyp verb check python checking for Python executable \u0026quot;python\u0026quot; in the PATH [ERROR] gyp verb `which` succeeded python C:\\Users\\XXX\\AppData\\Local\\Programs\\Python\\Python37\\python.EXE [ERROR] gyp ERR! configure error [ERROR] gyp ERR! stack Error: Command failed: C:\\Users\\XXX\\AppData\\Local\\Programs\\Python\\Python37\\python.EXE -c import sys; print \u0026quot;%s.%s.%s\u0026quot; % sys.version_info[:3]; [ERROR] gyp ERR! stack File \u0026quot;\u0026lt;string\u0026gt;\u0026quot;, line 1 [ERROR] gyp ERR! stack import sys; print \u0026quot;%s.%s.%s\u0026quot; % sys.version_info[:3]; [ERROR] gyp ERR! stack ^ [ERROR] gyp ERR! stack SyntaxError: invalid syntax [ERROR] gyp ERR! stack [ERROR] gyp ERR! stack at ChildProcess.exithandler (child_process.js:275:12) [ERROR] gyp ERR! stack at emitTwo (events.js:126:13) [ERROR] gyp ERR! stack at ChildProcess.emit (events.js:214:7) [ERROR] gyp ERR! stack at maybeClose (internal/child_process.js:925:16) [ERROR] gyp ERR! stack at Process.ChildProcess._handle.onexit (internal/child_process.js:209:5) [ERROR] gyp ERR! System Windows_NT 10.0.17134 ...... [INFO] server-starter-es7 ................................. SUCCESS [ 11.657 s] [INFO] apm-webapp ......................................... FAILURE [ 25.857 s] [INFO] apache-skywalking-apm .............................. SKIPPED [INFO] apache-skywalking-apm-es7 .......................... SKIPPED Reason It has nothing to do with SkyWalking.\nAccording to https://github.com/sass/node-sass/issues/1176, if you live in countries where requesting resources from GitHub and npmjs.org is very slowly, some precompiled binaries for dependency node-sass will fail to be downloaded during npm install, then npm will try to compile them itself. That\u0026rsquo;s why python2 is needed.\nResolve 1. Use mirror. Such as in China, please edit skywalking\\apm-webapp\\pom.xml Find\n\u0026lt;configuration\u0026gt; \u0026lt;arguments\u0026gt;install --registry=https://registry.npmjs.org/\u0026lt;/arguments\u0026gt; \u0026lt;/configuration\u0026gt; Replace it with\n\u0026lt;configuration\u0026gt; \u0026lt;arguments\u0026gt;install --registry=https://registry.npm.taobao.org/ --sass_binary_site=https://npm.taobao.org/mirrors/node-sass/\u0026lt;/arguments\u0026gt; \u0026lt;/configuration\u0026gt; 2. Get an enough powerful VPN ","excerpt":"Problem： Maven compilation failure with error like Error: not found: python2 When you compile the …","ref":"/docs/main/v8.3.0/en/faq/maven-compile-npm-failure/","title":"Problem： Maven compilation failure with error like `Error： not found： python2`"},{"body":"Protocols There are two types of protocols list here.\n  Probe Protocol. Include the descriptions and definitions about how agent send collected metrics data and traces, also the formats of each entities.\n  Query Protocol. The backend provide query capability to SkyWalking own UI and others. These queries are based on GraphQL.\n  Probe Protocols They also related to the probe group, for understand that, look Concepts and Designs document. These groups are Language based native agent protocol, Service Mesh protocol and 3rd-party instrument protocol.\nLanguage based native agent protocol There are two types of protocols to make language agents work in distributed environments.\n Cross Process Propagation Headers Protocol and Cross Process Correlation Headers Protocol are in wire data format, agent/SDK usually uses HTTP/MQ/HTTP2 headers to carry the data with rpc request. The remote agent will receive this in the request handler, and bind the context with this specific request. Trace Data Protocol is out of wire data, agent/SDK uses this to send traces and metrics to skywalking or other compatible backend.  Cross Process Propagation Headers Protocol v3 is the new protocol for in-wire context propagation, started in 8.0.0 release.\nCross Process Correlation Headers Protocol v1 is a new in-wire context propagation additional and optional protocols. Please read SkyWalking language agents documentations to see whether it is supported. This protocol defines the data format of transporting custom data with Cross Process Propagation Headers Protocol. SkyWalking javaagent begins to support this since 8.0.0.\nSkyWalking Trace Data Protocol v3 defines the communication way and format between agent and backend.\nBrowser probe protocol The browser probe, such as skywalking-client-js could use this protocol to send to backend. This service provided by gRPC.\nSkyWalking Browser Protocol define the communication way and format between skywalking-client-js and backend.\nService Mesh probe protocol The probe in sidecar or proxy could use this protocol to send data to backendEnd. This service provided by gRPC, requires the following key info:\n Service Name or ID at both sides. Service Instance Name or ID at both sides. Endpoint. URI in HTTP, service method full signature in gRPC. Latency. In milliseconds. Response code in HTTP Status. Success or fail. Protocol. HTTP, gRPC DetectPoint. In Service Mesh sidecar, client or server. In normal L7 proxy, value is proxy.  3rd-party instrument protocol 3rd-party instrument protocols are not defined by SkyWalking. They are just protocols/formats, which SkyWalking is compatible and could receive from their existed libraries. SkyWalking starts with supporting Zipkin v1, v2 data formats.\nBackend is based on modularization principle, so very easy to extend a new receiver to support new protocol/format.\nQuery Protocol Query protocol follows GraphQL grammar, provides data query capabilities, which depends on your analysis metrics. Read query protocol doc for more details.\n","excerpt":"Protocols There are two types of protocols list here.\n  Probe Protocol. Include the descriptions and …","ref":"/docs/main/v8.3.0/en/protocols/readme/","title":"Protocols"},{"body":"Query Protocol Query Protocol defines a set of APIs in GraphQL grammar to provide data query and interactive capabilities with SkyWalking native visualization tool or 3rd party system, including Web UI, CLI or private system.\nQuery protocol official repository, https://github.com/apache/skywalking-query-protocol.\nMetadata Metadata includes the brief info of the whole under monitoring services and their instances, endpoints, etc. Use multiple ways to query this meta data.\nextend type Query { getGlobalBrief(duration: Duration!): ClusterBrief # Normal service related metainfo  getAllServices(duration: Duration!): [Service!]! searchServices(duration: Duration!, keyword: String!): [Service!]! searchService(serviceCode: String!): Service # Fetch all services of Browser type getAllBrowserServices(duration: Duration!): [Service!]! # Service intance query getServiceInstances(duration: Duration!, serviceId: ID!): [ServiceInstance!]! # Endpoint query # Consider there are huge numbers of endpoint, # must use endpoint owner\u0026#39;s service id, keyword and limit filter to do query. searchEndpoint(keyword: String!, serviceId: ID!, limit: Int!): [Endpoint!]! getEndpointInfo(endpointId: ID!): EndpointInfo # Database related meta info. getAllDatabases(duration: Duration!): [Database!]! getTimeInfo: TimeInfo } Topology Show the topology and dependency graph of services or endpoints. Including direct relationship or global map.\nextend type Query { # Query the global topology getGlobalTopology(duration: Duration!): Topology # Query the topology, based on the given service getServiceTopology(serviceId: ID!, duration: Duration!): Topology # Query the instance topology, based on the given clientServiceId and serverServiceId getServiceInstanceTopology(clientServiceId: ID!, serverServiceId: ID!, duration: Duration!): ServiceInstanceTopology # Query the topology, based on the given endpoint getEndpointTopology(endpointId: ID!, duration: Duration!): Topology } Metrics Metrics query targets all the objects defined in OAL script. You could get the metrics data in linear or thermodynamic matrix formats based on the aggregation functions in script.\n3 types of metrics could be query\n Single value. The type of most default metrics is single value, consider this as default. getValues and getLinearIntValues are suitable for this. Multiple value. One metrics defined in OAL include multiple value calculations. Use getMultipleLinearIntValues to get all values. percentile is a typical multiple value func in OAL. Heatmap value. Read Heatmap in WIKI for detail. thermodynamic is the only OAL func. Use getThermodynamic to get the values.  extend type Query { getValues(metric: BatchMetricConditions!, duration: Duration!): IntValues getLinearIntValues(metric: MetricCondition!, duration: Duration!): IntValues # Query the type of metrics including multiple values, and format them as multiple linears. # The seq of these multiple lines base on the calculation func in OAL # Such as, should us this to query the result of func percentile(50,75,90,95,99) in OAL, # then five lines will be responsed, p50 is the first element of return value. getMultipleLinearIntValues(metric: MetricCondition!, numOfLinear: Int!, duration: Duration!): [IntValues!]! getThermodynamic(metric: MetricCondition!, duration: Duration!): Thermodynamic } Metrics are defined in the config/oal/*.oal files.\nAggregation Aggregation query means the metrics data need a secondary aggregation in query stage, which makes the query interfaces have some different arguments. Such as, TopN list of services is a very typical aggregation query, metrics stream aggregation just calculates the metrics values of each service, but the expected list needs ordering metrics data by the values.\nAggregation query is for single value metrics only.\n# The aggregation query is different with the metric query. # All aggregation queries require backend or/and storage do aggregation in query time. extend type Query { # TopN is an aggregation query. getServiceTopN(name: String!, topN: Int!, duration: Duration!, order: Order!): [TopNEntity!]! getAllServiceInstanceTopN(name: String!, topN: Int!, duration: Duration!, order: Order!): [TopNEntity!]! getServiceInstanceTopN(serviceId: ID!, name: String!, topN: Int!, duration: Duration!, order: Order!): [TopNEntity!]! getAllEndpointTopN(name: String!, topN: Int!, duration: Duration!, order: Order!): [TopNEntity!]! getEndpointTopN(serviceId: ID!, name: String!, topN: Int!, duration: Duration!, order: Order!): [TopNEntity!]! } Others The following query(s) are for specific features, including trace, alarm or profile.\n Trace. Query distributed traces by this. Alarm. Through alarm query, you can have alarm trend and details.  The actual query GraphQL scrips could be found inside query-protocol folder in here.\nCondition Duration Duration is a widely used parameter type as the APM data is time related. The explanations are as following. Step is related the precision.\n# The Duration defines the start and end time for each query operation. # Fields: `start` and `end` # represents the time span. And each of them matches the step. # ref https://www.ietf.org/rfc/rfc3339.txt # The time formats are # `SECOND` step: yyyy-MM-dd HHmmss # `MINUTE` step: yyyy-MM-dd HHmm # `HOUR` step: yyyy-MM-dd HH # `DAY` step: yyyy-MM-dd # `MONTH` step: yyyy-MM # Field: `step` # represents the accurate time point. # e.g. # if step==HOUR , start=2017-11-08 09, end=2017-11-08 19 # then # metrics from the following time points expected # 2017-11-08 9:00 -\u0026gt; 2017-11-08 19:00 # there are 11 time points (hours) in the time span. input Duration { start: String! end: String! step: Step! } enum Step { MONTH DAY HOUR MINUTE SECOND } ","excerpt":"Query Protocol Query Protocol defines a set of APIs in GraphQL grammar to provide data query and …","ref":"/docs/main/v8.3.0/en/protocols/query-protocol/","title":"Query Protocol"},{"body":"Scopes and Fields By using Aggregation Function, the requests will group by time and Group Key(s) in each scope.\nSCOPE All    Name Remarks Group Key Type     name Represent the service name of each request.  string   serviceInstanceName Represent the name of the service instance id referred.  string   endpoint Represent the endpoint path of each request.  string   latency Represent how much time of each request.  int(in ms)   status Represent whether success or fail of the request.  bool(true for success)   responseCode Represent the response code of HTTP response, if this request is the HTTP call. e.g. 200, 404, 302  int   type Represent the type of each request. Such as: Database, HTTP, RPC, gRPC.  enum   tags Represent the labels of each request and each value is made up with the TagKey:TagValue in the segment.  List\u0026lt;String\u0026gt;    SCOPE Service Calculate the metrics data from each request of the service.\n   Name Remarks Group Key Type     name Represent the name of the service  string   nodeType Represent which kind of node of Service or Network address represents to, Such as: Normal, Database, MQ, Cache.  enum   serviceInstanceName Represent the name of the service instance id referred  string   endpointName Represent the name of the endpoint, such a full path of HTTP URI  string   latency Represent how much time of each request.  int   status Represent whether success or fail of the request.  bool(true for success)   responseCode Represent the response code of HTTP response, if this request is the HTTP call  int   type Represent the type of each request. Such as: Database, HTTP, RPC, gRPC.  enum   tags Represent the labels of each request and each value is made up with the TagKey:TagValue in the segment.  List\u0026lt;String\u0026gt;   sideCar.internalErrorCode Represent the sidecar/gateway proxy internal error code, the value bases on the implementation.  string    SCOPE ServiceInstance Calculate the metrics data from each request of the service instance.\n   Name Remarks Group Key Type     name Represent the name of the service instance. Such as ip:port@Service Name. Notice: current native agent uses uuid@ipv4 as instance name, which is useless when you want to setup a filter in aggregation.  string   serviceName Represent the name of the service.  string   nodeType Represent which kind of node of Service or Network address represents to, Such as: Normal, Database, MQ, Cache.  enum   endpointName Represent the name of the endpoint, such a full path of HTTP URI.  string   latency Represent how much time of each request.  int   status Represent whether success or fail of the request.  bool(true for success)   responseCode Represent the response code of HTTP response, if this request is the HTTP call.  int   type Represent the type of each request. Such as: Database, HTTP, RPC, gRPC.  enum   tags Represent the labels of each request and each value is made up with the TagKey:TagValue in the segment.  List\u0026lt;String\u0026gt;   sideCar.internalErrorCode Represent the sidecar/gateway proxy internal error code, the value bases on the implementation.  string    Secondary scopes of ServiceInstance Calculate the metrics data if the service instance is a JVM and collected by javaagent.\n SCOPE ServiceInstanceJVMCPU     Name Remarks Group Key Type     name Represent the name of the service instance. Such as ip:port@Service Name. Notice: current native agent uses uuid@ipv4 as instance name, which is useless when you want to setup a filter in aggregation.  string   serviceName Represent the name of the service.  string   usePercent Represent how much percent of cpu time cost  double    SCOPE ServiceInstanceJVMMemory     Name Remarks Group Key Type     name Represent the name of the service instance. Such as ip:port@Service Name. Notice: current native agent uses uuid@ipv4 as instance name, which is useless when you want to setup a filter in aggregation.  string   serviceName Represent the name of the service.  string   heapStatus Represent this value the memory metrics values are heap or not  bool   init See JVM document  long   max See JVM document  long   used See JVM document  long   committed See JVM document  long    SCOPE ServiceInstanceJVMMemoryPool     Name Remarks Group Key Type     name Represent the name of the service instance. Such as ip:port@Service Name. Notice: current native agent uses uuid@ipv4 as instance name, which is useless when you want to setup a filter in aggregation.  string   serviceName Represent the name of the service.  string   poolType Include CODE_CACHE_USAGE, NEWGEN_USAGE, OLDGEN_USAGE, SURVIVOR_USAGE, PERMGEN_USAGE, METASPACE_USAGE based on different version of JVM.  enum   init See JVM document  long   max See JVM document  long   used See JVM document  long   committed See JVM document  long    SCOPE ServiceInstanceJVMGC     Name Remarks Group Key Type     name Represent the name of the service instance. Such as ip:port@Service Name. Notice: current native agent uses uuid@ipv4 as instance name, which is useless when you want to setup a filter in aggregation.  string   serviceName Represent the name of the service.  string   phrase Include NEW and OLD  Enum   time GC time cost  long   count Count of GC op  long    SCOPE ServiceInstanceJVMThread     Name Remarks Group Key Type     name Represent the name of the service instance. Such as ip:port@Service Name. Notice: current native agent uses uuid@ipv4 as instance name, which is useless when you want to setup a filter in aggregation.  string   serviceName Represent the name of the service.  string   liveCount Represent Current number of live threads  int   daemonCount Represent Current number of daemon threads  int   peakCount Represent Current number of peak threads  int    SCOPE Endpoint Calculate the metrics data from each request of the endpoint in the service.\n   Name Remarks Group Key Type     name Represent the name of the endpoint, such a full path of HTTP URI.  string   serviceName Represent the name of the service.  string   serviceNodeType Represent which kind of node of Service or Network address represents to, Such as: Normal, Database, MQ, Cache.  enum   serviceInstanceName Represent the name of the service instance id referred.  string   latency Represent how much time of each request.  int   status Represent whether success or fail of the request.  bool(true for success)   responseCode Represent the response code of HTTP response, if this request is the HTTP call.  int   type Represent the type of each request. Such as: Database, HTTP, RPC, gRPC.  enum   tags Represent the labels of each request and each value is made up with the TagKey:TagValue in the segment.  List\u0026lt;String\u0026gt;   sideCar.internalErrorCode Represent the sidecar/gateway proxy internal error code, the value bases on the implementation.  string    SCOPE ServiceRelation Calculate the metrics data from each request between one service and the other service\n   Name Remarks Group Key Type     sourceServiceName Represent the name of the source service.  string   sourceServiceNodeType Represent which kind of node of Service or Network address represents to, Such as: Normal, Database, MQ, Cache.  enum   sourceServiceInstanceName Represent the name of the source service instance.  string   destServiceName Represent the name of the destination service.  string   destServiceNodeType Represent which kind of node of Service or Network address represents to.  enum   destServiceInstanceName Represent the name of the destination service instance.  string   endpoint Represent the endpoint used in this call.  string   componentId Represent the id of component used in this call. yes string   latency Represent how much time of each request.  int   status Represent whether success or fail of the request.  bool(true for success)   responseCode Represent the response code of HTTP response, if this request is the HTTP call.  int   type Represent the type of each request. Such as: Database, HTTP, RPC, gRPC.  enum   detectPoint Represent where is the relation detected. Values: client, server, proxy. yes enum   tlsMode Represent TLS mode between source and destination services. For example service_relation_mtls_cpm = from(ServiceRelation.*).filter(tlsMode == \u0026quot;mTLS\u0026quot;).cpm()  string   sideCar.internalErrorCode Represent the sidecar/gateway proxy internal error code, the value bases on the implementation.  string    SCOPE ServiceInstanceRelation Calculate the metrics data from each request between one service instance and the other service instance\n   Name Remarks Group Key Type     sourceServiceName Represent the name of the source service.  string   sourceServiceNodeType Represent which kind of node of Service or Network address represents to, Such as: Normal, Database, MQ, Cache.  enum   sourceServiceInstanceName Represent the name of the source service instance.  string   destServiceName Represent the name of the destination service.     destServiceNodeType Represent which kind of node of Service or Network address represents to, Such as: Normal, Database, MQ, Cache.  string   destServiceInstanceName Represent the name of the destination service instance.  string   endpoint Represent the endpoint used in this call.  string   componentId Represent the id of component used in this call. yes string   latency Represent how much time of each request.  int   status Represent whether success or fail of the request.  bool(true for success)   responseCode Represent the response code of HTTP response, if this request is the HTTP call.  int   type Represent the type of each request. Such as: Database, HTTP, RPC, gRPC.  enum   detectPoint Represent where is the relation detected. Values: client, server, proxy. yes enum   tlsMode Represent TLS mode between source and destination service instances. For example, service_instance_relation_mtls_cpm = from(ServiceInstanceRelation.*).filter(tlsMode == \u0026quot;mTLS\u0026quot;).cpm()  string   sideCar.internalErrorCode Represent the sidecar/gateway proxy internal error code, the value bases on the implementation.  string    SCOPE EndpointRelation Calculate the metrics data of the dependency between one endpoint and the other endpoint. This relation is hard to detect, also depends on tracing lib to propagate the prev endpoint. So EndpointRelation scope aggregation effects only in service under tracing by SkyWalking native agents, including auto instrument agents(like Java, .NET), OpenCensus SkyWalking exporter implementation or others propagate tracing context in SkyWalking spec.\n   Name Remarks Group Key Type     endpoint Represent the endpoint as parent in the dependency.  string   serviceName Represent the name of the service.  string   serviceNodeType Represent which kind of node of Service or Network address represents to, Such as: Normal, Database, MQ, Cache.  enum   childEndpoint Represent the endpoint being used by the parent endpoint in row(1)  string   childServiceName Represent the endpoint being used by the parent service in row(1)  string   childServiceNodeType Represent which kind of node of Service or Network address represents to, Such as: Normal, Database, MQ, Cache.  string   childServiceInstanceName Represent the endpoint being used by the parent service instance in row(1)  string   rpcLatency Represent the latency of the RPC from some codes in the endpoint to the childEndpoint. Exclude the latency caused by the endpoint(1) itself.     componentId Represent the id of component used in this call. yes string   status Represent whether success or fail of the request.  bool(true for success)   responseCode Represent the response code of HTTP response, if this request is the HTTP call.  int   type Represent the type of each request. Such as: Database, HTTP, RPC, gRPC.  enum   detectPoint Represent where is the relation detected. Values: client, server, proxy. yes enum    SCOPE BrowserAppTraffic Calculate the metrics data form each request of the browser app (only browser).\n   Name Remarks Group Key Type     name Represent the browser app name of each request.  string   count Represents the number of request, fixed at 1.  int   trafficCategory Represents traffic category, Values: NORMAL, FIRST_ERROR, ERROR  enum   errorCategory Represents error category, Values: AJAX, RESOURCE, VUE, PROMISE, UNKNOWN  enum    SCOPE BrowserAppSingleVersionTraffic Calculate the metrics data form each request of the browser single version in the browser app (only browser).\n   Name Remarks Group Key Type     name Represent the single version name of each request.  string   serviceName Represent the name of the browser app.  string   count Represents the number of request, fixed at 1.  int   trafficCategory Represents traffic category, Values: NORMAL, FIRST_ERROR, ERROR  enum   errorCategory Represents error category, Values: AJAX, RESOURCE, VUE, PROMISE, UNKNOWN  enum    SCOPE BrowserAppPageTraffic Calculate the metrics data form each request of the page in the browser app (only browser).\n   Name Remarks Group Key Type     name Represent the page name of each request.  string   serviceName Represent the name of the browser app.  string   count Represents the number of request, fixed at 1.  int   trafficCategory Represents the traffic category, Values: NORMAL, FIRST_ERROR, ERROR  enum   errorCategory Represents the error category, Values: AJAX, RESOURCE, VUE, PROMISE, UNKNOWN  enum    SCOPE BrowserAppPagePerf Calculate the metrics data form each request of the page in the browser app (only browser).\n   Name Remarks Group Key Type     name Represent the page name of each request.  string   serviceName Represent the name of the browser app.  string   redirectTime Represents the time of redirection.  int(in ms)   dnsTime Represents the DNS query time.  int(in ms)   ttfbTime Time to first Byte.  int(in ms)   tcpTime TCP connection time.  int(in ms)   transTime Content transfer time.  int(in ms)   domAnalysisTime Dom parsing time.  int(in ms)   fptTime First paint time or blank screen time.  int(in ms)   domReadyTime Dom ready time.  int(in ms)   loadPageTime Page full load time.  int(in ms)   resTime Synchronous load resources in the page.  int(in ms)   sslTime Only valid for HTTPS.  int(in ms)   ttlTime Time to interact.  int(in ms)   firstPackTime First pack time.  int(in ms)   fmpTime First Meaningful Paint.  int(in ms)    ","excerpt":"Scopes and Fields By using Aggregation Function, the requests will group by time and Group Key(s) in …","ref":"/docs/main/v8.3.0/en/concepts-and-designs/scope-definitions/","title":"Scopes and Fields"},{"body":"Sending Envoy Metrics to SkyWalking OAP Server Example This is an example of sending Envoy Stats to SkyWalking OAP server through Metric Service.\nRunning the example The example requires docker and docker-compose to be installed in your local. It fetches images from Docker Hub.\nNote that in ths setup, we override the log4j2.xml config to set the org.apache.skywalking.oap.server.receiver.envoy logger level to DEBUG. This enables us to see the messages sent by Envoy to SkyWalking OAP server.\n$ make up $ docker-compose logs -f skywalking $ # Please wait for a moment until SkyWalking is ready and Envoy starts sending the stats. You will see similar messages like the following: skywalking_1 | 2019-08-31 23:57:40,672 - org.apache.skywalking.oap.server.receiver.envoy.MetricServiceGRPCHandler -26870 [grpc-default-executor-0] DEBUG [] - Received msg identifier { skywalking_1 | node { skywalking_1 | id: \u0026quot;ingress\u0026quot; skywalking_1 | cluster: \u0026quot;envoy-proxy\u0026quot; skywalking_1 | metadata { skywalking_1 | fields { skywalking_1 | key: \u0026quot;skywalking\u0026quot; skywalking_1 | value { skywalking_1 | string_value: \u0026quot;iscool\u0026quot; skywalking_1 | } skywalking_1 | } skywalking_1 | fields { skywalking_1 | key: \u0026quot;envoy\u0026quot; skywalking_1 | value { skywalking_1 | string_value: \u0026quot;isawesome\u0026quot; skywalking_1 | } skywalking_1 | } skywalking_1 | } skywalking_1 | locality { skywalking_1 | region: \u0026quot;ap-southeast-1\u0026quot; skywalking_1 | zone: \u0026quot;zone1\u0026quot; skywalking_1 | sub_zone: \u0026quot;subzone1\u0026quot; skywalking_1 | } skywalking_1 | build_version: \u0026quot;e349fb6139e4b7a59a9a359be0ea45dd61e589c5/1.11.1/Clean/RELEASE/BoringSSL\u0026quot; skywalking_1 | } skywalking_1 | } skywalking_1 | envoy_metrics { skywalking_1 | name: \u0026quot;cluster.service_skywalking.update_success\u0026quot; skywalking_1 | type: COUNTER skywalking_1 | metric { skywalking_1 | counter { skywalking_1 | value: 2.0 skywalking_1 | } skywalking_1 | timestamp_ms: 1567295859556 skywalking_1 | } skywalking_1 | } ... $ # To tear down: $ make down ","excerpt":"Sending Envoy Metrics to SkyWalking OAP Server Example This is an example of sending Envoy Stats to …","ref":"/docs/main/v8.3.0/en/setup/envoy/examples/metrics/readme/","title":"Sending Envoy Metrics to SkyWalking OAP Server Example"},{"body":"Service Auto Grouping SkyWalking supports various default and customized dashboard templates. Each template provides the reasonable layout for the services in the particular field. Such as, services with a language agent installed could have different metrics with service detected by the service mesh observability solution, and different with SkyWalking\u0026rsquo;s self-observability metrics dashboard.\nTherefore, since 8.3.0, SkyWalking OAP would generate the group based on this simple naming format.\n${service name} = [${group name}::]${logic name} Once the service name includes double colons(::), the literal string before the colons would be considered as the group name. In the latest GraphQL query, the group name has been provided as an option parameter.\n getAllServices(duration: Duration!, group: String): [Service!]!\n RocketBot UI dashboards(Standard type) support the group name for default and custom configurations.\n","excerpt":"Service Auto Grouping SkyWalking supports various default and customized dashboard templates. Each …","ref":"/docs/main/v8.3.0/en/setup/backend/service-auto-grouping/","title":"Service Auto Grouping"},{"body":"Service Auto Instrument Agent Service auto instrument agent is a subset of Language based native agents. In this kind of agent, it is based on some language specific features, usually a VM based languages.\nWhat does Auto Instrument mean? Many users know these agents from hearing They say don't need to change any single line of codes, SkyWalking used to put these words in our readme page too. But actually, it is right and wrong. For end user, YES, they don\u0026rsquo;t need to change codes, at least for most cases. But also NO, the codes are still changed by agent, usually called manipulate codes at runtime. Underlying, it is just auto instrument agent including codes about how to change codes through VM interface, such as change class in Java through javaagent premain.\nAlso, we said that the most auto instrument agents are VM based, but actually, you can build a tool at compiling time, rather than runtime.\nWhat are limits? Auto instrument is so cool, also you can create those in compiling time, that you don\u0026rsquo;t depend on VM features, then is there any limit?\nThe answer definitely YES. And they are:\n  In process propagation possible in most cases. In many high level languages, they are used to build business system, such as Java and .NET. Most codes of business logic are running in the same thread for per request, which make the propagation could be based on thread Id, and stack module to make sure the context is safe.\n  Just effect frameworks or libraries. Because of the changing codes by agents, it also means the codes are already known by agent plugin developers. So, there is always a supported list in this kind of probes. Like SkyWalking Java agent supported list.\n  Across thread can\u0026rsquo;t be supported all the time. Like we said about in process propagation, most codes run in a single thread per request, especially business codes. But in some other scenarios, they do things in different threads, such as job assignment, task pool or batch process. Or some languages provide coroutine or similar thing like Goroutine, then developer could run async process with low payload, even been encouraged. In those cases, auto instrument will face problems.\n  So, no mystery for auto instrument, in short words, agent developers write an activation to make instrument codes work you. That is all.\nWhat is next? If you want to learn about manual instrument libs in SkyWalking, see Manual instrument SDK section.\n","excerpt":"Service Auto Instrument Agent Service auto instrument agent is a subset of Language based native …","ref":"/docs/main/v8.3.0/en/concepts-and-designs/service-agent/","title":"Service Auto Instrument Agent"},{"body":"Service Mesh Probe Service Mesh probes use the extendable mechanism provided in Service Mesh implementor, like Istio.\nWhat is Service Mesh? The following explanation came from Istio documents.\n The term service mesh is often used to describe the network of microservices that make up such applications and the interactions between them. As a service mesh grows in size and complexity, it can become harder to understand and manage. Its requirements can include discovery, load balancing, failure recovery, metrics, and monitoring, and often more complex operational requirements such as A/B testing, canary releases, rate limiting, access control, and end-to-end authentication.\n Where does the probe collect data from? Istio is a very typical Service Mesh design and implementor. It defines Control Panel and Data Panel, which are wide used. Here is Istio Architecture:\nService Mesh probe can choose to collect data from Data Panel. In Istio, it means collecting telemetry data from Envoy sidecar(Data Panel). The probe collects two telemetry entities from client side and server side per request.\nHow does Service Mesh make backend work? From the probe, you can see there must have no trace related in this kind of probe, so why SkyWalking platform still works?\nService Mesh probes collects telemetry data from each request, so it knows the source, destination, endpoint, latency and status. By those, backend can tell the whole topology map by combining these call as lines, and also the metrics of each nodes through their incoming request. Backend asked for the same metrics data from parsing tracing data. So, the right expression is: Service Mesh metrics are exact the metrics, what the traces parsers generate. They are same.\nWhat is Next?  If you want to use the service mesh probe, read set SkyWalking on Service Mesh document.  ","excerpt":"Service Mesh Probe Service Mesh probes use the extendable mechanism provided in Service Mesh …","ref":"/docs/main/v8.3.0/en/concepts-and-designs/service-mesh-probe/","title":"Service Mesh Probe"},{"body":"Setting Override SkyWalking backend supports setting overrides by system properties and system environment variables. You could override the settings in application.yml\nSystem properties key rule ModuleName.ProviderName.SettingKey.\n  Example\nOverride restHost in this setting segment\n  core: default: restHost: ${SW_CORE_REST_HOST:0.0.0.0} restPort: ${SW_CORE_REST_PORT:12800} restContextPath: ${SW_CORE_REST_CONTEXT_PATH:/} gRPCHost: ${SW_CORE_GRPC_HOST:0.0.0.0} gRPCPort: ${SW_CORE_GRPC_PORT:11800} Use command arg\n-Dcore.default.restHost=172.0.4.12 System environment variables   Example\nOverride restHost in this setting segment through environment variables\n  core: default: restHost: ${REST_HOST:0.0.0.0} restPort: ${SW_CORE_REST_PORT:12800} restContextPath: ${SW_CORE_REST_CONTEXT_PATH:/} gRPCHost: ${SW_CORE_GRPC_HOST:0.0.0.0} gRPCPort: ${SW_CORE_GRPC_PORT:11800} If the REST_HOST  environment variable exists in your operating system and its value is 172.0.4.12, then the value of restHost here will be overwritten to 172.0.4.12, otherwise, it will be set to 0.0.0.0.\nBy the way, Placeholder nesting is also supported, like ${REST_HOST:${ANOTHER_REST_HOST:127.0.0.1}}. In this case, if the REST_HOST  environment variable not exists, but the REST_ANOTHER_REST_HOSTHOST environment variable exists and its value is 172.0.4.12, then the value of restHost here will be overwritten to 172.0.4.12, otherwise, it will be set to 127.0.0.1.\n","excerpt":"Setting Override SkyWalking backend supports setting overrides by system properties and system …","ref":"/docs/main/v8.3.0/en/setup/backend/backend-setting-override/","title":"Setting Override"},{"body":"Setting Override In default, SkyWalking provide agent.config for agent.\nSetting override means end user can override the settings in these config file, through using system properties or agent options.\nSystem properties Use skywalking. + key in config file as system properties key, to override the value.\n  Why need this prefix?\nThe agent system properties and env share with target application, this prefix can avoid variable conflict.\n  Example\nOverride agent.application_code by this.\n  -Dskywalking.agent.application_code=31200 Agent options Add the properties after the agent path in JVM arguments.\n-javaagent:/path/to/skywalking-agent.jar=[option1]=[value1],[option2]=[value2]   Example\nOverride agent.application_code and logging.level by this.\n  -javaagent:/path/to/skywalking-agent.jar=agent.application_code=31200,logging.level=debug   Special characters\nIf a separator(, or =) in the option or value, it should be wrapped in quotes.\n  -javaagent:/path/to/skywalking-agent.jar=agent.ignore_suffix='.jpg,.jpeg' System environment variables   Example\nOverride agent.application_code and logging.level by this.\n  # The service name in UI agent.service_name=${SW_AGENT_NAME:Your_ApplicationName} # Logging level logging.level=${SW_LOGGING_LEVEL:INFO} If the SW_AGENT_NAME  environment variable exists in your operating system and its value is skywalking-agent-demo, then the value of agent.service_name here will be overwritten to skywalking-agent-demo, otherwise, it will be set to Your_ApplicationName.\nBy the way, Placeholder nesting is also supported, like ${SW_AGENT_NAME:${ANOTHER_AGENT_NAME:Your_ApplicationName}}. In this case, if the SW_AGENT_NAME  environment variable not exists, but the ANOTHER_AGENT_NAME environment variable exists and its value is skywalking-agent-demo, then the value of agent.service_name here will be overwritten to skywalking-agent-demo,otherwise, it will be set to Your_ApplicationName.\nOverride priority Agent Options \u0026gt; System.Properties(-D) \u0026gt; System environment variables \u0026gt; Config file\n","excerpt":"Setting Override In default, SkyWalking provide agent.config for agent.\nSetting override means end …","ref":"/docs/main/v8.3.0/en/setup/service-agent/java-agent/setting-override/","title":"Setting Override"},{"body":"Setup The document explains how to install Skywalking based on the kind of probes you are going to use. If you don\u0026rsquo;t understand, please read Concepts and Designs first.\nImportant: Don\u0026rsquo;t forget to configure the timezone on your UI, and you also need to be sure your OAP backend servers are also using the same timezone.\nIf you have any issues, please check that your issue is not already described in the FAQ.\nDownload official releases  Backend, UI and Java agent are Apache official release, you can find them on the Apache SkyWalking download page.  Language agents in Service   Java agent. Introduces how to install java agent to your service, without any impact in your code.\n  LUA agent. Introduce how to install the lua agent in Nginx + LUA module or OpenResty.\n  Python Agent. Introduce how to install the Python Agent in a Python service.\n  Node.js agent. Introduce how to install the NodeJS Agent in a NodeJS service.\n  The following agents and SDKs are compatible with the SkyWalking\u0026rsquo;s data formats and network protocols, but are maintained by 3rd-parties. You can go to their project repositories for additional info about guides and releases.\n  SkyAPM .NET Core agent. See .NET Core agent project document for more details.\n  SkyAPM Node.js agent. See Node.js server side agent project document for more details.\n  SkyAPM PHP SDK. See PHP agent project document for more details.\n  SkyAPM GO2Sky. See GO2Sky project document for more details.\n  Browser Monitoring Apache SkyWalking Client JS. Support collecting metrics and error logs for the Browser or JavaScript based mobile app.\nNote, make sure the receiver-browser has been opened, default is ON since 8.2.0.\nService Mesh  Istio  SkyWalking on Istio. Introduces how to analyze Istio metrics.   Envoy  Use ALS (access log service) to observe service mesh, without Mixer. Follow document for guides.    Proxy  Envoy Proxy  Sending metrics to Skywalking from Envoy. How to send metrics from Envoy to SkyWalking using Metrics service.    Setup backend Follow backend and UI setup document to understand how the backend and UI configuration works. Different scenarios and advanced features are also explained.\nChanges log Backend, UI and Java agent changes are available here.\n","excerpt":"Setup The document explains how to install Skywalking based on the kind of probes you are going to …","ref":"/docs/main/v8.3.0/en/setup/readme/","title":"Setup"},{"body":"Setup java agent  Agent is available for JDK 8 - 14 in 7.x releases. JDK 1.6 - JDK 12 are supported in all 6.x releases NOTICE¹ Find agent folder in SkyWalking release package Set agent.service_name in config/agent.config. Could be any String in English. Set collector.backend_service in config/agent.config. Default point to 127.0.0.1:11800, only works for local backend. Add -javaagent:/path/to/skywalking-package/agent/skywalking-agent.jar to JVM argument. And make sure to add it before the -jar argument.  The agent release dist is included in Apache official release. New agent package looks like this.\n+-- agent +-- activations apm-toolkit-log4j-1.x-activation.jar apm-toolkit-log4j-2.x-activation.jar apm-toolkit-logback-1.x-activation.jar ... +-- config agent.config +-- plugins apm-dubbo-plugin.jar apm-feign-default-http-9.x.jar apm-httpClient-4.x-plugin.jar ..... +-- optional-plugins apm-gson-2.x-plugin.jar ..... +-- bootstrap-plugins jdk-http-plugin.jar ..... +-- logs skywalking-agent.jar  Start your application.  Supported middleware, framework and library SkyWalking agent has supported various middlewares, frameworks and libraries. Read supported list to get them and supported version. If the plugin is in Optional² catalog, go to optional plugins section to learn how to active it.\nAdvanced features  All plugins are in /plugins folder. The plugin jar is active when it is in there. Remove the plugin jar, it disabled. The default logging output folder is /logs.  Install javaagent FAQs  Linux Tomcat 7, Tomcat 8, Tomcat 9\nChange the first line of tomcat/bin/catalina.sh.  CATALINA_OPTS=\u0026#34;$CATALINA_OPTS-javaagent:/path/to/skywalking-agent/skywalking-agent.jar\u0026#34;; export CATALINA_OPTS  Windows Tomcat 7, Tomcat 8, Tomcat 9\nChange the first line of tomcat/bin/catalina.bat.  set \u0026#34;CATALINA_OPTS=-javaagent:/path/to/skywalking-agent/skywalking-agent.jar\u0026#34;  JAR file\nAdd -javaagent argument to command line in which you start your app. eg:  java -javaagent:/path/to/skywalking-agent/skywalking-agent.jar -jar yourApp.jar  Jetty\nModify jetty.sh, add -javaagent argument to command line in which you start your app. eg:  export JAVA_OPTIONS=\u0026#34;${JAVA_OPTIONS}-javaagent:/path/to/skywalking-agent/skywalking-agent.jar\u0026#34; Table of Agent Configuration Properties This is the properties list supported in agent/config/agent.config.\n   property key Description Default     agent.namespace Namespace isolates headers in cross process propagation. The HEADER name will be HeaderName:Namespace. Not set   agent.service_name The service name to represent a logic group providing the same capabilities/logic. Suggestion: set a unique name for every logic service group, service instance nodes share the same code, Max length is 50(UTF-8 char) Your_ApplicationName   agent.sample_n_per_3_secs Negative or zero means off, by default.SAMPLE_N_PER_3_SECS means sampling N TraceSegment in 3 seconds tops. Not set   agent.authentication Authentication active is based on backend setting, see application.yml for more details.For most scenarios, this needs backend extensions, only basic match auth provided in default implementation. Not set   agent.span_limit_per_segment The max number of spans in a single segment. Through this config item, SkyWalking keep your application memory cost estimated. 300   agent.ignore_suffix If the operation name of the first span is included in this set, this segment should be ignored. Not set   agent.is_open_debugging_class If true, skywalking agent will save all instrumented classes files in /debugging folder. SkyWalking team may ask for these files in order to resolve compatible problem. Not set   agent.is_cache_enhanced_class If true, SkyWalking agent will cache all instrumented classes files to memory or disk files (decided by class cache mode), allow another java agent to enhance those classes that enhanced by SkyWalking agent. To use some Java diagnostic tools (such as BTrace, Arthas) to diagnose applications or add a custom java agent to enhance classes, you need to enable this feature. Read this FAQ for more details false   agent.class_cache_mode The instrumented classes cache mode: MEMORY or FILE. MEMORY: cache class bytes to memory, if instrumented classes is too many or too large, it may take up more memory. FILE: cache class bytes in /class-cache folder, automatically clean up cached class files when the application exits. MEMORY   agent.instance_name Instance name is the identity of an instance, should be unique in the service. If empty, SkyWalking agent will generate an 32-bit uuid. Default, use UUID@hostname as the instance name. Max length is 50(UTF-8 char) \u0026quot;\u0026quot;   agent.instance_properties[key]=value Add service instance custom properties. Not set   agent.cause_exception_depth How depth the agent goes, when log all cause exceptions. 5   agent.force_reconnection_period  Force reconnection period of grpc, based on grpc_channel_check_interval. 1   agent.operation_name_threshold  The operationName max length, setting this value \u0026gt; 190 is not recommended. 150   agent.keep_tracing Keep tracing even the backend is not available if this value is true. false   osinfo.ipv4_list_size Limit the length of the ipv4 list size. 10   collector.grpc_channel_check_interval grpc channel status check interval. 30   collector.heartbeat_period agent heartbeat report period. Unit, second. 30   collector.properties_report_period_factor The agent sends the instance properties to the backend every collector.heartbeat_period * collector.properties_report_period_factor seconds 10   collector.backend_service Collector SkyWalking trace receiver service addresses. 127.0.0.1:11800   collector.grpc_upstream_timeout How long grpc client will timeout in sending data to upstream. Unit is second. 30 seconds   collector.get_profile_task_interval Sniffer get profile task list interval. 20   logging.level Log level: TRACE, DEBUG, INFO, WARN, ERROR, OFF. Default is info. INFO   logging.file_name Log file name. skywalking-api.log   logging.output Log output. Default is FILE. Use CONSOLE means output to stdout. FILE   logging.dir Log files directory. Default is blank string, means, use \u0026ldquo;{theSkywalkingAgentJarDir}/logs \u0026quot; to output logs. {theSkywalkingAgentJarDir} is the directory where the skywalking agent jar file is located \u0026quot;\u0026quot;   logging.resolver Logger resolver: PATTERN or JSON. The default is PATTERN, which uses logging.pattern to print traditional text logs. JSON resolver prints logs in JSON format. PATTERN   logging.pattern  Logging format. There are all conversion specifiers: * %level means log level. * %timestamp means now of time with format yyyy-MM-dd HH:mm:ss:SSS.\n* %thread means name of current thread.\n* %msg means some message which user logged. * %class means SimpleName of TargetClass. * %throwable means a throwable which user called. * %agent_name means agent.service_name. Only apply to the PatternLogger. %level %timestamp %thread %class : %msg %throwable   logging.max_file_size The max size of log file. If the size is bigger than this, archive the current file, and write into a new file. 300 * 1024 * 1024   logging.max_history_files The max history log files. When rollover happened, if log files exceed this number,then the oldest file will be delete. Negative or zero means off, by default. -1   statuscheck.ignored_exceptions Listed exceptions would not be treated as an error. Because in some codes, the exception is being used as a way of controlling business flow. \u0026quot;\u0026quot;   statuscheck.max_recursive_depth The max recursive depth when checking the exception traced by the agent. Typically, we don\u0026rsquo;t recommend setting this more than 10, which could cause a performance issue. Negative value and 0 would be ignored, which means all exceptions would make the span tagged in error status. 1   correlation.element_max_number Max element count in the correlation context. 3   correlation.value_max_length Max value length of each element. 128   correlation.auto_tag_keys Tag the span by the key/value in the correlation context, when the keys listed here exist. \u0026quot;\u0026quot;   jvm.buffer_size The buffer size of collected JVM info. 60 * 10   buffer.channel_size The buffer channel size. 5   buffer.buffer_size The buffer size. 300   profile.active If true, skywalking agent will enable profile when user create a new profile task. Otherwise disable profile. true   profile.max_parallel Parallel monitor segment count 5   profile.duration Max monitor segment time(minutes), if current segment monitor time out of limit, then stop it. 10   profile.dump_max_stack_depth Max dump thread stack depth 500   profile.snapshot_transport_buffer_size Snapshot transport to backend buffer size 50   meter.active If true, the agent collects and reports metrics to the backend. true   meter.report_interval Report meters interval. The unit is second 20   meter.max_meter_size Max size of the meter pool 500   plugin.mount Mount the specific folders of the plugins. Plugins in mounted folders would work. plugins,activations   plugin.peer_max_length  Peer maximum description limit. 200   plugin.exclude_plugins  Exclude some plugins define in plugins dir.Plugin names is defined in Agent plugin list \u0026quot;\u0026quot;   plugin.mongodb.trace_param If true, trace all the parameters in MongoDB access, default is false. Only trace the operation, not include parameters. false   plugin.mongodb.filter_length_limit If set to positive number, the WriteRequest.params would be truncated to this length, otherwise it would be completely saved, which may cause performance problem. 256   plugin.elasticsearch.trace_dsl If true, trace all the DSL(Domain Specific Language) in ElasticSearch access, default is false. false   plugin.springmvc.use_qualified_name_as_endpoint_name If true, the fully qualified method name will be used as the endpoint name instead of the request URL, default is false. false   plugin.toolit.use_qualified_name_as_operation_name If true, the fully qualified method name will be used as the operation name instead of the given operation name, default is false. false   plugin.jdbc.trace_sql_parameters If set to true, the parameters of the sql (typically java.sql.PreparedStatement) would be collected. false   plugin.jdbc.sql_parameters_max_length If set to positive number, the db.sql.parameters would be truncated to this length, otherwise it would be completely saved, which may cause performance problem. 512   plugin.jdbc.sql_body_max_length If set to positive number, the db.statement would be truncated to this length, otherwise it would be completely saved, which may cause performance problem. 2048   plugin.solrj.trace_statement If true, trace all the query parameters(include deleteByIds and deleteByQuery) in Solr query request, default is false. false   plugin.solrj.trace_ops_params If true, trace all the operation parameters in Solr request, default is false. false   plugin.light4j.trace_handler_chain If true, trace all middleware/business handlers that are part of the Light4J handler chain for a request. false   plugin.opgroup.* Support operation name customize group rules in different plugins. Read Group rule supported plugins Not set   plugin.springtransaction.simplify_transaction_definition_name If true, the transaction definition name will be simplified. false   plugin.jdkthreading.threading_class_prefixes Threading classes (java.lang.Runnable and java.util.concurrent.Callable) and their subclasses, including anonymous inner classes whose name match any one of the THREADING_CLASS_PREFIXES (splitted by ,) will be instrumented, make sure to only specify as narrow prefixes as what you\u0026rsquo;re expecting to instrument, (java. and javax. will be ignored due to safety issues) Not set   plugin.tomcat.collect_http_params This config item controls that whether the Tomcat plugin should collect the parameters of the request. Also, activate implicitly in the profiled trace. false   plugin.springmvc.collect_http_params This config item controls that whether the SpringMVC plugin should collect the parameters of the request, when your Spring application is based on Tomcat, consider only setting either plugin.tomcat.collect_http_params or plugin.springmvc.collect_http_params. Also, activate implicitly in the profiled trace. false   plugin.httpclient.collect_http_params This config item controls that whether the HttpClient plugin should collect the parameters of the request false   plugin.http.http_params_length_threshold When COLLECT_HTTP_PARAMS is enabled, how many characters to keep and send to the OAP backend, use negative values to keep and send the complete parameters, NB. this config item is added for the sake of performance. 1024   plugin.http.http_headers_length_threshold When include_http_headers declares header names, this threshold controls the length limitation of all header values. use negative values to keep and send the complete headers. Note. this config item is added for the sake of performance. 2048   plugin.http.include_http_headers Set the header names, which should be collected by the plugin. Header name must follow javax.servlet.http definition. Multiple names should be split by comma. ``(No header would be collected) |   plugin.feign.collect_request_body This config item controls that whether the Feign plugin should collect the http body of the request. false   plugin.feign.filter_length_limit When COLLECT_REQUEST_BODY is enabled, how many characters to keep and send to the OAP backend, use negative values to keep and send the complete body. 1024   plugin.feign.supported_content_types_prefix When COLLECT_REQUEST_BODY is enabled and content-type start with SUPPORTED_CONTENT_TYPES_PREFIX, collect the body of the request , multiple paths should be separated by , application/json,text/   plugin.influxdb.trace_influxql If true, trace all the influxql(query and write) in InfluxDB access, default is true. true   plugin.dubbo.collect_consumer_arguments Apache Dubbo consumer collect arguments in RPC call, use Object#toString to collect arguments. false   plugin.dubbo.consumer_arguments_length_threshold When plugin.dubbo.collect_consumer_arguments is true, Arguments of length from the front will to the OAP backend 256   plugin.dubbo.collect_provider_arguments Apache Dubbo provider collect arguments in RPC call, use Object#toString to collect arguments. false   plugin.dubbo.consumer_provider_length_threshold When plugin.dubbo.provider_consumer_arguments is true, Arguments of length from the front will to the OAP backend 256   plugin.kafka.bootstrap_servers A list of host/port pairs to use for establishing the initial connection to the Kafka cluster. localhost:9092   plugin.kafka.get_topic_timeout Timeout period of reading topics from the Kafka server, the unit is second. 10   plugin.kafka.consumer_config Kafka producer configuration.    plugin.kafka.producer_config Kafka producer configuration. Read producer configure to get more details. Check Kafka report doc for more details and examples.    plugin.kafka.topic_meter Specify which Kafka topic name for Meter System data to report to. skywalking_meters   plugin.kafka.topic_metrics Specify which Kafka topic name for JVM metrics data to report to. skywalking_metrics   plugin.kafka.topic_segment Specify which Kafka topic name for traces data to report to. skywalking_segments   plugin.kafka.topic_profilings Specify which Kafka topic name for Thread Profiling snapshot to report to. skywalking_profilings   plugin.kafka.topic_management Specify which Kafka topic name for the register or heartbeat data of Service Instance to report to. skywalking_managements   plugin.springannotation.classname_match_regex Match spring beans with regular expression for the class name. Multiple expressions could be separated by a comma. This only works when Spring annotation plugin has been activated. All the spring beans tagged with @Bean,@Service,@Dao, or @Repository.    Optional Plugins Java agent plugins are all pluggable. Optional plugins could be provided in optional-plugins folder under agent or 3rd party repositories. For using these plugins, you need to put the target plugin jar file into /plugins.\nNow, we have the following known optional plugins.\n Plugin of tracing Spring annotation beans Plugin of tracing Oracle and Resin Filter traces through specified endpoint name patterns Plugin of Gson serialization lib in optional plugin folder. Plugin of Zookeeper 3.4.x in optional plugin folder. The reason of being optional plugin is, many business irrelevant traces are generated, which cause extra payload to agents and backends. At the same time, those traces may be just heartbeat(s). Customize enhance Trace methods based on description files, rather than write plugin or change source codes. Plugin of Spring Cloud Gateway 2.1.x in optional plugin folder. Please only active this plugin when you install agent in Spring Gateway. spring-cloud-gateway-2.x-plugin and spring-webflux-5.x-plugin are both required. Plugin of Spring Transaction in optional plugin folder. The reason of being optional plugin is, many local span are generated, which also spend more CPU, memory and network. Plugin of Kotlin coroutine provides the tracing across coroutines automatically. As it will add local spans to all across routines scenarios, Please assess the performance impact. Plugin of quartz-scheduler-2.x in the optional plugin folder. The reason for being an optional plugin is, many task scheduling systems are based on quartz-scheduler, this will cause duplicate tracing and link different sub-tasks as they share the same quartz level trigger, such as ElasticJob. Plugin of spring-webflux-5.x in the optional plugin folder. Please only activate this plugin when you use webflux alone as a web container. If you are using SpringMVC 5 or Spring Gateway, you don\u0026rsquo;t need this plugin.  Bootstrap class plugins All bootstrap plugins are optional, due to unexpected risk. Bootstrap plugins are provided in bootstrap-plugins folder. For using these plugins, you need to put the target plugin jar file into /plugins.\nNow, we have the following known bootstrap plugins.\n Plugin of JDK HttpURLConnection. Agent is compatible with JDK 1.6+ Plugin of JDK Callable and Runnable. Agent is compatible with JDK 1.6+  The Logic Endpoint In default, all the RPC server-side names as entry spans, such as RESTFul API path and gRPC service name, would be endpoints with metrics. At the same time, SkyWalking introduces the logic endpoint concept, which allows plugins and users to add new endpoints without adding new spans. The following logic endpoints are added automatically by plugins.\n GraphQL Query and Mutation are logic endpoints by using the names of them. Spring\u0026rsquo;s ScheduledMethodRunnable jobs are logic endpoints. The name format is SpringScheduled/${className}/${methodName}. Apache ShardingSphere ElasticJob\u0026rsquo;s jobs are logic endpoints. The name format is ElasticJob/${jobName}. XXLJob\u0026rsquo;s jobs are logic endpoints. The name formats include xxl-job/MethodJob/${className}.${methodName}, xxl-job/ScriptJob/${GlueType}/id/${jobId}, and xxl-job/SimpleJob/${className}. Quartz(optional plugin)\u0026rsquo;s jobs are logic endpoints. the name format is quartz-scheduler/${jobName}.  User could use the SkyWalking\u0026rsquo;s application toolkits to add the tag into the local span to label the span as a logic endpoint in the analysis stage. The tag is, key=x-le and value = {\u0026quot;logic-span\u0026quot;:true}.\nAdvanced Features  Set the settings through system properties for config file override. Read setting override. Use gRPC TLS to link backend. See open TLS Monitor a big cluster by different SkyWalking services. Use Namespace to isolate the context propagation. Set client token if backend open token authentication. Application Toolkit, are a collection of libraries, provided by SkyWalking APM. Using them, you have a bridge between your application and SkyWalking APM agent.  If you want your codes to interact with SkyWalking agent, including getting trace id, setting tags, propagating custom data etc.. Try SkyWalking manual APIs. If you require customized metrics, try SkyWalking Meter System Toolkit. If you want to print trace context(e.g. traceId) in your logs, choose the log frameworks, log4j, log4j2, logback If you want to continue traces across thread manually, use across thread solution APIs. If you want to forward MicroMeter/Spring Sleuth metrics to Meter System, use SkyWalking MicroMeter Register. If you want to use OpenTracing Java APIs, try SkyWalking OpenTracing compatible tracer. More details you could find at http://opentracing.io If you want to tolerate some exceptions, read tolerate custom exception doc.   If you want to specify the path of your agent.config file. Read set config file through system properties  Advanced Reporters The advanced report provides an alternative way to submit the agent collected data to the backend. All of them are in the optional-reporter-plugins folder, move the one you needed into the reporter-plugins folder for the activation. Notice, don\u0026rsquo;t try to activate multiple reporters, that could cause unexpected fatal errors.\n Use Kafka to transport the traces, JVM metrics, instance properties, and profiled snapshots to the backend. Read the How to enable Kafka Reporter for more details.  Plugin Development Guide SkyWalking java agent supports plugin to extend the supported list. Please follow our Plugin Development Guide.\nTest If you are interested in plugin compatible tests or agent performance, see the following reports.\n Plugin Test in every Pull Request Java Agent Performance Test  Notice ¹ Due to gRPC didn\u0026rsquo;t support JDK 1.6 since 2018, SkyWalking abandoned the JDK 6/7 supports in all 7.x releases. But, with gRPC back forward compatibility(at least for now), all SkyWalking 6.x agents could work with 7.x, including the agent and backend.\n","excerpt":"Setup java agent  Agent is available for JDK 8 - 14 in 7.x releases. JDK 1.6 - JDK 12 are supported …","ref":"/docs/main/v8.3.0/en/setup/service-agent/java-agent/readme/","title":"Setup java agent"},{"body":"Skywalking Agent List  activemq-5.x armeria-063-084 armeria-085 armeria-086 armeria-098 async-http-client-2.x avro-1.x brpc-java canal-1.x cassandra-java-driver-3.x dbcp-2.x dubbo ehcache-2.x elastic-job-2.x elastic-job-3.x elasticsearch-5.x elasticsearch-6.x feign-default-http-9.x feign-pathvar-9.x finagle graphql grpc-1.x gson-2.8.x h2-1.x hbase-1.x httpasyncclient-4.x httpclient-3.x httpclient-4.x hystrix-1.x influxdb-2.x jdk-http-plugin jdk-threading-plugin jedis-2.x jetty-client-9.0 jetty-client-9.x jetty-server-9.x kafka-0.11.x/1.x/2.x kotlin-coroutine lettuce-5.x light4j mariadb-2.x memcache-2.x mongodb-2.x mongodb-3.x mongodb-4.x motan-0.x mysql-5.x mysql-6.x mysql-8.x netty-socketio nutz-http-1.x nutz-mvc-annotation-1.x okhttp-3.x play-2.x postgresql-8.x pulsar quasar quartz-scheduler-2.x rabbitmq-5.x redisson-3.x resteasy-server-3.x rocketMQ-3.x rocketMQ-4.x servicecomb-0.x servicecomb-1.x sharding-jdbc-1.5.x sharding-sphere-3.x sharding-sphere-4.0.0 sharding-sphere-4.1.0 sharding-sphere-4.x sharding-sphere-4.x-rc3 sofarpc solrj-7.x spring-annotation spring-async-annotation-5.x spring-cloud-feign-1.x spring-cloud-feign-2.x spring-cloud-gateway-2.0.x spring-cloud-gateway-2.1.x spring-concurrent-util-4.x spring-core-patch spring-kafka-1.x spring-kafka-2.x spring-mvc-annotation spring-mvc-annotation-3.x spring-mvc-annotation-4.x spring-mvc-annotation-5.x spring-resttemplate-4.x spring-scheduled-annotation spring-tx spring-webflux-5.x spring-webflux-5.x-webclient spymemcached-2.x struts2-2.x thrift tomcat-7.x/8.x toolkit-counter toolkit-gauge toolkit-histogram toolkit-kafka toolkit-log4j toolkit-log4j2 toolkit-logback toolkit-opentracing toolkit-tag toolkit-trace toolkit-exception undertow-2.x-plugin vertx-core-3.x xxl-job-2.x zookeeper-3.4.x mssql-jtds-1.x mssql-jdbc apache-cxf-3.x  ","excerpt":"Skywalking Agent List  activemq-5.x armeria-063-084 armeria-085 armeria-086 armeria-098 …","ref":"/docs/main/v8.3.0/en/setup/service-agent/java-agent/plugin-list/","title":"Skywalking Agent List"},{"body":"SkyWalking Cross Process Correlation Headers Protocol  Version 1.0  The Cross Process Correlation Headers Protocol is used to transport custom data by leveraging the capability of Cross Process Propagation Headers Protocol.\nThis is an optional and additional protocol for language tracer implementation. All tracer implementation could consider to implement this. Cross Process Correlation Header key is sw8-correlation. The value is the encoded(key):encoded(value) list with elements splitted by , such as base64(string key):base64(string value),base64(string key2):base64(string value2).\nRecommendations of language APIs Recommended implementation in different language API.\n TraceContext#putCorrelation and TraceContext#getCorrelation are recommended to write and read the correlation context, with key/value string. The key should be added if it is absent. The later writes should override the previous value. The total number of all keys should be less than 3, and the length of each value should be less than 128 bytes. The context should be propagated as well when tracing context is propagated across threads and processes.  ","excerpt":"SkyWalking Cross Process Correlation Headers Protocol  Version 1.0  The Cross Process Correlation …","ref":"/docs/main/v8.3.0/en/protocols/skywalking-cross-process-correlation-headers-protocol-v1/","title":"SkyWalking Cross Process Correlation Headers Protocol"},{"body":"SkyWalking Cross Process Propagation Headers Protocol  Version 3.0  SkyWalking is more likely an APM system, rather than the common distributed tracing system. The Headers are much more complex than them in order to improving analysis performance of OAP. You can find many similar mechanism in other commercial APM systems. (Some are even much more complex than our\u0026rsquo;s)\nAbstract SkyWalking Cross Process Propagation Headers Protocol v3 is also named as sw8 protocol, which is for context propagation.\nStandard Header Item The standard header should be the minimal requirement for the context propagation.\n Header Name: sw8. Header Value: 8 fields split by -. The length of header value should be less than 2k(default).  Value format example, XXXXX-XXXXX-XXXX-XXXX\nValues Values include the following segments, all String type values are in BASE64 encoding.\n Required(s)   Sample. 0 or 1. 0 means context exists, but could(most likely will) ignore. 1 means this trace need to be sampled and send to backend. Trace Id. String(BASE64 encoded). Literal String and unique globally. Parent trace segment Id. String(BASE64 encoded). Literal String and unique globally. Parent span Id. Integer. Begin with 0. This span id points to the parent span in parent trace segment. Parent service. String(BASE64 encoded). The length should not be less or equal than 50 UTF-8 characters. Parent service instance. String(BASE64 encoded). The length should be less or equal than 50 UTF-8 characters. Parent endpoint. String(BASE64 encoded). Operation Name of the first entry span in the parent segment. The length should be less than 150 UTF-8 characters. Target address used at client side of this request. String(BASE64 encoded). The network address(not must be IP + port) used at client side to access this target service.   Sample values, 1-TRACEID-SEGMENTID-3-PARENT_SERVICE-PARENT_INSTANCE-PARENT_ENDPOINT-IPPORT  Extension Header Item Extension header item is designed for the advanced features. It provides the interaction capabilities between the agents deployed in upstream and downstream services.\n Header Name: sw8-x Header Value: Split by -. The fields are extendable.  Values The current value includes fields.\n Tracing Mode. empty, 0 or 1. empty or 0 is default. 1 represents all spans generated in this context should skip analysis, spanObject#skipAnalysis=true. This context should be propagated to upstream in the default, unless it is changed in the tracing process. The timestamp of sending at the client-side. This is used in async RPC such as MQ. Once it is set, the consumer side would calculate the latency between sending and receiving, and tag the latency in the span by using key transmission.latency automatically.  ","excerpt":"SkyWalking Cross Process Propagation Headers Protocol  Version 3.0  SkyWalking is more likely an APM …","ref":"/docs/main/v8.3.0/en/protocols/skywalking-cross-process-propagation-headers-protocol-v3/","title":"SkyWalking Cross Process Propagation Headers Protocol"},{"body":"Apache SkyWalking release guide This document guides every committer to release SkyWalking in Apache Way, and also help committers to check the release for vote.\nSetup your development environment Follow Apache maven deployment environment document to set gpg tool and encrypt passwords\nUse the following block as a template and place it in ~/.m2/settings.xml\n\u0026lt;settings\u0026gt; ... \u0026lt;servers\u0026gt; \u0026lt;!-- To publish a snapshot of some part of Maven --\u0026gt; \u0026lt;server\u0026gt; \u0026lt;id\u0026gt;apache.snapshots.https\u0026lt;/id\u0026gt; \u0026lt;username\u0026gt; \u0026lt;!-- YOUR APACHE LDAP USERNAME --\u0026gt; \u0026lt;/username\u0026gt; \u0026lt;password\u0026gt; \u0026lt;!-- YOUR APACHE LDAP PASSWORD (encrypted) --\u0026gt; \u0026lt;/password\u0026gt; \u0026lt;/server\u0026gt; \u0026lt;!-- To stage a release of some part of Maven --\u0026gt; \u0026lt;server\u0026gt; \u0026lt;id\u0026gt;apache.releases.https\u0026lt;/id\u0026gt; \u0026lt;username\u0026gt; \u0026lt;!-- YOUR APACHE LDAP USERNAME --\u0026gt; \u0026lt;/username\u0026gt; \u0026lt;password\u0026gt; \u0026lt;!-- YOUR APACHE LDAP PASSWORD (encrypted) --\u0026gt; \u0026lt;/password\u0026gt; \u0026lt;/server\u0026gt; ... \u0026lt;/servers\u0026gt; \u0026lt;/settings\u0026gt; Add your GPG public key  Add your GPG public key into SkyWalking GPG KEYS file, only if you are a committer, use your Apache id and password login this svn, and update file. Don\u0026rsquo;t override the existing file. Upload your GPG public key to public GPG site. Such as MIT\u0026rsquo;s site. This site should be in Apache maven staging repository check list.  Test your settings This step is only for test, if your env is set right, don\u0026rsquo;t need to check every time.\n./mvnw clean install -Pall (this will build artifacts, sources and sign) Prepare the release ./mvnw release:clean ./mvnw release:prepare -DautoVersionSubmodules=true -Pall  Set version number as x.y.z, and tag as vx.y.z (version tag must start with v, you will find the purpose in next step.)  You could do a GPG sign before doing release, if you need input the password to sign, and the maven don\u0026rsquo;t give the chance, but just failure. Run gpg --sign xxx to any file could remember the password for enough time to do release.\nStage the release ./mvnw release:perform -DskipTests -Pall  The release will automatically be inserted into a temporary staging repository for you.  Build and sign the source code package export RELEASE_VERSION=x.y.z (example: RELEASE_VERSION=5.0.0-alpha) cd tools/releasing sh create_source_release.sh NOTICE, create_source_release.sh is just suitable for MacOS. Welcome anyone to contribute Windows bat and Linux shell.\nThis scripts should do following things\n Use v + RELEASE_VERSION as tag to clone the codes. Make git submodule init/update done. Exclude all unnecessary files in the target source tar, such as .git, .github, .gitmodules. See the script for the details. Do gpg and shasum 512.  The apache-skywalking-apm-x.y.z-src.tgz should be found in tools/releasing folder, with .asc, .sha512.\nFind and download distribution in Apache Nexus Staging repositories  Use ApacheId to login https://repository.apache.org/ Go to https://repository.apache.org/#stagingRepositories Search skywalking and find your staging repository Close the repository and wait for all checks pass. In this step, your GPG KEYS will be checked. See set PGP document, if you haven\u0026rsquo;t done it before. Go to {REPO_URL}/org/apache/skywalking/apache-skywalking-apm/x.y.z Download .tar.gz and .zip with .asc and .sha1  Upload to Apache svn  Use ApacheId to login https://dist.apache.org/repos/dist/dev/skywalking/ Create folder, named by release version and round, such as: x.y.z Upload Source code package to the folder with .asc, .sha512  Package name: apache-skywalking-x.y.z-src.tar.gz See Section \u0026ldquo;Build and sign the source code package\u0026rdquo; for more details   Upload distribution package to the folder with .asc, .sha512  Package name: apache-skywalking-bin-x.y.z.tar.gz, apache-skywalking-bin-x.y.z.zip See Section \u0026ldquo;Find and download distribution in Apache Nexus Staging repositories\u0026rdquo; for more details Create .sha512 package: shasum -a 512 file \u0026gt; file.sha512    Make the internal announcements Send an announcement mail in dev mail list.\nMail title: [ANNOUNCE] SkyWalking x.y.z test build available Mail content: The test build of x.y.z is available. We welcome any comments you may have, and will take all feedback into account if a quality vote is called for this build. Release notes: * https://github.com/apache/skywalking/blob/master/CHANGES.md Release Candidate: * https://dist.apache.org/repos/dist/dev/skywalking/xxxx * sha512 checksums - sha512xxxxyyyzzz apache-skywalking-apm-x.x.x-src.tgz - sha512xxxxyyyzzz apache-skywalking-apm-bin-x.x.x.tar.gz - sha512xxxxyyyzzz apache-skywalking-apm-bin-x.x.x.zip Maven 2 staging repository: * https://repository.apache.org/content/repositories/xxxx/org/apache/skywalking/ Release Tag : * (Git Tag) x.y.z Release CommitID : * https://github.com/apache/skywalking/tree/(Git Commit ID) * Git submodule * skywalking-ui: https://github.com/apache/skywalking-rocketbot-ui/tree/(Git Commit ID) * apm-protocol/apm-network/src/main/proto: https://github.com/apache/skywalking-data-collect-protocol/tree/(Git Commit ID) * oap-server/server-query-plugin/query-graphql-plugin/src/main/resources/query-protocol https://github.com/apache/skywalking-query-protocol/tree/(Git Commit ID) Keys to verify the Release Candidate : * https://dist.apache.org/repos/dist/release/skywalking/KEYS Guide to build the release from source : * https://github.com/apache/skywalking/blob/x.y.z/docs/en/guides/How-to-build.md A vote regarding the quality of this test build will be initiated within the next couple of days. Wait at least 48 hours for test responses Any PMC, committer or contributor can test features for releasing, and feedback. Based on that, PMC will decide whether start a vote.\nCall a vote in dev Call a vote in dev@skywalking.apache.org\nMail title: [VOTE] Release Apache SkyWalking version x.y.z Mail content: Hi All, This is a call for vote to release Apache SkyWalking version x.y.z. Release notes: * https://github.com/apache/skywalking/blob/x.y.z/CHANGES.md Release Candidate: * https://dist.apache.org/repos/dist/dev/skywalking/xxxx * sha512 checksums - sha512xxxxyyyzzz apache-skywalking-apm-x.x.x-src.tgz - sha512xxxxyyyzzz apache-skywalking-apm-bin-x.x.x.tar.gz - sha512xxxxyyyzzz apache-skywalking-apm-bin-x.x.x.zip Maven 2 staging repository: * https://repository.apache.org/content/repositories/xxxx/org/apache/skywalking/ Release Tag : * (Git Tag) x.y.z Release CommitID : * https://github.com/apache/skywalking/tree/(Git Commit ID) * Git submodule * skywalking-ui: https://github.com/apache/skywalking-rocketbot-ui/tree/(Git Commit ID) * apm-protocol/apm-network/src/main/proto: https://github.com/apache/skywalking-data-collect-protocol/tree/(Git Commit ID) * oap-server/server-query-plugin/query-graphql-plugin/src/main/resources/query-protocol https://github.com/apache/skywalking-query-protocol/tree/(Git Commit ID) Keys to verify the Release Candidate : * https://dist.apache.org/repos/dist/release/skywalking/KEYS Guide to build the release from source : * https://github.com/apache/skywalking/blob/x.y.z/docs/en/guides/How-to-build.md Voting will start now (xxxx date) and will remain open for at least 72 hours, Request all PMC members to give their vote. [ ] +1 Release this package. [ ] +0 No opinion. [ ] -1 Do not release this package because.... Vote Check All PMC members and committers should check these before vote +1.\n Features test. All artifacts in staging repository are published with .asc, .md5, *sha1 files Source code and distribution package (apache-skywalking-x.y.z-src.tar.gz, apache-skywalking-bin-x.y.z.tar.gz, apache-skywalking-bin-x.y.z.zip) are in https://dist.apache.org/repos/dist/dev/skywalking/x.y.z with .asc, .sha512 LICENSE and NOTICE are in Source code and distribution package. Check shasum -c apache-skywalking-apm-x.y.z-src.tgz.sha512 Check gpg --verify apache-skywalking-apm-x.y.z-src.tgz.asc apache-skywalking-apm-x.y.z-src.tgz Build distribution from source code package (apache-skywalking-x.y.z-src.tar.gz) by following this doc. Apache RAT check. Run ./mvnw apache-rat:check. (No binary in source codes)  Vote result should follow these.\n PMC vote is +1 binding, all others is +1 no binding. In 72 hours, you get at least 3 (+1 binding), and have more +1 than -1. Vote pass.  Publish release  Move source codes tar balls and distributions to https://dist.apache.org/repos/dist/release/skywalking/.  \u0026gt; export SVN_EDITOR=vim \u0026gt; svn mv https://dist.apache.org/repos/dist/dev/skywalking/x.y.z https://dist.apache.org/repos/dist/release/skywalking .... enter your apache password .... Do release in nexus staging repo. Public download source and distribution tar/zip locate in http://www.apache.org/dyn/closer.cgi/skywalking/x.y.z/xxx. We only publish Apache mirror path as release info. Public asc and sha512 locate in https://www.apache.org/dist/skywalking/x.y.z/xxx Public KEYS pointing to https://www.apache.org/dist/skywalking/KEYS Update website download page. http://skywalking.apache.org/downloads/ . Include new download source, distribution, sha512, asc and document links. Links could be found by following above rules(3)-(6). Add a release event on website homepage and event page. Announce the public release with changelog or key features. Send ANNOUNCE email to dev@skywalking.apache.org, announce@apache.org, the sender should use Apache email account.  Mail title: [ANNOUNCE] Apache SkyWalking x.y.z released Mail content: Hi all, Apache SkyWalking Team is glad to announce the first release of Apache SkyWalking x.y.z. SkyWalking: APM (application performance monitor) tool for distributed systems, especially designed for microservices, cloud native and container-based (Docker, Kubernetes, Mesos) architectures. This release contains a number of new features, bug fixes and improvements compared to version a.b.c(last release). The notable changes since x.y.z include: (Highlight key changes) 1. ... 2. ... 3. ... Please refer to the change log for the complete list of changes: https://github.com/apache/skywalking/blob/vx.y.z/CHANGES.md Apache SkyWalking website: http://skywalking.apache.org/ Downloads: http://skywalking.apache.org/downloads/ Twitter: https://twitter.com/ASFSkyWalking SkyWalking Resources: - GitHub: https://github.com/apache/skywalking - Issue: https://github.com/apache/skywalking/issues - Mailing list: dev@skywalkiing.apache.org - Apache SkyWalking Team ","excerpt":"Apache SkyWalking release guide This document guides every committer to release SkyWalking in Apache …","ref":"/docs/main/v8.3.0/en/guides/how-to-release/","title":"SkyWalking release guide"},{"body":"Skywalking with Kotlin coroutine This Plugin provides an auto instrument support plugin for Kotlin coroutine based on context snapshot.\nDescription SkyWalking provide tracing context propagation inside thread. In order to support Kotlin Coroutine, we provide this additional plugin.\nImplementation principle As we know, Kotlin coroutine switches the execution thread by CoroutineDispatcher.\n Create a snapshot of the current context before dispatch the continuation. Then create a coroutine span after thread switched, mark the span continued with the snapshot. Every new span which created in the new thread will be a child of this coroutine span. So we can link those span together in a tracing. After the original runnable executed, we need to stop the coroutine span for cleaning thread state.  Some screenshots Run without the plugin We run a Kotlin coroutine based gRPC server without this coroutine plugin.\nYou can find, the one call (client -\u0026gt; server1 -\u0026gt; server2) has been split two tracing paths.\n Server1 without exit span and server2 tracing path.  Server2 tracing path.   Run with the plugin Without changing codes manually, just install the plugin. We can find the spans be connected together. We can get all info of one client call.\n","excerpt":"Skywalking with Kotlin coroutine This Plugin provides an auto instrument support plugin for Kotlin …","ref":"/docs/main/v8.3.0/en/setup/service-agent/java-agent/agent-optional-plugins/kotlin-coroutine-plugin/","title":"Skywalking with Kotlin coroutine"},{"body":"Slow Database Statement Slow Database statements are significant important to find out the bottleneck of the system, which relied on Database.\nSlow DB statements are based on sampling, right now, the core samples top 50 slowest in every 10 minutes. But duration of those statements must be slower than threshold.\nThe setting format is following, unit is millisecond.\n database-type:thresholdValue,database-type2:thresholdValue2\n Default setting is default:200,mongodb:100. Reserved DB type is default, which be as default threshold for all database types, except set explicitly.\nNotice, the threshold should not be too small, like 1ms. Functionally, it works, but would cost OAP performance issue, if your system statement access time are mostly more than 1ms.\n","excerpt":"Slow Database Statement Slow Database statements are significant important to find out the …","ref":"/docs/main/v8.3.0/en/setup/backend/slow-db-statement/","title":"Slow Database Statement"},{"body":"Source and Scope extension for new metrics From OAL scope introduction, you should already have understood what the scope is. At here, as you want to do more extension, you need understand deeper, which is the Source.\nSource and Scope are binding concepts. Scope declare the id(int) and name, Source declare the attributes. Please follow these steps to create a new Source and Scope.\n In the OAP core module, it provide SourceReceiver internal service.  public interface SourceReceiver extends Service { void receive(Source source); } All analysis data must be a org.apache.skywalking.oap.server.core.source.Source sub class, tagged by @SourceType annotation, and in org.apache.skywalking package. Then it could be supported by OAL script and OAP core.  Such as existed source, Service.\n@ScopeDeclaration(id = SERVICE_INSTANCE, name = \u0026#34;ServiceInstance\u0026#34;, catalog = SERVICE_INSTANCE_CATALOG_NAME) @ScopeDefaultColumn.VirtualColumnDefinition(fieldName = \u0026#34;entityId\u0026#34;, columnName = \u0026#34;entity_id\u0026#34;, isID = true, type = String.class) public class ServiceInstance extends Source { @Override public int scope() { return DefaultScopeDefine.SERVICE_INSTANCE; } @Override public String getEntityId() { return String.valueOf(id); } @Getter @Setter private int id; @Getter @Setter @ScopeDefaultColumn.DefinedByField(columnName = \u0026#34;service_id\u0026#34;) private int serviceId; @Getter @Setter private String name; @Getter @Setter private String serviceName; @Getter @Setter private String endpointName; @Getter @Setter private int latency; @Getter @Setter private boolean status; @Getter @Setter private int responseCode; @Getter @Setter private RequestType type; }  The scope() method in Source, returns an ID, which is not a random number. This ID need to be declared through @ScopeDeclaration annotation too. The ID in @ScopeDeclaration and ID in scope() method should be same for this Source.\n  The String getEntityId() method in Source, requests the return value representing unique entity which the scope related. Such as, in this Service scope, the id is service id, representing a particular service, like Order service. This value is used in OAL group mechanism.\n  @ScopeDefaultColumn.VirtualColumnDefinition and @ScopeDefaultColumn.DefinedByField are required, all declared fields(virtual/byField) are going to be pushed into persistent entity, mapping to such as ElasticSearch index and Database table column. Such as, include entity id mostly, and service id for endpoint and service instance level scope. Take a reference to all existing scopes. All these fields are detected by OAL Runtime, and required in query stage.\n  Add scope name as keyword to oal grammar definition file, OALLexer.g4, which is at antlr4 folder of generate-tool-grammar module.\n  Add scope name keyword as source in parser definition file, OALParser.g4, which is at same fold of OALLexer.g4.\n   After you done all of these, you could build a receiver, which do\n Get the original data of the metrics, Build the source, send into SourceReceiver. Write your whole OAL scripts. Repackage the project.  ","excerpt":"Source and Scope extension for new metrics From OAL scope introduction, you should already have …","ref":"/docs/main/v8.3.0/en/guides/source-extension/","title":"Source and Scope extension for new metrics"},{"body":"Spring annotation plugin This plugin allows to trace all methods of beans in Spring context, which are annotated with @Bean, @Service, @Component and @Repository.\n Why does this plugin optional?  Tracing all methods in Spring context all creates a lot of spans, which also spend more CPU, memory and network. Of course you want to have spans as many as possible, but please make sure your system payload can support these.\n","excerpt":"Spring annotation plugin This plugin allows to trace all methods of beans in Spring context, which …","ref":"/docs/main/v8.3.0/en/setup/service-agent/java-agent/agent-optional-plugins/spring-annotation-plugin/","title":"Spring annotation plugin"},{"body":"Spring sleuth setup Spring Sleuth provides Spring Boot auto-configuration for distributed tracing. Skywalking integrates it\u0026rsquo;s micrometer part, and it can send metrics to the Skywalking Meter System.\nSet up agent  Add the Micrometer and Skywalking meter registry dependency into project pom.xml file. Also you could found more detail at Toolkit micrometer.  \u0026lt;dependency\u0026gt; \u0026lt;groupId\u0026gt;org.springframework.boot\u0026lt;/groupId\u0026gt; \u0026lt;artifactId\u0026gt;spring-boot-starter-actuator\u0026lt;/artifactId\u0026gt; \u0026lt;/dependency\u0026gt; \u0026lt;dependency\u0026gt; \u0026lt;groupId\u0026gt;org.apache.skywalking\u0026lt;/groupId\u0026gt; \u0026lt;artifactId\u0026gt;apm-toolkit-micrometer-registry\u0026lt;/artifactId\u0026gt; \u0026lt;version\u0026gt;${skywalking.version}\u0026lt;/version\u0026gt; \u0026lt;/dependency\u0026gt; Create the Skywalking meter resgitry into spring bean management.  @Bean SkywalkingMeterRegistry skywalkingMeterRegistry() { // Add rate configs If you need, otherwise using none args construct  SkywalkingConfig config = new SkywalkingConfig(Arrays.asList(\u0026#34;\u0026#34;)); return new SkywalkingMeterRegistry(config); } Set up backend receiver  Enable meter receiver in the applicaiton.yml.  receiver-meter: selector: ${SW_RECEIVER_METER:default} default: Configure the meter config file, It already has the spring sleuth meter config. If you also has some customized meter at the agent side, please read meter document to configure meter.  Add UI dashboard   Open the dashboard view, click edit button to edit the templates.\n  Create a new template. Template type: Standard -\u0026gt; Template Configuration: Spring -\u0026gt; Input the Template Name.\n  Click view button, Finally get the spring sleuth dashboard.\n  Supported meter Supported 3 types information: Application, System, JVM.\n Application: HTTP request count and duration, JDBC max/idle/active connection count, Tomcat session active/reject count. System: CPU system/process usage, OS System load, OS Process file count. JVM: GC pause count and duration, Memory max/used/committed size, Thread peak/live/daemon count, Classes loaded/unloaded count.  ","excerpt":"Spring sleuth setup Spring Sleuth provides Spring Boot auto-configuration for distributed tracing. …","ref":"/docs/main/v8.3.0/en/setup/backend/spring-sleuth-setup/","title":"Spring sleuth setup"},{"body":"Start up mode In different deployment tool, such as k8s, you may need different startup mode. We provide another two optional startup modes.\nDefault mode Default mode. Do initialization works if necessary, start listen and provide service.\nRun /bin/oapService.sh(.bat) to start in this mode. Also when use startup.sh(.bat) to start.\nInit mode In this mode, oap server starts up to do initialization works, then exit. You could use this mode to init your storage, such as ElasticSearch indexes, MySQL and TiDB tables, and init data.\nRun /bin/oapServiceInit.sh(.bat) to start in this mode.\nNo-init mode In this mode, oap server starts up without initialization works, but it waits for ElasticSearch indexes, MySQL and TiDB tables existed, start listen and provide service. Meaning, this oap server expect another oap server to do the initialization.\nRun /bin/oapServiceNoInit.sh(.bat) to start in this mode.\n","excerpt":"Start up mode In different deployment tool, such as k8s, you may need different startup mode. We …","ref":"/docs/main/v8.3.0/en/setup/backend/backend-start-up-mode/","title":"Start up mode"},{"body":"Support custom enhance Here is an optional plugin apm-customize-enhance-plugin\nIntroduce SkyWalking has provided Java agent plugin development guide to help developers to build new plugin.\nThis plugin is not designed for replacement but for user convenience. The behaviour is very similar with @Trace toolkit, but without code change requirement, and more powerful, such as provide tag and log.\nHow to configure Implementing enhancements to custom classes requires two steps.\n Active the plugin, move the optional-plugins/apm-customize-enhance-plugin.jar to plugin/apm-customize-enhance-plugin.jar. Set plugin.customize.enhance_file in agent.config, which targets to rule file, such as /absolute/path/to/customize_enhance.xml. Set enhancement rules in customize_enhance.xml. \u0026lt;?xml version=\u0026#34;1.0\u0026#34; encoding=\u0026#34;UTF-8\u0026#34;?\u0026gt; \u0026lt;enhanced\u0026gt; \u0026lt;class class_name=\u0026#34;test.apache.skywalking.testcase.customize.service.TestService1\u0026#34;\u0026gt; \u0026lt;method method=\u0026#34;staticMethod()\u0026#34; operation_name=\u0026#34;/is_static_method\u0026#34; static=\u0026#34;true\u0026#34;/\u0026gt; \u0026lt;method method=\u0026#34;staticMethod(java.lang.String,int.class,java.util.Map,java.util.List,[Ljava.lang.Object;)\u0026#34; operation_name=\u0026#34;/is_static_method_args\u0026#34; static=\u0026#34;true\u0026#34;\u0026gt; \u0026lt;operation_name_suffix\u0026gt;arg[0]\u0026lt;/operation_name_suffix\u0026gt; \u0026lt;operation_name_suffix\u0026gt;arg[1]\u0026lt;/operation_name_suffix\u0026gt; \u0026lt;operation_name_suffix\u0026gt;arg[3].[0]\u0026lt;/operation_name_suffix\u0026gt; \u0026lt;tag key=\u0026#34;tag_1\u0026#34;\u0026gt;arg[2].[\u0026#39;k1\u0026#39;]\u0026lt;/tag\u0026gt; \u0026lt;tag key=\u0026#34;tag_2\u0026#34;\u0026gt;arg[4].[1]\u0026lt;/tag\u0026gt; \u0026lt;log key=\u0026#34;log_1\u0026#34;\u0026gt;arg[4].[2]\u0026lt;/log\u0026gt; \u0026lt;/method\u0026gt; \u0026lt;method method=\u0026#34;method()\u0026#34; static=\u0026#34;false\u0026#34;/\u0026gt; \u0026lt;method method=\u0026#34;method(java.lang.String,int.class)\u0026#34; operation_name=\u0026#34;/method_2\u0026#34; static=\u0026#34;false\u0026#34;\u0026gt; \u0026lt;operation_name_suffix\u0026gt;arg[0]\u0026lt;/operation_name_suffix\u0026gt; \u0026lt;tag key=\u0026#34;tag_1\u0026#34;\u0026gt;arg[0]\u0026lt;/tag\u0026gt; \u0026lt;log key=\u0026#34;log_1\u0026#34;\u0026gt;arg[1]\u0026lt;/log\u0026gt; \u0026lt;/method\u0026gt; \u0026lt;method method=\u0026#34;method(test.apache.skywalking.testcase.customize.model.Model0,java.lang.String,int.class)\u0026#34; operation_name=\u0026#34;/method_3\u0026#34; static=\u0026#34;false\u0026#34;\u0026gt; \u0026lt;operation_name_suffix\u0026gt;arg[0].id\u0026lt;/operation_name_suffix\u0026gt; \u0026lt;operation_name_suffix\u0026gt;arg[0].model1.name\u0026lt;/operation_name_suffix\u0026gt; \u0026lt;operation_name_suffix\u0026gt;arg[0].model1.getId()\u0026lt;/operation_name_suffix\u0026gt; \u0026lt;tag key=\u0026#34;tag_os\u0026#34;\u0026gt;arg[0].os.[1]\u0026lt;/tag\u0026gt; \u0026lt;log key=\u0026#34;log_map\u0026#34;\u0026gt;arg[0].getM().[\u0026#39;k1\u0026#39;]\u0026lt;/log\u0026gt; \u0026lt;/method\u0026gt; \u0026lt;/class\u0026gt; \u0026lt;class class_name=\u0026#34;test.apache.skywalking.testcase.customize.service.TestService2\u0026#34;\u0026gt; \u0026lt;method method=\u0026#34;staticMethod(java.lang.String,int.class)\u0026#34; operation_name=\u0026#34;/is_2_static_method\u0026#34; static=\u0026#34;true\u0026#34;\u0026gt; \u0026lt;tag key=\u0026#34;tag_2_1\u0026#34;\u0026gt;arg[0]\u0026lt;/tag\u0026gt; \u0026lt;log key=\u0026#34;log_1_1\u0026#34;\u0026gt;arg[1]\u0026lt;/log\u0026gt; \u0026lt;/method\u0026gt; \u0026lt;method method=\u0026#34;method([Ljava.lang.Object;)\u0026#34; operation_name=\u0026#34;/method_4\u0026#34; static=\u0026#34;false\u0026#34;\u0026gt; \u0026lt;tag key=\u0026#34;tag_4_1\u0026#34;\u0026gt;arg[0].[0]\u0026lt;/tag\u0026gt; \u0026lt;/method\u0026gt; \u0026lt;method method=\u0026#34;method(java.util.List,int.class)\u0026#34; operation_name=\u0026#34;/method_5\u0026#34; static=\u0026#34;false\u0026#34;\u0026gt; \u0026lt;tag key=\u0026#34;tag_5_1\u0026#34;\u0026gt;arg[0].[0]\u0026lt;/tag\u0026gt; \u0026lt;log key=\u0026#34;log_5_1\u0026#34;\u0026gt;arg[1]\u0026lt;/log\u0026gt; \u0026lt;/method\u0026gt; \u0026lt;/class\u0026gt; \u0026lt;/enhanced\u0026gt; ``\n   Explanation of the configuration in the file    configuration explanation     class_name The enhanced class   method The interceptor method of the class   operation_name If fill it out, will use it instead of the default operation_name.   operation_name_suffix What it means adding dynamic data after the operation_name.   static Is this method static.   tag Will add a tag in local span. The value of key needs to be represented on the XML node.   log Will add a log in local span. The value of key needs to be represented on the XML node.   arg[x] What it means is to get the input arguments. such as arg[0] is means get first arguments.   .[x] When the parsing object is Array or List, you can use it to get the object at the specified index.   .[\u0026lsquo;key\u0026rsquo;] When the parsing object is Map, you can get the map \u0026lsquo;key\u0026rsquo; through it.      ","excerpt":"Support custom enhance Here is an optional plugin apm-customize-enhance-plugin\nIntroduce SkyWalking …","ref":"/docs/main/v8.3.0/en/setup/service-agent/java-agent/customize-enhance-trace/","title":"Support custom enhance"},{"body":"Support custom trace ignore Here is an optional plugin apm-trace-ignore-plugin\nIntroduce  The purpose of this plugin is to filter endpoint which are expected to be ignored by the tracing system. You can setup multiple URL path patterns, The endpoints match these patterns wouldn\u0026rsquo;t be traced. The current matching rules follow Ant Path match style , like /path/*, /path/**, /path/?. Copy apm-trace-ignore-plugin-x.jar to agent/plugins, restarting the agent can effect the plugin.  How to configure There are two ways to configure ignore patterns. Settings through system env has higher priority.\n Set through the system environment variable,you need to add skywalking.trace.ignore_path to the system variables, the value is the path that you need to ignore, multiple paths should be separated by , Copy/agent/optional-plugins/apm-trace-ignore-plugin/apm-trace-ignore-plugin.config to /agent/config/ dir, and add rules to filter traces  trace.ignore_path=/your/path/1/**,/your/path/2/** ","excerpt":"Support custom trace ignore Here is an optional plugin apm-trace-ignore-plugin\nIntroduce  The …","ref":"/docs/main/v8.3.0/en/setup/service-agent/java-agent/agent-optional-plugins/trace-ignore-plugin/","title":"Support custom trace ignore"},{"body":"Support gRPC SSL transportation for OAP server For OAP communication we are currently using gRPC, a multi-platform RPC framework that uses protocol buffers for message serialization. The nice part about gRPC is that it promotes the use of SSL/TLS to authenticate and encrypt exchanges. Now OAP supports to enable SSL transportation for gRPC receivers.\nYou can follow below steps to enable this feature\nCreating SSL/TLS Certificates It seems like step one is to generate certificates and key files for encrypting communication. I thought this would be fairly straightforward using openssl from the command line.\nUse this script if you are not familiar with how to generate key files.\nWe need below files:\n server.pem a private RSA key to sign and authenticate the public key. It\u0026rsquo;s either a PKCS#8(PEM) or PKCS#1(DER). server.crt self-signed X.509 public keys for distribution. ca.crt a certificate authority public key for a client to validate the server\u0026rsquo;s certificate.  Config OAP server You can enable gRPC SSL by add following lines to application.yml/core/default.\ngRPCSslEnabled: true gRPCSslKeyPath: /path/to/server.pem gRPCSslCertChainPath: /path/to/server.crt gRPCSslTrustedCAPath: /path/to/ca.crt gRPCSslKeyPath and gRPCSslCertChainPath are loaded by OAP server to encrypt the communication. gRPCSslTrustedCAPath helps gRPC client to verify server certificates in cluster mode.\nWhen new files are in place, they can be load dynamically instead of restarting OAP instance.\nIf you enable sharding-server to ingest data from external, add following lines to application.yml/receiver-sharing-server/default:\ngRPCSslEnabled: true gRPCSslKeyPath: /path/to/server.pem gRPCSslCertChainPath: /path/to/server.crt Because sharding-server only receives data from external, so it doesn\u0026rsquo;t need CA at all.\nIf you port to java agent, refer to TLS.md to config java agent to enable TLS.\n","excerpt":"Support gRPC SSL transportation for OAP server For OAP communication we are currently using gRPC, a …","ref":"/docs/main/v8.3.0/en/setup/backend/grpc-ssl/","title":"Support gRPC SSL transportation for OAP server"},{"body":"Support Transport Layer Security (TLS) Transport Layer Security (TLS) is a very common security way when transport data through Internet. In some use cases, end users report the background:\n Target(under monitoring) applications are in a region, which also named VPC, at the same time, the SkyWalking backend is in another region (VPC).\nBecause of that, security requirement is very obvious.\n Authentication Mode Only support no mutual auth.\n Use this script if you are not familiar with how to generate key files. Find ca.crt, and use it at client side Find server.crt ,server.pem and ca.crt. Use them at server side. Please refer to gRPC SSL for more details.  Open and config TLS Agent config  Place ca.crt into /ca folder in agent package. Notice, /ca is not created in distribution, please create it by yourself.  Agent open TLS automatically after the /ca/ca.crt file detected.\no make sure can\u0026rsquo;t access other ports out of region (VPC), such as firewall, proxy.\n","excerpt":"Support Transport Layer Security (TLS) Transport Layer Security (TLS) is a very common security way …","ref":"/docs/main/v8.3.0/en/setup/service-agent/java-agent/tls/","title":"Support Transport Layer Security (TLS)"},{"body":"Telemetry for backend By default, the telemetry is disabled by setting selector to none, like this\ntelemetry: selector: ${SW_TELEMETRY:none} none: prometheus: host: ${SW_TELEMETRY_PROMETHEUS_HOST:0.0.0.0} port: ${SW_TELEMETRY_PROMETHEUS_PORT:1234} sslEnabled: ${SW_TELEMETRY_PROMETHEUS_SSL_ENABLED:false} sslKeyPath: ${SW_TELEMETRY_PROMETHEUS_SSL_KEY_PATH:\u0026#34;\u0026#34;} sslCertChainPath: ${SW_TELEMETRY_PROMETHEUS_SSL_CERT_CHAIN_PATH:\u0026#34;\u0026#34;} but you can set one of prometheus to enable them, for more information, refer to the details below.\nPrometheus Prometheus is supported as telemetry implementor. By using this, prometheus collects metrics from SkyWalking backend.\nSet prometheus to provider. The endpoint open at http://0.0.0.0:1234/ and http://0.0.0.0:1234/metrics.\ntelemetry: selector: ${SW_TELEMETRY:prometheus} prometheus: Set host and port if needed.\ntelemetry: selector: ${SW_TELEMETRY:prometheus} prometheus: host: 127.0.0.1 port: 1543 Set SSL relevant settings to expose a secure endpoint. Notice private key file and cert chain file could be uploaded once changes are applied to them.\ntelemetry: selector: ${SW_TELEMETRY:prometheus} prometheus: host: 127.0.0.1 port: 1543 sslEnabled: true sslKeyPath: /etc/ssl/key.pem sslCertChainPath: /etc/ssl/cert-chain.pem Grafana Visualization Provide the grafana dashboard settings. Check SkyWalking Telemetry dashboard config.\nSelf Observability SkyWalking supports to collect telemetry data into OAP backend directly. Users could check them out through UI or GraphQL API then.\nAdding following configuration to enable self-observability related modules.\n Setting up prometheus telemetry.  telemetry: selector: ${SW_TELEMETRY:prometheus} prometheus: host: 127.0.0.1 port: 1543 Setting up prometheus fetcher  prometheus-fetcher: selector: ${SW_PROMETHEUS_FETCHER:default} default: enabledRules: ${SW_PROMETHEUS_FETCHER_ENABLED_RULES:\u0026#34;self\u0026#34;} Make sure config/fetcher-prom-rules/self.yaml exists.  Once you deploy an oap-server cluster, the target host should be replaced with a dedicated IP or hostname. For instances, there are three oap server in your cluster, their host is service1, service2 and service3 respectively. You should update each self.yaml to twist target host.\nservice1:\nfetcherInterval: PT15S fetcherTimeout: PT10S metricsPath: /metrics staticConfig: # targets will be labeled as \u0026#34;instance\u0026#34; targets: - service1:1234 labels: service: oap-server ... service2:\nfetcherInterval: PT15S fetcherTimeout: PT10S metricsPath: /metrics staticConfig: # targets will be labeled as \u0026#34;instance\u0026#34; targets: - service2:1234 labels: service: oap-server ... service3:\nfetcherInterval: PT15S fetcherTimeout: PT10S metricsPath: /metrics staticConfig: # targets will be labeled as \u0026#34;instance\u0026#34; targets: - service3:1234 labels: service: oap-server ... ","excerpt":"Telemetry for backend By default, the telemetry is disabled by setting selector to none, like this …","ref":"/docs/main/v8.3.0/en/setup/backend/backend-telemetry/","title":"Telemetry for backend"},{"body":"Thread dump merging mechanism The performance profile is an enhancement feature in the APM system. We are using the thread dump to estimate the method execution time, rather than adding many local spans. In this way, the resource cost would be much less than using distributed tracing to locate slow method. This feature is suitable in the production environment. This document introduces how thread dumps are merged into the final report as a stack tree(s).\nThread analyst Read data and transform Read data from the database and convert it to a data structure in gRPC.\nst=\u0026gt;start: Start e=\u0026gt;end: End op1=\u0026gt;operation: Load data using paging op2=\u0026gt;operation: Transform data using parallel st(right)-\u0026gt;op1(right)-\u0026gt;op2 op2(right)-\u0026gt;e Copy code and paste it into this link to generate flow chart.\n Use the stream to read data by page (50 records per page). Convert data into gRPC data structures in the form of parallel streams. Merge into a list of data.  Data analyze Use the group by and collector modes in the Java parallel stream to group according to the first stack element in the database records, and use the collector to perform data aggregation. Generate a multi-root tree.\nst=\u0026gt;start: Start e=\u0026gt;end: End op1=\u0026gt;operation: Group by first stack element sup=\u0026gt;operation: Generate empty stack tree acc=\u0026gt;operation: Accumulator data to stack tree com=\u0026gt;operation: Combine stack trees fin=\u0026gt;operation: Calculate durations and build result st(right)-\u0026gt;op1-\u0026gt;sup(right)-\u0026gt;acc acc(right)-\u0026gt;com(right)-\u0026gt;fin-\u0026gt;e Copy code and paste it into this link to generate flow chart.\n Group by first stack element: Use the first level element in each stack to group, ensuring that the stacks have the same root node. Generate empty stack tree: Generate multiple top-level empty trees for preparation of the following steps, The reason for generating multiple top-level trees is that original data can be add in parallel without generating locks. Accumulator data to stack tree: Add every thread dump into the generated trees.  Iterate through each element in the thread dump to find if there is any child element with the same code signature and same stack depth in the parent element. If not, then add this element. Keep the dump sequences and timestamps in each nodes from the source.   Combine stack trees: Combine all trees structures into one by using the rules as same as Accumulator.  Use LDR to traversal tree node. Use the Stack data structure to avoid recursive calls, each stack element represents the node that needs to be merged. The task of merging two nodes is to merge the list of children nodes. If they have the same code signature and same parents, save the dump sequences and timestamps in this node. Otherwise, the node needs to be added into the target node as a new child.   Calculate durations and build result: Calculate relevant statistics and generate response.  Use the same traversal node logic as in the Combine stack trees step. Convert to a GraphQL data structure, and put all nodes into a list for subsequent duration calculations. Calculate each node\u0026rsquo;s duration in parallel. For each node, sort the sequences, if there are two continuous sequences, the duration should add the duration of these two seq\u0026rsquo;s timestamp. Calculate each node execution in parallel. For each node, the duration of the current node should minus the time consumed by all children.    Profile data debug Please follow the exporter tool to package profile data. Unzip the profile data and using analyzer main function to run it.\n","excerpt":"Thread dump merging mechanism The performance profile is an enhancement feature in the APM system. …","ref":"/docs/main/v8.3.0/en/guides/backend-profile/","title":"Thread dump merging mechanism"},{"body":"Token Authentication Supported version 7.0.0+\nWhy need token authentication after we have TLS? TLS is about transport security, which makes sure the network can be trusted. The token authentication is about monitoring application data can be trusted.\nToken In current version, Token is considered as a simple string.\nSet Token  Set token in agent.config file  # Authentication active is based on backend setting, see application.yml for more details. agent.authentication = ${SW_AGENT_AUTHENTICATION:xxxx} Set token in application.yml file  ······ receiver-sharing-server: default: authentication: ${SW_AUTHENTICATION:\u0026#34;\u0026#34;} ······ Authentication fails The Skywalking OAP verifies every request from agent, only allows requests whose token matches the one configured in application.yml.\nIf the token is not right, you will see the following log in agent\norg.apache.skywalking.apm.dependencies.io.grpc.StatusRuntimeException: PERMISSION_DENIED FAQ Can I use token authentication instead of TLS? No, you shouldn\u0026rsquo;t. In tech way, you can of course, but token and TLS are used for untrusted network env. In that circumstance, TLS has higher priority than this. Token can be trusted only under TLS protection.Token can be stolen easily if you send it through a non-TLS network.\nDo you support other authentication mechanisms? Such as ak/sk? For now, no. But we appreciate someone contributes this feature.\n","excerpt":"Token Authentication Supported version 7.0.0+\nWhy need token authentication after we have TLS? TLS …","ref":"/docs/main/v8.3.0/en/setup/backend/backend-token-auth/","title":"Token Authentication"},{"body":"Token Authentication Token In current version, Token is considered as a simple string.\nSet Token Set token in agent.config file\n# Authentication active is based on backend setting, see application.yml for more details. agent.authentication = xxxx Meanwhile, open the backend token authentication.\nAuthentication fails The Collector verifies every request from agent, allowed only the token match.\nIf the token is not right, you will see the following log in agent\norg.apache.skywalking.apm.dependencies.io.grpc.StatusRuntimeException: PERMISSION_DENIED FAQ Can I use token authentication instead of TLS? No, you shouldn\u0026rsquo;t. In tech way, you can of course, but token and TLS are used for untrusted network env. In that circumstance, TLS has higher priority than this. Token can be trusted only under TLS protection.Token can be stolen easily if you send it through a non-TLS network.\nDo you support other authentication mechanisms? Such as ak/sk? For now, no. But we appreciate someone contributes this feature.\n","excerpt":"Token Authentication Token In current version, Token is considered as a simple string.\nSet Token Set …","ref":"/docs/main/v8.3.0/en/setup/service-agent/java-agent/token-auth/","title":"Token Authentication"},{"body":"trace cross thread  Dependency the toolkit, such as using maven or gradle  \u0026lt;dependency\u0026gt; \u0026lt;groupId\u0026gt;org.apache.skywalking\u0026lt;/groupId\u0026gt; \u0026lt;artifactId\u0026gt;apm-toolkit-trace\u0026lt;/artifactId\u0026gt; \u0026lt;version\u0026gt;${skywalking.version}\u0026lt;/version\u0026gt; \u0026lt;/dependency\u0026gt;  usage 1.  @TraceCrossThread public static class MyCallable\u0026lt;String\u0026gt; implements Callable\u0026lt;String\u0026gt; { @Override public String call() throws Exception { return null; } } ... ExecutorService executorService = Executors.newFixedThreadPool(1); executorService.submit(new MyCallable());  usage 2.  ExecutorService executorService = Executors.newFixedThreadPool(1); executorService.submit(CallableWrapper.of(new Callable\u0026lt;String\u0026gt;() { @Override public String call() throws Exception { return null; } })); or\nExecutorService executorService = Executors.newFixedThreadPool(1); executorService.execute(RunnableWrapper.of(new Runnable() { @Override public void run() { //your code  } }));  usage 3.  @TraceCrossThread public class MySupplier\u0026lt;String\u0026gt; implements Supplier\u0026lt;String\u0026gt; { @Override public String get() { return null; } } ... CompletableFuture.supplyAsync(new MySupplier\u0026lt;String\u0026gt;()); or\nCompletableFuture.supplyAsync(SupplierWrapper.of(()-\u0026gt;{ return \u0026#34;SupplierWrapper\u0026#34;; })).thenAccept(System.out::println); Sample codes only\n","excerpt":"trace cross thread  Dependency the toolkit, such as using maven or gradle  \u0026lt;dependency\u0026gt; …","ref":"/docs/main/v8.3.0/en/setup/service-agent/java-agent/application-toolkit-trace-cross-thread/","title":"trace cross thread"},{"body":"Trace Data Protocol v3 Trace Data Protocol describes the data format between SkyWalking agent/sniffer and backend.\nOverview Trace data protocol is defined and provided in gRPC format, also implemented in HTTP 1.1\nReport service instance status   Service Instance Properties Service instance has more information than a name, once the agent wants to report this, use ManagementService#reportInstanceProperties service providing a string-key/string-value pair list as the parameter. language of target instance is expected at least.\n  Service Ping Service instance should keep alive with the backend. The agent should set a scheduler using ManagementService#keepAlive service in every minute.\n  Send trace and metrics After you have service id and service instance id, you could send traces and metrics. Now we have\n TraceSegmentReportService#collect for skywalking native trace format JVMMetricReportService#collect for skywalking native jvm format  For trace format, there are some notices\n Segment is a concept in SkyWalking, it should include all span for per request in a single OS process, usually single thread based on language. Span has 3 different groups.    EntrySpan EntrySpan represents a service provider, also the endpoint of server side. As an APM system, we are targeting the application servers. So almost all the services and MQ-consumer are EntrySpan(s).\n  LocalSpan LocalSpan represents a normal Java method, which don\u0026rsquo;t relate with remote service, neither a MQ producer/consumer nor a service(e.g. HTTP service) provider/consumer.\n  ExitSpan ExitSpan represents a client of service or MQ-producer, as named as LeafSpan at early age of SkyWalking. e.g. accessing DB by JDBC, reading Redis/Memcached are cataloged an ExitSpan.\n   Span across thread or process parent info is called Reference. Reference carries trace id, segment id, span id, service name, service instance name, endpoint name and target address used at client side(not required in across thread) of this request in the parent. Follow Cross Process Propagation Headers Protocol v3 to get more details.\n  Span#skipAnalysis could be TRUE, if this span doesn\u0026rsquo;t require backend analysis.\n  ","excerpt":"Trace Data Protocol v3 Trace Data Protocol describes the data format between SkyWalking …","ref":"/docs/main/v8.3.0/en/protocols/trace-data-protocol-v3/","title":"Trace Data Protocol v3"},{"body":"Trace Sampling at server side When we run a distributed tracing system, the trace bring us detailed info, but cost a lot at storage. Open server side trace sampling mechanism, the metrics of service, service instance, endpoint and topology are all accurate as before, but only don\u0026rsquo;t save all the traces into storage.\nOf course, even you open sampling, the traces will be kept as consistent as possible. Consistent means, once the trace segments have been collected and reported by agents, the backend would do their best to don\u0026rsquo;t break the trace. See Recommendation to understand why we called it as consistent as possible and do their best to don't break the trace.\nSet the sample rate In agent-analyzer module, you will find sampleRate setting.\nagent-analyzer: default: ... sampleRate: ${SW_TRACE_SAMPLE_RATE:10000} # The sample rate precision is 1/10000. 10000 means 100% sample in default. forceSampleErrorSegment: ${SW_FORCE_SAMPLE_ERROR_SEGMENT:true} # When sampling mechanism activated, this config would make the error status segment sampled, ignoring the sampling rate. slowTraceSegmentThreshold: ${SW_SLOW_TRACE_SEGMENT_THRESHOLD:-1} # Setting this threshold about the latency would make the slow trace segments sampled if they cost more time, even the sampling mechanism activated. The default value is `-1`, which means would not sample slow traces. Unit, millisecond. sampleRate is for you to set sample rate to this backend. The sample rate precision is 1/10000. 10000 means 100% sample in default.\nforceSampleErrorSegment is for you to save all error segments when sampling mechanism actived. When sampling mechanism activated, this config would make the error status segment sampled, ignoring the sampling rate.\nslowTraceSegmentThreshold is for you to save all slow trace segments when sampling mechanism actived. Setting this threshold about the latency would make the slow trace segments sampled if they cost more time, even the sampling mechanism activated. The default value is -1, which means would not sample slow traces. Unit, millisecond.\nRecommendation You could set different backend instances with different sampleRate values, but we recommend you to set the same.\nWhen you set the rate different, let\u0026rsquo;s say\n Backend-InstanceA.sampleRate = 35 Backend-InstanceB.sampleRate = 55  And we assume the agents reported all trace segments to backend, Then the 35% traces in the global will be collected and saved in storage consistent/complete, with all spans. 20% trace segments, which reported to Backend-InstanceB, will saved in storage, maybe miss some trace segments, because they are reported to Backend-InstanceA and ignored.\nNote When you open sampling, the actual sample rate could be over sampleRate. Because currently, all error/slow segments will be saved, meanwhile, the upstream and downstream may not be sampled. This feature is going to make sure you could have the error/slow stacks and segments, but don\u0026rsquo;t guarantee you would have the whole trace.\nAlso, the side effect would be, if most of the accesses are fail/slow, the sampling rate would be closing to 100%, which could crash the backend or storage clusters.\n","excerpt":"Trace Sampling at server side When we run a distributed tracing system, the trace bring us detailed …","ref":"/docs/main/v8.3.0/en/setup/backend/trace-sampling/","title":"Trace Sampling at server side"},{"body":"Tracing and Tracing based Metrics Analyze Plugins The following plugins provide the distributed tracing capability, and the OAP backend would analyze the topology and metrics based on the tracing data.\n HTTP Server  Tomcat 7 Tomcat 8 Tomcat 9 Spring Boot Web 4.x Spring MVC 3.x, 4.x 5.x with servlet 3.x Nutz Web Framework 1.x Struts2 MVC 2.3.x -\u0026gt; 2.5.x Resin 3 (Optional¹) Resin 4 (Optional¹) Jetty Server 9 Spring WebFlux 5.x (Optional¹) Undertow 1.3.0.Final -\u0026gt; 2.0.27.Final RESTEasy 3.1.0.Final -\u0026gt; 3.7.0.Final Play Framework 2.6.x -\u0026gt; 2.8.x Light4J Microservices Framework 1.6.x -\u0026gt; 2.x Netty SocketIO 1.x   HTTP Client  Feign 9.x Netflix Spring Cloud Feign 1.1.x -\u0026gt; 2.x Okhttp 3.x Apache httpcomponent HttpClient 2.0 -\u0026gt; 3.1, 4.2, 4.3 Spring RestTemplete 4.x Jetty Client 9 Apache httpcomponent AsyncClient 4.x AsyncHttpClient 2.x   HTTP Gateway  Spring Cloud Gateway 2.0.2.RELEASE -\u0026gt; 2.2.x.RELEASE (Optional²)   JDBC  Mysql Driver 5.x, 6.x, 8.x Oracle Driver (Optional¹) H2 Driver 1.3.x -\u0026gt; 1.4.x Sharding-JDBC 1.5.x ShardingSphere 3.0.0, 4.0.0-RC1, 4.0.0, 4.0.1, 4.1.0, 4.1.1 PostgreSQL Driver 8.x, 9.x, 42.x Mariadb Driver 2.x, 1.8 InfluxDB 2.5 -\u0026gt; 2.17 Mssql-Jtds 1.x Mssql-jdbc 6.x -\u0026gt; 8.x   RPC Frameworks  Dubbo 2.5.4 -\u0026gt; 2.6.0 Dubbox 2.8.4 Apache Dubbo 2.7.0 Motan 0.2.x -\u0026gt; 1.1.0 gRPC 1.x Apache ServiceComb Java Chassis 0.1 -\u0026gt; 0.5,1.x SOFARPC 5.4.0 Armeria 0.63.0 -\u0026gt; 0.98.0 Apache Avro 1.7.0 - 1.8.x Finagle 6.44.0 -\u0026gt; 20.1.0 (6.25.0 -\u0026gt; 6.44.0 not tested) Brpc-Java 2.3.7 -\u0026gt; 2.5.3 Thrift 0.10.0 -\u0026gt; 0.12.0 Apache CXF 3.x   MQ  RocketMQ 4.x Kafka 0.11.0.0 -\u0026gt; 1.0 Spring-Kafka Spring Kafka Consumer 1.3.x -\u0026gt; 2.3.x (2.0.x and 2.1.x not tested and not recommended by the official document) ActiveMQ 5.10.0 -\u0026gt; 5.15.4 RabbitMQ 5.x Pulsar 2.2.x -\u0026gt; 2.4.x   NoSQL  Redis  Jedis 2.x Redisson Easy Java Redis client 3.5.2+ Lettuce 5.x   MongoDB Java Driver 2.13-2.14, 3.4.0-3.12.7, 4.0.0-4.1.0 Memcached Client  Spymemcached 2.x Xmemcached 2.x   Elasticsearch  transport-client 5.2.x-5.6.x transport-client 6.7.1-6.8.4 rest-high-level-client 6.7.1-6.8.4 rest-high-level-client 7.0.0-7.5.2   Solr  SolrJ 7.x   Cassandra 3.x  cassandra-java-driver 3.7.0-3.7.2   HBase  hbase-client HTable 1.x     Service Discovery  Netflix Eureka   Distributed Coordination  Zookeeper 3.4.x (Optional² \u0026amp; Except 3.4.4)   Spring Ecosystem  Spring Bean annotations(@Bean, @Service, @Component, @Repository) 3.x and 4.x (Optional²) Spring Core Async SuccessCallback/FailureCallback/ListenableFutureCallback 4.x Spring Transaction 4.x and 5.x (Optional²)   Hystrix: Latency and Fault Tolerance for Distributed Systems 1.4.20 -\u0026gt; 1.5.18 Scheduler  Elastic Job 2.x Apache ShardingSphere-Elasticjob 3.0.0-alpha Spring @Scheduled 3.1+ Quartz Scheduler 2.x (Optional²) XXL Job 2.x   OpenTracing community supported Canal: Alibaba mysql database binlog incremental subscription \u0026amp; consumer components 1.0.25 -\u0026gt; 1.1.2 JSON  GSON 2.8.x (Optional²)   Vert.x Ecosystem  Vert.x Eventbus 3.2+ Vert.x Web 3.x   Thread Schedule Framework  Spring @Async 4.x and 5.x Quasar 0.7.x   Cache  Ehcache 2.x   Kotlin  Coroutine 1.0.1 -\u0026gt; 1.3.x (Optional²)   GraphQL  Graphql 8.0 -\u0026gt; 15.x   Pool  Apache Commons DBCP 2.x    Meter Plugins The meter plugin provides the advanced metrics collections, which are not a part of tracing.\n ¹Due to license incompatibilities/restrictions these plugins are hosted and released in 3rd part repository, go to SkyAPM java plugin extension repository to get these.\n²These plugins affect the performance or must be used under some conditions, from experiences. So only released in /optional-plugins, copy to /plugins in order to make them work.\n","excerpt":"Tracing and Tracing based Metrics Analyze Plugins The following plugins provide the distributed …","ref":"/docs/main/v8.3.0/en/setup/service-agent/java-agent/supported-list/","title":"Tracing and Tracing based Metrics Analyze Plugins"},{"body":"TTL In SkyWalking, there are two types of observability data, besides metadata.\n Record, including trace and alarm. Maybe log in the future. Metric, including such as percentile, heat map, success rate, cpm(rpm) etc.  You have following settings for different types.\n# Set a timeout on metrics data. After the timeout has expired, the metrics data will automatically be deleted. recordDataTTL: ${SW_CORE_RECORD_DATA_TTL:3} # Unit is day metricsDataTTL: ${SW_CORE_METRICS_DATA_TTL:7} # Unit is day  recordDataTTL affects Record data, including tracing and alarm. metricsDataTTL affects all metrics, including service, instance, endpoint metrics and topology map metrics.  ","excerpt":"TTL In SkyWalking, there are two types of observability data, besides metadata.\n Record, including …","ref":"/docs/main/v8.3.0/en/setup/backend/ttl/","title":"TTL"},{"body":"UI SkyWalking UI distribution is already included in our Apache official release.\nStartup Startup script is also in /bin/webappService.sh(.bat). UI runs as an OS Java process, powered-by Zuul.\nSettings Setting file of UI is webapp/webapp.yml in distribution package. It is constituted by three parts.\n Listening port. Backend connect info.  server: port: 8080 collector: path: /graphql ribbon: ReadTimeout: 10000 # Point to all backend\u0026#39;s restHost:restPort, split by ,  listOfServers: 10.2.34.1:12800,10.2.34.2:12800 ","excerpt":"UI SkyWalking UI distribution is already included in our Apache official release.\nStartup Startup …","ref":"/docs/main/v8.3.0/en/setup/backend/ui-setup/","title":"UI"},{"body":"UI Introduction SkyWalking official UI provides the default and powerful visualization capabilities for SkyWalking observing distributed cluster.\nThe latest introduction video could be found on the Youtube\n\nSkyWalking dashboard includes the following part.\n Feature Tab Selector Zone. The key features are list there. The more details will be introduced below. Reload Zone. Control the reload mechanism, including reload periodically or manually. Time Selector Zone. Control the timezone and time range. And a Chinese/English switch button here, default, the UI uses the browser language setting. We also welcome to contribute more languages.  Dashboard Dashboard provide metrics of service, service instance and endpoint. There are a few metrics terms you need to understand\n Throughput CPM , represents calls per minute. Apdex score, Read Apdex in WIKI Response Time Percentile, including p99, p95, p90, p75, p50. Read percentile in WIKI SLA, represents the successful rate. For HTTP, it means the rate of 200 response code.  Service, Instance and Dashboard selector could reload manually rather than reload the whole page. NOTICE, the Reload Zone wouldn\u0026rsquo;t reload these selectors.\nTwo default dashboards are provided to visualize the metrics of service and database.\nUser could click the lock button left aside the Service/Instance/Endpoint Reload button to custom your own dashboard.\nCustom Dashboard Users could customize the dashboard. The default dashboards are provided through the default templates located in /ui-initialized-templates folders.\nThe template file follows this format.\ntemplates: - name: template name # The unique name # The type includes DASHBOARD, TOPOLOGY_INSTANCE, TOPOLOGY_ENDPOINT. # DASHBOARD type templates could have multiple definitions, by using different names. # TOPOLOGY_INSTANCE, TOPOLOGY_ENDPOINT type templates should be defined once,  # as they are used in the topology page only. type: \u0026#34;DASHBOARD\u0026#34; # Custom the dashboard or create a new one on the UI, set the metrics as you like in the edit mode. # Then, you could export this configuration through the page and add it here. configuration: |-[ { \u0026#34;name\u0026#34;:\u0026#34;Spring Sleuth\u0026#34;, \u0026#34;type\u0026#34;:\u0026#34;service\u0026#34;, \u0026#34;children\u0026#34;:[ { \u0026#34;name\u0026#34;:\u0026#34;Sleuth\u0026#34;, \u0026#34;children\u0026#34;: [{ \u0026#34;width\u0026#34;: \u0026#34;3\u0026#34;, \u0026#34;title\u0026#34;: \u0026#34;HTTP Request\u0026#34;, \u0026#34;height\u0026#34;: \u0026#34;200\u0026#34;, \u0026#34;entityType\u0026#34;: \u0026#34;ServiceInstance\u0026#34;, \u0026#34;independentSelector\u0026#34;: false, \u0026#34;metricType\u0026#34;: \u0026#34;REGULAR_VALUE\u0026#34;, \u0026#34;metricName\u0026#34;: \u0026#34;meter_http_server_requests_count\u0026#34;, \u0026#34;queryMetricType\u0026#34;: \u0026#34;readMetricsValues\u0026#34;, \u0026#34;chartType\u0026#34;: \u0026#34;ChartLine\u0026#34;, \u0026#34;unit\u0026#34;: \u0026#34;Count\u0026#34; } ... ] } ] } ] # Activated means this templates added into the UI page automatically. # False means providing a basic template, user needs to add it manually on the page. activated: false # True means wouldn\u0026#39;t show up on the dashboard. Only keeps the definition in the storage. disabled: false NOTE, UI initialized templates would only be initialized if there is no template in the storage has the same name. Check the entity named as ui_template in your storage.\nTopology Topology map shows the relationship among the services and instances with metrics.\n Topology shows the default global topology including all services. Service Selector supports to show direct relationships including upstream and downstream. Custom Group provides the any sub topology capability of service group. Service Deep Dive opens when you click any service. The honeycomb could do metrics, trace and alarm query of the selected service. Service Relationship Metrics gives the metrics of service RPC interactions and instances of these two services.  Trace Query Trace query is a typical feature as SkyWalking provided distributed agents.\n Trace Segment List is not the trace list. Every trace has several segments belonging to different services. If\nquery by all services or by trace id, different segments with same trace id could be list there. Span is clickable, the detail of each span will pop up at the left side. Trace Views provides 3 typical and different usage views to visualize the trace.  Profile Profile is an interaction feature. It provides the method level performance diagnosis.\nTo start the profile analysis, user need to create the profile task\n Select the specific service. Set the endpoint name. This endpoint name typically is the operation name of the first span. Find this on the trace segment list view. Monitor time could start right now or from any given future time. Monitor duration defines the observation time window to find the suitable request to do performance analysis. Even the profile add a very limited performance impact to the target system, but it is still an additional load. This duration make the impact controllable. Min duration threshold provides a filter mechanism, if a request of the given endpoint response quickly, it wouldn\u0026rsquo;t be profiled. This could make sure, the profiled data is the expected one. Max sampling count gives the max dataset of agent will collect. It helps to reduce the memory and network load. One implicit condition, in any moment, SkyWalking only accept one profile task for each service. Agent could have different settings to control or limit this feature, read document setup for more details. Not all SkyWalking ecosystem agent supports this feature, java agent from 7.0.0 supports this in default.  Once the profile done, the profiled trace segments would show up. And you could request for analysis for any span. Typically, we analysis spans having long self duration, if the span and its children both have long duration, you could choose include children or exclude childrend to set the analysis boundaries.\nAfter choose the right span, and click the analysis button, you will see the stack based analysis result. The slowest methods have been highlighted.\nAdvanced features  Since 7.1.0, the profiled trace collects the HTTP request parameters for Tomcat and SpringMVC Controller automatically.  Alarm Alarm page lists all triggered alarm. Read the backend setup documentation to know how to set up the alarm rule or integrate with 3rd party system.\n","excerpt":"UI Introduction SkyWalking official UI provides the default and powerful visualization capabilities …","ref":"/docs/main/v8.3.0/en/ui/readme/","title":"UI Introduction"},{"body":"Uninstrumented Gateways/Proxies The uninstrumented gateways are not instrumented by SkyWalking agent plugin when they are started, but they can be configured in gateways.yml file or via Dynamic Configuration. The reason why they can\u0026rsquo;t register to backend automatically is that there\u0026rsquo;re no suitable agent plugins, for example, there is no agent plugins for Nginx, haproxy, etc. So in order to visualize the real topology, we provide a way to configure the gateways/proxies manually.\nConfiguration Format The configuration content includes the gateways' names and their instances:\ngateways: - name: proxy0 # the name is not used for now instances: - host: 127.0.0.1 # the host/ip of this gateway instance port: 9099 # the port of this gateway instance, defaults to 80 Note that the host of the instance must be the one that is actually used in client side, for example, if the instance proxyA has 2 IPs, say 192.168.1.110 and 192.168.1.111, both of which delegates the target service, and the client connects to 192.168.1.110, then configuring 192.168.1.111 as the host won\u0026rsquo;t work properly.\n","excerpt":"Uninstrumented Gateways/Proxies The uninstrumented gateways are not instrumented by SkyWalking agent …","ref":"/docs/main/v8.3.0/en/setup/backend/uninstrumented-gateways/","title":"Uninstrumented Gateways/Proxies"},{"body":"Using E2E local remote debugging The E2E remote debugging port of service containers is 5005. If the developer wants to use remote debugging, he needs to add remote debugging parameters to the start service command, and then expose the port 5005.\nFor example, this is the configuration of a container in the skywalking/test/e2e/e2e-test/docker/base-compose.yml. JAVA_OPTS is a preset variable for passing additional parameters in the AOP service startup command, so we only need to add the JAVA remote debugging parameters agentlib:jdwp=transport=dt_socket,server=y,suspend=n,address=5005 to the configuration and exposes the port 5005.\noap: image: skywalking/oap:latest expose: ... - 5005 ... environment: ... JAVA_OPTS: \u0026gt;-... -agentlib:jdwp=transport=dt_socket,server=y,suspend=n,address=5005 ... At last, if the E2E test failed and is retrying, the developer can get the ports mapping in the file skywalking/test/e2e/e2e-test/remote_real_port and selects the host port of the corresponding service for remote debugging. For example,\n#remote_real_port #The remote debugging port on the host is 32783 oap-localhost:32783 #The remote debugging port on the host is 32782 provider-localhost:32782 ","excerpt":"Using E2E local remote debugging The E2E remote debugging port of service containers is 5005. If the …","ref":"/docs/main/v8.3.0/en/guides/e2e-local-remote-debug/","title":"Using E2E local remote debugging"},{"body":"V6 upgrade SkyWalking v6 is widely used in many production environments. Users may wants to upgrade to an old release to new. This is a guidance to tell users how to do that.\nNOTICE, the following ways are not the only ways to do upgrade.\nUse Canary Release Like all applications, SkyWalking could use canary release method to upgrade by following these steps\n Deploy a new cluster by using the latest(or new) version of SkyWalking OAP cluster with new database cluster. Once the target(being monitored) service has chance to upgrade the agent.jar(or just simply reboot), change the collector.backend_service pointing to the new OAP backend, and use/add a new namespace(agent.namespace in Table of Agent Configuration Properties). The namespace will avoid the conflict between different versions. When all target services have been rebooted, the old OAP clusters could be discarded.  Canary Release methods works for any version upgrade.\nOnline Hot Reboot Upgrade The reason we required Canary Release is, SkyWalking agent has cache mechanisms, switching to a new cluster makes the cache unavailable for new OAP cluster. In the 6.5.0+(especially for agent version), we have Agent hot reboot trigger mechanism. By using that, we could do upgrade an easier way, deploy a new cluster by using the latest(or new) version of SkyWalking OAP cluster with new database cluster, and shift the traffic to the new cluster once for all. Based on the mechanism, all agents will go into cool_down mode, then back online. More detail, read the backend setup document.\nNOTICE, as a known bug in 6.4.0, its agent could have re-connection issue, so, even this bot reboot mechanism included in 6.4.0, it may not work in some network scenarios, especially in k8s.\nAgent Compatibility All versions of SkyWalking 6.x(even 7.x) are compatible with each others, so users could only upgrade the OAP servers first. The agent is also enhanced from version to version, so from SkyWalking team\u0026rsquo;s recommendations, upgrade the agent once you have the chance.\n","excerpt":"V6 upgrade SkyWalking v6 is widely used in many production environments. Users may wants to upgrade …","ref":"/docs/main/v8.3.0/en/faq/v6-version-upgrade/","title":"V6 upgrade"},{"body":"V8 upgrade SkyWalking v8 begins to use v3 protocol, so, it is incompatible with previous releases. Users who intend to upgrade in v8 series releases could follow this guidance.\nRegister in v6 and v7 has been removed in v8 for better scaling out performance, please upgrade in the following ways.\n Use a different storage or a new namespace. Also, could consider erasing the whole storage index/table(s) related to SkyWalking. Deploy the whole SkyWalking cluster, and expose in a new network address. If you are using the language agents, upgrade the new agents too, meanwhile, make sure the agent has supported the different language. And set up the backend address to the new SkyWalking OAP cluster.  ","excerpt":"V8 upgrade SkyWalking v8 begins to use v3 protocol, so, it is incompatible with previous releases. …","ref":"/docs/main/v8.3.0/en/faq/v8-version-upgrade/","title":"V8 upgrade"},{"body":"Version 3.x -\u0026gt; 5.0.0-alpha Upgrade FAQs Collector Problem There is no information showing in the UI.\nCause In upgrate from 3.2.6 to 5.0.0, Elasticsearch indexes aren\u0026rsquo;t recreated, because not indexes exist, but aren\u0026rsquo;t compatible with 5.0.0-alpha. When service name registered, the es will create this column by default type string, which is wrong.\nSolution Clean the data folder in ElasticSearch and restart ElasticSearch, collector and your under monitoring application.\n","excerpt":"Version 3.x -\u0026gt; 5.0.0-alpha Upgrade FAQs Collector Problem There is no information showing in the …","ref":"/docs/main/v8.3.0/en/faq/v3-version-upgrade/","title":"Version 3.x -\u003e 5.0.0-alpha Upgrade FAQs"},{"body":"Visualization SkyWalking native UI provides the default visualization solution. It provides observability related graphs about overview, service, service instance, endpoint, trace and alarm, including topology, dependency graph, heatmap, etc.\nAlso, we have already known, many of our users have integrated SkyWalking into their products. If you want to do that too, please use SkyWalking query protocol.\n","excerpt":"Visualization SkyWalking native UI provides the default visualization solution. It provides …","ref":"/docs/main/v8.3.0/en/concepts-and-designs/ui-overview/","title":"Visualization"},{"body":"Welcome Here are SkyWalking 8 official documentation. You\u0026rsquo;re welcome to join us.\nFrom here you can learn all about SkyWalking’s architecture, how to deploy and use SkyWalking, and develop based on SkyWalking contributions guidelines.\nNOTICE, SkyWalking 8 uses brand new tracing APIs, it is incompatible with all previous releases.\n  Concepts and Designs. You\u0026rsquo;ll find the the most important core ideas about SkyWalking. You can learn from here if you want to understand what is going on under our cool features and visualization.\n  Setup. Guides for installing SkyWalking in different scenarios. As a platform, it provides several ways of the observability.\n  UI Introduction. Introduce the UI usage and features.\n  Contributing Guides. Guides are for PMC member, committer or new contributor. Here, you can find how to start contributing.\n  Protocols. Protocols show the communication ways between agents/probes and backend. Anyone interested in uplink telemetry data should definitely read this.\n  FAQs. A manifest of already known setup problems, secondary developments experiments. When you are facing a problem, check here first.\n  In addition, you might find these links interesting:\n  The latest and old releases are all available at Apache SkyWalking release page. The change logs are here.\n  SkyWalking WIKI hosts the context of some changes and events.\n  You can find the speaking schedules at Conf, online videos and articles about SkyWalking in Community resource catalog.\n  We\u0026rsquo;re always looking for help improving our documentation and codes, so please don’t hesitate to file an issue if you see any problem. Or better yet, submit your own contributions through pull request to help make them better.\n","excerpt":"Welcome Here are SkyWalking 8 official documentation. You\u0026rsquo;re welcome to join us.\nFrom here you …","ref":"/docs/main/v8.3.0/readme/","title":"Welcome"},{"body":"What is VNode? In the trace page, sometimes, people could find there are nodes named VNode as the span name, and there is no attribute for this span.\nVNode is created by the UI itself, rather than reported from the agent or tracing SDK. It represents there are some span(s) missed from the trace data in this query.\nHow does the UI detect the missing span(s)? The UI real check the parent spans and reference segments of all spans, if a parent id(segment id + span id) can\u0026rsquo;t be found, then it creates a VNode automatically.\nHow does this happen? The VNode was introduced, because there are some cases which could cause the trace data are not always completed.\n The agent fail-safe mechanism activated. The SkyWalking agent has the capability to abandon the trace data, if there is agent-\u0026gt;OAP network issue(unconnected, slow network speed), or the performance of the OAP cluster is not enough to process all traces. Some plugins could have bugs, then some segments in the trace never stop correctly, it is hold in the memory.  In these cases, the trace would not exist in the query. Then VNode shows up.\n","excerpt":"What is VNode? In the trace page, sometimes, people could find there are nodes named VNode as the …","ref":"/docs/main/v8.3.0/en/faq/vnode/","title":"What is VNode?"},{"body":"Why can\u0026rsquo;t I see any data in the UI? There are three main reasons no data can be shown by the UI:\n No traces have been sent to the collector. Traces have been sent, but the timezone of your containers is incorrect. Traces are in the collector, but you\u0026rsquo;re not watching the correct timeframe in the UI.  No traces Be sure to check the logs of your agents to see if they are connected to the collector and traces are being sent.\nIncorrect timezone in containers Be sure to check the time in your containers.\nThe UI isn\u0026rsquo;t showing any data Be sure to configure the timeframe shown by the UI.\n","excerpt":"Why can\u0026rsquo;t I see any data in the UI? There are three main reasons no data can be shown by the …","ref":"/docs/main/v8.3.0/en/faq/time-and-timezone/","title":"Why can't I see any data in the UI?"},{"body":"Why doesn\u0026rsquo;t SkyWalking involve MQ in the architecture? People usually ask about these questions when they know SkyWalking at the first time. They think MQ should be better in the performance and supporting high throughput, like the following\nHere are the reasons the SkyWalking\u0026rsquo;s opinions.\nIs MQ a good or right way to communicate with OAP backend? This question comes out when people think about what happens when the OAP cluster is not powerful enough or offline. But I want to ask the questions before answer this.\n Why do you think OAP should be not powerful enough? As it is not, the speed of data analysis wouldn\u0026rsquo;t catch up with producers(agents). Then what is the point of adding new deployment requirement? Maybe you will argue says, the payload is sometimes higher than usual as there is hot business time. But, my question is how much higher? If less than 40%, how many resources will you use for the new MQ cluster? How about moving them to new OAP and ES nodes? If higher than 40%, such as 70%-2x times? Then, I could say, your MQ wastes more resources than it saves. Your MQ would support 2x-3x payload, and with 10%-20% cost in general time. Furthermore, in this case, if the payload/throughput are so high, how long the OAP cluster could catch up. I would say never before it catches up, the next hot time event is coming.  Besides all this analysis, why do you want the traces still 100%, as you are costing so many resources? Better than this, we could consider adding a better dynamic trace sampling mechanism at the backend, when throughput goes over the threshold, active the sampling rate to 100%-\u0026gt;10% step by step, which means you could get the OAP and ES 3 times more powerful than usual, just ignore the traces at hot time.\nIs MQ transport acceptable even there are several side effects? Even MQ transport is not recommended from the production perspective, SkyWalking still has optional plugins named kafka-reporter and kafka-fetcher for this feature since 8.1.0.\nHow about MQ metrics data exporter? I would say, it is already available there. Exporter module with gRPC default mechanism is there. It is easy to provide a new implementor of that module.\n","excerpt":"Why doesn\u0026rsquo;t SkyWalking involve MQ in the architecture? People usually ask about these …","ref":"/docs/main/v8.3.0/en/faq/why_mq_not_involved/","title":"Why doesn't SkyWalking involve MQ in the architecture?"},{"body":"Why metrics indexes in Hour and Day precisions stop update after upgrade to 7.x? This is an expected case when 6.x-\u0026gt;7.x upgrade. Read Downsampling Data Packing feature of the ElasticSearch storage.\nThe users could simply delete all expired *-day_xxxxx and *-hour_xxxxx(xxxxx is a timestamp) indexes. SkyWalking is using metrics name-xxxxx and metrics name-month_xxxxx indexes only.\n","excerpt":"Why metrics indexes in Hour and Day precisions stop update after upgrade to 7.x? This is an expected …","ref":"/docs/main/v8.3.0/en/faq/hour-day-metrics-stopping/","title":"Why metrics indexes in Hour and Day precisions stop update after upgrade to 7.x?"},{"body":"Work with Istio Instructions for transport Istio\u0026rsquo;s metrics to the SkyWalking OAP server.\nPrerequisites Istio should be installed in the kubernetes cluster. Follow Istio getting start to finish it.\nDeploy Skywalking backend Follow the deploying backend in kubernetes to install the OAP server in the kubernetes cluster. Referring to OpenTelemetry receiver to ingest metrics. otel-receiver defaults to be inactive. Set env var SW_OTEL_RECEIVER to default to enable it.\nDeploy OpenTelemetry collector OpenTelemetry collector is the location Istio telemetry sends metrics, then processing and sending them to SkyWalking backend.\nFollowing the Getting Started to deploy this collector. There are several components available in the collector, and they could be combined for different scenarios. For the sake of brevity, we use the Prometheus receiver to retrieve metrics from Istio control and data plane, then send them to SkyWalking by OpenCensus exporter.\nPrometheus receiver Refer to Prometheus Receiver to set up this receiver. you could find more configuration details in Prometheus Integration of Istio to figure out how to direct Prometheus receiver to query Istio metrics.\nSkyWalking supports receiving multi-cluster metrics in a single OAP cluster. A cluster label should be appended to every metric fetched by this receiver even there\u0026rsquo;s only a single cluster needed to be collected. You could leverage relabel to add it like below:\nrelabel_configs: - source_labels: [] target_label: cluster replacement: \u0026lt;cluster name\u0026gt; or opt to Resource Processor:\nprocessors: resource: attributes: - key: cluster value: \u0026quot;\u0026lt;cluster name\u0026gt;\u0026quot; action: upsert Notice, if you try the sample of istio Prometheus Kubernetes configuration, the issues described here might block you. Try to use the solution indicated in this issue if it\u0026rsquo;s not fixed.\nOpenCensus exporter Follow OpenCensus exporter configuration to set up a connection between OpenTelemetry collector and OAP cluster. endpoint is the address of OAP gRPC service.\nObserve Istio Open Istio Dashboard in SkyWaling UI by clicking Dashboard -\u0026gt; Istio, then you\u0026rsquo;re able to view charts and diagrams generated by Istio metrics. You also could view them by swctl and set up alarm rules based on them.\nNOTICE, if you want metrics of Istio managed services, including topology among them, we recommend you to consider our ALS solution\n","excerpt":"Work with Istio Instructions for transport Istio\u0026rsquo;s metrics to the SkyWalking OAP server. …","ref":"/docs/main/v8.3.0/en/setup/istio/readme/","title":"Work with Istio"},{"body":"Advanced deployment OAP servers inter communicate with each other in a cluster environment. In the cluster mode, you could run in different roles.\n Mixed(default) Receiver Aggregator  In some time, users want to deploy cluster nodes with explicit role. Then could use this.\nMixed Default role, the OAP should take responsibilities of\n Receive agent traces or metrics. Do L1 aggregation Internal communication(send/receive) Do L2 aggregation Persistence Alarm  Receiver The OAP should take responsibilities of\n Receive agent traces or metrics. Do L1 aggregation Internal communication(send)  Aggregator The OAP should take responsibilities of\n Internal communication(receive) Do L2 aggregation Persistence Alarm   These roles are designed for complex deployment requirements based on security and network policy.\nKubernetes If you are using our native Kubernetes coordinator, the labelSelector setting is used for Aggregator choose rules. Choose the right OAP deployment based on your requirements.\n","excerpt":"Advanced deployment OAP servers inter communicate with each other in a cluster environment. In the …","ref":"/docs/main/v8.2.0/en/setup/backend/advanced-deployment/","title":"Advanced deployment"},{"body":"Alarm Alarm core is driven by a collection of rules, which are defined in config/alarm-settings.yml. There are three parts in alarm rule definition.\n Alarm rules. They define how metrics alarm should be triggered, what conditions should be considered. Webhooks. The list of web service endpoint, which should be called after the alarm is triggered. gRPCHook. The host and port of remote gRPC method, which should be called after the alarm is triggered.  Entity name Define the relation between scope and entity name.\n Service: Service name Instance: {Instance name} of {Service name} Endpoint: {Endpoint name} in {Service name} Database: Database service name Service Relation: {Source service name} to {Dest service name} Instance Relation: {Source instance name} of {Source service name} to {Dest instance name} of {Dest service name} Endpoint Relation: {Source endpoint name} in {Source Service name} to {Dest endpoint name} in {Dest service name}  Rules There are two types of rules, individual rule and composite rule, composite rule is the combination of individual rules\nIndividual rules Alarm rule is constituted by following keys\n Rule name. Unique name, show in alarm message. Must end with _rule. Metrics name. A.K.A. metrics name in oal script. Only long, double, int types are supported. See List of all potential metrics name. Include names. The following entity names are included in this rule. Please follow Entity name define. Exclude names. The following entity names are excluded in this rule. Please follow Entity name define. Include names regex. Provide a regex to include the entity names. If both setting the include name list and include name regex, both rules will take effect. Exclude names regex. Provide a regex to exclude the exclude names. If both setting the exclude name list and exclude name regex, both rules will take effect. Include labels. The following labels of the metric are included in this rule. Exclude labels. The following labels of the metric are excluded in this rule. Include labels regex. Provide a regex to include labels. If both setting the include label list and include label regex, both rules will take effect. Exclude labels regex. Provide a regex to exclude labels. If both setting the exclude label list and exclude label regex, both rules will take effect.  The settings of labels is required by meter-system which intends to store metrics from label-system platform, just like Prometheus, Micrometer, etc. The function supports the above four settings should implement LabeledValueHolder.\n Threshold. The target value. For multiple values metrics, such as percentile, the threshold is an array. Described like value1, value2, value3, value4, value5. Each value could the threshold for each value of the metrics. Set the value to - if don\u0026rsquo;t want to trigger alarm by this or some of the values.\nSuch as in percentile, value1 is threshold of P50, and -, -, value3, value4, value5 means, there is no threshold for P50 and P75 in percentile alarm rule. OP. Operator, support \u0026gt;, \u0026gt;=, \u0026lt;, \u0026lt;=, =. Welcome to contribute all OPs. Period. How long should the alarm rule should be checked. This is a time window, which goes with the backend deployment env time. Count. In the period window, if the number of values over threshold(by OP), reaches count, alarm should send. Only as condition. Specify if the rule can send notification or just as an condition of composite rule. Silence period. After alarm is triggered in Time-N, then keep silence in the TN -\u0026gt; TN + period. By default, it is as same as Period, which means in a period, same alarm(same ID in same metrics name) will be trigger once.  Composite rules NOTE. Composite rules only work for alarm rules targeting the same entity level, such as alarm rules of the service level. For example, service_percent_rule \u0026amp;\u0026amp; service_resp_time_percentile_rule. You shouldn\u0026rsquo;t compose alarm rules of different entity levels. such as one alarm rule of the service metrics with another rule of the endpoint metrics.\nComposite rule is constituted by the following keys\n Rule name. Unique name, show in alarm message. Must end with _rule. Expression. Specify how to compose rules, support \u0026amp;\u0026amp;, ||, (). Message. Specify the notification message when rule triggered.  rules: # Rule unique name, must be ended with `_rule`. endpoint_percent_rule: # Metrics value need to be long, double or int metrics-name: endpoint_percent threshold: 75 op: \u0026lt; # The length of time to evaluate the metrics period: 10 # How many times after the metrics match the condition, will trigger alarm count: 3 # How many times of checks, the alarm keeps silence after alarm triggered, default as same as period. silence-period: 10 # Specify if the rule can send notification or just as an condition of composite rule only-as-condition: false service_percent_rule: metrics-name: service_percent # [Optional] Default, match all services in this metrics include-names: - service_a - service_b exclude-names: - service_c # Single value metrics threshold. threshold: 85 op: \u0026lt; period: 10 count: 4 only-as-condition: false service_resp_time_percentile_rule: # Metrics value need to be long, double or int metrics-name: service_percentile op: \u0026#34;\u0026gt;\u0026#34; # Multiple value metrics threshold. Thresholds for P50, P75, P90, P95, P99. threshold: 1000,1000,1000,1000,1000 period: 10 count: 3 silence-period: 5 message: Percentile response time of service {name} alarm in 3 minutes of last 10 minutes, due to more than one condition of p50 \u0026gt; 1000, p75 \u0026gt; 1000, p90 \u0026gt; 1000, p95 \u0026gt; 1000, p99 \u0026gt; 1000 only-as-condition: false meter_service_status_code_rule: metrics-name: meter_status_code exclude-labels: - \u0026#34;200\u0026#34; op: \u0026#34;\u0026gt;\u0026#34; threshold: 10 period: 10 count: 3 silence-period: 5 message: The request number of entity {name} non-200 status is more than expected. only-as-condition: false composite-rules: comp_rule: # Must satisfied percent rule and resp time rule  expression: service_percent_rule \u0026amp;\u0026amp; service_resp_time_percentile_rule message: Service {name} successful rate is less than 80% and P50 of response time is over 1000ms Default alarm rules We provided a default alarm-setting.yml in our distribution only for convenience, which including following rules\n Service average response time over 1s in last 3 minutes. Service success rate lower than 80% in last 2 minutes. Percentile of service response time is over 1s in last 3 minutes Service Instance average response time over 1s in last 2 minutes, and the instance name matches the regex. Endpoint average response time over 1s in last 2 minutes. Database access average response time over 1s in last 2 minutes. Endpoint relation average response time over 1s in last 2 minutes.  List of all potential metrics name The metrics names are defined in official OAL scripts, right now metrics from Service, Service Instance, Endpoint, Service Relation, Service Instance Relation, Endpoint Relation scopes could be used in Alarm, and the Database access same with Service scope.\nSubmit issue or pull request if you want to support any other scope in alarm.\nWebhook Webhook requires the peer is a web container. The alarm message will send through HTTP post by application/json content type. The JSON format is based on List\u0026lt;org.apache.skywalking.oap.server.core.alarm.AlarmMessage\u0026gt; with following key information.\n scopeId, scope. All scopes are defined in org.apache.skywalking.oap.server.core.source.DefaultScopeDefine. name. Target scope entity name. Please follow Entity name define. id0. The ID of the scope entity matched the name. When using relation scope, it is the source entity ID. id1. When using relation scope, it will be the dest entity ID. Otherwise, it is empty. ruleName. The rule name you configured in alarm-settings.yml. alarmMessage. Alarm text message. startTime. Alarm time measured in milliseconds, between the current time and midnight, January 1, 1970 UTC.  Example as following\n[{ \u0026#34;scopeId\u0026#34;: 1, \u0026#34;scope\u0026#34;: \u0026#34;SERVICE\u0026#34;, \u0026#34;name\u0026#34;: \u0026#34;serviceA\u0026#34;, \u0026#34;id0\u0026#34;: \u0026#34;12\u0026#34;, \u0026#34;id1\u0026#34;: \u0026#34;\u0026#34;, \u0026#34;ruleName\u0026#34;: \u0026#34;service_resp_time_rule\u0026#34;, \u0026#34;alarmMessage\u0026#34;: \u0026#34;alarmMessage xxxx\u0026#34;, \u0026#34;startTime\u0026#34;: 1560524171000 }, { \u0026#34;scopeId\u0026#34;: 1, \u0026#34;scope\u0026#34;: \u0026#34;SERVICE\u0026#34;, \u0026#34;name\u0026#34;: \u0026#34;serviceB\u0026#34;, \u0026#34;id0\u0026#34;: \u0026#34;23\u0026#34;, \u0026#34;id1\u0026#34;: \u0026#34;\u0026#34;, \u0026#34;ruleName\u0026#34;: \u0026#34;service_resp_time_rule\u0026#34;, \u0026#34;alarmMessage\u0026#34;: \u0026#34;alarmMessage yyy\u0026#34;, \u0026#34;startTime\u0026#34;: 1560524171000 }] gRPCHook The alarm message will send through remote gRPC method by Protobuf content type. The message format with following key information which are defined in oap-server/server-alarm-plugin/src/main/proto/alarm-hook.proto.\nPart of protocol looks as following:\nmessage AlarmMessage { int64 scopeId = 1; string scope = 2; string name = 3; string id0 = 4; string id1 = 5; string ruleName = 6; string alarmMessage = 7; int64 startTime = 8;}Slack Chat Hook To do this you need to follow the Getting Started with Incoming Webhooks guide and create new Webhooks.\nThe alarm message will send through HTTP post by application/json content type if you configured Slack Incoming Webhooks as following:\nslackHooks: textTemplate: |-{ \u0026#34;type\u0026#34;: \u0026#34;section\u0026#34;, \u0026#34;text\u0026#34;: { \u0026#34;type\u0026#34;: \u0026#34;mrkdwn\u0026#34;, \u0026#34;text\u0026#34;: \u0026#34;:alarm_clock: *Apache Skywalking Alarm* \\n **%s**.\u0026#34; } } webhooks: - https://hooks.slack.com/services/x/y/z WeChat Hook Note, only WeCom(WeChat Company Edition) supports webhook. To use the WeChat webhook you need to follow the Wechat Webhooks guide. The alarm message would send through HTTP post by application/json content type after you set up Wechat Webhooks as following:\nwechatHooks: textTemplate: |-{ \u0026#34;msgtype\u0026#34;: \u0026#34;text\u0026#34;, \u0026#34;text\u0026#34;: { \u0026#34;content\u0026#34;: \u0026#34;Apache SkyWalking Alarm: \\n %s.\u0026#34; } } webhooks: - https://qyapi.weixin.qq.com/cgi-bin/webhook/send?key=dummy_key Dingtalk Hook To do this you need to follow the Dingtalk Webhooks guide and create new Webhooks. For security issue, you can config optional secret for individual webhook url. The alarm message will send through HTTP post by application/json content type if you configured Dingtalk Webhooks as following:\ndingtalkHooks: textTemplate: |-{ \u0026#34;msgtype\u0026#34;: \u0026#34;text\u0026#34;, \u0026#34;text\u0026#34;: { \u0026#34;content\u0026#34;: \u0026#34;Apache SkyWalking Alarm: \\n %s.\u0026#34; } } webhooks: - url: https://oapi.dingtalk.com/robot/send?access_token=dummy_token secret: dummysecret Update the settings dynamically Since 6.5.0, the alarm settings can be updated dynamically at runtime by Dynamic Configuration, which will override the settings in alarm-settings.yml.\nIn order to determine that whether an alarm rule is triggered or not, SkyWalking needs to cache the metrics of a time window for each alarm rule, if any attribute (metrics-name, op, threshold, period, count, etc.) of a rule is changed, the sliding window will be destroyed and re-created, causing the alarm of this specific rule to restart again.\n","excerpt":"Alarm Alarm core is driven by a collection of rules, which are defined in config/alarm-settings.yml. …","ref":"/docs/main/v8.2.0/en/setup/backend/backend-alarm/","title":"Alarm"},{"body":"Apache SkyWalking committer SkyWalking Project Management Committee(PMC) takes the responsibilities to assess the contributions of candidates.\nIn the SkyWalking, like many Apache projects, we treat contributions including, but not limited to, code contributions. Such as writing blog, guiding new users, give public speak, prompting project in various ways, are all treated as significant contributions.\nCommitter New committer nomination In the SkyWalking, new committer nomination could only be started by existing PMC members officially. The new contributor could contact any existing PMC member if he/she feels he/she is qualified. Talk with the PMC member, if some members agree, they could start the process.\nThe following steps are recommended, and could only be started by existing PMC member.\n Send the [DISCUSS] Promote xxx as new committer mail to private@skywalking.a.o. List the important contributions of the candidates, in order to help the PMC members supporting your proposal. Keep discussion open in more than 3 days, but not more than 1 week, unless there is any explicit objection or concern. Send the [VOTE] Promote xxx as new committer mail to private@skywalking.a.o, when the PMC seems to agree the proposal. Keep vote more than 3 days, but not more than 1 week. Consider the result as Consensus Approval if there 3 +1 votes and +1 votes \u0026gt; -1 votes Send the [RESULT][VOTE] Promote xxx as new committer mail to private@skywalking.a.o, and list the vote detail including the voters.  Invite new committer The PMC member, who start the promotion, takes the responsibilities to send the invitation to new committer and guide him/her to set up the ASF env.\nYou should send the mail like the following template to new committer\nTo: JoeBloggs@foo.net Cc: private@skywalking.apache.org Subject: Invitation to become SkyWalking committer: Joe Bloggs Hello [invitee name], The SkyWalking Project Management Committee] (PMC) hereby offers you committer privileges to the project . These privileges are offered on the understanding that you'll use them reasonably and with common sense. We like to work on trust rather than unnecessary constraints. Being a committer enables you to more easily make changes without needing to go through the patch submission process. Being a committer does not require you to participate any more than you already do. It does tend to make one even more committed. You will probably find that you spend more time here. Of course, you can decline and instead remain as a contributor, participating as you do now. A. This personal invitation is a chance for you to accept or decline in private. Either way, please let us know in reply to the [private@skywalking.apache.org] address only. B. If you accept, the next step is to register an iCLA: 1. Details of the iCLA and the forms are found through this link: http://www.apache.org/licenses/#clas 2. Instructions for its completion and return to the Secretary of the ASF are found at http://www.apache.org/licenses/#submitting 3. When you transmit the completed iCLA, request to notify the Apache SkyWalking and choose a unique Apache id. Look to see if your preferred id is already taken at http://people.apache.org/committer-index.html This will allow the Secretary to notify the PMC when your iCLA has been recorded. When recording of your iCLA is noticed, you will receive a follow-up message with the next steps for establishing you as a committer. Invitation acceptance process And the new committer should reply the mail to private@skywalking.apache.org(Choose reply all), and express the will to accept the invitation explicitly. Then this invitation will be treated as accepted by project PMC. Of course, the new committer could just say NO, and reject the invitation.\nIf they accepted, then they need to do the following things.\n Make sure they have subscribed the dev@skywalking.apache.org. Usually they already have. Choose a Apache ID that is not in the apache committers list page. Download the ICLA (If they are going to contribute to the project as day job, CCLA is expected). After filling the icla.pdf (or ccla.pdf) with information correctly, print, sign it manually (by hand), scan it as an pdf, and send it in mail as an attachment to the secretary@apache.org. (If they prefer to sign electronically, please follow the steps of this page) Then the PMC will wait the Apache secretary confirmed the ICLA (or CCLA) filed. The new committer and PMC will receive the mail like following  Dear XXX, This message acknowledges receipt of your ICLA, which has been filed in the Apache Software Foundation records. Your account has been requested for you and you should receive email with next steps within the next few days (can take up to a week). Please refer to https://www.apache.org/foundation/how-it-works.html#developers for more information about roles at Apache. If in some case, the account has not be requested(rarely to see), the PMC member should contact the project V.P.. The V.P. could request through the Apache Account Submission Helper Form.\nAfter several days, the new committer will receive the account created mail, as this title, Welcome to the Apache Software Foundation (ASF)!. At this point, congratulate! You have the official Apache ID.\nThe PMC member should add the new committer to official committer list through roster.\nSet up the Apache ID and dev env  Go to Apache Account Utility Platform, initial your password, set up your personal mailbox(Forwarding email address) and GitHub account(Your GitHub Username). An organisational invite will be sent to you via email shortly thereafter (within 2 hours). If you want to use xxx@apache.org to send mail, please refer to here. Gmail is recommended, because in other mailbox service settings, this forwarding mode is not easy to find. Following the authorized GitHub 2FA wiki to enable two-factors authorization (2FA) on github. When you set 2FA to \u0026ldquo;off\u0026rdquo;, it will be delisted by the corresponding Apache committer write permission group until you set it up again. (NOTE: Treat your recovery codes with the same level of attention as you would your password !) Use GitBox Account Linking Utility to obtain write permission of the SkyWalking project. Follow this doc to update the website.  If you want others could see you are in the Apache GitHub org, you need to go to Apache GitHub org people page, search for yourself, and choose Organization visibility to Public.\nCommitter rights, duties and responsibilities SkyWalking project doesn\u0026rsquo;t require the continue contributions after you become a committer, but we hope and truly want you could.\nBeing a committer, you could\n Review and merge the pull request to the master branch in the Apache repo. A pull request often contains multiple commits. Those commits must be squashed and merged into a single commit with explanatory comments. For new committer, we hope you could request some senior committer to recheck the pull request. Create and push codes to new branch in the Apache repo. Follow the Release process to process new release. Of course, you need to ask committer team to confirm it is the right time to release.  The PMC hope the new committer to take part in the release and release vote, even still be consider +1 no binding. But be familiar with the release is one of the key to be promoted as a PMC member.\nProject Management Committee Project Management Committee(PMC) member has no special rights in code contributions. They just cover and make sure the project following the Apache requirement, including\n Release binding vote and license check New committer and PMC member recognition Identify branding issue and do branding protection. Response the ASF board question, take necessary actions.  V.P. and chair of the PMC is the secretary, take responsibility of initializing the board report.\nIn the normal case, the new PMC member should be nominated from committer team. But becoming a PMC member directly is not forbidden, if the PMC could agree and be confidence that the candidate is ready, such as he/she has been a PMC member of another project, Apache member or Apache officer.\nThe process of new PMC vote should also follow the same [DISCUSS], [VOTE] and [RESULT][VOTE] in private mail list as new committer vote. One more step before sending the invitation, the PMC need to send NOTICE mail to Apache board.\nTo: board@apache.org Cc: private@skywalking.apache.org Subject: [NOTICE] Jane Doe for SkyWalking PMC SkyWalking proposes to invite Jane Doe (janedoe) to join the PMC. (include if a vote was held) The vote result is available here: https://lists.apache.org/... After 72 hours, if the board doesn\u0026rsquo;t object(usually it wouldn\u0026rsquo;t be), send the invitation.\nAfter the committer accepted the invitation, The PMC member should add the new committer to official PMC list through roster.\n","excerpt":"Apache SkyWalking committer SkyWalking Project Management Committee(PMC) takes the responsibilities …","ref":"/docs/main/v8.2.0/en/guides/asf/committer/","title":"Apache SkyWalking committer"},{"body":"Apdex threshold Apdex is a measure of response time based against a set threshold. It measures the ratio of satisfactory response times to unsatisfactory response times. The response time is measured from an asset request to completed delivery back to the requestor.\nA user defines a response time threshold T. All responses handled in T or less time satisfy the user.\nFor example, if T is 1.2 seconds and a response completes in 0.5 seconds, then the user is satisfied. All responses greater than 1.2 seconds dissatisfy the user. Responses greater than 4.8 seconds frustrate the user.\nThe apdex threshold T can be configured in service-apdex-threshold.yml file or via Dynamic Configuration. The default item will be apply to a service isn\u0026rsquo;t defined in this configuration as the default threshold.\nConfiguration Format The configuration content includes the service' names and their threshold:\n# default threshold is 500ms default: 500 # example: # the threshold of service \u0026#34;tomcat\u0026#34; is 1s # tomcat: 1000 # the threshold of service \u0026#34;springboot1\u0026#34; is 50ms # springboot1: 50 ","excerpt":"Apdex threshold Apdex is a measure of response time based against a set threshold. It measures the …","ref":"/docs/main/v8.2.0/en/setup/backend/apdex-threshold/","title":"Apdex threshold"},{"body":"Backend setup First and most important thing is, SkyWalking backend startup behaviours are driven by config/application.yml. Understood the setting file will help you to read this document.\nStartup script The default startup scripts are /bin/oapService.sh(.bat). Read start up mode document to know other options of starting backend.\napplication.yml The core concept behind this setting file is, SkyWalking collector is based on pure modularization design. End user can switch or assemble the collector features by their own requirements.\nSo, in application.yml, there are three levels.\n Level 1, module name. Meaning this module is active in running mode. Level 2, provider option list and provider selector. Available providers are listed here with a selector to indicate which one will actually take effect, if there is only one provider listed, the selector is optional and can be omitted. Level 3. settings of the provider.  Example:\nstorage: selector: mysql # the mysql storage will actually be activated, while the h2 storage takes no effect h2: driver: ${SW_STORAGE_H2_DRIVER:org.h2.jdbcx.JdbcDataSource} url: ${SW_STORAGE_H2_URL:jdbc:h2:mem:skywalking-oap-db} user: ${SW_STORAGE_H2_USER:sa} metadataQueryMaxSize: ${SW_STORAGE_H2_QUERY_MAX_SIZE:5000} mysql: properties: jdbcUrl: ${SW_JDBC_URL:\u0026#34;jdbc:mysql://localhost:3306/swtest\u0026#34;} dataSource.user: ${SW_DATA_SOURCE_USER:root} dataSource.password: ${SW_DATA_SOURCE_PASSWORD:root@1234} dataSource.cachePrepStmts: ${SW_DATA_SOURCE_CACHE_PREP_STMTS:true} dataSource.prepStmtCacheSize: ${SW_DATA_SOURCE_PREP_STMT_CACHE_SQL_SIZE:250} dataSource.prepStmtCacheSqlLimit: ${SW_DATA_SOURCE_PREP_STMT_CACHE_SQL_LIMIT:2048} dataSource.useServerPrepStmts: ${SW_DATA_SOURCE_USE_SERVER_PREP_STMTS:true} metadataQueryMaxSize: ${SW_STORAGE_MYSQL_QUERY_MAX_SIZE:5000} # other configurations  storage is the module. selector selects one out of the all providers listed below, the unselected ones take no effect as if they were deleted. default is the default implementor of core module. driver, url, \u0026hellip; metadataQueryMaxSize are all setting items of the implementor.  At the same time, modules includes required and optional, the required modules provide the skeleton of backend, even modularization supported pluggable, removing those modules are meaningless, for optional modules, some of them have a provider implementation called none, meaning it only provides a shell with no actual logic, typically such as telemetry. Setting - to the selector means this whole module will be excluded at runtime. We highly recommend you don\u0026rsquo;t try to change APIs of those modules, unless you understand SkyWalking project and its codes very well.\nList the required modules here\n Core. Do basic and major skeleton of all data analysis and stream dispatch. Cluster. Manage multiple backend instances in a cluster, which could provide high throughputs process capabilities. Storage. Make the analysis result persistence. Query. Provide query interfaces to UI.  For Cluster and Storage have provided multiple implementors(providers), see Cluster management and Choose storage documents in the link list.\nAlso, several receiver modules are provided. Receiver is the module in charge of accepting incoming data requests to backend. Most(all) provide service by some network(RPC) protocol, such as gRPC, HTTPRestful.\nThe receivers have many different module names, you could read Set receivers document in the link list.\nConfiguration Vocabulary All available configurations in application.yml could be found in Configuration Vocabulary.\nAdvanced feature document link list After understand the setting file structure, you could choose your interesting feature document. We recommend you to read the feature documents in our following order.\n Overriding settings in application.yml is supported IP and port setting. Introduce how IP and port set and be used. Backend init mode startup. How to init the environment and exit graciously. Read this before you try to initial a new cluster. Cluster management. Guide you to set backend server in cluster mode. Deploy in kubernetes. Guide you to build and use SkyWalking image, and deploy in k8s. Choose storage. As we know, in default quick start, backend is running with H2 DB. But clearly, it doesn\u0026rsquo;t fit the product env. In here, you could find what other choices do you have. Choose the ones you like, we are also welcome anyone to contribute new storage implementor. Set receivers. You could choose receivers by your requirements, most receivers are harmless, at least our default receivers are. You would set and active all receivers provided. Open fetchers. You could open different fetchers to read metrics from the target applications. These ones works like receivers, but in pulling mode, typically like Prometheus. Token authentication. You could add token authentication mechanisms to avoid OAP receiving untrusted data. Do trace sampling at backend. This sample keep the metrics accurate, only don\u0026rsquo;t save some of traces in storage based on rate. Follow slow DB statement threshold config document to understand that, how to detect the Slow database statements(including SQL statements) in your system. Official OAL scripts. As you known from our OAL introduction, most of backend analysis capabilities based on the scripts. Here is the description of official scripts, which helps you to understand which metrics data are in process, also could be used in alarm. Alarm. Alarm provides a time-series based check mechanism. You could set alarm rules targeting the analysis oal metrics objects. Advanced deployment options. If you want to deploy backend in very large scale and support high payload, you may need this. Metrics exporter. Use metrics data exporter to forward metrics data to 3rd party system. Time To Live (TTL). Metrics and trace are time series data, TTL settings affect the expired time of them. Dynamic Configuration. Make configuration of OAP changed dynamic, from remote service or 3rd party configuration management system. Uninstrumented Gateways. Configure gateways/proxies that are not supported by SkyWalking agent plugins, to reflect the delegation in topology graph. Apdex threshold. Configure the thresholds for different services if Apdex calculation is activated in the OAL. Group Parameterized Endpoints. Configure the grouping rules for parameterized endpoints, to improve the meaning of the metrics. Spring Sleuth Metrics Analysis. Configure the agent and backend to receiver metrics from micrometer.  Telemetry for backend OAP backend cluster itself underlying is a distributed streaming process system. For helping the Ops team, we provide the telemetry for OAP backend itself. Follow document to use it.\nAt the same time, we provide Health Check to get a score for the health status.\n 0 means healthy, more than 0 means unhealthy and less than 0 means oap doesn\u0026rsquo;t startup.\n FAQs When and why do we need to set Timezone? SkyWalking provides downsampling time series metrics features. Query and storage at each time dimension(minute, hour, day, month metrics indexes) related to timezone when doing time format.\nFor example, metrics time will be formatted like YYYYMMDDHHmm in minute dimension metrics, which format process is timezone related.\nIn default, SkyWalking OAP backend choose the OS default timezone. If you want to override it, please follow Java and OS documents to do so.\nHow to query the storage directly from 3rd party tool? SkyWalking provides browser UI, CLI and GraphQL ways to support extensions. But some users may have the idea to query data directly from the storage. Such as in ElasticSearch case, Kibana is a great tool to do this.\nIn default, due to reduce memory, network and storage space usages, SkyWalking saves based64-encoded id(s) only in the metrics entities. But these tools usually don\u0026rsquo;t support nested query, or don\u0026rsquo;t work conveniently. In this special case, SkyWalking provide a config to add all necessary name column(s) into the final metrics entities with ID as a trade-off.\nTake a look at core/default/activeExtraModelColumns config in the application.yaml, and set it as true to open this feature.\nThis feature wouldn\u0026rsquo;t provide any new feature to the native SkyWalking scenarios, just for the 3rd party integration.\n","excerpt":"Backend setup First and most important thing is, SkyWalking backend startup behaviours are driven by …","ref":"/docs/main/v8.2.0/en/setup/backend/backend-setup/","title":"Backend setup"},{"body":"Backend storage SkyWalking storage is pluggable, we have provided the following storage solutions, you could easily use one of them by specifying it as the selector in the application.yml：\nstorage: selector: ${SW_STORAGE:elasticsearch7} Native supported storage\n H2 ElasticSearch 6, 7 MySQL TiDB InfluxDB  Redistribution version with supported storage.\n ElasticSearch 5  H2 Active H2 as storage, set storage provider to H2 In-Memory Databases. Default in distribution package. Please read Database URL Overview in H2 official document, you could set the target to H2 in Embedded, Server and Mixed modes.\nSetting fragment example\nstorage: selector: ${SW_STORAGE:h2} h2: driver: org.h2.jdbcx.JdbcDataSource url: jdbc:h2:mem:skywalking-oap-db user: sa ElasticSearch  In order to activate ElasticSearch 6 as storage, set storage provider to elasticsearch In order to activate ElasticSearch 7 as storage, set storage provider to elasticsearch7  Required ElasticSearch 6.3.2 or higher. HTTP RestHighLevelClient is used to connect server.\n For ElasticSearch 6.3.2 ~ 7.0.0 (excluded), please download the apache-skywalking-bin.tar.gz or apache-skywalking-bin.zip, For ElasticSearch 7.0.0 ~ 8.0.0 (excluded), please download the apache-skywalking-bin-es7.tar.gz or apache-skywalking-bin-es7.zip.  For now, ElasticSearch 6 and ElasticSearch 7 share the same configurations, as follows:\nstorage: selector: ${SW_STORAGE:elasticsearch} elasticsearch: nameSpace: ${SW_NAMESPACE:\u0026#34;\u0026#34;} clusterNodes: ${SW_STORAGE_ES_CLUSTER_NODES:localhost:9200} protocol: ${SW_STORAGE_ES_HTTP_PROTOCOL:\u0026#34;http\u0026#34;} trustStorePath: ${SW_STORAGE_ES_SSL_JKS_PATH:\u0026#34;\u0026#34;} trustStorePass: ${SW_STORAGE_ES_SSL_JKS_PASS:\u0026#34;\u0026#34;} user: ${SW_ES_USER:\u0026#34;\u0026#34;} password: ${SW_ES_PASSWORD:\u0026#34;\u0026#34;} secretsManagementFile: ${SW_ES_SECRETS_MANAGEMENT_FILE:\u0026#34;\u0026#34;} # Secrets management file in the properties format includes the username, password, which are managed by 3rd party tool. dayStep: ${SW_STORAGE_DAY_STEP:1} # Represent the number of days in the one minute/hour/day index. indexShardsNumber: ${SW_STORAGE_ES_INDEX_SHARDS_NUMBER:1} # Shard number of new indexes indexReplicasNumber: ${SW_STORAGE_ES_INDEX_REPLICAS_NUMBER:1} # Replicas number of new indexes # Super data set has been defined in the codes, such as trace segments.The following 3 config would be improve es performance when storage super size data in es. superDatasetDayStep: ${SW_SUPERDATASET_STORAGE_DAY_STEP:-1} # Represent the number of days in the super size dataset record index, the default value is the same as dayStep when the value is less than 0 superDatasetIndexShardsFactor: ${SW_STORAGE_ES_SUPER_DATASET_INDEX_SHARDS_FACTOR:5} # This factor provides more shards for the super data set, shards number = indexShardsNumber * superDatasetIndexShardsFactor. Also, this factor effects Zipkin and Jaeger traces. superDatasetIndexReplicasNumber: ${SW_STORAGE_ES_SUPER_DATASET_INDEX_REPLICAS_NUMBER:0} # Represent the replicas number in the super size dataset record index, the default value is 0. bulkActions: ${SW_STORAGE_ES_BULK_ACTIONS:1000} # Execute the async bulk record data every ${SW_STORAGE_ES_BULK_ACTIONS} requests syncBulkActions: ${SW_STORAGE_ES_SYNC_BULK_ACTIONS:50000} # Execute the sync bulk metrics data every ${SW_STORAGE_ES_SYNC_BULK_ACTIONS} requests flushInterval: ${SW_STORAGE_ES_FLUSH_INTERVAL:10} # flush the bulk every 10 seconds whatever the number of requests concurrentRequests: ${SW_STORAGE_ES_CONCURRENT_REQUESTS:2} # the number of concurrent requests resultWindowMaxSize: ${SW_STORAGE_ES_QUERY_MAX_WINDOW_SIZE:10000} metadataQueryMaxSize: ${SW_STORAGE_ES_QUERY_MAX_SIZE:5000} segmentQueryMaxSize: ${SW_STORAGE_ES_QUERY_SEGMENT_SIZE:200} profileTaskQueryMaxSize: ${SW_STORAGE_ES_QUERY_PROFILE_TASK_SIZE:200} advanced: ${SW_STORAGE_ES_ADVANCED:\u0026#34;\u0026#34;} In order to use ElasticSearch 7, comment/remove the section storage/elasticsearch and find the corresponding config section(storage/elasticsearch7), uncomment to enable it.\nElasticSearch 6 With Https SSL Encrypting communications. example:\nstorage: selector: ${SW_STORAGE:elasticsearch} elasticsearch: # nameSpace: ${SW_NAMESPACE:\u0026#34;\u0026#34;} user: ${SW_ES_USER:\u0026#34;\u0026#34;} # User needs to be set when Http Basic authentication is enabled password: ${SW_ES_PASSWORD:\u0026#34;\u0026#34;} # Password to be set when Http Basic authentication is enabled clusterNodes: ${SW_STORAGE_ES_CLUSTER_NODES:localhost:443} trustStorePath: ${SW_SW_STORAGE_ES_SSL_JKS_PATH:\u0026#34;../es_keystore.jks\u0026#34;} trustStorePass: ${SW_SW_STORAGE_ES_SSL_JKS_PASS:\u0026#34;\u0026#34;} protocol: ${SW_STORAGE_ES_HTTP_PROTOCOL:\u0026#34;https\u0026#34;} ...  File at trustStorePath is being monitored, once it is changed, the ElasticSearch client will do reconnecting. trustStorePass could be changed on the runtime through Secrets Management File Of ElasticSearch Authentication.  Daily Index Step Daily index step(storage/elasticsearch/dayStep, default 1) represents the index creation period. In this period, several days(dayStep value)' metrics are saved.\nMostly, users don\u0026rsquo;t need to change the value manually. As SkyWalking is designed to observe large scale distributed system. But in some specific cases, users want to set a long TTL value, such as more than 60 days, but their ElasticSearch cluster isn\u0026rsquo;t powerful due to the low traffic in the production environment. This value could be increased to 5(or more), if users could make sure single one index could support these days(5 in this case) metrics and traces.\nSuch as, if dayStep == 11,\n data in [2000-01-01, 2000-01-11] will be merged into the index-20000101. data in [2000-01-12, 2000-01-22] will be merged into the index-20000112.  storage/elasticsearch/superDatasetDayStep override the storage/elasticsearch/dayStep if the value is positive. This would affect the record related entities, such as the trace segment. In some cases, the size of metrics is much less than the record(trace), this would help the shards balance in the ElasticSearch cluster.\nNOTICE, TTL deletion would be affected by these. You should set an extra more dayStep in your TTL. Such as you want to TTL == 30 days and dayStep == 10, you actually need to set TTL = 40;\nSecrets Management File Of ElasticSearch Authentication The value of secretsManagementFile should point to the secrets management file absolute path. The file includes username, password and JKS password of ElasticSearch server in the properties format.\nuser=xxx password=yyy trustStorePass=zzz The major difference between using user, password, trustStorePass configs in the application.yaml file is, the Secrets Management File is being watched by the OAP server. Once it is changed manually or through 3rd party tool, such as Vault, the storage provider will use the new username, password and JKS password to establish the connection and close the old one. If the information exist in the file, the user/password will be overrided.\nAdvanced Configurations For Elasticsearch Index You can add advanced configurations in JSON format to set ElasticSearch index settings by following ElasticSearch doc\nFor example, set translog settings:\nstorage: elasticsearch: # ...... advanced: ${SW_STORAGE_ES_ADVANCED:\u0026#34;{\\\u0026#34;index.translog.durability\\\u0026#34;:\\\u0026#34;request\\\u0026#34;,\\\u0026#34;index.translog.sync_interval\\\u0026#34;:\\\u0026#34;5s\\\u0026#34;}\u0026#34;} Recommended ElasticSearch server-side configurations You could add following config to elasticsearch.yml, set the value based on your env.\n# In tracing scenario, consider to set more than this at least. thread_pool.index.queue_size: 1000 # Only suitable for ElasticSearch 6 thread_pool.write.queue_size: 1000 # Suitable for ElasticSearch 6 and 7 # When you face query error at trace page, remember to check this. index.max_result_window: 1000000 We strongly advice you to read more about these configurations from ElasticSearch official document. This effects the performance of ElasticSearch very much.\nElasticSearch 6 with Zipkin trace extension This implementation shares most of elasticsearch, just extend to support zipkin span storage. It has all same configs.\nstorage: selector: ${SW_STORAGE:zipkin-elasticsearch} zipkin-elasticsearch: nameSpace: ${SW_NAMESPACE:\u0026#34;\u0026#34;} clusterNodes: ${SW_STORAGE_ES_CLUSTER_NODES:localhost:9200} protocol: ${SW_STORAGE_ES_HTTP_PROTOCOL:\u0026#34;http\u0026#34;} user: ${SW_ES_USER:\u0026#34;\u0026#34;} password: ${SW_ES_PASSWORD:\u0026#34;\u0026#34;} indexShardsNumber: ${SW_STORAGE_ES_INDEX_SHARDS_NUMBER:2} indexReplicasNumber: ${SW_STORAGE_ES_INDEX_REPLICAS_NUMBER:0} # Batch process setting, refer to https://www.elastic.co/guide/en/elasticsearch/client/java-api/5.5/java-docs-bulk-processor.html bulkActions: ${SW_STORAGE_ES_BULK_ACTIONS:2000} # Execute the bulk every 2000 requests bulkSize: ${SW_STORAGE_ES_BULK_SIZE:20} # flush the bulk every 20mb flushInterval: ${SW_STORAGE_ES_FLUSH_INTERVAL:10} # flush the bulk every 10 seconds whatever the number of requests concurrentRequests: ${SW_STORAGE_ES_CONCURRENT_REQUESTS:2} # the number of concurrent requests ElasticSearch 6 with Jaeger trace extension This implementation shares most of elasticsearch, just extend to support jaeger span storage. It has all same configs.\nstorage: selector: ${SW_STORAGE:jaeger-elasticsearch} jaeger-elasticsearch: nameSpace: ${SW_NAMESPACE:\u0026#34;\u0026#34;} clusterNodes: ${SW_STORAGE_ES_CLUSTER_NODES:localhost:9200} protocol: ${SW_STORAGE_ES_HTTP_PROTOCOL:\u0026#34;http\u0026#34;} user: ${SW_ES_USER:\u0026#34;\u0026#34;} password: ${SW_ES_PASSWORD:\u0026#34;\u0026#34;} indexShardsNumber: ${SW_STORAGE_ES_INDEX_SHARDS_NUMBER:2} indexReplicasNumber: ${SW_STORAGE_ES_INDEX_REPLICAS_NUMBER:0} # Batch process setting, refer to https://www.elastic.co/guide/en/elasticsearch/client/java-api/5.5/java-docs-bulk-processor.html bulkActions: ${SW_STORAGE_ES_BULK_ACTIONS:2000} # Execute the bulk every 2000 requests bulkSize: ${SW_STORAGE_ES_BULK_SIZE:20} # flush the bulk every 20mb flushInterval: ${SW_STORAGE_ES_FLUSH_INTERVAL:10} # flush the bulk every 10 seconds whatever the number of requests concurrentRequests: ${SW_STORAGE_ES_CONCURRENT_REQUESTS:2} # the number of concurrent requests About Namespace When namespace is set, names of all indexes in ElasticSearch will use it as prefix.\nMySQL Active MySQL as storage, set storage provider to mysql.\nNOTICE: MySQL driver is NOT allowed in Apache official distribution and source codes. Please download MySQL driver by yourself. Copy the connection driver jar to oap-libs.\nstorage: selector: ${SW_STORAGE:mysql} mysql: properties: jdbcUrl: ${SW_JDBC_URL:\u0026#34;jdbc:mysql://localhost:3306/swtest\u0026#34;} dataSource.user: ${SW_DATA_SOURCE_USER:root} dataSource.password: ${SW_DATA_SOURCE_PASSWORD:root@1234} dataSource.cachePrepStmts: ${SW_DATA_SOURCE_CACHE_PREP_STMTS:true} dataSource.prepStmtCacheSize: ${SW_DATA_SOURCE_PREP_STMT_CACHE_SQL_SIZE:250} dataSource.prepStmtCacheSqlLimit: ${SW_DATA_SOURCE_PREP_STMT_CACHE_SQL_LIMIT:2048} dataSource.useServerPrepStmts: ${SW_DATA_SOURCE_USE_SERVER_PREP_STMTS:true} metadataQueryMaxSize: ${SW_STORAGE_MYSQL_QUERY_MAX_SIZE:5000} All connection related settings including link url, username and password are in application.yml. Here are some of the settings, please follow HikariCP connection pool document for all the settings.\nTiDB Currently tested TiDB in version 2.0.9, and Mysql Client driver in version 8.0.13. Active TiDB as storage, set storage provider to mysql.\nstorage: selector: ${SW_STORAGE:mysql} mysql: properties: jdbcUrl: ${SW_JDBC_URL:\u0026#34;jdbc:mysql://localhost:3306/swtest\u0026#34;} dataSource.user: ${SW_DATA_SOURCE_USER:root} dataSource.password: ${SW_DATA_SOURCE_PASSWORD:root@1234} dataSource.cachePrepStmts: ${SW_DATA_SOURCE_CACHE_PREP_STMTS:true} dataSource.prepStmtCacheSize: ${SW_DATA_SOURCE_PREP_STMT_CACHE_SQL_SIZE:250} dataSource.prepStmtCacheSqlLimit: ${SW_DATA_SOURCE_PREP_STMT_CACHE_SQL_LIMIT:2048} dataSource.useServerPrepStmts: ${SW_DATA_SOURCE_USE_SERVER_PREP_STMTS:true} metadataQueryMaxSize: ${SW_STORAGE_MYSQL_QUERY_MAX_SIZE:5000} All connection related settings including link url, username and password are in application.yml. These settings can refer to the configuration of MySQL above.\nInfluxDB InfluxDB storage provides a time-series database as a new storage option.\nstorage: selector: ${SW_STORAGE:influxdb} influxdb: url: ${SW_STORAGE_INFLUXDB_URL:http://localhost:8086} user: ${SW_STORAGE_INFLUXDB_USER:root} password: ${SW_STORAGE_INFLUXDB_PASSWORD:} database: ${SW_STORAGE_INFLUXDB_DATABASE:skywalking} actions: ${SW_STORAGE_INFLUXDB_ACTIONS:1000} # the number of actions to collect duration: ${SW_STORAGE_INFLUXDB_DURATION:1000} # the time to wait at most (milliseconds) fetchTaskLogMaxSize: ${SW_STORAGE_INFLUXDB_FETCH_TASK_LOG_MAX_SIZE:5000} # the max number of fetch task log in a request All connection related settings including link url, username and password are in application.yml. The Metadata storage provider settings can refer to the configuration of H2/MySQL above.\nElasticSearch 5 ElasticSearch 5 is incompatible with ElasticSearch 6 Java client jar, so it could not be included in native distribution. OpenSkyWalking/SkyWalking-With-Es5x-Storage repo includes the distribution version.\nMore storage solution extension Follow Storage extension development guide in Project Extensions document in development guide.\n","excerpt":"Backend storage SkyWalking storage is pluggable, we have provided the following storage solutions, …","ref":"/docs/main/v8.2.0/en/setup/backend/backend-storage/","title":"Backend storage"},{"body":"Backend, UI, and CLI setup SkyWalking backend distribution package includes the following parts:\n  bin/cmd scripts, in /bin folder. Includes startup linux shell and Windows cmd scripts for Backend server and UI startup.\n  Backend config, in /config folder. Includes settings files of the backend, which are:\n application.yml log4j.xml alarm-settings.yml    Libraries of backend, in /oap-libs folder. All the dependencies of the backend are in it.\n  Webapp env, in webapp folder. UI frontend jar file is here, with its webapp.yml setting file.\n  Quick start Requirements and default settings Requirement: JDK8 to JDK12 are tested, other versions are not tested and may or may not work.\nBefore you start, you should know that the quickstart aims to get you a basic configuration mostly for previews/demo, performance and long-term running are not our goals.\nFor production/QA/tests environments, you should head to Backend and UI deployment documents.\nYou can use bin/startup.sh (or cmd) to startup the backend and UI with their default settings, which are:\n Backend storage uses H2 by default (for an easier start) Backend listens 0.0.0.0/11800 for gRPC APIs and 0.0.0.0/12800 for http rest APIs.  In Java, .NetCore, Node.js, Istio agents/probe, you should set the gRPC service address to ip/host:11800, with ip/host where your backend is.\n UI listens on 8080 port and request 127.0.0.1/12800 to do GraphQL query.  Deploy Backend and UI Before deploying Skywalking in your distributed environment, you should know how agents/probes, backend, UI communicates with each other:\n All native agents and probes, either language based or mesh probe, are using gRPC service (core/default/gRPC* in application.yml) to report data to the backend. Also, jetty service supported in JSON format. UI uses GraphQL (HTTP) query to access the backend also in Jetty service (core/default/rest* in application.yml).  Now, let\u0026rsquo;s continue with the backend, UI and CLI setting documents.\n Backend setup document UI setup document CLI set up document  ","excerpt":"Backend, UI, and CLI setup SkyWalking backend distribution package includes the following parts: …","ref":"/docs/main/v8.2.0/en/setup/backend/backend-ui-setup/","title":"Backend, UI, and CLI setup"},{"body":"Browser Protocol Browser protocol describes the data format between skywalking-client-js and backend.\nOverview Browser protocol is defined and provided in gRPC format, also implemented in HTTP 1.1\nSend performance data and error log You can send performance data and error logs via the following services:\n BrowserPerfService#collectPerfData for performance data format. BrowserPerfService#collectErrorLogs for error log format.  For error log format, there are some notices\n BrowserErrorLog#uniqueId should be unique in the whole distributed environments.  ","excerpt":"Browser Protocol Browser protocol describes the data format between skywalking-client-js and …","ref":"/docs/main/v8.2.0/en/protocols/browser-protocol/","title":"Browser Protocol"},{"body":"Choose receiver Receiver is a concept in SkyWalking backend. All modules, which are responsible for receiving telemetry or tracing data from other being monitored system, are all being called Receiver. If you are looking for the pull mode, Take a look at fetcher document.\nWe have following receivers, and default implementors are provided in our Apache distribution.\n receiver-trace. gRPC and HTTPRestful services to accept SkyWalking format traces. receiver-register. gRPC and HTTPRestful services to provide service, service instance and endpoint register. service-mesh. gRPC services accept data from inbound mesh probes. receiver-jvm. gRPC services accept JVM metrics data. istio-telemetry. Istio telemetry is from Istio official bypass adaptor, this receiver match its gRPC services. envoy-metric. Envoy metrics_service and ALS(access log service) supported by this receiver. OAL script support all GAUGE type metrics. receiver-profile. gRPC services accept profile task status and snapshot reporter. receiver_zipkin. See details. receiver_jaeger. See details. receiver-oc. See details. receiver-meter. See details. receiver-browser. gRPC services to accept browser performance data and error log.  The sample settings of these receivers should be already in default application.yml, and also list here\nreceiver-register: selector: ${SW_RECEIVER_REGISTER:default} default: receiver-trace: selector: ${SW_RECEIVER_TRACE:default} default: receiver-jvm: selector: ${SW_RECEIVER_JVM:default} default: service-mesh: selector: ${SW_SERVICE_MESH:default} default: istio-telemetry: selector: ${SW_ISTIO_TELEMETRY:default} default: envoy-metric: selector: ${SW_ENVOY_METRIC:default} default: acceptMetricsService: ${SW_ENVOY_METRIC_SERVICE:true} alsHTTPAnalysis: ${SW_ENVOY_METRIC_ALS_HTTP_ANALYSIS:\u0026#34;\u0026#34;} receiver_zipkin: selector: ${SW_RECEIVER_ZIPKIN:-} default: host: ${SW_RECEIVER_ZIPKIN_HOST:0.0.0.0} port: ${SW_RECEIVER_ZIPKIN_PORT:9411} contextPath: ${SW_RECEIVER_ZIPKIN_CONTEXT_PATH:/} jettyMinThreads: ${SW_RECEIVER_ZIPKIN_JETTY_MIN_THREADS:1} jettyMaxThreads: ${SW_RECEIVER_ZIPKIN_JETTY_MAX_THREADS:200} jettyIdleTimeOut: ${SW_RECEIVER_ZIPKIN_JETTY_IDLE_TIMEOUT:30000} jettyAcceptorPriorityDelta: ${SW_RECEIVER_ZIPKIN_JETTY_DELTA:0} jettyAcceptQueueSize: ${SW_RECEIVER_ZIPKIN_QUEUE_SIZE:0} receiver-profile: selector: ${SW_RECEIVER_PROFILE:default} default: receiver-browser: selector: ${SW_RECEIVER_BROWSER:default} default: sampleRate: ${SW_RECEIVER_BROWSER_SAMPLE_RATE:10000} gRPC/HTTP server for receiver In default, all gRPC/HTTP services should be served at core/gRPC and core/rest. But the receiver-sharing-server module provide a way to make all receivers serving at different ip:port, if you set them explicitly.\nreceiver-sharing-server: selector: ${SW_RECEIVER_SHARING_SERVER:default} default: host: ${SW_RECEIVER_JETTY_HOST:0.0.0.0} contextPath: ${SW_RECEIVER_JETTY_CONTEXT_PATH:/} authentication: ${SW_AUTHENTICATION:\u0026#34;\u0026#34;} jettyMinThreads: ${SW_RECEIVER_SHARING_JETTY_MIN_THREADS:1} jettyMaxThreads: ${SW_RECEIVER_SHARING_JETTY_MAX_THREADS:200} jettyIdleTimeOut: ${SW_RECEIVER_SHARING_JETTY_IDLE_TIMEOUT:30000} jettyAcceptorPriorityDelta: ${SW_RECEIVER_SHARING_JETTY_DELTA:0} jettyAcceptQueueSize: ${SW_RECEIVER_SHARING_JETTY_QUEUE_SIZE:0} Notice, if you add these settings, make sure they are not as same as core module, because gRPC/HTTP servers of core are still used for UI and OAP internal communications.\nZipkin receiver Zipkin receiver could work in two different mode.\n Tracing mode(default). Tracing mode is that, skywalking OAP acts like zipkin collector, fully supports Zipkin v1/v2 formats through HTTP service, also provide persistence and query in skywalking UI. But it wouldn\u0026rsquo;t analysis metrics from them. In most case, I suggest you could use this feature, when metrics come from service mesh. Notice, in this mode, Zipkin receiver requires zipkin-elasticsearch storage implementation active. Read this to know how to active.  Use following config to active.\nreceiver_zipkin: selector: ${SW_RECEIVER_ZIPKIN:-} default: host: ${SW_RECEIVER_ZIPKIN_HOST:0.0.0.0} port: ${SW_RECEIVER_ZIPKIN_PORT:9411} contextPath: ${SW_RECEIVER_ZIPKIN_CONTEXT_PATH:/} jettyMinThreads: ${SW_RECEIVER_ZIPKIN_JETTY_MIN_THREADS:1} jettyMaxThreads: ${SW_RECEIVER_ZIPKIN_JETTY_MAX_THREADS:200} jettyIdleTimeOut: ${SW_RECEIVER_ZIPKIN_JETTY_IDLE_TIMEOUT:30000} jettyAcceptorPriorityDelta: ${SW_RECEIVER_ZIPKIN_JETTY_DELTA:0} jettyAcceptQueueSize: ${SW_RECEIVER_ZIPKIN_QUEUE_SIZE:0} Analysis mode(Not production ready), receive Zipkin v1/v2 formats through HTTP service. Transform the trace to skywalking native format, and analysis like skywalking trace. This feature can\u0026rsquo;t work in production env right now, because of Zipkin tag/endpoint value unpredictable, we can\u0026rsquo;t make sure it fits production env requirements.  Active analysis mode, you should set needAnalysis config.\nreceiver_zipkin: selector: ${SW_RECEIVER_ZIPKIN:-} default: host: ${SW_RECEIVER_ZIPKIN_HOST:0.0.0.0} port: ${SW_RECEIVER_ZIPKIN_PORT:9411} contextPath: ${SW_RECEIVER_ZIPKIN_CONTEXT_PATH:/} jettyMinThreads: ${SW_RECEIVER_ZIPKIN_JETTY_MIN_THREADS:1} jettyMaxThreads: ${SW_RECEIVER_ZIPKIN_JETTY_MAX_THREADS:200} jettyIdleTimeOut: ${SW_RECEIVER_ZIPKIN_JETTY_IDLE_TIMEOUT:30000} jettyAcceptorPriorityDelta: ${SW_RECEIVER_ZIPKIN_JETTY_DELTA:0} jettyAcceptQueueSize: ${SW_RECEIVER_ZIPKIN_QUEUE_SIZE:0} needAnalysis: true NOTICE, Zipkin receiver is only provided in apache-skywalking-apm-x.y.z.tar.gz tar.\nJaeger receiver Jaeger receiver right now only works in Tracing Mode, and no analysis. Jaeger receiver provides extra gRPC host/port, if absent, sharing-server host/port will be used, then core gRPC host/port. Receiver requires jaeger-elasticsearch storage implementation active. Read this to know how to active.\nRight now, you need jaeger agent to batch send spans to SkyWalking oap server. Read Jaeger Architecture to get more details.\nActive the receiver.\nreceiver_jaeger: selector: ${SW_RECEIVER_JAEGER:-} default: gRPCHost: ${SW_RECEIVER_JAEGER_HOST:0.0.0.0} gRPCPort: ${SW_RECEIVER_JAEGER_PORT:14250} NOTICE, Jaeger receiver is only provided in apache-skywalking-apm-x.y.z.tar.gz tar.\nOpencensus receiver Opencensus receiver supports to ingest agent metrics by meter-system. OAP can load the configuration at bootstrap. If the new configuration is not well-formed, OAP fails to start up. The files are located at $CLASSPATH/oc-rules.\nThe file is written in YAML format, defined by the scheme described in prometheus-fetcher. Notice, receiver-oc only support metricsRules node of scheme due to the push mode it opts to.\nTo active the default implementation:\nreceiver-oc: selector: ${SW_OC_RECEIVER:-} default: gRPCHost: ${SW_OC_RECEIVER_GRPC_HOST:0.0.0.0} gRPCPort: ${SW_OC_RECEIVER_GRPC_PORT:55678} Meter receiver Meter receiver supports accept the metrics into the meter-system. OAP can load the configuration at bootstrap.\nThe file is written in YAML format, defined by the scheme described in backend-meter.\nTo active the default implementation:\nreceiver-meter: selector: ${SW_RECEIVER_METER:default} default: ","excerpt":"Choose receiver Receiver is a concept in SkyWalking backend. All modules, which are responsible for …","ref":"/docs/main/v8.2.0/en/setup/backend/backend-receivers/","title":"Choose receiver"},{"body":"Cluster Management In many product environments, backend needs to support high throughput and provides HA to keep robustness, so you should need cluster management always in product env.\nBackend provides several ways to do cluster management. Choose the one you need/want.\n Zookeeper coordinator. Use Zookeeper to let backend instance detects and communicates with each other. Kubernetes. When backend cluster are deployed inside kubernetes, you could choose this by using k8s native APIs to manage cluster. Consul. Use Consul as backend cluster management implementor, to coordinate backend instances. Etcd. Use Etcd to coordinate backend instances. Nacos. Use Nacos to coordinate backend instances. In the application.yml, there\u0026rsquo;re default configurations for the aforementioned coordinators under the section cluster, you can specify one of them in the selector property to enable it.  Zookeeper coordinator Zookeeper is a very common and wide used cluster coordinator. Set the cluster/selector to zookeeper in the yml to enable.\nRequired Zookeeper version, 3.4+\ncluster: selector: ${SW_CLUSTER:zookeeper} # other configurations  hostPort is the list of zookeeper servers. Format is IP1:PORT1,IP2:PORT2,...,IPn:PORTn enableACL enable Zookeeper ACL to control access to its znode. schema is Zookeeper ACL schemas. expression is a expression of ACL. The format of the expression is specific to the schema. hostPort, baseSleepTimeMs and maxRetries are settings of Zookeeper curator client.  Note:\n If Zookeeper ACL is enabled and /skywalking existed, must be sure SkyWalking has CREATE, READ and WRITE permissions. If /skywalking is not exists, it will be created by SkyWalking and grant all permissions to the specified user. Simultaneously, znode is granted READ to anyone. If set schema as digest, the password of expression is set in clear text.  In some cases, oap default gRPC host and port in core are not suitable for internal communication among the oap nodes. The following setting are provided to set the host and port manually, based on your own LAN env.\n internalComHost, the host registered and other oap node use this to communicate with current node. internalComPort, the port registered and other oap node use this to communicate with current node.  zookeeper: nameSpace: ${SW_NAMESPACE:\u0026#34;\u0026#34;} hostPort: ${SW_CLUSTER_ZK_HOST_PORT:localhost:2181} #Retry Policy baseSleepTimeMs: ${SW_CLUSTER_ZK_SLEEP_TIME:1000} # initial amount of time to wait between retries maxRetries: ${SW_CLUSTER_ZK_MAX_RETRIES:3} # max number of times to retry internalComHost: 172.10.4.10 internalComPort: 11800 # Enable ACL enableACL: ${SW_ZK_ENABLE_ACL:false} # disable ACL in default schema: ${SW_ZK_SCHEMA:digest} # only support digest schema expression: ${SW_ZK_EXPRESSION:skywalking:skywalking} Kubernetes Require backend cluster are deployed inside kubernetes, guides are in Deploy in kubernetes. Set the selector to kubernetes.\ncluster: selector: ${SW_CLUSTER:kubernetes} # other configurations Consul Now, consul is becoming a famous system, many of companies and developers using consul to be their service discovery solution. Set the cluster/selector to consul in the yml to enable.\ncluster: selector: ${SW_CLUSTER:consul} # other configurations Same as Zookeeper coordinator, in some cases, oap default gRPC host and port in core are not suitable for internal communication among the oap nodes. The following setting are provided to set the host and port manually, based on your own LAN env.\n internalComHost, the host registered and other oap node use this to communicate with current node. internalComPort, the port registered and other oap node use this to communicate with current node.  Etcd Set the cluster/selector to etcd in the yml to enable.\ncluster: selector: ${SW_CLUSTER:etcd} # other configurations Same as Zookeeper coordinator, in some cases, oap default gRPC host and port in core are not suitable for internal communication among the oap nodes. The following setting are provided to set the host and port manually, based on your own LAN env.\n internalComHost, the host registered and other oap node use this to communicate with current node. internalComPort, the port registered and other oap node use this to communicate with current node.  Nacos Set the cluster/selector to nacos in the yml to enable.\ncluster: selector: ${SW_CLUSTER:nacos} # other configurations Nacos support authenticate by username or accessKey, empty means no need auth. extra config is bellow:\nnacos: username: password: accessKey: secretKey: Same as Zookeeper coordinator, in some cases, oap default gRPC host and port in core are not suitable for internal communication among the oap nodes. The following setting are provided to set the host and port manually, based on your own LAN env.\n internalComHost, the host registered and other oap node use this to communicate with current node. internalComPort, the port registered and other oap node use this to communicate with current node.  ","excerpt":"Cluster Management In many product environments, backend needs to support high throughput and …","ref":"/docs/main/v8.2.0/en/setup/backend/backend-cluster/","title":"Cluster Management"},{"body":"Compatible with other javaagent bytecode processing Problem   When use skywalking agent, some other agent, such as Arthas, can\u0026rsquo;t work well https://github.com/apache/skywalking/pull/4858\n  Java agent retransforming class fails with Skywalking agent, such as in this demo\n  Reason SkyWalking agent uses ByteBuddy to transform classes when the Java application starts. ByteBuddy generates auxiliary classes with different random names every time.\nWhen another java agent retransforms the same class, it triggers the SkyWalking agent to enhance the class again. The bytecode regenerated by ByteBuddy is changed, the fields and imported class names are modified, the JVM verifications about class bytecode fail, causing the retransform fails.\nResolve 1.Enable the class cache feature\nAdd JVM parameters:\n-Dskywalking.agent.is_cache_enhanced_class=true -Dskywalking.agent.class_cache_mode=MEMORY\nOr uncomment options in agent.conf:\n# If true, SkyWalking agent will cache all instrumented classes files to memory or disk files (decided by class cache mode), # allow other javaagent to enhance those classes that enhanced by SkyWalking agent. agent.is_cache_enhanced_class = ${SW_AGENT_CACHE_CLASS:false} # The instrumented classes cache mode: MEMORY or FILE # MEMORY: cache class bytes to memory, if instrumented classes is too many or too large, it may take up more memory # FILE: cache class bytes to user temp folder starts with 'class-cache', automatically clean up cached class files when the application exits agent.class_cache_mode = ${SW_AGENT_CLASS_CACHE_MODE:MEMORY} If the class cache feature is enabled, save the instrumented class bytecode to memory or a temporary file. When other java agents retransform the same class, SkyWalking agent first attempts to load from the cache.\nIf the cached class is found, it will be used directly without regenerating a new random name auxiliary class, which will not affect the processing of the subsequent java agent.\n2.Class cache save mode\nIt is recommended to put the cache class in memory, meanwhile if it costs more memory resources. Another option is using the local file system. Set the class cache mode through the following options:\n-Dskywalking.agent.class_cache_mode=MEMORY : save cache classes to java memory. -Dskywalking.agent.class_cache_mode=FILE : save cache classes to SkyWalking agent path \u0026lsquo;/class-cache\u0026rsquo;.\nOr modify the option in agent.conf:\nagent.class_cache_mode = ${SW_AGENT_CLASS_CACHE_MODE:MEMORY} agent.class_cache_mode = ${SW_AGENT_CLASS_CACHE_MODE:FILE}\n","excerpt":"Compatible with other javaagent bytecode processing Problem   When use skywalking agent, some other …","ref":"/docs/main/v8.2.0/en/faq/compatible-with-other-javaagent-bytecode-processing/","title":"Compatible with other javaagent bytecode processing"},{"body":"Component library settings Component library settings are about your own or 3rd part libraries used in monitored application.\nIn agent or SDK, no matter library name collected as ID or String(literally, e.g. SpringMVC), collector formats data in ID for better performance and less storage requirements.\nAlso, collector conjectures the remote service based on the component library, such as: the component library is MySQL Driver library, then the remote service should be MySQL Server.\nFor those two reasons, collector require two parts of settings in this file:\n Component Library id, name and languages. Remote server mapping, based on local library.  All component names and IDs must be defined in this file.\nComponent Library id Define all component libraries' names and IDs, used in monitored application. This is a both-way mapping, agent or SDK could use the value(ID) to represent the component name in uplink data.\n Name: the component name used in agent and UI id: Unique ID. All IDs are reserved, once it is released. languages: Program languages may use this component. Multi languages should be separated by ,  ID rules  Java and multi languages shared: (0, 3000) .NET Platform reserved: [3000, 4000) Node.js Platform reserved: [4000, 5000) Go reserved: [5000, 6000) Lua reserved: [6000, 7000) Python reserved: [7000, 8000) PHP reserved: [8000, 9000) C++ reserved: [9000, 10000)  Example\nTomcat: id: 1 languages: Java HttpClient: id: 2 languages: Java,C#,Node.js Dubbo: id: 3 languages: Java H2: id: 4 languages: Java Remote server mapping Remote server will be conjectured by the local component. The mappings are based on Component library names.\n Key: client component library name Value: server component name  Component-Server-Mappings: Jedis: Redis StackExchange.Redis: Redis Redisson: Redis Lettuce: Redis Zookeeper: Zookeeper SqlClient: SqlServer Npgsql: PostgreSQL MySqlConnector: Mysql EntityFrameworkCore.InMemory: InMemoryDatabase ","excerpt":"Component library settings Component library settings are about your own or 3rd part libraries used …","ref":"/docs/main/v8.2.0/en/guides/component-library-settings/","title":"Component library settings"},{"body":"Concepts and Designs Concepts and Designs help you to learn and understand the SkyWalking and the landscape.\n What is SkyWalking?  Overview and Core concepts. Provides a high-level description and introduction, including the problems the project solves. Project Goals. Provides the goals, which SkyWalking is trying to focus and provide features about them.    After you read the above documents, you should understand the SkyWalking basic goals. Now, you can choose which following parts you are interested, then dive in.\n Probe  Introduction. Lead readers to understand what the probe is, how many different probes existed and why need them. Service auto instrument agent. Introduce what the auto instrument agents do and which languages does SkyWalking already support. Manual instrument SDK. Introduce the role of the manual instrument SDKs in SkyWalking ecosystem. Service Mesh probe. Introduce why and how SkyWalking receive telemetry data from Service mesh and proxy probe.   Backend  Overview. Provides a high level introduction about the OAP backend. Observability Analysis Language. Introduces the core languages, which is designed for aggregation behaviour definition. Query in OAP. A set of query protocol provided, based on the Observability Analysis Language metrics definition.   UI  Overview. A simple brief about SkyWalking UI.   CLI  SkyWalking CLI. A command line interface for SkyWalking.    ","excerpt":"Concepts and Designs Concepts and Designs help you to learn and understand the SkyWalking and the …","ref":"/docs/main/v8.2.0/en/concepts-and-designs/readme/","title":"Concepts and Designs"},{"body":"Configuration Vocabulary Configuration Vocabulary lists all available configurations provided by application.yml.\n   Module Provider Settings Value(s) and Explanation System Environment Variable¹ Default     core default role Option values, Mixed/Receiver/Aggregator. Receiver mode OAP open the service to the agents, analysis and aggregate the results and forward the results for distributed aggregation. Aggregator mode OAP receives data from Mixer and Receiver role OAP nodes, and do 2nd level aggregation. Mixer means being Receiver and Aggregator both. SW_CORE_ROLE Mixed   - - restHost Binding IP of restful service. Services include GraphQL query and HTTP data report SW_CORE_REST_HOST 0.0.0.0   - - restPort Binding port of restful service SW_CORE_REST_PORT 12800   - - restContextPath Web context path of restful service SW_CORE_REST_CONTEXT_PATH /   - - restMinThreads Min threads number of restful service SW_CORE_REST_JETTY_MIN_THREADS 1   - - restMaxThreads Max threads number of restful service SW_CORE_REST_JETTY_MAX_THREADS 200   - - restIdleTimeOut Connector idle timeout in milliseconds of restful service SW_CORE_REST_JETTY_IDLE_TIMEOUT 30000   - - restAcceptorPriorityDelta Thread priority delta to give to acceptor threads of restful service SW_CORE_REST_JETTY_DELTA 0   - - restAcceptQueueSize ServerSocketChannel backlog of restful service SW_CORE_REST_JETTY_QUEUE_SIZE 0   - - gRPCHost Binding IP of gRPC service. Services include gRPC data report and internal communication among OAP nodes SW_CORE_GRPC_HOST 0.0.0.0   - - gRPCPort Binding port of gRPC service SW_CORE_GRPC_PORT 11800   - - gRPCSslEnabled Activate SSL for gRPC service SW_CORE_GRPC_SSL_ENABLED false   - - gRPCSslKeyPath The file path of gRPC SSL key SW_CORE_GRPC_SSL_KEY_PATH -   - - gRPCSslCertChainPath The file path of gRPC SSL cert chain SW_CORE_GRPC_SSL_CERT_CHAIN_PATH -   - - gRPCSslTrustedCAPath The file path of gRPC trusted CA SW_CORE_GRPC_SSL_TRUSTED_CA_PATH -   - - downsampling The activated level of down sampling aggregation  Hour,Day   - - enableDataKeeperExecutor Controller of TTL scheduler. Once disabled, TTL wouldn\u0026rsquo;t work. SW_CORE_ENABLE_DATA_KEEPER_EXECUTOR true   - - dataKeeperExecutePeriod The execution period of TTL scheduler, unit is minute. Execution doesn\u0026rsquo;t mean deleting data. The storage provider could override this, such as ElasticSearch storage. SW_CORE_DATA_KEEPER_EXECUTE_PERIOD 5   - - recordDataTTL The lifecycle of record data. Record data includes traces, top n sampled records, and logs. Unit is day. Minimal value is 2. SW_CORE_RECORD_DATA_TTL 3   - - metricsDataTTL The lifecycle of metrics data, including the metadata. Unit is day. Recommend metricsDataTTL \u0026gt;= recordDataTTL. Minimal value is 2. SW_CORE_METRICS_DATA_TTL 7   - - enableDatabaseSession Cache metrics data for 1 minute to reduce database queries, and if the OAP cluster changes within that minute. SW_CORE_ENABLE_DATABASE_SESSION true   - - topNReportPeriod The execution period of top N sampler, which saves sampled data into the storage. Unit is minute SW_CORE_TOPN_REPORT_PERIOD 10   - - activeExtraModelColumns Append the names of entity, such as service name, into the metrics storage entities. SW_CORE_ACTIVE_EXTRA_MODEL_COLUMNS false   - - serviceNameMaxLength Max length limitation of service name. SW_SERVICE_NAME_MAX_LENGTH 70   - - instanceNameMaxLength Max length limitation of service instance name. The max length of service + instance names should be less than 200. SW_INSTANCE_NAME_MAX_LENGTH 70   - - endpointNameMaxLength Max length limitation of endpoint name. The max length of service + endpoint names should be less than 240. SW_ENDPOINT_NAME_MAX_LENGTH 150   - - searchableTracesTags Define the set of span tag keys, which should be searchable through the GraphQL. Multiple values should be separated through the comma. SW_SEARCHABLE_TAG_KEYS http.method,status_code,db.type,db.instance,mq.queue,mq.topic,mq.broker   - - gRPCThreadPoolSize Pool size of gRPC server SW_CORE_GRPC_THREAD_POOL_SIZE CPU core * 4   - - gRPCThreadPoolQueueSize The queue size of gRPC server SW_CORE_GRPC_POOL_QUEUE_SIZE 10000   - - maxConcurrentCallsPerConnection The maximum number of concurrent calls permitted for each incoming connection. Defaults to no limit. SW_CORE_GRPC_MAX_CONCURRENT_CALL -   - - maxMessageSize Sets the maximum message size allowed to be received on the server. Empty means 4 MiB SW_CORE_GRPC_MAX_MESSAGE_SIZE 4M(based on Netty)   - - remoteTimeout Timeout for cluster internal communication, in seconds. - 20   - - maxSizeOfNetworkAddressAlias Max size of network address detected in the be monitored system. - 1_000_000   - - maxPageSizeOfQueryProfileSnapshot The max size in every OAP query for snapshot analysis - 500   - - maxSizeOfAnalyzeProfileSnapshot The max number of snapshots analyzed by OAP - 12000   cluster standalone - standalone is not suitable for one node running, no available configuration. - -   - zookeeper nameSpace The namespace, represented by root path, isolates the configurations in the zookeeper. SW_NAMESPACE /, root path   - - hostPort hosts and ports of Zookeeper Cluster SW_CLUSTER_ZK_HOST_PORT localhost:2181   - - baseSleepTimeMs The period of Zookeeper client between two retries. Unit is ms. SW_CLUSTER_ZK_SLEEP_TIME 1000   - - maxRetries The max retry time of re-trying. SW_CLUSTER_ZK_MAX_RETRIES 3   - - enableACL Open ACL by using schema and expression SW_ZK_ENABLE_ACL false   - - schema schema for the authorization SW_ZK_SCHEMA digest   - - expression expression for the authorization SW_ZK_EXPRESSION skywalking:skywalking   - - internalComHost The hostname registered in the Zookeeper for the internal communication of OAP cluster. - -   - - internalComPort The port registered in the Zookeeper for the internal communication of OAP cluster. - -1   - kubernetes namespace Namespace SkyWalking deployed in the k8s SW_CLUSTER_K8S_NAMESPACE default   - - labelSelector Labels used for filtering the OAP deployment in the k8s SW_CLUSTER_K8S_LABEL app=collector,release=skywalking   - - uidEnvName Environment variable name for reading uid. SW_CLUSTER_K8S_UID SKYWALKING_COLLECTOR_UID   - consul serviceName Service name used for SkyWalking cluster. SW_SERVICE_NAME SkyWalking_OAP_Cluster   - - hostPort hosts and ports used of Consul cluster. SW_CLUSTER_CONSUL_HOST_PORT localhost:8500   - - aclToken ALC Token of Consul. Empty string means without ALC token. SW_CLUSTER_CONSUL_ACLTOKEN -   - - internalComHost The hostname registered in the Consul for the internal communication of OAP cluster. - -   - - internalComPort The port registered in the Consul for the internal communication of OAP cluster. - -1   - etcd serviceName Service name used for SkyWalking cluster. SW_SERVICE_NAME SkyWalking_OAP_Cluster   - - hostPort hosts and ports used of etcd cluster. SW_CLUSTER_ETCD_HOST_PORT localhost:2379   - - isSSL Open SSL for the connection between SkyWalking and etcd cluster. - -   - - internalComHost The hostname registered in the etcd for the internal communication of OAP cluster. - -   - - internalComPort The port registered in the etcd for the internal communication of OAP cluster. - -1   - Nacos serviceName Service name used for SkyWalking cluster. SW_SERVICE_NAME SkyWalking_OAP_Cluster   - - hostPort hosts and ports used of Nacos cluster. SW_CLUSTER_NACOS_HOST_PORT localhost:8848   - - namespace Namespace used by SkyWalking node coordination. SW_CLUSTER_NACOS_NAMESPACE public   - - internalComHost The hostname registered in the Nacos for the internal communication of OAP cluster. - -   - - internalComPort The port registered in the Nacos for the internal communication of OAP cluster. - -1   - - username Nacos Auth username SW_CLUSTER_NACOS_USERNAME -   - - password Nacos Auth password SW_CLUSTER_NACOS_PASSWORD -   - - accessKey Nacos Auth accessKey SW_CLUSTER_NACOS_ACCESSKEY -   - - secretKey Nacos Auth secretKey SW_CLUSTER_NACOS_SECRETKEY -   storage elasticsearch - ElasticSearch 6 storage implementation - -   - - nameSpace Prefix of indexes created and used by SkyWalking. SW_NAMESPACE -   - - clusterNodes ElasticSearch cluster nodes for client connection. SW_STORAGE_ES_CLUSTER_NODES localhost   - - protocol HTTP or HTTPs. SW_STORAGE_ES_HTTP_PROTOCOL HTTP   - - user User name of ElasticSearch cluster SW_ES_USER -   - - password Password of ElasticSearch cluster SW_ES_PASSWORD -   - - trustStorePath Trust JKS file path. Only work when user name and password opened SW_STORAGE_ES_SSL_JKS_PATH -   - - trustStorePass Trust JKS file password. Only work when user name and password opened SW_STORAGE_ES_SSL_JKS_PASS -   - - secretsManagementFile Secrets management file in the properties format includes the username, password, which are managed by 3rd party tool. Provide the capability to update them in the runtime. SW_ES_SECRETS_MANAGEMENT_FILE -   - - dayStep Represent the number of days in the one minute/hour/day index. SW_STORAGE_DAY_STEP 1   - - indexShardsNumber Shard number of new indexes SW_STORAGE_ES_INDEX_SHARDS_NUMBER 1   - - indexReplicasNumber Replicas number of new indexes SW_STORAGE_ES_INDEX_REPLICAS_NUMBER 0   - - superDatasetDayStep Represent the number of days in the super size dataset record index, the default value is the same as dayStep when the value is less than 0. SW_SUPERDATASET_STORAGE_DAY_STEP -1   - - superDatasetIndexShardsFactor Super data set has been defined in the codes, such as trace segments. This factor provides more shards for the super data set, shards number = indexShardsNumber * superDatasetIndexShardsFactor. Also, this factor effects Zipkin and Jaeger traces. SW_STORAGE_ES_SUPER_DATASET_INDEX_SHARDS_FACTOR 5   - - superDatasetIndexReplicasNumber Represent the replicas number in the super size dataset record index. SW_STORAGE_ES_SUPER_DATASET_INDEX_REPLICAS_NUMBER 0   - - bulkActions Async bulk size of the record data batch execution. SW_STORAGE_ES_BULK_ACTIONS 1000   - - syncBulkActions Sync bulk size of the metrics data batch execution. SW_STORAGE_ES_SYNC_BULK_ACTIONS 50000   - - flushInterval Period of flush, no matter bulkActions reached or not. Unit is second. SW_STORAGE_ES_FLUSH_INTERVAL 10   - - concurrentRequests The number of concurrent requests allowed to be executed. SW_STORAGE_ES_CONCURRENT_REQUESTS 2   - - resultWindowMaxSize The max size of dataset when OAP loading cache, such as network alias. SW_STORAGE_ES_QUERY_MAX_WINDOW_SIZE 10000   - - metadataQueryMaxSize The max size of metadata per query. SW_STORAGE_ES_QUERY_MAX_SIZE 5000   - - segmentQueryMaxSize The max size of trace segments per query. SW_STORAGE_ES_QUERY_SEGMENT_SIZE 200   - - profileTaskQueryMaxSize The max size of profile task per query. SW_STORAGE_ES_QUERY_PROFILE_TASK_SIZE 200   - - advanced All settings of ElasticSearch index creation. The value should be in JSON format SW_STORAGE_ES_ADVANCED -   - elasticsearch7 - ElasticSearch 7 storage implementation - -   - - nameSpace Prefix of indexes created and used by SkyWalking. SW_NAMESPACE -   - - clusterNodes ElasticSearch cluster nodes for client connection. SW_STORAGE_ES_CLUSTER_NODES localhost   - - protocol HTTP or HTTPs. SW_STORAGE_ES_HTTP_PROTOCOL HTTP   - - user User name of ElasticSearch cluster SW_ES_USER -   - - password Password of ElasticSearch cluster SW_ES_PASSWORD -   - - trustStorePath Trust JKS file path. Only work when user name and password opened SW_STORAGE_ES_SSL_JKS_PATH -   - - trustStorePass Trust JKS file password. Only work when user name and password opened SW_STORAGE_ES_SSL_JKS_PASS -   - - secretsManagementFile Secrets management file in the properties format includes the username, password, which are managed by 3rd party tool. Provide the capability to update them in the runtime. SW_ES_SECRETS_MANAGEMENT_FILE -   - - dayStep Represent the number of days in the one minute/hour/day index. SW_STORAGE_DAY_STEP 1   - - indexShardsNumber Shard number of new indexes SW_STORAGE_ES_INDEX_SHARDS_NUMBER 1   - - indexReplicasNumber Replicas number of new indexes SW_STORAGE_ES_INDEX_REPLICAS_NUMBER 0   - - superDatasetDayStep Represent the number of days in the super size dataset record index, the default value is the same as dayStep when the value is less than 0. SW_SUPERDATASET_STORAGE_DAY_STEP -1   - - superDatasetIndexShardsFactor Super data set has been defined in the codes, such as trace segments. This factor provides more shards for the super data set, shards number = indexShardsNumber * superDatasetIndexShardsFactor. Also, this factor effects Zipkin and Jaeger traces. SW_STORAGE_ES_SUPER_DATASET_INDEX_SHARDS_FACTOR 5   - - superDatasetIndexReplicasNumber Represent the replicas number in the super size dataset record index. SW_STORAGE_ES_SUPER_DATASET_INDEX_REPLICAS_NUMBER 0   - - bulkActions Async bulk size of the record data batch execution. SW_STORAGE_ES_BULK_ACTIONS 1000   - - syncBulkActions Sync bulk size of the metrics data batch execution. SW_STORAGE_ES_SYNC_BULK_ACTIONS 50000   - - flushInterval Period of flush, no matter bulkActions reached or not. Unit is second. SW_STORAGE_ES_FLUSH_INTERVAL 10   - - concurrentRequests The number of concurrent requests allowed to be executed. SW_STORAGE_ES_CONCURRENT_REQUESTS 2   - - resultWindowMaxSize The max size of dataset when OAP loading cache, such as network alias. SW_STORAGE_ES_QUERY_MAX_WINDOW_SIZE 10000   - - metadataQueryMaxSize The max size of metadata per query. SW_STORAGE_ES_QUERY_MAX_SIZE 5000   - - segmentQueryMaxSize The max size of trace segments per query. SW_STORAGE_ES_QUERY_SEGMENT_SIZE 200   - - profileTaskQueryMaxSize The max size of profile task per query. SW_STORAGE_ES_QUERY_PROFILE_TASK_SIZE 200   - - advanced All settings of ElasticSearch index creation. The value should be in JSON format SW_STORAGE_ES_ADVANCED -   - h2 - H2 storage is designed for demonstration and running in short term(1-2 hours) only - -   - - driver H2 JDBC driver. SW_STORAGE_H2_DRIVER org.h2.jdbcx.JdbcDataSource   - - url H2 connection URL. Default is H2 memory mode SW_STORAGE_H2_URL jdbc:h2:mem:skywalking-oap-db   - - user User name of H2 database. SW_STORAGE_H2_USER sa   - - password Password of H2 database. - -   - - metadataQueryMaxSize The max size of metadata per query. SW_STORAGE_H2_QUERY_MAX_SIZE 5000   - - maxSizeOfArrayColumn Some entities, such as trace segment, include the logic column with multiple values. In the H2, we use multiple physical columns to host the values, such as, Change column_a with values [1,2,3,4,5] to column_a_0 = 1, column_a_1 = 2, column_a_2 = 3 , column_a_3 = 4, column_a_4 = 5 SW_STORAGE_MAX_SIZE_OF_ARRAY_COLUMN 20   - - numOfSearchableValuesPerTag In a trace segment, it includes multiple spans with multiple tags. Different spans could have same tag keys, such as multiple HTTP exit spans all have their own http.method tag. This configuration set the limitation of max num of values for the same tag key. SW_STORAGE_NUM_OF_SEARCHABLE_VALUES_PER_TAG 2   - mysql - MySQL Storage. The MySQL JDBC Driver is not in the dist, please copy it into oap-lib folder manually - -   - - properties Hikari connection pool configurations - Listed in the application.yaml.   - - metadataQueryMaxSize The max size of metadata per query. SW_STORAGE_MYSQL_QUERY_MAX_SIZE 5000   - - maxSizeOfArrayColumn Some entities, such as trace segment, include the logic column with multiple values. In the MySQL, we use multiple physical columns to host the values, such as, Change column_a with values [1,2,3,4,5] to column_a_0 = 1, column_a_1 = 2, column_a_2 = 3 , column_a_3 = 4, column_a_4 = 5 SW_STORAGE_MAX_SIZE_OF_ARRAY_COLUMN 20   - - numOfSearchableValuesPerTag In a trace segment, it includes multiple spans with multiple tags. Different spans could have same tag keys, such as multiple HTTP exit spans all have their own http.method tag. This configuration set the limitation of max num of values for the same tag key. SW_STORAGE_NUM_OF_SEARCHABLE_VALUES_PER_TAG 2   - influxdb - InfluxDB storage. - -   - - url InfluxDB connection URL. SW_STORAGE_INFLUXDB_URL http://localhost:8086   - - user User name of InfluxDB. SW_STORAGE_INFLUXDB_USER root   - - password Password of InfluxDB. SW_STORAGE_INFLUXDB_PASSWORD -   - - database Database of InfluxDB. SW_STORAGE_INFLUXDB_DATABASE skywalking   - - actions The number of actions to collect. SW_STORAGE_INFLUXDB_ACTIONS 1000   - - duration The time to wait at most (milliseconds). SW_STORAGE_INFLUXDB_DURATION 1000   - - fetchTaskLogMaxSize The max number of fetch task log in a request. SW_STORAGE_INFLUXDB_FETCH_TASK_LOG_MAX_SIZE 5000   agent-analyzer default Agent Analyzer. SW_AGENT_ANALYZER default    - - sampleRate Sampling rate for receiving trace. The precision is 1/10000. 10000 means 100% sample in default. SW_TRACE_SAMPLE_RATE 10000   - - slowDBAccessThreshold The slow database access thresholds. Unit ms. SW_SLOW_DB_THRESHOLD default:200,mongodb:100   - - forceSampleErrorSegment When sampling mechanism activated, this config would make the error status segment sampled, ignoring the sampling rate. SW_FORCE_SAMPLE_ERROR_SEGMENT true   - - segmentStatusAnalysisStrategy Determine the final segment status from the status of spans. Available values are FROM_SPAN_STATUS , FROM_ENTRY_SPAN and FROM_FIRST_SPAN. FROM_SPAN_STATUS represents the segment status would be error if any span is in error status. FROM_ENTRY_SPAN means the segment status would be determined by the status of entry spans only. FROM_FIRST_SPAN means the segment status would be determined by the status of the first span only. SW_SEGMENT_STATUS_ANALYSIS_STRATEGY FROM_SPAN_STATUS   - - noUpstreamRealAddressAgents Exit spans with the component in the list would not generate the client-side instance relation metrics. As some tracing plugins can\u0026rsquo;t collect the real peer ip address, such as Nginx-LUA and Envoy. SW_NO_UPSTREAM_REAL_ADDRESS 6000,9000   receiver-sharing-server default Sharing server provides new gRPC and restful servers for data collection. Ana make the servers in the core module working for internal communication only. - -    - - restHost Binding IP of restful service. Services include GraphQL query and HTTP data report - -   - - restPort Binding port of restful service - -   - - restContextPath Web context path of restful service - -   - - gRPCHost Binding IP of gRPC service. Services include gRPC data report and internal communication among OAP nodes SW_RECEIVER_GRPC_HOST 0.0.0.0. Not Activated   - - gRPCPort Binding port of gRPC service SW_RECEIVER_GRPC_PORT Not Activated   - - gRPCThreadPoolSize Pool size of gRPC server SW_RECEIVER_GRPC_THREAD_POOL_SIZE CPU core * 4   - - gRPCThreadPoolQueueSize The queue size of gRPC server SW_RECEIVER_GRPC_POOL_QUEUE_SIZE 10000   - - gRPCSslEnabled Activate SSL for gRPC service SW_RECEIVER_GRPC_SSL_ENABLED false   - - gRPCSslKeyPath The file path of gRPC SSL key SW_RECEIVER_GRPC_SSL_KEY_PATH -   - - gRPCSslCertChainPath The file path of gRPC SSL cert chain SW_RECEIVER_GRPC_SSL_CERT_CHAIN_PATH -   - - maxConcurrentCallsPerConnection The maximum number of concurrent calls permitted for each incoming connection. Defaults to no limit. SW_RECEIVER_GRPC_MAX_CONCURRENT_CALL -   - - authentication The token text for the authentication. Work for gRPC connection only. Once this is set, the client is required to use the same token. SW_AUTHENTICATION -   receiver-register default Read receiver doc for more details - -    receiver-trace default Read receiver doc for more details - -    receiver-jvm default Read receiver doc for more details - -    receiver-clr default Read receiver doc for more details - -    receiver-profile default Read receiver doc for more details - -    service-mesh default Read receiver doc for more details - -    istio-telemetry default Read receiver doc for more details - -    envoy-metric default Read receiver doc for more details - -    - - acceptMetricsService Open Envoy Metrics Service analysis SW_ENVOY_METRIC_SERVICE true   - - alsHTTPAnalysis Open Envoy Access Log Service analysis. Value = k8s-mesh means open the analysis SW_ENVOY_METRIC_ALS_HTTP_ANALYSIS -   receiver-oc default Read receiver doc for more details - -    - - gRPCHost Binding IP of gRPC service. Services include gRPC data report and internal communication among OAP nodes SW_OC_RECEIVER_GRPC_HOST -   - - gRPCPort Binding port of gRPC service SW_OC_RECEIVER_GRPC_PORT -   - - gRPCThreadPoolSize Pool size of gRPC server - CPU core * 4   - - gRPCThreadPoolQueueSize The queue size of gRPC server - 10000   - - maxConcurrentCallsPerConnection The maximum number of concurrent calls permitted for each incoming connection. Defaults to no limit. - -   - - maxMessageSize Sets the maximum message size allowed to be received on the server. Empty means 4 MiB - 4M(based on Netty)   receiver_zipkin default Read receiver doc - -    - - restHost Binding IP of restful service. SW_RECEIVER_ZIPKIN_HOST 0.0.0.0   - - restPort Binding port of restful service SW_RECEIVER_ZIPKIN_PORT 9411   - - restContextPath Web context path of restful service SW_RECEIVER_ZIPKIN_CONTEXT_PATH /   - - needAnalysis Analysis zipkin span to generate metrics - false   - - maxCacheSize Max cache size for span analysis - 1_000_000   - - expireTime The expire time of analysis cache, unit is second. - 20   receiver_jaeger default Read receiver doc - -    - - gRPCHost Binding IP of gRPC service. Services include gRPC data report and internal communication among OAP nodes SW_RECEIVER_JAEGER_HOST -   - - gRPCPort Binding port of gRPC service SW_RECEIVER_JAEGER_PORT -   - - gRPCThreadPoolSize Pool size of gRPC server - CPU core * 4   - - gRPCThreadPoolQueueSize The queue size of gRPC server - 10000   - - maxConcurrentCallsPerConnection The maximum number of concurrent calls permitted for each incoming connection. Defaults to no limit. - -   - - maxMessageSize Sets the maximum message size allowed to be received on the server. Empty means 4 MiB - 4M(based on Netty)   prometheus-fetcher default Read fetcher doc for more details - -    - - active Activate the Prometheus fetcher. SW_PROMETHEUS_FETCHER_ACTIVE false   kafka-fetcher default Read fetcher doc for more details - -    - - bootstrapServers A list of host/port pairs to use for establishing the initial connection to the Kafka cluster. SW_KAFKA_FETCHER_SERVERS localhost:9092   - - groupId A unique string that identifies the consumer group this consumer belongs to. - skywalking-consumer   - - consumePartitions Which PartitionId(s) of the topics assign to the OAP server. If more than one, is separated by commas. SW_KAFKA_FETCHER_CONSUME_PARTITIONS -   - - isSharding it was true when OAP Server in cluster. SW_KAFKA_FETCHER_IS_SHARDING false   - - createTopicIfNotExist If true, create the Kafka topic when it does not exist. - true   - - partitions The number of partitions for the topic being created. SW_KAFKA_FETCHER_PARTITIONS 3   - - enableMeterSystem To enable to fetch and handle Meter System data. SW_KAFKA_FETCHER_ENABLE_METER_SYSTEM false   - - replicationFactor The replication factor for each partition in the topic being created. SW_KAFKA_FETCHER_PARTITIONS_FACTOR 2   - - topicNameOfMeters Specifying Kafka topic name for Meter system data. - skywalking-meters   - - topicNameOfMetrics Specifying Kafka topic name for JVM Metrics data. - skywalking-metrics   - - topicNameOfProfiling Specifying Kafka topic name for Profiling data. - skywalking-profilings   - - topicNameOfTracingSegments Specifying Kafka topic name for Tracing data. - skywalking-segments   - - topicNameOfManagements Specifying Kafka topic name for service instance reporting and registering. - skywalking-managements   receiver-browser default Read receiver doc for more details - - -   - - sampleRate Sampling rate for receiving trace. The precision is 1/10000. 10000 means 100% sample in default. SW_RECEIVER_BROWSER_SAMPLE_RATE 10000   query graphql - GraphQL query implementation -    - - path Root path of GraphQL query and mutation. SW_QUERY_GRAPHQL_PATH /graphql   alarm default - Read alarm doc for more details. -    telemetry - - Read telemetry doc for more details. -    - none - No op implementation -    - prometheus host Binding host for Prometheus server fetching data SW_TELEMETRY_PROMETHEUS_HOST 0.0.0.0   - - port Binding port for Prometheus server fetching data SW_TELEMETRY_PROMETHEUS_PORT 1234   configuration - - Read dynamic configuration doc for more details. -    - grpc host DCS server binding hostname SW_DCS_SERVER_HOST -   - - port DCS server binding port SW_DCS_SERVER_PORT 80   - - clusterName Cluster name when reading latest configuration from DSC server. SW_DCS_CLUSTER_NAME SkyWalking   - - period The period of OAP reading data from DSC server. Unit is second. SW_DCS_PERIOD 20   - apollo apolloMeta apollo.meta in Apollo SW_CONFIG_APOLLO http://106.12.25.204:8080   - - apolloCluster apollo.cluster in Apollo SW_CONFIG_APOLLO_CLUSTER default   - - apolloEnv env in Apollo SW_CONFIG_APOLLO_ENV -   - - appId app.id in Apollo SW_CONFIG_APOLLO_APP_ID skywalking   - - period The period of data sync. Unit is second. SW_CONFIG_APOLLO_PERIOD 60   - zookeeper nameSpace The namespace, represented by root path, isolates the configurations in the zookeeper. SW_CONFIG_ZK_NAMESPACE /, root path   - - hostPort hosts and ports of Zookeeper Cluster SW_CONFIG_ZK_HOST_PORT localhost:2181   - - baseSleepTimeMs The period of Zookeeper client between two retries. Unit is ms. SW_CONFIG_ZK_BASE_SLEEP_TIME_MS 1000   - - maxRetries The max retry time of re-trying. SW_CONFIG_ZK_MAX_RETRIES 3   - - period The period of data sync. Unit is second. SW_CONFIG_ZK_PERIOD 60   - etcd clusterName Service name used for SkyWalking cluster. SW_CONFIG_ETCD_CLUSTER_NAME default   - - serverAddr hosts and ports used of etcd cluster. SW_CONFIG_ETCD_SERVER_ADDR localhost:2379   - - group Additional prefix of the configuration key SW_CONFIG_ETCD_GROUP skywalking   - - period The period of data sync. Unit is second. SW_CONFIG_ZK_PERIOD 60   - consul hostPort hosts and ports used of Consul cluster. SW_CONFIG_CONSUL_HOST_AND_PORTS localhost:8500   - - aclToken ALC Token of Consul. Empty string means without ALC token. SW_CONFIG_CONSUL_ACL_TOKEN -   - - period The period of data sync. Unit is second. SW_CONFIG_CONSUL_PERIOD 60   - k8s-configmap namespace Deployment namespace of the config map. SW_CLUSTER_K8S_NAMESPACE default   - - labelSelector Labels used for locating configmap. SW_CLUSTER_K8S_LABEL app=collector,release=skywalking   - - period The period of data sync. Unit is second. SW_CONFIG_ZK_PERIOD 60   - nacos serverAddr Nacos Server Host SW_CONFIG_NACOS_SERVER_ADDR 127.0.0.1   - - port Nacos Server Port SW_CONFIG_NACOS_SERVER_PORT 8848   - - group Nacos Configuration namespace SW_CONFIG_NACOS_SERVER_NAMESPACE -   - - period The period of data sync. Unit is second. SW_CONFIG_CONFIG_NACOS_PERIOD 60   - - username Nacos Auth username SW_CONFIG_NACOS_USERNAME -   - - password Nacos Auth password SW_CONFIG_NACOS_PASSWORD -   - - accessKey Nacos Auth accessKey SW_CONFIG_NACOS_ACCESSKEY -   - - secretKey Nacos Auth secretKey SW_CONFIG_NACOS_SECRETKEY -   exporter grpc targetHost The host of target grpc server for receiving export data. SW_EXPORTER_GRPC_HOST 127.0.0.1   - - targetPort The port of target grpc server for receiving export data. SW_EXPORTER_GRPC_PORT 9870   health-checker default checkIntervalSeconds The period of check OAP internal health status. Unit is second. SW_HEALTH_CHECKER_INTERVAL_SECONDS 5    Notice ¹ System Environment Variable name could be declared and changed in the application.yml. The names listed here, are just provided in the default application.yml file.\n","excerpt":"Configuration Vocabulary Configuration Vocabulary lists all available configurations provided by …","ref":"/docs/main/v8.2.0/en/setup/backend/configuration-vocabulary/","title":"Configuration Vocabulary"},{"body":"Configuring Envoy to send metrics to SkyWalking In order to let Envoy to send metrics to SkyWalking, we need to feed Envoy with a configuration which contains stats_sinks that includes envoy.metrics_service. This envoy.metrics_service should be configured as a config.grpc_service entry.\nThe interesting parts of the config is shown in the config below:\nstats_sinks: - name: envoy.metrics_service config: grpc_service: # Note: we can use google_grpc implementation as well. envoy_grpc: cluster_name: service_skywalking static_resources: ... clusters: - name: service_skywalking connect_timeout: 5s type: LOGICAL_DNS http2_protocol_options: {} dns_lookup_family: V4_ONLY lb_policy: ROUND_ROBIN load_assignment: cluster_name: service_skywalking endpoints: - lb_endpoints: - endpoint: address: socket_address: address: skywalking # This is the port where SkyWalking serving the Envoy Metrics Service gRPC stream. port_value: 11800 A more complete static configuration, can be observed here.\nNote that Envoy can also be configured dynamically through xDS Protocol.\nMetrics data Some of the Envoy statistics are listed in this list. A sample data that contains identifier can be found here, while the metrics only can be observed here.\n","excerpt":"Configuring Envoy to send metrics to SkyWalking In order to let Envoy to send metrics to SkyWalking, …","ref":"/docs/main/v8.2.0/en/setup/envoy/metrics_service_setting/","title":"Configuring Envoy to send metrics to SkyWalking"},{"body":" Dependency the toolkit, such as using maven or gradle  \u0026lt;dependency\u0026gt; \u0026lt;groupId\u0026gt;org.apache.skywalking\u0026lt;/groupId\u0026gt; \u0026lt;artifactId\u0026gt;apm-toolkit-log4j-1.x\u0026lt;/artifactId\u0026gt; \u0026lt;version\u0026gt;{project.release.version}\u0026lt;/version\u0026gt; \u0026lt;/dependency\u0026gt;  Config a layout  log4j.appender.CONSOLE.layout=TraceIdPatternLayout  set %T in layout.ConversionPattern ( In 2.0-2016, you should use %x, Why change? )  log4j.appender.CONSOLE.layout.ConversionPattern=%d [%T] %-5p %c{1}:%L - %m%n  When you use -javaagent to active the sky-walking tracer, log4j will output traceId, if it existed. If the tracer is inactive, the output will be TID: N/A.  ","excerpt":"Dependency the toolkit, such as using maven or gradle  \u0026lt;dependency\u0026gt; …","ref":"/docs/main/v8.2.0/en/setup/service-agent/java-agent/application-toolkit-log4j-1.x/","title":"Dependency the toolkit, such as using maven or gradle"},{"body":" Dependency the toolkit, such as using maven or gradle  \u0026lt;dependency\u0026gt; \u0026lt;groupId\u0026gt;org.apache.skywalking\u0026lt;/groupId\u0026gt; \u0026lt;artifactId\u0026gt;apm-toolkit-log4j-2.x\u0026lt;/artifactId\u0026gt; \u0026lt;version\u0026gt;{project.release.version}\u0026lt;/version\u0026gt; \u0026lt;/dependency\u0026gt;  Config the [%traceId] pattern in your log4j2.xml  \u0026lt;Appenders\u0026gt; \u0026lt;Console name=\u0026#34;Console\u0026#34; target=\u0026#34;SYSTEM_OUT\u0026#34;\u0026gt; \u0026lt;PatternLayout pattern=\u0026#34;%d [%traceId] %-5p %c{1}:%L - %m%n\u0026#34;/\u0026gt; \u0026lt;/Console\u0026gt; \u0026lt;/Appenders\u0026gt;  Support log4j2 AsyncRoot , No additional configuration is required. Refer to the demo of log4j2.xml below. For details: Log4j2 Async Loggers  \u0026lt;Configuration\u0026gt; \u0026lt;Appenders\u0026gt; \u0026lt;Console name=\u0026#34;Console\u0026#34; target=\u0026#34;SYSTEM_OUT\u0026#34;\u0026gt; \u0026lt;PatternLayout pattern=\u0026#34;%d [%traceId] %-5p %c{1}:%L - %m%n\u0026#34;/\u0026gt; \u0026lt;/Console\u0026gt; \u0026lt;/Appenders\u0026gt; \u0026lt;Loggers\u0026gt; \u0026lt;AsyncRoot level=\u0026#34;INFO\u0026#34;\u0026gt; \u0026lt;AppenderRef ref=\u0026#34;Console\u0026#34;/\u0026gt; \u0026lt;/AsyncRoot\u0026gt; \u0026lt;/Loggers\u0026gt; \u0026lt;/Configuration\u0026gt;   Support log4j2 AsyncAppender , No additional configuration is required. Refer to the demo of log4j2.xml below.\nFor details: All Loggers Async\nLog4j-2.9 and higher require disruptor-3.3.4.jar or higher on the classpath. Prior to Log4j-2.9, disruptor-3.0.0.jar or higher was required. This is simplest to configure and gives the best performance. To make all loggers asynchronous, add the disruptor jar to the classpath and set the system property log4j2.contextSelector to org.apache.logging.log4j.core.async.AsyncLoggerContextSelector.\n\u0026lt;Configuration status=\u0026#34;WARN\u0026#34;\u0026gt; \u0026lt;Appenders\u0026gt; \u0026lt;!-- Async Loggers will auto-flush in batches, so switch off immediateFlush. --\u0026gt; \u0026lt;RandomAccessFile name=\u0026#34;RandomAccessFile\u0026#34; fileName=\u0026#34;async.log\u0026#34; immediateFlush=\u0026#34;false\u0026#34; append=\u0026#34;false\u0026#34;\u0026gt; \u0026lt;PatternLayout\u0026gt; \u0026lt;Pattern\u0026gt;%d %p %c{1.} [%t] [%traceId] %m %ex%n\u0026lt;/Pattern\u0026gt; \u0026lt;/PatternLayout\u0026gt; \u0026lt;/RandomAccessFile\u0026gt; \u0026lt;/Appenders\u0026gt; \u0026lt;Loggers\u0026gt; \u0026lt;Root level=\u0026#34;info\u0026#34; includeLocation=\u0026#34;false\u0026#34;\u0026gt; \u0026lt;AppenderRef ref=\u0026#34;RandomAccessFile\u0026#34;/\u0026gt; \u0026lt;/Root\u0026gt; \u0026lt;/Loggers\u0026gt; \u0026lt;/Configuration\u0026gt; For details: Mixed Sync \u0026amp; Async\nLog4j-2.9 and higher require disruptor-3.3.4.jar or higher on the classpath. Prior to Log4j-2.9, disruptor-3.0.0.jar or higher was required. There is no need to set system property Log4jContextSelector to any value.\n\u0026lt;Configuration status=\u0026#34;WARN\u0026#34;\u0026gt; \u0026lt;Appenders\u0026gt; \u0026lt;!-- Async Loggers will auto-flush in batches, so switch off immediateFlush. --\u0026gt; \u0026lt;RandomAccessFile name=\u0026#34;RandomAccessFile\u0026#34; fileName=\u0026#34;asyncWithLocation.log\u0026#34; immediateFlush=\u0026#34;false\u0026#34; append=\u0026#34;false\u0026#34;\u0026gt; \u0026lt;PatternLayout\u0026gt; \u0026lt;Pattern\u0026gt;%d %p %class{1.} [%t] [%traceId] %location %m %ex%n\u0026lt;/Pattern\u0026gt; \u0026lt;/PatternLayout\u0026gt; \u0026lt;/RandomAccessFile\u0026gt; \u0026lt;/Appenders\u0026gt; \u0026lt;Loggers\u0026gt; \u0026lt;!-- pattern layout actually uses location, so we need to include it --\u0026gt; \u0026lt;AsyncLogger name=\u0026#34;com.foo.Bar\u0026#34; level=\u0026#34;trace\u0026#34; includeLocation=\u0026#34;true\u0026#34;\u0026gt; \u0026lt;AppenderRef ref=\u0026#34;RandomAccessFile\u0026#34;/\u0026gt; \u0026lt;/AsyncLogger\u0026gt; \u0026lt;Root level=\u0026#34;info\u0026#34; includeLocation=\u0026#34;true\u0026#34;\u0026gt; \u0026lt;AppenderRef ref=\u0026#34;RandomAccessFile\u0026#34;/\u0026gt; \u0026lt;/Root\u0026gt; \u0026lt;/Loggers\u0026gt; \u0026lt;/Configuration\u0026gt;   Support log4j2 AsyncAppender, For details: Log4j2 AsyncAppender\n  \u0026lt;Configuration\u0026gt; \u0026lt;Appenders\u0026gt; \u0026lt;Console name=\u0026#34;Console\u0026#34; target=\u0026#34;SYSTEM_OUT\u0026#34;\u0026gt; \u0026lt;PatternLayout pattern=\u0026#34;%d [%traceId] %-5p %c{1}:%L - %m%n\u0026#34;/\u0026gt; \u0026lt;/Console\u0026gt; \u0026lt;Async name=\u0026#34;Async\u0026#34;\u0026gt; \u0026lt;AppenderRef ref=\u0026#34;Console\u0026#34;/\u0026gt; \u0026lt;/Async\u0026gt; \u0026lt;/Appenders\u0026gt; \u0026lt;Loggers\u0026gt; \u0026lt;Root level=\u0026#34;INFO\u0026#34;\u0026gt; \u0026lt;AppenderRef ref=\u0026#34;Async\u0026#34;/\u0026gt; \u0026lt;/Root\u0026gt; \u0026lt;/Loggers\u0026gt; \u0026lt;/Configuration\u0026gt;  When you use -javaagent to active the sky-walking tracer, log4j2 will output traceId, if it existed. If the tracer is inactive, the output will be TID: N/A.  ","excerpt":"Dependency the toolkit, such as using maven or gradle  \u0026lt;dependency\u0026gt; …","ref":"/docs/main/v8.2.0/en/setup/service-agent/java-agent/application-toolkit-log4j-2.x/","title":"Dependency the toolkit, such as using maven or gradle"},{"body":" Dependency the toolkit, such as using maven or gradle  \u0026lt;dependency\u0026gt; \u0026lt;groupId\u0026gt;org.apache.skywalking\u0026lt;/groupId\u0026gt; \u0026lt;artifactId\u0026gt;apm-toolkit-meter\u0026lt;/artifactId\u0026gt; \u0026lt;version\u0026gt;${skywalking.version}\u0026lt;/version\u0026gt; \u0026lt;/dependency\u0026gt; If you\u0026rsquo;re using Spring sleuth, you could use Spring Sleuth Setup.\n Counter API represents a single monotonically increasing counter, automatic collect data and report to backend.  import org.apache.skywalking.apm.toolkit.meter.MeterFactory; Counter counter = MeterFactory.counter(meterName).tag(\u0026#34;tagKey\u0026#34;, \u0026#34;tagValue\u0026#34;).mode(Counter.Mode.INCREMENT).build(); counter.increment(1d);  MeterFactory.counter Create a new counter builder with the meter name. Counter.Builder.tag(String key, String value) Mark a tag key/value pair. Counter.Builder.mode(Counter.Mode mode) Change the counter mode, RATE mode means reporting rate to the backend. Counter.Builder.build() Build a new Counter which is collected and reported to the backend. Counter.increment(double count) Increment count to the Counter, It could be a positive value.   Gauge API represents a single numerical value.  import org.apache.skywalking.apm.toolkit.meter.MeterFactory; ThreadPoolExecutor threadPool = ...; Gauge gauge = MeterFactory.gauge(meterName, () -\u0026gt; threadPool.getActiveCount()).tag(\u0026#34;tagKey\u0026#34;, \u0026#34;tagValue\u0026#34;).build();  MeterFactory.gauge(String name, Supplier\u0026lt;Double\u0026gt; getter) Create a new gauge builder with the meter name and supplier function, this function need to return a double value. Gauge.Builder.tag(String key, String value) Mark a tag key/value pair. Gauge.Builder.build() Build a new Gauge which is collected and reported to the backend.   Histogram API represents a summary sample observations with customize buckets.  import org.apache.skywalking.apm.toolkit.meter.MeterFactory; Histogram histogram = MeterFactory.histogram(\u0026#34;test\u0026#34;).tag(\u0026#34;tagKey\u0026#34;, \u0026#34;tagValue\u0026#34;).steps(Arrays.asList(1, 5, 10)).minValue(0).build(); histogram.addValue(3);  MeterFactory.histogram(String name) Create a new histogram builder with the meter name. Histogram.Builder.tag(String key, String value) Mark a tag key/value pair. Histogram.Builder.steps(List\u0026lt;Double\u0026gt; steps) Set up the max values of every histogram buckets. Histogram.Builder.minValue(double value) Set up the minimal value of this histogram, default is 0. Histogram.Builder.build() Build a new Histogram which is collected and reported to the backend. Histogram.addValue(double value) Add value into the histogram, automatically analyze what bucket count needs to be increment. rule: count into [step1, step2).  ","excerpt":"Dependency the toolkit, such as using maven or gradle  \u0026lt;dependency\u0026gt; …","ref":"/docs/main/v8.2.0/en/setup/service-agent/java-agent/application-toolkit-meter/","title":"Dependency the toolkit, such as using maven or gradle"},{"body":" Dependency the toolkit, such as using maven or gradle  \u0026lt;dependency\u0026gt; \u0026lt;groupId\u0026gt;org.apache.skywalking\u0026lt;/groupId\u0026gt; \u0026lt;artifactId\u0026gt;apm-toolkit-micrometer-registry\u0026lt;/artifactId\u0026gt; \u0026lt;version\u0026gt;${skywalking.version}\u0026lt;/version\u0026gt; \u0026lt;/dependency\u0026gt;  Using org.apache.skywalking.apm.meter.micrometer.SkywalkingMeterRegistry as the registry, it could forward the MicroMeter collected metrics to OAP server.  import org.apache.skywalking.apm.meter.micrometer.SkywalkingMeterRegistry; SkywalkingMeterRegistry registry = new SkywalkingMeterRegistry(); // If you has some counter want to rate by agent side SkywalkingConfig config = new SkywalkingConfig(Arrays.asList(\u0026#34;test_rate_counter\u0026#34;)); new SkywalkingMeterRegistry(config); // Also you could using composite registry to combine multiple meter registry, such as collect to Skywalking and prometheus CompositeMeterRegistry compositeRegistry = new CompositeMeterRegistry(); compositeRegistry.add(new PrometheusMeterRegistry(PrometheusConfig.DEFAULT)); compositeRegistry.add(new SkywalkingMeterRegistry());   Using snake case as the naming convention. Such as test.meter will be send to test_meter.\n  Using Millisecond as the time unit.\n  Adapt micrometer data convention.\n     Micrometer data type Transform to meter name Skywalking data type Description     Counter Counter name Counter Same with counter   Gauges Gauges name Gauges Same with gauges   Timer Timer name + \u0026ldquo;_count\u0026rdquo; Counter Execute finished count    Timer name + \u0026ldquo;_sum\u0026rdquo; Counter Total execute finished duration    Timer name + \u0026ldquo;_max\u0026rdquo; Gauges Max duration of execute finished time    Timer name + \u0026ldquo;_histogram\u0026rdquo; Histogram Histogram of execute finished duration   LongTaskTimer Timer name + \u0026ldquo;_active_count\u0026rdquo; Gauges Executing task count    Timer name + \u0026ldquo;_duration_sum\u0026rdquo; Counter All of executing task sum duration    Timer name + \u0026ldquo;_max\u0026rdquo; Counter Current longest running task execute duration   Function Timer Timer name + \u0026ldquo;_count\u0026rdquo; Gauges Execute finished timer count    Timer name + \u0026ldquo;_sum\u0026rdquo; Gauges Execute finished timer total duration   Function Counter Counter name Counter Custom counter value   Distribution summary Summary name + \u0026ldquo;_count\u0026rdquo; Counter Total record count    Summary name + \u0026ldquo;_sum\u0026rdquo; Counter Total record amount sum    Summary name + \u0026ldquo;_max\u0026rdquo; Gauges Max record amount    Summary name + \u0026ldquo;_histogram\u0026rdquo; Gauges Histogram of the amount     Not Adapt data convention.     Micrometer data type Data type     LongTaskTimer Histogram    ","excerpt":"Dependency the toolkit, such as using maven or gradle  \u0026lt;dependency\u0026gt; …","ref":"/docs/main/v8.2.0/en/setup/service-agent/java-agent/application-toolkit-micrometer/","title":"Dependency the toolkit, such as using maven or gradle"},{"body":" Dependency the toolkit, such as using maven or gradle  \u0026lt;dependency\u0026gt; \u0026lt;groupId\u0026gt;org.apache.skywalking\u0026lt;/groupId\u0026gt; \u0026lt;artifactId\u0026gt;apm-toolkit-trace\u0026lt;/artifactId\u0026gt; \u0026lt;version\u0026gt;${skywalking.version}\u0026lt;/version\u0026gt; \u0026lt;/dependency\u0026gt;  Use TraceContext.traceId() API to obtain traceId.  import TraceContext; ... modelAndView.addObject(\u0026#34;traceId\u0026#34;, TraceContext.traceId()); Sample codes only\n  Add @Trace to any method you want to trace. After that, you can see the span in the Stack.\n  Methods annotated with @Tag will try to tag the current active span with the given key (Tag#key()) and (Tag#value()), if there is no active span at all, this annotation takes no effect. @Tag can be repeated, and can be used in companion with @Trace, see examples below. The value of Tag is the same as what are supported in Customize Enhance Trace.\n  Add custom tag in the context of traced method, ActiveSpan.tag(\u0026quot;key\u0026quot;, \u0026quot;val\u0026quot;).\n  ActiveSpan.error() Mark the current span as error status.\n  ActiveSpan.error(String errorMsg) Mark the current span as error status with a message.\n  ActiveSpan.error(Throwable throwable) Mark the current span as error status with a Throwable.\n  ActiveSpan.debug(String debugMsg) Add a debug level log message in the current span.\n  ActiveSpan.info(String infoMsg) Add an info level log message in the current span.\n  ActiveSpan.setOperationName(String operationName) Customize an operation name.\n  ActiveSpan.tag(\u0026#34;my_tag\u0026#34;, \u0026#34;my_value\u0026#34;); ActiveSpan.error(); ActiveSpan.error(\u0026#34;Test-Error-Reason\u0026#34;); ActiveSpan.error(new RuntimeException(\u0026#34;Test-Error-Throwable\u0026#34;)); ActiveSpan.info(\u0026#34;Test-Info-Msg\u0026#34;); ActiveSpan.debug(\u0026#34;Test-debug-Msg\u0026#34;); /** * The codes below will generate a span, * and two types of tags, one type tag: keys are `tag1` and `tag2`, values are the passed-in parameters, respectively, the other type tag: keys are `username` and `age`, values are the return value in User, respectively */ @Trace @Tag(key = \u0026#34;tag1\u0026#34;, value = \u0026#34;arg[0]\u0026#34;) @Tag(key = \u0026#34;tag2\u0026#34;, value = \u0026#34;arg[1]\u0026#34;) @Tag(key = \u0026#34;username\u0026#34;, value = \u0026#34;returnedObj.username\u0026#34;) @Tag(key = \u0026#34;age\u0026#34;, value = \u0026#34;returnedObj.age\u0026#34;) public User methodYouWantToTrace(String param1, String param2) { // ActiveSpan.setOperationName(\u0026#34;Customize your own operation name, if this is an entry span, this would be an endpoint name\u0026#34;);  // ... }  Use TraceContext.putCorrelation() API to put custom data in tracing context.  Optional\u0026lt;String\u0026gt; previous = TraceContext.putCorrelation(\u0026#34;customKey\u0026#34;, \u0026#34;customValue\u0026#34;); CorrelationContext will remove the item when the value is null or empty.\n Use TraceContext.getCorrelation() API to get custom data.  Optional\u0026lt;String\u0026gt; value = TraceContext.getCorrelation(\u0026#34;customKey\u0026#34;); CorrelationContext configuration descriptions could be found in the agent configuration documentation, with correlation. as the prefix.\n","excerpt":"Dependency the toolkit, such as using maven or gradle  \u0026lt;dependency\u0026gt; …","ref":"/docs/main/v8.2.0/en/setup/service-agent/java-agent/application-toolkit-trace/","title":"Dependency the toolkit, such as using maven or gradle"},{"body":" Dependency the toolkit, such as using maven or gradle  \u0026lt;dependency\u0026gt; \u0026lt;groupId\u0026gt;org.apache.skywalking\u0026lt;/groupId\u0026gt; \u0026lt;artifactId\u0026gt;apm-toolkit-opentracing\u0026lt;/artifactId\u0026gt; \u0026lt;version\u0026gt;{project.release.version}\u0026lt;/version\u0026gt; \u0026lt;/dependency\u0026gt;  Use our OpenTracing tracer implementation  Tracer tracer = new SkywalkingTracer(); Tracer.SpanBuilder spanBuilder = tracer.buildSpan(\u0026#34;/yourApplication/yourService\u0026#34;); ","excerpt":" Dependency the toolkit, such as using maven or gradle  \u0026lt;dependency\u0026gt; …","ref":"/docs/main/v8.2.0/en/setup/service-agent/java-agent/opentracing/","title":"Dependency the toolkit, such as using maven or gradle"},{"body":"Deploy SkyWalking backend and UI in kubernetes Follow instructions in the deploying SkyWalking backend to Kubernetes cluster to deploy oap and ui to a kubernetes cluster.\nPlease read the Readme file.\n","excerpt":"Deploy SkyWalking backend and UI in kubernetes Follow instructions in the deploying SkyWalking …","ref":"/docs/main/v8.2.0/en/setup/backend/backend-k8s/","title":"Deploy SkyWalking backend and UI in kubernetes"},{"body":"Design Goals The document outlines the core design goals for SkyWalking project.\n  Keep Observability. No matter how does the target system deploy, SkyWalking could provide a solution or integration way to keep observability for it. Based on this, SkyWalking provides several runtime forms and probes.\n  Topology, Metrics and Trace Together. The first step to see and understand a distributed system should be from topology map. It visualizes the whole complex system as an easy map. Under that topology, OSS people requires more about metrics for service, instance, endpoint and calls. Trace exists as detail logs for making sense of those metrics. Such as when endpoint latency becomes long, you want to see the slowest the trace to find out why. So you can see, they are from big picture to details, they are all needed. SkyWalking integrates and provides a lot of features to make this possible and easy understand.\n  Light Weight. There two parts of light weight are needed. (1) In probe, we just depend on network communication framework, prefer gRPC. By that, the probe should be as small as possible, to avoid the library conflicts and the payload of VM, such as permsize requirement in JVM. (2) As an observability platform, it is secondary and third level system in your project environment. So we are using our own light weight framework to build the backend core. Then you don\u0026rsquo;t need to deploy big data tech platform and maintain them. SkyWalking should be simple in tech stack.\n  Pluggable. SkyWalking core team provides many default implementations, but definitely it is not enough, and also don\u0026rsquo;t fit every scenario. So, we provide a lot of features for being pluggable.\n  Portability. SkyWalking can run in multiple environments, including: (1) Use traditional register center like eureka. (2) Use RPC framework including service discovery, like Spring Cloud, Apache Dubbo. (3) Use Service Mesh in modern infrastructure. (4) Use cloud services. (5) Across cloud deployment. SkyWalking should run well in all these cases.\n  Interop. Observability is a big landscape, SkyWalking is impossible to support all, even by its community. As that, it supports to interop with other OSS system, mostly probes, such as Zipkin, Jaeger, OpenTracing, OpenCensus. To accept and understand their data formats makes sure SkyWalking more useful for end users. And don\u0026rsquo;t require the users to switch their libraries.\n  What is next?  See probe Introduction to know SkyWalking\u0026rsquo;s probe groups. From backend overview, you can understand what backend does after it received probe data. If you want to customize UI, start with UI overview document.  ","excerpt":"Design Goals The document outlines the core design goals for SkyWalking project.\n  Keep …","ref":"/docs/main/v8.2.0/en/concepts-and-designs/project-goals/","title":"Design Goals"},{"body":"Disable plugins Delete or remove the specific libraries / jars in skywalking-agent/plugins/*.jar\n+-- skywalking-agent +-- activations apm-toolkit-log4j-1.x-activation.jar apm-toolkit-log4j-2.x-activation.jar apm-toolkit-logback-1.x-activation.jar ... +-- config agent.config +-- plugins apm-dubbo-plugin.jar apm-feign-default-http-9.x.jar apm-httpClient-4.x-plugin.jar ..... skywalking-agent.jar ","excerpt":"Disable plugins Delete or remove the specific libraries / jars in skywalking-agent/plugins/*.jar\n+-- …","ref":"/docs/main/v8.2.0/en/setup/service-agent/java-agent/how-to-disable-plugin/","title":"Disable plugins"},{"body":"Dynamic Configuration SkyWalking Configurations mostly are set through application.yml and OS system environment variables. At the same time, some of them are supporting dynamic settings from upstream management system.\nRight now, SkyWalking supports following dynamic configurations.\n   Config Key Value Description Value Format Example     agent-analyzer.default.slowDBAccessThreshold Thresholds of slow Database statement, override receiver-trace/default/slowDBAccessThreshold of applciation.yml. default:200,mongodb:50   agent-analyzer.default.uninstrumentedGateways The uninstrumented gateways, override gateways.yml. same as gateways.yml   alarm.default.alarm-settings The alarm settings, will override alarm-settings.yml. same as alarm-settings.yml   core.default.apdexThreshold The apdex threshold settings, will override service-apdex-threshold.yml. same as service-apdex-threshold.yml   core.default.endpoint-name-grouping The endpoint name grouping setting, will override endpoint-name-grouping.yml. same as endpoint-name-grouping.yml   agent-analyzer.default.sampleRate Trace sampling , override receiver-trace/default/sampleRate of applciation.yml. 10000    This feature depends on upstream service, so it is DISABLED by default.\nconfiguration: selector: ${SW_CONFIGURATION:none} none: grpc: host: ${SW_DCS_SERVER_HOST:\u0026#34;\u0026#34;} port: ${SW_DCS_SERVER_PORT:80} clusterName: ${SW_DCS_CLUSTER_NAME:SkyWalking} period: ${SW_DCS_PERIOD:20} # ... other implementations Dynamic Configuration Service, DCS Dynamic Configuration Service is a gRPC service, which requires the upstream system implemented. The SkyWalking OAP fetches the configuration from the implementation(any system), after you open this implementation like this.\nconfiguration: selector: ${SW_CONFIGURATION:grpc} grpc: host: ${SW_DCS_SERVER_HOST:\u0026#34;\u0026#34;} port: ${SW_DCS_SERVER_PORT:80} clusterName: ${SW_DCS_CLUSTER_NAME:SkyWalking} period: ${SW_DCS_PERIOD:20} Dynamic Configuration Zookeeper Implementation Zookeeper is also supported as DCC(Dynamic Configuration Center), to use it, please configure as follows:\nconfiguration: selector: ${SW_CONFIGURATION:zookeeper} zookeeper: period: ${SW_CONFIG_ZK_PERIOD:60} # Unit seconds, sync period. Default fetch every 60 seconds. nameSpace: ${SW_CONFIG_ZK_NAMESPACE:/default} hostPort: ${SW_CONFIG_ZK_HOST_PORT:localhost:2181} # Retry Policy baseSleepTimeMs: ${SW_CONFIG_ZK_BASE_SLEEP_TIME_MS:1000} # initial amount of time to wait between retries maxRetries: ${SW_CONFIG_ZK_MAX_RETRIES:3} # max number of times to retry The nameSpace is the ZooKeeper path. The config key and value are the properties of the namespace folder.\nDynamic Configuration Etcd Implementation Etcd is also supported as DCC(Dynamic Configuration Center), to use it, please configure as follows:\nconfiguration: selector: ${SW_CONFIGURATION:etcd} etcd: period: ${SW_CONFIG_ETCD_PERIOD:60} # Unit seconds, sync period. Default fetch every 60 seconds. group: ${SW_CONFIG_ETCD_GROUP:skywalking} serverAddr: ${SW_CONFIG_ETCD_SERVER_ADDR:localhost:2379} clusterName: ${SW_CONFIG_ETCD_CLUSTER_NAME:default} Dynamic Configuration Consul Implementation Consul is also supported as DCC(Dynamic Configuration Center), to use it, please configure as follows:\nconfiguration: selector: ${SW_CONFIGURATION:consul} consul: # Consul host and ports, separated by comma, e.g. 1.2.3.4:8500,2.3.4.5:8500 hostAndPorts: ${SW_CONFIG_CONSUL_HOST_AND_PORTS:1.2.3.4:8500} # Sync period in seconds. Defaults to 60 seconds. period: ${SW_CONFIG_CONSUL_PERIOD:1} # Consul aclToken aclToken: ${SW_CONFIG_CONSUL_ACL_TOKEN:\u0026#34;\u0026#34;} Dynamic Configuration Apollo Implementation Apollo is also supported as DCC(Dynamic Configuration Center), to use it, just configured as follows:\nconfiguration: selector: ${SW_CONFIGURATION:apollo} apollo: apolloMeta: ${SW_CONFIG_APOLLO:http://106.12.25.204:8080} apolloCluster: ${SW_CONFIG_APOLLO_CLUSTER:default} apolloEnv: ${SW_CONFIG_APOLLO_ENV:\u0026#34;\u0026#34;} appId: ${SW_CONFIG_APOLLO_APP_ID:skywalking} period: ${SW_CONFIG_APOLLO_PERIOD:5} Dynamic Configuration Kuberbetes Configmap Implementation configmap is also supported as DCC(Dynamic Configuration Center), to use it, just configured as follows:\nconfiguration: selector: ${SW_CONFIGURATION:k8s-configmap} # [example] (../../../../oap-server/server-configuration/configuration-k8s-configmap/src/test/resources/skywalking-dynamic-configmap.example.yaml) k8s-configmap: # Sync period in seconds. Defaults to 60 seconds. period: ${SW_CONFIG_CONFIGMAP_PERIOD:60} # Which namespace is confiigmap deployed in. namespace: ${SW_CLUSTER_K8S_NAMESPACE:default} # Labelselector is used to locate specific configmap labelSelector: ${SW_CLUSTER_K8S_LABEL:app=collector,release=skywalking} Dynamic Configuration Nacos Implementation Nacos is also supported as DCC(Dynamic Configuration Center), to use it, please configure as follows:\nconfiguration: selector: ${SW_CONFIGURATION:nacos} nacos: # Nacos Server Host serverAddr: ${SW_CONFIG_NACOS_SERVER_ADDR:127.0.0.1} # Nacos Server Port port: ${SW_CONFIG_NACOS_SERVER_PORT:8848} # Nacos Configuration Group group: ${SW_CONFIG_NACOS_SERVER_GROUP:skywalking} # Nacos Configuration namespace namespace: ${SW_CONFIG_NACOS_SERVER_NAMESPACE:} # Unit seconds, sync period. Default fetch every 60 seconds. period: ${SW_CONFIG_NACOS_PERIOD:60} # the name of current cluster, set the name if you want to upstream system known. clusterName: ${SW_CONFIG_NACOS_CLUSTER_NAME:default} ","excerpt":"Dynamic Configuration SkyWalking Configurations mostly are set through application.yml and OS system …","ref":"/docs/main/v8.2.0/en/setup/backend/dynamic-config/","title":"Dynamic Configuration"},{"body":"ElasticSearch Some new users may face\n ElasticSearch performance is not as good as expected. Such as, can\u0026rsquo;t have latest data after a while.  Or\n ERROR CODE 429.   Suppressed: org.elasticsearch.client.ResponseException: method [POST], host [http://127.0.0.1:9200], URI [/service_instance_inventory/type/6_tcc-app-gateway-77b98ff6ff-crblx.cards_0_0/_update?refresh=true\u0026amp;timeout=1m], status line [HTTP/1.1 429 Too Many Requests] {\u0026quot;error\u0026quot;:{\u0026quot;root_cause\u0026quot;:[{\u0026quot;type\u0026quot;:\u0026quot;remote_transport_exception\u0026quot;,\u0026quot;reason\u0026quot;:\u0026quot;[elasticsearch-0][10.16.9.130:9300][indices:data/write/update[s]]\u0026quot;}],\u0026quot;type\u0026quot;:\u0026quot;es_rejected_execution_exception\u0026quot;,\u0026quot;reason\u0026quot;:\u0026quot;rejected execution of org.elasticsearch.transport.TransportService$7@19a5cf02 on EsThreadPoolExecutor[name = elasticsearch-0/write, queue capacity = 200, org.elasticsearch.common.util.concurrent.EsThreadPoolExecutor@389297ad[Running, pool size = 2, active threads = 2, queued tasks = 200, completed tasks = 147611]]\u0026quot;},\u0026quot;status\u0026quot;:429} at org.elasticsearch.client.RestClient$SyncResponseListener.get(RestClient.java:705) ~[elasticsearch-rest-client-6.3.2.jar:6.3.2] at org.elasticsearch.client.RestClient.performRequest(RestClient.java:235) ~[elasticsearch-rest-client-6.3.2.jar:6.3.2] at org.elasticsearch.client.RestClient.performRequest(RestClient.java:198) ~[elasticsearch-rest-client-6.3.2.jar:6.3.2] at org.elasticsearch.client.RestHighLevelClient.performRequest(RestHighLevelClient.java:522) ~[elasticsearch You could add following config to elasticsearch.yml, set the value based on your env.\n# In tracing scenario, consider to set more than this at least. thread_pool.index.queue_size: 1000 thread_pool.write.queue_size: 1000 # When you face query error at trace page, remember to check this. index.max_result_window: 1000000 Read ElasticSearch official documents to get more information.\n","excerpt":"ElasticSearch Some new users may face\n ElasticSearch performance is not as good as expected. Such …","ref":"/docs/main/v8.2.0/en/faq/es-server-faq/","title":"ElasticSearch"},{"body":"Exporter tool of profile raw data When the visualization doesn\u0026rsquo;t work well through the official UI, users could submit the issue to report. This tool helps the users to package the original profile data for helping the community to locate the issue in the user case. NOTICE, this report includes the class name, method name, line number, etc. Before submit this, please make sure this wouldn\u0026rsquo;t become your system vulnerability.\nExport command line Usage  Set the storage in tools/profile-exporter/application.yml file by following your use case. Prepare data  Profile task id: Profile task id Trace id: Wrong profiled trace id Export dir: Directory of the data will export   Enter the Skywalking root path Execute shell command bash tools/profile-exporter/profile_exporter.sh --taskid={profileTaskId} --traceid={traceId} {exportDir}  The file {traceId}.tar.gz will be generated after execution shell.  Exported data content  basic.yml: Contains the complete information of the profiled segments in the trace. snapshot.data: All monitored thread snapshot data in the current segment.  Report profile issue  Provide exported data generated from this tool. Provide span operation name, analyze mode(include/exclude children). Issue description. (If there have the UI screenshots, it\u0026rsquo;s better)  ","excerpt":"Exporter tool of profile raw data When the visualization doesn\u0026rsquo;t work well through the …","ref":"/docs/main/v8.2.0/en/guides/backend-profile-export/","title":"Exporter tool of profile raw data"},{"body":"Extend storage SkyWalking has already provided several storage solutions. In this document, you could learn how to implement a new storage easily.\nDefine your storage provider  Define a class extends org.apache.skywalking.oap.server.library.module.ModuleProvider. Set this provider targeting to Storage module.  @Override public Class\u0026lt;? extends ModuleDefine\u0026gt; module() { return StorageModule.class; } Implement all DAOs Here is the list of all DAO interfaces in storage\n IServiceInventoryCacheDAO IServiceInstanceInventoryCacheDAO IEndpointInventoryCacheDAO INetworkAddressInventoryCacheDAO IBatchDAO StorageDAO IRegisterLockDAO ITopologyQueryDAO IMetricsQueryDAO ITraceQueryDAO IMetadataQueryDAO IAggregationQueryDAO IAlarmQueryDAO IHistoryDeleteDAO IMetricsDAO IRecordDAO IRegisterDAO ILogQueryDAO ITopNRecordsQueryDAO IBrowserLogQueryDAO  Register all service implementations In public void prepare(), use this#registerServiceImplementation method to do register binding your implementation with the above interfaces.\nExample Take org.apache.skywalking.oap.server.storage.plugin.elasticsearch.StorageModuleElasticsearchProvider or org.apache.skywalking.oap.server.storage.plugin.jdbc.mysql.MySQLStorageProvider as a good example.\nRedistribution with new storage implementation. You don\u0026rsquo;t have to clone the main repo just for implementing the storage. You could just easy depend our Apache releases. Take a look at SkyAPM/SkyWalking-With-Es5x-Storage repo, SkyWalking v6 redistribution with ElasticSearch 5 TCP connection storage implementation.\n","excerpt":"Extend storage SkyWalking has already provided several storage solutions. In this document, you …","ref":"/docs/main/v8.2.0/en/guides/storage-extention/","title":"Extend storage"},{"body":"FAQs These are known and common FAQs. We welcome you to contribute yours.\nDesign  Why doesn\u0026rsquo;t SkyWalking involve MQ in the architecture?  Compiling  Protoc plugin fails in maven build Required items could not be found, when import project into Eclipse Maven compilation failure with python2 not found error  Runtime  8.x+ upgrade Why metrics indexes(ElasticSearch) in Hour and Day precisions stop update after upgrade to 7.x? 6.x version upgrade Why only traces in UI? The trace doesn\u0026rsquo;t continue in kafka consumer side Agent or collector version upgrade, 3.x -\u0026gt; 5.0.0-alpha EnhanceRequireObjectCache class cast exception ElasticSearch server performance FAQ, including ERROR CODE:429 IllegalStateException when install Java agent on WebSphere 7 \u0026ldquo;FORBIDDEN/12/index read-only / allow delete (api)\u0026rdquo; appears in the log No data shown and backend replies with \u0026ldquo;Variable \u0026lsquo;serviceId\u0026rsquo; has coerced Null value for NonNull type \u0026lsquo;ID!'\u0026quot; Unexpected endpoint register warning after 6.6.0 Use the profile exporter tool if the profile analysis is not right Compatible with other javaagent bytecode processing Java agent memory leak when enhance Worker thread at use Thread Pool Thrift plugin  ","excerpt":"FAQs These are known and common FAQs. We welcome you to contribute yours.\nDesign  Why doesn\u0026rsquo;t …","ref":"/docs/main/v8.2.0/en/faq/readme/","title":"FAQs"},{"body":"Group Parameterized Endpoints In most cases, the endpoint should be detected automatically through the language agents, service mesh observability solution, or configuration of meter system.\nThere are some special cases, especially when people use REST style URI, the application codes put the parameter in the endpoint name, such as putting order id in the URI, like /prod/ORDER123 and /prod/ORDER123. But logically, people expect they could have an endpoint name like prod/{order-id}. This is the feature of parameterized endpoint grouping designed for.\nCurrent, user could set up grouping rules through the static YAML file, named endpoint-name-grouping.yml, or use Dynamic Configuration to initial and update the endpoint grouping rule.\nConfiguration Format No matter in static local file or dynamic configuration value, they are sharing the same YAML format.\ngrouping: # Endpoint of the service would follow the following rules - service-name: serviceA rules: # Logic name when the regex expression matched. - endpoint-name: /prod/{id} regex: \\/prod\\/.+ ","excerpt":"Group Parameterized Endpoints In most cases, the endpoint should be detected automatically through …","ref":"/docs/main/v8.2.0/en/setup/backend/endpoint-grouping-rules/","title":"Group Parameterized Endpoints"},{"body":"Guides There are many ways that you can help the SkyWalking community.\n Go through our documents, point out or fix unclear things. Translate the documents to other languages. Download our releases, try to monitor your applications, and feedback to us about what you think. Read our source codes, Ask questions for details. Find some bugs, submit issue, and try to fix it. Find help wanted issues, which are good for you to start. Submit issue or start discussion through GitHub issue. See all mail list discussion through website list review. If you are a SkyWalking committer, could login and use the mail list in browser mode. Otherwise, follow the next step to subscribe. Issue report and discussion also could take place in dev@skywalking.apache.org. Mail to dev-subscribe@skywalking.apache.org, follow the reply to subscribe the mail list.  Contact Us All the following channels are open to the community, you could choose the way you like.\n Submit an issue Mail list: dev@skywalking.apache.org. Mail to dev-subscribe@skywalking.apache.org, follow the reply to subscribe the mail list. Gitter QQ Group: 392443393  Become official Apache SkyWalking Committer The PMC will assess the contributions of every contributor, including, but not limited to, code contributions, and follow the Apache guides to promote, vote and invite new committer and PMC member. Read Become official Apache SkyWalking Committer to get details.\nFor code developer For developers, first step, read Compiling Guide. It teaches developer how to build the project in local and set up the environment.\nIntegration Tests After setting up the environment and writing your codes, in order to make it more easily accepted by SkyWalking project, you\u0026rsquo;ll need to run the tests locally to verify that your codes don\u0026rsquo;t break any existed features, and write some unit test (UT) codes to verify that the new codes work well, preventing them being broke by future contributors. If the new codes involve other components or libraries, you\u0026rsquo;re also supposed to write integration tests (IT).\nSkyWalking leverages plugin maven-surefire-plugin to run the UTs while using maven-failsafe-plugin to run the ITs, maven-surefire-plugin will exclude ITs (whose class name starts with IT) and leave them for maven-failsafe-plugin to run, which is bound to the verify goal, CI-with-IT profile. Therefore, to run the UTs, try ./mvnw clean test, this will only run the UTs, not including ITs.\nIf you want to run the ITs please activate the CI-with-IT profile as well as the the profiles of the modules whose ITs you want to run. e.g. if you want to run the ITs in oap-server, try ./mvnw -Pbackend,CI-with-IT clean verify, and if you\u0026rsquo;d like to run all the ITs, simply run ./mvnw -Pall,CI-with-IT clean verify.\nPlease be advised that if you\u0026rsquo;re writing integration tests, name it with the pattern IT* to make them only run in CI-with-IT profile.\nEnd to End Tests (E2E for short) Since version 6.3.0, we have introduced more automatic tests to perform software quality assurance, E2E is one of the most important parts.\n End-to-end testing is a methodology used to test whether the flow of an application is performing as designed from start to finish. The purpose of carrying out end-to-end tests is to identify system dependencies and to ensure that the right information is passed between various system components and systems.\n The e2e test involves some/all of the OAP server, storage, coordinator, webapp, and the instrumented services, all of which are orchestrated by docker-compose, besides, there is a test controller(JUnit test) running outside of the container that sends traffics to the instrumented service, and then verifies the corresponding results after those requests, by GraphQL API of the SkyWalking Web App.\nWriting E2E Cases  Set up environment in IntelliJ IDEA  The e2e test is an separated project under the SkyWalking root directory and the IDEA cannot recognize it by default, right click on the file test/e2e/pom.xml and click Add as Maven Project, things should be ready now. But we recommend to open the directory skywalking/test/e2e in a separated IDE window for better experience because there may be shaded classes issues.\n Orchestrate the components  Our goal of E2E tests is to test the SkyWalking project in a whole, including the OAP server, storage, coordinator, webapp, and even the frontend UI(not now), in single node mode as well as cluster mode, therefore the first step is to determine what case we are going to verify and orchestrate the components.\nIn order to make it more easily to orchestrate, we\u0026rsquo;re using a docker-compose that provides a convenient file format (docker-compose.yml) to orchestrate the needed containers, and gives us possibilities to define the dependencies of the components.\nBasically you will need:\n Decide what (and how many) containers will be needed, e.g. for cluster testing, you\u0026rsquo;ll need \u0026gt; 2 OAP nodes, coordinators like zookeeper, storage like ElasticSearch, and instrumented services; Define the containers in docker-compose.yml, and carefully specify the dependencies, starting orders, and most importantly, link them together, e.g. set correct OAP address in the agent side, set correct coordinator address in OAP, etc. Write (or hopefully reuse) the test codes, to verify the results is correct.  As for the last step, we have a friendly framework to help you get started more quickly, which provides annotation @DockerCompose(\u0026quot;docker-compose.yml\u0026quot;) to load/parse and start up all the containers in a proper order, @ContainerHost/@ContainerPort to get the real host/port of the container, @ContainerHostAndPort to get both, @DockerContainer to get the running container.\n Write test controller  To put it simple, test controllers are basically tests that can be bound to the Maven integration-test/verify phase. They send designed requests to the instrumented service, and expect to get corresponding traces/metrics/metadata from the SkyWalking webapp GraphQL API.\nIn the test framework, we provide a TrafficController to periodically send traffic data to the instrumented services, you can simply enable it by giving a url and traffic data, refer to [../../../test/e2e/e2e-test/src/test/java/org/apache/skywalking/e2e/base/TrafficController.java].\n Troubleshooting  We expose all the logs from all containers to the stdout in non-CI (local) mode, but save/and upload them all to the GitHub server and you can download them (only when tests failed) in the right-up button \u0026ldquo;Artifacts/Download artifacts/logs\u0026rdquo; for debugging.\nNOTE: Please verify the newly-added E2E test case locally first, however, if you find it passed locally but failed in the PR check status, make sure all the updated/newly-added files (especially those in submodules) are committed and included in that PR, or reset the git HEAD to the remote and verify locally again.\nE2E local remote debugging When the E2E test is executed locally, if any test case fails, the E2E local remote debugging function can be used to quickly troubleshoot the bug.\nProject Extensions SkyWalking project supports many ways to extend existing features. If you are interesting in these ways, read the following guides.\n Java agent plugin development guide. This guide helps you to develop SkyWalking agent plugin to support more frameworks. Both open source plugin and private plugin developer should read this. If you want to build a new probe or plugin in any language, please read Component library definition and extension document. Storage extension development guide. Help potential contributors to build a new storage implementor besides the official. Customize analysis by oal script. OAL scripts locate in config/oal/*.oal. You could change it and reboot the OAP server. Read Observability Analysis Language Introduction if you need to learn about OAL script. Source and scope extension for new metrics. If you want to analysis a new metrics, which SkyWalking haven\u0026rsquo;t provide. You need to add a new receiver rather than choosing existed receiver. At that moment, you most likely need to add a new source and scope. This document will teach you how to do.  UI developer Our UI is constituted by static pages and web container.\n RocketBot UI is SkyWalking primary UI since 6.1 release. It is built with vue + typescript. You could know more at the rocketbot repository. Web container source codes are in apm-webapp module. This is a just an easy zuul proxy to host static resources and send GraphQL query requests to backend. Legacy UI repository is still there, but not included in SkyWalking release, after 6.0.0-GA.  OAP backend dependency management  This section is only applicable to the dependencies of the backend module\n Being one of the Top Level Projects of The Apache Software Foundation (ASF), SkyWalking is supposed to follow the ASF 3RD PARTY LICENSE POLICY, so if you\u0026rsquo;re adding new dependencies to the project, you\u0026rsquo;re responsible to check the newly-added dependencies won\u0026rsquo;t break the policy, and add their LICENSE\u0026rsquo;s and NOTICES\u0026rsquo;s to the project.\nWe have a simple script to help you make sure that you didn\u0026rsquo;t miss any newly-added dependency:\n Build a distribution package and unzip/untar it to folder dist. Run the script in the root directory, it will print out all newly-added dependencies. Check the LICENSE\u0026rsquo;s and NOTICE\u0026rsquo;s of those dependencies, if they can be included in an ASF project, add them in the apm-dist/release-docs/{LICENSE,NOTICE} file. Add those dependencies' names to the tools/dependencies/known-oap-backend-dependencies.txt file (alphabetical order), the next run of check-LICENSE.sh should pass.  Profile The performance profile is an enhancement feature in the APM system. We are using the thread dump to estimate the method execution time, rather than adding many local spans. In this way, the resource cost would be much less than using distributed tracing to locate slow method. This feature is suitable in the production environment. The following documents are important for developers to understand the key parts of this feature\n Profile data report procotol is provided like other trace, JVM data through gRPC. Thread dump merging mechanism introduces the merging mechanism, which helps the end users to understand the profile report. Exporter tool of profile raw data introduces when the visualization doesn\u0026rsquo;t work well through the official UI, how to package the original profile data, which helps the users report the issue.  For release Apache Release Guide introduces to the committer team about doing official Apache version release, to avoid breaking any Apache rule. Apache license allows everyone to redistribute if you keep our licenses and NOTICE in your redistribution.\n","excerpt":"Guides There are many ways that you can help the SkyWalking community.\n Go through our documents, …","ref":"/docs/main/v8.2.0/en/guides/readme/","title":"Guides"},{"body":"Health Check Health check intends to provide a unique approach to check the healthy status of OAP server. It includes the health status of modules, GraphQL and gRPC services readiness.\nHealth Checker Module. Health Checker module could solute how to observe the health status of modules. We can active it by below:\nhealth-checker: selector: ${SW_HEALTH_CHECKER:default} default: checkIntervalSeconds: ${SW_HEALTH_CHECKER_INTERVAL_SECONDS:5} Notice, we should enable telemetry module at the same time. That means the provider should not be - and none.\nAfter that, we can query OAP server health status by querying GraphQL:\nquery{ checkHealth{ score details } } If the OAP server is healthy, the response should be\n{ \u0026#34;data\u0026#34;: { \u0026#34;checkHealth\u0026#34;: { \u0026#34;score\u0026#34;: 0, \u0026#34;details\u0026#34;: \u0026#34;\u0026#34; } } } Once some modules are unhealthy, for instance, storage H2 is down. The result might be like below:\n{ \u0026#34;data\u0026#34;: { \u0026#34;checkHealth\u0026#34;: { \u0026#34;score\u0026#34;: 1, \u0026#34;details\u0026#34;: \u0026#34;storage_h2,\u0026#34; } } } You could refer to checkHealth query for more details.\nThe readiness of GraphQL and gRPC We could opt to above query to check the readiness of GraphQL.\nOAP has implemented gRPC Health Checking Protocol. We could use grpc-health-probe or any other tools to check the health of OAP gRPC services.\nCLI tool Please follow the CLI doc to get the health status score directly through the checkhealth command.\n","excerpt":"Health Check Health check intends to provide a unique approach to check the healthy status of OAP …","ref":"/docs/main/v8.2.0/en/setup/backend/backend-health-check/","title":"Health Check"},{"body":"How to build project This document helps people to compile and build the project in your maven and set your IDE.\nBuild Project Because we are using Git submodule, we recommend don\u0026rsquo;t use GitHub tag or release page to download source codes for compiling.\nMaven behind Proxy If you need to execute build behind the proxy, edit the .mvn/jvm.config and put the follow properties:\n-Dhttp.proxyHost=proxy_ip -Dhttp.proxyPort=proxy_port -Dhttps.proxyHost=proxy_ip -Dhttps.proxyPort=proxy_port -Dhttp.proxyUser=username -Dhttp.proxyPassword=password Build from GitHub   Prepare git, JDK8+ and Maven 3.6+\n  Clone project\nIf you want to build a release from source codes, provide a tag name by using git clone -b [tag_name] ... while cloning.\ngit clone --recurse-submodules https://github.com/apache/skywalking.git cd skywalking/ OR git clone https://github.com/apache/skywalking.git cd skywalking/ git submodule init git submodule update   Run ./mvnw clean package -DskipTests\n  All packages are in /dist (.tar.gz for Linux and .zip for Windows).\n  Build from Apache source code release  What is Apache source code release?  For each official Apache release, there is a complete and independent source code tar, which is including all source codes. You could download it from SkyWalking Apache download page. No git related stuff required when compiling this. Just follow these steps.\n Prepare JDK8+ and Maven 3.6+ Run ./mvnw clean package -DskipTests All packages are in /dist.(.tar.gz for Linux and .zip for Windows).  Advanced compile SkyWalking is a complex maven project, including many modules, which could cause long compiling time. If you just want to recompile part of the project, you have following options\n Compile agent and package   ./mvnw package -Pagent,dist\n or\n make build.agent\n If you intend to compile a single one plugin, such as in the dev stage, you could\n cd plugin_module_dir \u0026amp; mvn clean package\n  Compile backend and package   ./mvnw package -Pbackend,dist\n or\n make build.backend\n  Compile UI and package   ./mvnw package -Pui,dist\n or\n make build.ui\n Build docker images We can build docker images of backend and ui with Makefile located in root folder.\nRefer to Build docker image for more details.\nSetup your IntelliJ IDEA NOTICE: If you clone the codes from GitHub, please make sure that you had finished step 1 to 3 in section Build from GitHub, if you download the source codes from the official website of SkyWalking, please make sure that you had followed the steps in section Build from Apache source code release.\n Import the project as a maven project Run ./mvnw compile -Dmaven.test.skip=true to compile project and generate source codes. Because we use gRPC and protobuf. Set Generated Source Codes folders.  grpc-java and java folders in apm-protocol/apm-network/target/generated-sources/protobuf grpc-java and java folders in oap-server/server-core/target/generated-sources/protobuf grpc-java and java folders in oap-server/server-receiver-plugin/receiver-proto/target/generated-sources/protobuf grpc-java and java folders in oap-server/exporter/target/generated-sources/protobuf grpc-java and java folders in oap-server/server-configuration/grpc-configuration-sync/target/generated-sources/protobuf antlr4 folder in oap-server/oal-grammar/target/generated-sources    ","excerpt":"How to build project This document helps people to compile and build the project in your maven and …","ref":"/docs/main/v8.2.0/en/guides/how-to-build/","title":"How to build project"},{"body":"How to enable Kafka Reporter The Kafka reporter plugin support report traces, JVM metrics, Instance Properties, and profiled snapshots to Kafka cluster, which is disabled in default. Move the jar of the plugin, kafka-reporter-plugin-x.y.z.jar, from agent/optional-reporter-plugins to agent/plugins for activating.\nNotice, currently, the agent still needs to configure GRPC receiver for delivering the task of profiling. In other words, the following configure cannot be omitted.\n# Backend service addresses. collector.backend_service=${SW_AGENT_COLLECTOR_BACKEND_SERVICES:127.0.0.1:11800} # Kafka producer configuration plugin.kafka.bootstrap_servers=${SW_KAFKA_BOOTSTRAP_SERVERS:localhost:9092} plugin.kafka.producer_config[delivery.timeout.ms]=12000 plugin.kafka.get_topic_timeout=${SW_GET_TOPIC_TIMEOUT:10} Kafka reporter plugin support to customize all configurations of listed in here.\nBefore you activated the Kafka reporter, you have to make sure that Kafka fetcher has been opened in service.\n","excerpt":"How to enable Kafka Reporter The Kafka reporter plugin support report traces, JVM metrics, Instance …","ref":"/docs/main/v8.2.0/en/setup/service-agent/java-agent/how-to-enable-kafka-reporter/","title":"How to enable Kafka Reporter"},{"body":"How to tolerate custom exceptions In some codes, the exception is being used as a way of controlling business flow. Skywalking provides 2 ways to tolerate an exception which is traced in a span.\n Set the names of exception classes in the agent config Use our annotation in the codes.  Set the names of exception classes in the agent config The property named \u0026ldquo;statuscheck.ignored_exceptions\u0026rdquo; is used to set up class names in the agent configuration file. if the exception listed here are detected in the agent, the agent core would flag the related span as the error status.\nDemo   A custom exception.\n TestNamedMatchException  package org.apache.skywalking.apm.agent.core.context.status; public class TestNamedMatchException extends RuntimeException { public TestNamedMatchException() { } public TestNamedMatchException(final String message) { super(message); } ... }  TestHierarchyMatchException  package org.apache.skywalking.apm.agent.core.context.status; public class TestHierarchyMatchException extends TestNamedMatchException { public TestHierarchyMatchException() { } public TestHierarchyMatchException(final String message) { super(message); } ... }   When the above exceptions traced in some spans, the status is like the following.\n   The traced exception Final span status     org.apache.skywalking.apm.agent.core.context.status.TestNamedMatchException true   org.apache.skywalking.apm.agent.core.context.status.TestHierarchyMatchException true      After set these class names through \u0026ldquo;statuscheck.ignored_exceptions\u0026rdquo;, the status of spans would be changed.\nstatuscheck.ignored_exceptions=org.apache.skywalking.apm.agent.core.context.status.TestNamedMatchException    The traced exception Final span status     org.apache.skywalking.apm.agent.core.context.status.TestNamedMatchException false   org.apache.skywalking.apm.agent.core.context.status.TestHierarchyMatchException false      Use our annotation in the codes. If an exception has the @IgnoredException annotation, the exception wouldn\u0026rsquo;t be marked as error status when tracing. Because the annotation supports inheritance, also affects the subclasses.\nDependency  Dependency the toolkit, such as using maven or gradle. Since 8.2.0.  \u0026lt;dependency\u0026gt; \u0026lt;groupId\u0026gt;org.apache.skywalking\u0026lt;/groupId\u0026gt; \u0026lt;artifactId\u0026gt;apm-toolkit-log4j-1.x\u0026lt;/artifactId\u0026gt; \u0026lt;version\u0026gt;{project.release.version}\u0026lt;/version\u0026gt; \u0026lt;/dependency\u0026gt; Demo   A custom exception.\npackage org.apache.skywalking.apm.agent.core.context.status; public class TestAnnotatedException extends RuntimeException { public TestAnnotatedException() { } public TestAnnotatedException(final String message) { super(message); } ... }   When the above exception traced in some spans, the status is like the following.\n   The traced exception Final span status     org.apache.skywalking.apm.agent.core.context.status.TestAnnotatedException true      However, when the exception annotated with the annotation, the status would be changed.\npackage org.apache.skywalking.apm.agent.core.context.status; @IgnoredException public class TestAnnotatedException extends RuntimeException { public TestAnnotatedException() { } public TestAnnotatedException(final String message) { super(message); } ... }    The traced exception Final span status     org.apache.skywalking.apm.agent.core.context.status.TestAnnotatedException false      Recursive check Due to the wrapper nature of Java exceptions, sometimes users need recursive checking. Skywalking also supports it. Typically, we don\u0026rsquo;t recommend setting this more than 10, which could cause a performance issue. Negative value and 0 would be ignored, which means all exceptions would make the span tagged in error status.\n statuscheck.max_recursive_depth=${SW_STATUSCHECK_MAX_RECURSIVE_DEPTH:1} ","excerpt":"How to tolerate custom exceptions In some codes, the exception is being used as a way of controlling …","ref":"/docs/main/v8.2.0/en/setup/service-agent/java-agent/how-to-tolerate-exceptions/","title":"How to tolerate custom exceptions"},{"body":"HTTP API Protocol HTTP API Protocol defines the API data format, including api request and response data format. They use the HTTP1.1 wrapper of the official SkyWalking Browser Protocol. Read it for more details.\nPerformance Data Report Detail information about data format can be found in BrowserPerf.proto.\nPOST http://localhost:12800/browser/perfData Send a performance data object with JSON format.\nInput:\n{ \u0026#34;service\u0026#34;: \u0026#34;web\u0026#34;, \u0026#34;serviceVersion\u0026#34;: \u0026#34;v0.0.1\u0026#34;, \u0026#34;pagePath\u0026#34;: \u0026#34;/index.html\u0026#34;, \u0026#34;redirectTime\u0026#34;: 10, \u0026#34;dnsTime\u0026#34;: 10, \u0026#34;ttfbTime\u0026#34;: 10, \u0026#34;tcpTime\u0026#34;: 10, \u0026#34;transTime\u0026#34;: 10, \u0026#34;domAnalysisTime\u0026#34;: 10, \u0026#34;fptTime\u0026#34;: 10, \u0026#34;domReadyTime\u0026#34;: 10, \u0026#34;loadPageTime\u0026#34;: 10, \u0026#34;resTime\u0026#34;: 10, \u0026#34;sslTime\u0026#34;: 10, \u0026#34;ttlTime\u0026#34;: 10, \u0026#34;firstPackTime\u0026#34;: 10, \u0026#34;fmpTime\u0026#34;: 10 } OutPut:\nHttp Status: 204\nError Log Report Detail information about data format can be found in BrowserPerf.proto.\nPOST http://localhost:12800/browser/errorLogs Send an error log object list with JSON format.\nInput:\n[ { \u0026#34;uniqueId\u0026#34;: \u0026#34;55ec6178-3fb7-43ef-899c-a26944407b01\u0026#34;, \u0026#34;service\u0026#34;: \u0026#34;web\u0026#34;, \u0026#34;serviceVersion\u0026#34;: \u0026#34;v0.0.1\u0026#34;, \u0026#34;pagePath\u0026#34;: \u0026#34;/index.html\u0026#34;, \u0026#34;category\u0026#34;: \u0026#34;ajax\u0026#34;, \u0026#34;message\u0026#34;: \u0026#34;error\u0026#34;, \u0026#34;line\u0026#34;: 1, \u0026#34;col\u0026#34;: 1, \u0026#34;stack\u0026#34;: \u0026#34;error\u0026#34;, \u0026#34;errorUrl\u0026#34;: \u0026#34;/index.html\u0026#34; }, { \u0026#34;uniqueId\u0026#34;: \u0026#34;55ec6178-3fb7-43ef-899c-a26944407b02\u0026#34;, \u0026#34;service\u0026#34;: \u0026#34;web\u0026#34;, \u0026#34;serviceVersion\u0026#34;: \u0026#34;v0.0.1\u0026#34;, \u0026#34;pagePath\u0026#34;: \u0026#34;/index.html\u0026#34;, \u0026#34;category\u0026#34;: \u0026#34;ajax\u0026#34;, \u0026#34;message\u0026#34;: \u0026#34;error\u0026#34;, \u0026#34;line\u0026#34;: 1, \u0026#34;col\u0026#34;: 1, \u0026#34;stack\u0026#34;: \u0026#34;error\u0026#34;, \u0026#34;errorUrl\u0026#34;: \u0026#34;/index.html\u0026#34; } ] OutPut:\nHttp Status: 204\nPOST http://localhost:12800/browser/errorLog Send a single error log object with JSON format.\nInput:\n{ \u0026#34;uniqueId\u0026#34;: \u0026#34;55ec6178-3fb7-43ef-899c-a26944407b01\u0026#34;, \u0026#34;service\u0026#34;: \u0026#34;web\u0026#34;, \u0026#34;serviceVersion\u0026#34;: \u0026#34;v0.0.1\u0026#34;, \u0026#34;pagePath\u0026#34;: \u0026#34;/index.html\u0026#34;, \u0026#34;category\u0026#34;: \u0026#34;ajax\u0026#34;, \u0026#34;message\u0026#34;: \u0026#34;error\u0026#34;, \u0026#34;line\u0026#34;: 1, \u0026#34;col\u0026#34;: 1, \u0026#34;stack\u0026#34;: \u0026#34;error\u0026#34;, \u0026#34;errorUrl\u0026#34;: \u0026#34;/index.html\u0026#34; } OutPut:\nHttp Status: 204\n","excerpt":"HTTP API Protocol HTTP API Protocol defines the API data format, including api request and response …","ref":"/docs/main/v8.2.0/en/protocols/browser-http-api-protocol/","title":"HTTP API Protocol"},{"body":"HTTP API Protocol HTTP API Protocol defines the API data format, including api request and response data format. They use the HTTP1.1 wrapper of the official SkyWalking Trace Data Protocol v3. Read it for more details.\nInstance Management Detail information about data format can be found in Instance Management.\n Report service instance properties   POST http://localhost:12800/v3/management/reportProperties\n Input:\n{ \u0026#34;service\u0026#34;: \u0026#34;User Service Name\u0026#34;, \u0026#34;serviceInstance\u0026#34;: \u0026#34;User Service Instance Name\u0026#34;, \u0026#34;properties\u0026#34;: [{ \u0026#34;language\u0026#34;: \u0026#34;Lua\u0026#34; }] } Output JSON Array:\n{}  Service instance ping   POST http://localhost:12800/v3/management/keepAlive\n Input:\n{ \u0026#34;service\u0026#34;: \u0026#34;User Service Name\u0026#34;, \u0026#34;serviceInstance\u0026#34;: \u0026#34;User Service Instance Name\u0026#34; } OutPut:\n{} Trace Report Detail information about data format can be found in Instance Management. There are two ways to report segment data, one segment per request or segment array in the bulk mode.\nPOST http://localhost:12800/v3/segment Send a single segment object with JSON format.\nInput:\n{ \u0026#34;traceId\u0026#34;: \u0026#34;a12ff60b-5807-463b-a1f8-fb1c8608219e\u0026#34;, \u0026#34;serviceInstance\u0026#34;: \u0026#34;User_Service_Instance_Name\u0026#34;, \u0026#34;spans\u0026#34;: [{ \u0026#34;operationName\u0026#34;: \u0026#34;/ingress\u0026#34;, \u0026#34;startTime\u0026#34;: 1588664577013, \u0026#34;endTime\u0026#34;: 1588664577028, \u0026#34;spanType\u0026#34;: \u0026#34;Exit\u0026#34;, \u0026#34;spanId\u0026#34;: 1, \u0026#34;isError\u0026#34;: false, \u0026#34;parentSpanId\u0026#34;: 0, \u0026#34;componentId\u0026#34;: 6000, \u0026#34;peer\u0026#34;: \u0026#34;upstream service\u0026#34;, \u0026#34;spanLayer\u0026#34;: \u0026#34;Http\u0026#34; }, { \u0026#34;operationName\u0026#34;: \u0026#34;/ingress\u0026#34;, \u0026#34;startTime\u0026#34;: 1588664577013, \u0026#34;tags\u0026#34;: [{ \u0026#34;key\u0026#34;: \u0026#34;http.method\u0026#34;, \u0026#34;value\u0026#34;: \u0026#34;GET\u0026#34; }, { \u0026#34;key\u0026#34;: \u0026#34;http.params\u0026#34;, \u0026#34;value\u0026#34;: \u0026#34;http://localhost/ingress\u0026#34; }], \u0026#34;endTime\u0026#34;: 1588664577028, \u0026#34;spanType\u0026#34;: \u0026#34;Entry\u0026#34;, \u0026#34;spanId\u0026#34;: 0, \u0026#34;parentSpanId\u0026#34;: -1, \u0026#34;isError\u0026#34;: false, \u0026#34;spanLayer\u0026#34;: \u0026#34;Http\u0026#34;, \u0026#34;componentId\u0026#34;: 6000 }], \u0026#34;service\u0026#34;: \u0026#34;User_Service_Name\u0026#34;, \u0026#34;traceSegmentId\u0026#34;: \u0026#34;a12ff60b-5807-463b-a1f8-fb1c8608219e\u0026#34; } OutPut:\nPOST http://localhost:12800/v3/segments Send a segment object list with JSON format.\nInput:\n[{ \u0026#34;traceId\u0026#34;: \u0026#34;a12ff60b-5807-463b-a1f8-fb1c8608219e\u0026#34;, \u0026#34;serviceInstance\u0026#34;: \u0026#34;User_Service_Instance_Name\u0026#34;, \u0026#34;spans\u0026#34;: [{ \u0026#34;operationName\u0026#34;: \u0026#34;/ingress\u0026#34;, \u0026#34;startTime\u0026#34;: 1588664577013, \u0026#34;endTime\u0026#34;: 1588664577028, \u0026#34;spanType\u0026#34;: \u0026#34;Exit\u0026#34;, \u0026#34;spanId\u0026#34;: 1, \u0026#34;isError\u0026#34;: false, \u0026#34;parentSpanId\u0026#34;: 0, \u0026#34;componentId\u0026#34;: 6000, \u0026#34;peer\u0026#34;: \u0026#34;upstream service\u0026#34;, \u0026#34;spanLayer\u0026#34;: \u0026#34;Http\u0026#34; }, { \u0026#34;operationName\u0026#34;: \u0026#34;/ingress\u0026#34;, \u0026#34;startTime\u0026#34;: 1588664577013, \u0026#34;tags\u0026#34;: [{ \u0026#34;key\u0026#34;: \u0026#34;http.method\u0026#34;, \u0026#34;value\u0026#34;: \u0026#34;GET\u0026#34; }, { \u0026#34;key\u0026#34;: \u0026#34;http.params\u0026#34;, \u0026#34;value\u0026#34;: \u0026#34;http://localhost/ingress\u0026#34; }], \u0026#34;endTime\u0026#34;: 1588664577028, \u0026#34;spanType\u0026#34;: \u0026#34;Entry\u0026#34;, \u0026#34;spanId\u0026#34;: 0, \u0026#34;parentSpanId\u0026#34;: -1, \u0026#34;isError\u0026#34;: false, \u0026#34;spanLayer\u0026#34;: \u0026#34;Http\u0026#34;, \u0026#34;componentId\u0026#34;: 6000 }], \u0026#34;service\u0026#34;: \u0026#34;User_Service_Name\u0026#34;, \u0026#34;traceSegmentId\u0026#34;: \u0026#34;a12ff60b-5807-463b-a1f8-fb1c8608219e\u0026#34; }, { \u0026#34;traceId\u0026#34;: \u0026#34;f956699e-5106-4ea3-95e5-da748c55bac1\u0026#34;, \u0026#34;serviceInstance\u0026#34;: \u0026#34;User_Service_Instance_Name\u0026#34;, \u0026#34;spans\u0026#34;: [{ \u0026#34;operationName\u0026#34;: \u0026#34;/ingress\u0026#34;, \u0026#34;startTime\u0026#34;: 1588664577250, \u0026#34;endTime\u0026#34;: 1588664577250, \u0026#34;spanType\u0026#34;: \u0026#34;Exit\u0026#34;, \u0026#34;spanId\u0026#34;: 1, \u0026#34;isError\u0026#34;: false, \u0026#34;parentSpanId\u0026#34;: 0, \u0026#34;componentId\u0026#34;: 6000, \u0026#34;peer\u0026#34;: \u0026#34;upstream service\u0026#34;, \u0026#34;spanLayer\u0026#34;: \u0026#34;Http\u0026#34; }, { \u0026#34;operationName\u0026#34;: \u0026#34;/ingress\u0026#34;, \u0026#34;startTime\u0026#34;: 1588664577250, \u0026#34;tags\u0026#34;: [{ \u0026#34;key\u0026#34;: \u0026#34;http.method\u0026#34;, \u0026#34;value\u0026#34;: \u0026#34;GET\u0026#34; }, { \u0026#34;key\u0026#34;: \u0026#34;http.params\u0026#34;, \u0026#34;value\u0026#34;: \u0026#34;http://localhost/ingress\u0026#34; }], \u0026#34;endTime\u0026#34;: 1588664577250, \u0026#34;spanType\u0026#34;: \u0026#34;Entry\u0026#34;, \u0026#34;spanId\u0026#34;: 0, \u0026#34;parentSpanId\u0026#34;: -1, \u0026#34;isError\u0026#34;: false, \u0026#34;spanLayer\u0026#34;: \u0026#34;Http\u0026#34;, \u0026#34;componentId\u0026#34;: 6000 }], \u0026#34;service\u0026#34;: \u0026#34;User_Service_Name\u0026#34;, \u0026#34;traceSegmentId\u0026#34;: \u0026#34;f956699e-5106-4ea3-95e5-da748c55bac1\u0026#34; }] OutPut:\n","excerpt":"HTTP API Protocol HTTP API Protocol defines the API data format, including api request and response …","ref":"/docs/main/v8.2.0/en/protocols/http-api-protocol/","title":"HTTP API Protocol"},{"body":" HTTP Server  Tomcat 7 Tomcat 8 Tomcat 9 Spring Boot Web 4.x Spring MVC 3.x, 4.x 5.x with servlet 3.x Nutz Web Framework 1.x Struts2 MVC 2.3.x -\u0026gt; 2.5.x Resin 3 (Optional¹) Resin 4 (Optional¹) Jetty Server 9 Spring WebFlux 5.x (Optional¹) Undertow 1.3.0.Final -\u0026gt; 2.0.27.Final RESTEasy 3.1.0.Final -\u0026gt; 3.7.0.Final Play Framework 2.6.x -\u0026gt; 2.8.x Light4J Microservices Framework 1.6.x -\u0026gt; 2.x Netty SocketIO 1.x   HTTP Client  Feign 9.x Netflix Spring Cloud Feign 1.1.x -\u0026gt; 2.x Okhttp 3.x Apache httpcomponent HttpClient 2.0 -\u0026gt; 3.1, 4.2, 4.3 Spring RestTemplete 4.x Jetty Client 9 Apache httpcomponent AsyncClient 4.x   HTTP Gateway  Spring Cloud Gateway 2.0.2.RELEASE -\u0026gt; 2.2.x.RELEASE (Optional²)   JDBC  Mysql Driver 5.x, 6.x, 8.x Oracle Driver (Optional¹) H2 Driver 1.3.x -\u0026gt; 1.4.x Sharding-JDBC 1.5.x ShardingSphere 3.0.0, 4.0.0-RC1, 4.0.0, 4.0.1, 4.1.0, 4.1.1 PostgreSQL Driver 8.x, 9.x, 42.x Mariadb Driver 2.x, 1.8 InfluxDB 2.5 -\u0026gt; 2.17   RPC Frameworks  Dubbo 2.5.4 -\u0026gt; 2.6.0 Dubbox 2.8.4 Apache Dubbo 2.7.0 Motan 0.2.x -\u0026gt; 1.1.0 gRPC 1.x Apache ServiceComb Java Chassis 0.1 -\u0026gt; 0.5,1.x SOFARPC 5.4.0 Armeria 0.63.0 -\u0026gt; 0.98.0 Apache Avro 1.7.0 - 1.8.x Finagle 6.25.0 -\u0026gt; 20.1.0 Brpc-Java 2.3.7 -\u0026gt; 2.5.3 Thrift 0.10.0 -\u0026gt; 0.12.0   MQ  RocketMQ 4.x Kafka 0.11.0.0 -\u0026gt; 1.0 Spring-Kafka Spring Kafka Consumer 2.2.x ActiveMQ 5.10.0 -\u0026gt; 5.15.4 RabbitMQ 5.x Pulsar 2.2.x -\u0026gt; 2.4.x   NoSQL  Redis  Jedis 2.x Redisson Easy Java Redis client 3.5.2+ Lettuce 5.x   MongoDB Java Driver 2.13-2.14, 3.4.0-3.12.7, 4.0.0-4.1.0 Memcached Client  Spymemcached 2.x Xmemcached 2.x   Elasticsearch  transport-client 5.2.x-5.6.x transport-client 6.7.1-6.8.4 rest-high-level-client 6.7.1-6.8.4 rest-high-level-client 7.0.0-7.5.2   Solr  SolrJ 7.x   Cassandra 3.x  cassandra-java-driver 3.7.0-3.7.2   HBase  hbase-client HTable 1.x     Service Discovery  Netflix Eureka   Distributed Coordination  Zookeeper 3.4.x (Optional² \u0026amp; Except 3.4.4)   Spring Ecosystem  Spring Bean annotations(@Bean, @Service, @Component, @Repository) 3.x and 4.x (Optional²) Spring Core Async SuccessCallback/FailureCallback/ListenableFutureCallback 4.x Spring Transaction 4.x and 5.x (Optional²)   Hystrix: Latency and Fault Tolerance for Distributed Systems 1.4.20 -\u0026gt; 1.5.12 Scheduler  Elastic Job 2.x Apache ShardingSphere-Elasticjob 3.0.0-alpha Spring @Scheduled 3.1+ Quartz Scheduler 2.x (Optional²) XXL Job 2.x   OpenTracing community supported Canal: Alibaba mysql database binlog incremental subscription \u0026amp; consumer components 1.0.25 -\u0026gt; 1.1.2 JSON  GSON 2.8.x (Optional²)   Vert.x Ecosystem  Vert.x Eventbus 3.2+ Vert.x Web 3.x   Thread Schedule Framework  Spring @Async 4.x and 5.x Quasar 0.7.x   Cache  Ehcache 2.x   Kotlin  Coroutine 1.0.1 -\u0026gt; 1.3.x (Optional²)   GraphQL  Graphql 8.0 -\u0026gt; 15.x    ¹Due to license incompatibilities/restrictions these plugins are hosted and released in 3rd part repository, go to SkyAPM java plugin extension repository to get these.\n²These plugins affect the performance or must be used under some conditions, from experiences. So only released in /optional-plugins, copy to /plugins in order to make them work.\n","excerpt":"HTTP Server  Tomcat 7 Tomcat 8 Tomcat 9 Spring Boot Web 4.x Spring MVC 3.x, 4.x 5.x with servlet 3.x …","ref":"/docs/main/v8.2.0/en/setup/service-agent/java-agent/supported-list/","title":"HTTP Server"},{"body":"IllegalStateException when install Java agent on WebSphere This FAQ came from community discussion and feedback. This user installed SkyWalking Java agent on WebSphere 7.0.0.11 and ibm jdk 1.8_20160719 and 1.7.0_20150407, and had following error logs\nWARN 2019-05-09 17:01:35:905 SkywalkingAgent-1-GRPCChannelManager-0 ProtectiveShieldMatcher : Byte-buddy occurs exception when match type. java.lang.IllegalStateException: Cannot resolve type description for java.security.PrivilegedAction at org.apache.skywalking.apm.dependencies.net.bytebuddy.pool.TypePool$Resolution$Illegal.resolve(TypePool.java:144) at org.apache.skywalking.apm.dependencies.net.bytebuddy.pool.TypePool$Default$WithLazyResolution$LazyTypeDescription.delegate(TypePool.java:1392) at org.apache.skywalking.apm.dependencies.net.bytebuddy.description.type.TypeDescription$AbstractBase$OfSimpleType$WithDelegation.getInterfaces(TypeDescription.java:8016) at org.apache.skywalking.apm.dependencies.net.bytebuddy.description.type.TypeDescription$Generic$OfNonGenericType.getInterfaces(TypeDescription.java:3621) at org.apache.skywalking.apm.dependencies.net.bytebuddy.matcher.HasSuperTypeMatcher.hasInterface(HasSuperTypeMatcher.java:53) at org.apache.skywalking.apm.dependencies.net.bytebuddy.matcher.HasSuperTypeMatcher.hasInterface(HasSuperTypeMatcher.java:54) at org.apache.skywalking.apm.dependencies.net.bytebuddy.matcher.HasSuperTypeMatcher.matches(HasSuperTypeMatcher.java:38) at org.apache.skywalking.apm.dependencies.net.bytebuddy.matcher.HasSuperTypeMatcher.matches(HasSuperTypeMatcher.java:15) at org.apache.skywalking.apm.dependencies.net.bytebuddy.matcher.ElementMatcher$Junction$Conjunction.matches(ElementMatcher.java:107) at org.apache.skywalking.apm.dependencies.net.bytebuddy.matcher.ElementMatcher$Junction$Disjunction.matches(ElementMatcher.java:147) at org.apache.skywalking.apm.dependencies.net.bytebuddy.matcher.ElementMatcher$Junction$Disjunction.matches(ElementMatcher.java:147) at org.apache.skywalking.apm.dependencies.net.bytebuddy.matcher.ElementMatcher$Junction$Disjunction.matches(ElementMatcher.java:147) at org.apache.skywalking.apm.dependencies.net.bytebuddy.matcher.ElementMatcher$Junction$Disjunction.matches(ElementMatcher.java:147) at org.apache.skywalking.apm.dependencies.net.bytebuddy.matcher.ElementMatcher$Junction$Disjunction.matches(ElementMatcher.java:147) ... The exception has been addressed as access grant required in WebSphere. You could follow these steps.\n Set the agent\u0026rsquo;s owner to the owner of WebSphere. Add \u0026ldquo;grant codeBase \u0026ldquo;file:${agent_dir}/-\u0026rdquo; { permission java.security.AllPermission; };\u0026rdquo; in the file of \u0026ldquo;server.policy\u0026rdquo;.  ","excerpt":"IllegalStateException when install Java agent on WebSphere This FAQ came from community discussion …","ref":"/docs/main/v8.2.0/en/faq/install_agent_on_websphere/","title":"IllegalStateException when install Java agent on WebSphere"},{"body":"Init mode SkyWalking backend supports multiple storage implementors. Most of them could initialize the storage, such as Elastic Search, Database automatically when the backend startup in first place.\nBut there are some unexpected happens based on the storage, such as When create Elastic Search indexes concurrently, because of several backend instances startup at the same time., there is a change, the APIs of Elastic Search would be blocked without any exception. And this has more chances happen in container management platform, like k8s.\nThat is where you need Init mode startup.\nSolution Only one single instance should run in Init mode before other instances start up. And this instance will exit graciously after all initialization steps are done.\nUse oapServiceInit.sh/oapServiceInit.bat to start up backend. You should see the following logs\n 2018-11-09 23:04:39,465 - org.apache.skywalking.oap.server.starter.OAPServerStartUp -2214 [main] INFO [] - OAP starts up in init mode successfully, exit now\u0026hellip;\n Kubernetes Initialization in this mode would be included in our Kubernetes scripts and Helm.\n","excerpt":"Init mode SkyWalking backend supports multiple storage implementors. Most of them could initialize …","ref":"/docs/main/v8.2.0/en/setup/backend/backend-init-mode/","title":"Init mode"},{"body":"IP and port setting Backend is using IP and port binding, in order to support the OS having multiple IPs. The binding/listening IP and port are specified by core module\ncore: default: restHost: 0.0.0.0 restPort: 12800 restContextPath: / gRPCHost: 0.0.0.0 gRPCPort: 11800 There are two IP/port pair for gRPC and HTTP rest services.\n Most agents and probes use gRPC service for better performance and code readability. Few agent use rest service, because gRPC may be not supported in that language. UI uses rest service, but data in GraphQL format, always.  Notice IP binding In case some users are not familiar with IP binding, you should know, after you did that, the client could only use this IP to access the service. For example, bind 172.09.13.28, even you are in this machine, must use 172.09.13.28 rather than 127.0.0.1 or localhost to access the service.\nModule provider specified IP and port The IP and port in core are only default provided by core. But some module provider may provide other IP and port settings, this is common. Such as many receiver modules provide this.\n","excerpt":"IP and port setting Backend is using IP and port binding, in order to support the OS having multiple …","ref":"/docs/main/v8.2.0/en/setup/backend/backend-ip-port/","title":"IP and port setting"},{"body":"JVM Metrics Service Abstract Uplink the JVM metrics, including PermSize, HeapSize, CPU, Memory, etc., every second.\ngRPC service define\n","excerpt":"JVM Metrics Service Abstract Uplink the JVM metrics, including PermSize, HeapSize, CPU, Memory, …","ref":"/docs/main/v8.2.0/en/protocols/jvm-protocol/","title":"JVM Metrics Service"},{"body":"Local span and Exit span should not be register Since 6.6.0, SkyWalking cancelled the local span and exit span register. If old java agent(before 6.6.0) is still running, and do register to 6.6.0+ backend, you will face the following warning message.\nclass=RegisterServiceHandler, message = Unexpected endpoint register, endpoint isn't detected from server side. This will not harm the backend or cause any issue. This is a reminder that, your agent or other client should follow the new protocol requirements.\nYou could simply use log4j2.xml to filter this warning message out.\n","excerpt":"Local span and Exit span should not be register Since 6.6.0, SkyWalking cancelled the local span and …","ref":"/docs/main/v8.2.0/en/faq/unexpected-endpoint-register/","title":"Local span and Exit span should not be register"},{"body":"Locate agent config file by system property Supported version 5.0.0-RC+\nWhat is Locate agent config file by system property ？ In Default. The agent will try to locate agent.config, which should be in the /config dictionary of agent package. If User sets the specified agent config file through system properties, The agent will try to load file from there. By the way, This function has no conflict with Setting Override\nOverride priority The specified agent config \u0026gt; The default agent config\nHow to use The content formats of the specified config must be same as the default config.\nUsing System.Properties(-D) to set the specified config path\n-Dskywalking_config=/path/to/agent.config /path/to/agent.config is the absolute path of the specified config file\n","excerpt":"Locate agent config file by system property Supported version 5.0.0-RC+\nWhat is Locate agent config …","ref":"/docs/main/v8.2.0/en/setup/service-agent/java-agent/specified-agent-config/","title":"Locate agent config file by system property"},{"body":"logback plugin  Dependency the toolkit, such as using maven or gradle  \u0026lt;dependency\u0026gt; \u0026lt;groupId\u0026gt;org.apache.skywalking\u0026lt;/groupId\u0026gt; \u0026lt;artifactId\u0026gt;apm-toolkit-logback-1.x\u0026lt;/artifactId\u0026gt; \u0026lt;version\u0026gt;{project.release.version}\u0026lt;/version\u0026gt; \u0026lt;/dependency\u0026gt;  set %tid in Pattern section of logback.xml  \u0026lt;appender name=\u0026#34;STDOUT\u0026#34; class=\u0026#34;ch.qos.logback.core.ConsoleAppender\u0026#34;\u0026gt; \u0026lt;encoder class=\u0026#34;ch.qos.logback.core.encoder.LayoutWrappingEncoder\u0026#34;\u0026gt; \u0026lt;layout class=\u0026#34;org.apache.skywalking.apm.toolkit.log.logback.v1.x.TraceIdPatternLogbackLayout\u0026#34;\u0026gt; \u0026lt;Pattern\u0026gt;%d{yyyy-MM-dd HH:mm:ss.SSS} [%tid] [%thread] %-5level %logger{36} -%msg%n\u0026lt;/Pattern\u0026gt; \u0026lt;/layout\u0026gt; \u0026lt;/encoder\u0026gt; \u0026lt;/appender\u0026gt;  with the MDC, set %X{tid} in Pattern section of logback.xml  \u0026lt;appender name=\u0026#34;STDOUT\u0026#34; class=\u0026#34;ch.qos.logback.core.ConsoleAppender\u0026#34;\u0026gt; \u0026lt;encoder class=\u0026#34;ch.qos.logback.core.encoder.LayoutWrappingEncoder\u0026#34;\u0026gt; \u0026lt;layout class=\u0026#34;org.apache.skywalking.apm.toolkit.log.logback.v1.x.mdc.TraceIdMDCPatternLogbackLayout\u0026#34;\u0026gt; \u0026lt;Pattern\u0026gt;%d{yyyy-MM-dd HH:mm:ss.SSS} [%X{tid}] [%thread] %-5level %logger{36} -%msg%n\u0026lt;/Pattern\u0026gt; \u0026lt;/layout\u0026gt; \u0026lt;/encoder\u0026gt; \u0026lt;/appender\u0026gt;  Support logback AsyncAppender(MDC also support), No additional configuration is required. Refer to the demo of logback.xml below. For details: Logback AsyncAppender  \u0026lt;configuration scan=\u0026#34;true\u0026#34; scanPeriod=\u0026#34; 5 seconds\u0026#34;\u0026gt; \u0026lt;appender name=\u0026#34;STDOUT\u0026#34; class=\u0026#34;ch.qos.logback.core.ConsoleAppender\u0026#34;\u0026gt; \u0026lt;encoder class=\u0026#34;ch.qos.logback.core.encoder.LayoutWrappingEncoder\u0026#34;\u0026gt; \u0026lt;layout class=\u0026#34;org.apache.skywalking.apm.toolkit.log.logback.v1.x.mdc.TraceIdMDCPatternLogbackLayout\u0026#34;\u0026gt; \u0026lt;Pattern\u0026gt;%d{yyyy-MM-dd HH:mm:ss.SSS} [%X{tid}] [%thread] %-5level %logger{36} -%msg%n\u0026lt;/Pattern\u0026gt; \u0026lt;/layout\u0026gt; \u0026lt;/encoder\u0026gt; \u0026lt;/appender\u0026gt; \u0026lt;appender name=\u0026#34;ASYNC\u0026#34; class=\u0026#34;ch.qos.logback.classic.AsyncAppender\u0026#34;\u0026gt; \u0026lt;discardingThreshold\u0026gt;0\u0026lt;/discardingThreshold\u0026gt; \u0026lt;queueSize\u0026gt;1024\u0026lt;/queueSize\u0026gt; \u0026lt;neverBlock\u0026gt;true\u0026lt;/neverBlock\u0026gt; \u0026lt;appender-ref ref=\u0026#34;STDOUT\u0026#34;/\u0026gt; \u0026lt;/appender\u0026gt; \u0026lt;root level=\u0026#34;INFO\u0026#34;\u0026gt; \u0026lt;appender-ref ref=\u0026#34;ASYNC\u0026#34;/\u0026gt; \u0026lt;/root\u0026gt; \u0026lt;/configuration\u0026gt;  When you use -javaagent to active the sky-walking tracer, logback will output traceId, if it existed. If the tracer is inactive, the output will be TID: N/A.  logstash logback plugin  Dependency the toolkit, such as using maven or gradle  \u0026lt;dependency\u0026gt; \u0026lt;groupId\u0026gt;org.apache.skywalking\u0026lt;/groupId\u0026gt; \u0026lt;artifactId\u0026gt;apm-toolkit-logback-1.x\u0026lt;/artifactId\u0026gt; \u0026lt;version\u0026gt;${skywalking.version}\u0026lt;/version\u0026gt; \u0026lt;/dependency\u0026gt;  set LogstashEncoder of logback.xml  \u0026lt;encoder charset=\u0026#34;UTF-8\u0026#34; class=\u0026#34;net.logstash.logback.encoder.LogstashEncoder\u0026#34;\u0026gt; \u0026lt;provider class=\u0026#34;org.apache.skywalking.apm.toolkit.log.logback.v1.x.logstash.TraceIdJsonProvider\u0026#34;\u0026gt; \u0026lt;/provider\u0026gt; \u0026lt;/encoder\u0026gt;  set LoggingEventCompositeJsonEncoder of logstash in logback-spring.xml for custom json format  1.add converter for %tid as child of  node\n\u0026lt;!--add converter for %tid --\u0026gt; \u0026lt;conversionRule conversionWord=\u0026#34;tid\u0026#34; converterClass=\u0026#34;org.apache.skywalking.apm.toolkit.log.logback.v1.x.LogbackPatternConverter\u0026#34;/\u0026gt; 2.add json encoder for custom json format\n\u0026lt;encoder class=\u0026#34;net.logstash.logback.encoder.LoggingEventCompositeJsonEncoder\u0026#34;\u0026gt; \u0026lt;providers\u0026gt; \u0026lt;timestamp\u0026gt; \u0026lt;timeZone\u0026gt;UTC\u0026lt;/timeZone\u0026gt; \u0026lt;/timestamp\u0026gt; \u0026lt;pattern\u0026gt; \u0026lt;pattern\u0026gt; { \u0026#34;level\u0026#34;: \u0026#34;%level\u0026#34;, \u0026#34;tid\u0026#34;: \u0026#34;%tid\u0026#34;, \u0026#34;thread\u0026#34;: \u0026#34;%thread\u0026#34;, \u0026#34;class\u0026#34;: \u0026#34;%logger{1.}:%L\u0026#34;, \u0026#34;message\u0026#34;: \u0026#34;%message\u0026#34;, \u0026#34;stackTrace\u0026#34;: \u0026#34;%exception{10}\u0026#34; } \u0026lt;/pattern\u0026gt; \u0026lt;/pattern\u0026gt; \u0026lt;/providers\u0026gt; \u0026lt;/encoder\u0026gt; ","excerpt":"logback plugin  Dependency the toolkit, such as using maven or gradle  \u0026lt;dependency\u0026gt; …","ref":"/docs/main/v8.2.0/en/setup/service-agent/java-agent/application-toolkit-logback-1.x/","title":"logback plugin"},{"body":"Manual instrument SDK We have manual instrument SDK contributed from the community.\n Go2Sky. Go SDK follows SkyWalking format.  Welcome to consider contributing in following languages:\n Python C++  What is SkyWalking formats and propagation protocols? See these protocols in protocols document.\nCan SkyWalking provide OpenCensus exporter in above languages? At the moment I am writing this document, NO. Because, OC(OpenCensus) don\u0026rsquo;t support context extendable mechanism, and no hook mechanism when manipulate spans. SkyWalking relied on those to propagate more things than trace id and span id.\nWe are already in the middle of discussion, see https://github.com/census-instrumentation/opencensus-specs/issues/70. After OC provides this officially, we can.\nHow about Zipkin instrument SDKs? See Zipkin receiver in backend Choose receiver section.\n","excerpt":"Manual instrument SDK We have manual instrument SDK contributed from the community.\n Go2Sky. Go SDK …","ref":"/docs/main/v8.2.0/en/concepts-and-designs/manual-sdk/","title":"Manual instrument SDK"},{"body":"Meter Analysis Language Meter system provides a functional analysis language called MAL(Meter Analysis Language) that lets the user analyze and aggregate meter data in OAP streaming system. The result of an expression can either be ingested by agent analyzer, or OC/Prometheus analyzer.\nLanguage data type In MAL, an expression or sub-expression can evaluate to one of two types:\n Sample family - a set of samples(metrics) containing a range of metrics whose name is identical. Scalar - a simple numeric value. it supports integer/long, floating/double,  Sample family A set of samples, which is as the basic unit in MAL. For example:\ninstance_trace_count The above sample family might contains following simples which are provided by external modules, for instance, agent analyzer:\ninstance_trace_count{region=\u0026quot;us-west\u0026quot;,az=\u0026quot;az-1\u0026quot;} 100 instance_trace_count{region=\u0026quot;us-east\u0026quot;,az=\u0026quot;az-3\u0026quot;} 20 instance_trace_count{region=\u0026quot;asia-north\u0026quot;,az=\u0026quot;az-1\u0026quot;} 33 Tag filter MAL support four type operations to filter samples in a sample family:\n tagEqual: Filter tags that are exactly equal to the provided string. tagNotEqual: Filter tags that are not equal to the provided string. tagMatch: Filter tags that regex-match the provided string. tagNotMatch: Filter labels that do not regex-match the provided string.  For example, this filters all instance_trace_count samples for us-west and asia-north region and az-1 az:\ninstance_trace_count.tagMatch(\u0026quot;region\u0026quot;, \u0026quot;us-west|asia-north\u0026quot;).tagEqual(\u0026quot;az\u0026quot;, \u0026quot;az-1\u0026quot;) Binary operators The following binary arithmetic operators are available in MAL:\n + (addition) - (subtraction) * (multiplication) / (division)  Binary operators are defined between scalar/scalar, sampleFamily/scalar and sampleFamily/sampleFamily value pairs.\nBetween two scalars: they evaluate to another scalar that is the result of the operator applied to both scalar operands:\n1 + 2 Between a sample family and a scalar, the operator is applied to the value of every sample in the smaple family. For example:\ninstance_trace_count + 2 or\n2 + instance_trace_count results in\ninstance_trace_count{region=\u0026quot;us-west\u0026quot;,az=\u0026quot;az-1\u0026quot;} 102 // 100 + 2 instance_trace_count{region=\u0026quot;us-east\u0026quot;,az=\u0026quot;az-3\u0026quot;} 22 // 20 + 2 instance_trace_count{region=\u0026quot;asia-north\u0026quot;,az=\u0026quot;az-1\u0026quot;} 35 // 33 + 2 Between two sample families, a binary operator is applied to each sample in the left-hand side sample family and its matching sample in the right-hand sample family. A new sample family with empty name will be generated. Only the matched tags will be reserved. Samples for which no matching sample in the right-hand sample family are not in the result.\nAnother sample family instance_trace_analysis_error_count is\ninstance_trace_analysis_error_count{region=\u0026quot;us-west\u0026quot;,az=\u0026quot;az-1\u0026quot;} 20 instance_trace_analysis_error_count{region=\u0026quot;asia-north\u0026quot;,az=\u0026quot;az-1\u0026quot;} 11 Example expression:\ninstance_trace_analysis_error_count / instance_trace_count This returns a result sample family containing the error rate of trace analysis. The samples with region us-west and az az-3 have no match and will not show up in the result:\n{region=\u0026quot;us-west\u0026quot;,az=\u0026quot;az-1\u0026quot;} 0.8 // 20 / 100 {region=\u0026quot;asia-north\u0026quot;,az=\u0026quot;az-1\u0026quot;} 0.3333 // 11 / 33 Aggregation Operation Sample family supports the following aggregation operations that can be used to aggregate the samples of a single sample family, resulting in a new sample family of fewer samples(even single one) with aggregated values:\n sum (calculate sum over dimensions) min (select minimum over dimensions) (TODO) max (select maximum over dimensions) (TODO) avg (calculate the average over dimensions) (TODO)  These operations can be used to aggregate over all label dimensions or preserve distinct dimensions by inputting by parameter.\n\u0026lt;aggr-op\u0026gt;(by: \u0026lt;tag1, tag2, ...\u0026gt;) Example expression:\ninstance_trace_count.sum(by: ['az']) will output a result:\ninstance_trace_count{az=\u0026quot;az-1\u0026quot;} 133 // 100 + 33 instance_trace_count{az=\u0026quot;az-3\u0026quot;} 20 Function Duraton is a textual representation of a time range. The formats accepted are based on the ISO-8601 duration format {@code PnDTnHnMn.nS} with days considered to be exactly 24 hours.\nExamples:\n \u0026ldquo;PT20.345S\u0026rdquo; \u0026ndash; parses as \u0026ldquo;20.345 seconds\u0026rdquo; \u0026ldquo;PT15M\u0026rdquo; \u0026ndash; parses as \u0026ldquo;15 minutes\u0026rdquo; (where a minute is 60 seconds) \u0026ldquo;PT10H\u0026rdquo; \u0026ndash; parses as \u0026ldquo;10 hours\u0026rdquo; (where an hour is 3600 seconds) \u0026ldquo;P2D\u0026rdquo; \u0026ndash; parses as \u0026ldquo;2 days\u0026rdquo; (where a day is 24 hours or 86400 seconds) \u0026ldquo;P2DT3H4M\u0026rdquo; \u0026ndash; parses as \u0026ldquo;2 days, 3 hours and 4 minutes\u0026rdquo; \u0026ldquo;P-6H3M\u0026rdquo; \u0026ndash; parses as \u0026ldquo;-6 hours and +3 minutes\u0026rdquo; \u0026ldquo;-P6H3M\u0026rdquo; \u0026ndash; parses as \u0026ldquo;-6 hours and -3 minutes\u0026rdquo; \u0026ldquo;-P-6H+3M\u0026rdquo; \u0026ndash; parses as \u0026ldquo;+6 hours and -3 minutes\u0026rdquo;  increase increase(Duration). Calculates the increase in the time range.\nrate rate(Duration). Calculates the per-second average rate of increase of the time range.\nirate irate(). Calculates the per-second instant rate of increase of the time range.\ntag tag({allTags -\u0026gt; }). Update tags of samples. User can add, drop, rename and update tags.\nhistogram histogram(le: '\u0026lt;the tag name of le\u0026gt;'). Transforms less based histogram buckets to meter system histogram buckets. le parameter hints the tag name of a bucket.\nhistogram_percentile histogram_percentile([\u0026lt;p scalar\u0026gt;]). Hints meter-system to calculates the p-percentile (0 ≤ p ≤ 100) from the buckets.\ntime time(). returns the number of seconds since January 1, 1970 UTC.\nDown Sampling Operation MAL should instruct meter-system how to do downsampling for metrics. It doesn\u0026rsquo;t only refer to aggregate raw samples to minute level, but also hints data from minute to higher levels, for instance, hour and day.\nDown sampling operations are as global function in MAL:\n avg latest (TODO) min (TODO) max (TODO) mean (TODO) sum (TODO) count (TODO)  The default one is avg if not specific an operation.\nIf user want get latest time from last_server_state_sync_time_in_seconds:\nlatest(last_server_state_sync_time_in_seconds.tagEqual('production', 'catalog')) or latest last_server_state_sync_time_in_seconds.tagEqual('production', 'catalog') Metric level function Metric has three level, service, instance and endpoint. They extract level relevant labels from metric labels, then hints meter-system which level this metrics should be.\n servcie([svc_label1, svc_label2...]) extracts service level labels from the array argument. instance([svc_label1, svc_label2...], [ins_label1, ins_label2...]) extracts service level labels from the first array argument, extracts instance level labels from the second array argument. endpoint([svc_label1, svc_label2...], [ep_label1, ep_label2...]) extracts service level labels from the first array argument, extracts endpoint level labels from the second array argument.  More Examples Please refer to OAP Self-Observability\n","excerpt":"Meter Analysis Language Meter system provides a functional analysis language called MAL(Meter …","ref":"/docs/main/v8.2.0/en/concepts-and-designs/mal/","title":"Meter Analysis Language"},{"body":"Meter Receiver Meter receiver is accepting the metrics of meter protocol format into the Meter System.\nModule define receiver-meter: selector: ${SW_RECEIVER_METER:default} default: In Kafka Fetcher, we need to follow the configuration to enable it.\nkafka-fetcher: selector: ${SW_KAFKA_FETCHER:default} default: bootstrapServers: ${SW_KAFKA_FETCHER_SERVERS:localhost:9092} enableMeterSystem: ${SW_KAFKA_FETCHER_ENABLE_METER_SYSTEM:true} Configuration file Meter receiver is configured via a configuration file. The configuration file defines everything related to receiving from agents, as well as which rule files to load.\nOAP can load the configuration at bootstrap. If the new configuration is not well-formed, OAP fails to start up. The files are located at $CLASSPATH/meter-receive-config.\nThe file is written in YAML format, defined by the scheme described below. Brackets indicate that a parameter is optional.\nA example can be found here. If you\u0026rsquo;re using Spring sleuth, you could use Spring Sleuth Setup.\nMeters configure # Meter config allow your to recompute meters: # Meter name which combines with a prefix \u0026#39;meter_\u0026#39; as the index/table name in storage. - name: \u0026lt;string\u0026gt; # The meter scope scope: # Scope type should be one of SERVICE, SERVICE_INSTANCE, ENDPOINT type: \u0026lt;string\u0026gt; # \u0026lt;Optional\u0026gt; Appoint the endpoint name if using ENDPOINT scope endpoint: \u0026lt;string\u0026gt; # The agent source of the transformation operation. meter: # The transformation operation from prometheus metrics to Skywalking ones.  operation: \u0026lt;string\u0026gt; # Meter value parse groovy script. value: \u0026lt;string\u0026gt; # Aggregate metrics group by dedicated labels groupBy: - \u0026lt;labelName\u0026gt; # \u0026lt;Optional\u0026gt; Appoint percentiles if using avgHistogramPercentile operation. percentile: - \u0026lt;rank\u0026gt; Meter transform operation The available operations are avg, avgLabeled, avgHistogram and avgHistogramPercentile. The avg and avgXXX mean to average the raw received metrics.\nWhen you specify avgHistogram and avgHistogramPercentile, the source should be the type of histogram.\nMeter value script The script is provide a easy way to custom build a complex value, and it also support combine multiple meter into one.\nMeter value grammar // Declare the meter value. meter[METER_NAME] [.tagFilter(TAG_KEY, TAG_VALUE)] .FUNCTION(VALUE | METER) Meter Name Use name to refer the metrics raw data from agent side.\nTag Filter Use the meter tag to filter the meter value.\n meter[\u0026ldquo;test_meter\u0026rdquo;].tagFilter(\u0026ldquo;k1\u0026rdquo;, \u0026ldquo;v1\u0026rdquo;)\n In this case, filter the tag key equals k1 and tag value equals v1 value from test_meter.\nAggregation Function Use multiple build-in methods to help operate the value.\nProvided functions\n add. Add value into meter. Support single value.   meter[\u0026ldquo;test_meter\u0026rdquo;].add(2)\n In this case, all of the meter values will add 2.\n meter[\u0026ldquo;test_meter1\u0026rdquo;].add(meter[\u0026ldquo;test_meter2\u0026rdquo;])\n In this case, all of the test_meter1 values will add value from test_meter2, ensure test_meter2 only has single value to operate, could use tagFilter.\n subtract. Subtract value into meter. Support single value.   meter[\u0026ldquo;test_meter\u0026rdquo;].subtract(2)\n In this case, all of the meter values will subtract 2.\n meter[\u0026ldquo;test_meter1\u0026rdquo;].subtract(meter[\u0026ldquo;test_meter2\u0026rdquo;])\n In this case, all of the test_meter1 values will subtract value from test_meter2, ensure test_meter2 only has single value to operate, could use tagFilter.\n multiply. Multiply value into meter. Support single value.   meter[\u0026ldquo;test_meter\u0026rdquo;].multiply(2)\n In this case, all of the meter values will multiply 2.\n meter[\u0026ldquo;test_meter1\u0026rdquo;].multiply(meter[\u0026ldquo;test_meter2\u0026rdquo;])\n In this case, all of the test_meter1 values will multiply value from test_meter2, ensure test_meter2 only has single value to operate, could use tagFilter.\n divide. Divide value into meter. Support single value.   meter[\u0026ldquo;test_meter\u0026rdquo;].divide(2)\n In this case, all of the meter values will divide 2.\n meter[\u0026ldquo;test_meter1\u0026rdquo;].divide(meter[\u0026ldquo;test_meter2\u0026rdquo;])\n In this case, all of the test_meter1 values will divide value from test_meter2, ensure test_meter2 only has single value to operate, could use tagFilter.\n scale. Scale value into meter. Support single value.   meter[\u0026ldquo;test_meter\u0026rdquo;].scale(2)\n In this case, all of the meter values will scale 2. For example, meter[\u0026quot;test_meter\u0026quot;] value is 1, then using scale(2), the result will be 100.\n rate.(Not Recommended) Rate value from the time range. Support single value and Histogram.   meter[\u0026ldquo;test_meter\u0026rdquo;].rate(\u0026ldquo;P15S\u0026rdquo;)\n In this case, all of the meter values will rate from 15s before.\n irate.(Not Recommended) IRate value from the time range. Support single value and Histogram.   meter[\u0026ldquo;test_meter\u0026rdquo;].irate(\u0026ldquo;P15S\u0026rdquo;)\n In this case, all of the meter values will irate from 15s before.\n increase.(Not Recommended) increase value from the time range. Support single value and Histogram.   meter[\u0026ldquo;test_meter\u0026rdquo;].increase(\u0026ldquo;P15S\u0026rdquo;)\n In this case, all of the meter values will increase from 15s before.\nEven we supported rate, irate, increase function in the backend, but we still recommend user to consider using client-side APIs to do these. Because\n The OAP has to set up caches to calculate the value. Once the agent reconnected to another OAP instance, the time windows of rate calculation will break. Then, the result would not be accurate.  ","excerpt":"Meter Receiver Meter receiver is accepting the metrics of meter protocol format into the Meter …","ref":"/docs/main/v8.2.0/en/setup/backend/backend-meter/","title":"Meter Receiver"},{"body":"Meter System Meter system is another streaming calculation mode, especially for metrics data. In the OAL, there are clear Scope Definitions, including native objects. Meter system is focusing on the data type itself, and provides more flexible to the end user to define the scope entity.\nThe meter system is open to different receivers and fetchers in the backend, follow the backend setup document for more details.\nEvery metrics is declared in the meter system should include following attribute\n Metrics Name. An unique name globally, should avoid overlap the OAL variable names. Function Name. The function used for this metrics, distributed aggregation, value calculation and down sampling calculation based on the function implementation. Also, the data structure is determined by the function too, such as function Avg is for Long. Scope Type. Unlike inside the OAL, there are plenty of logic scope definitions, in meter system, only type is required. Type values include service, instance, and endpoint, like we introduced in the Overview. The values of scope entity name, such as service name, are required when metrics data generated with the metrics data value.  NOTICE, the metrics must be declared in the bootstrap stage, no runtime changed.\nMeter System supports following binding functions\n avg. Calculate the avg value for every entity in the same metrics name. histogram. Aggregate the counts in the configurable buckets, buckets is configurable but must be assigned in the declaration stage. percentile. Read percentile in WIKI. Unlike in the OAL, we provide 50/75/90/95/99 in default, in the meter system function, percentile function accepts several ranks, which should be in the (0, 100) range.  ","excerpt":"Meter System Meter system is another streaming calculation mode, especially for metrics data. In the …","ref":"/docs/main/v8.2.0/en/concepts-and-designs/meter/","title":"Meter System"},{"body":"Metrics Exporter SkyWalking provides basic and most important metrics aggregation, alarm and analysis. In real world, people may want to forward the data to their 3rd party system, for deeper analysis or anything else. Metrics Exporter makes that possible.\nMetrics exporter is an independent module, you need manually active it.\nRight now, we provide the following exporters\n gRPC exporter  gRPC exporter gRPC exporter uses SkyWalking native exporter service definition. Here is proto definition.\nservice MetricExportService { rpc export (stream ExportMetricValue) returns (ExportResponse) { } rpc subscription (SubscriptionReq) returns (SubscriptionsResp) { }}message ExportMetricValue { string metricName = 1; string entityName = 2; string entityId = 3; ValueType type = 4; int64 timeBucket = 5; int64 longValue = 6; double doubleValue = 7; repeated int64 longValues = 8;}message SubscriptionsResp { repeated string metricNames = 1;}enum ValueType { LONG = 0; DOUBLE = 1; MULTI_LONG = 2;}message SubscriptionReq {}message ExportResponse {}To active the exporter, you should add this into your application.yml\nexporter: grpc: targetHost: 127.0.0.1 targetPort: 9870  targetHost:targetPort is the expected target service address. You could set any gRPC server to receive the data. Target gRPC service needs to be standby, otherwise, the OAP starts up failure.  For target exporter service subscription implementation Return the expected metrics name list, all the names must match the OAL script definition. Return empty list, if you want to export all metrics.\nexport implementation Stream service, all subscribed metrics will be sent to here, based on OAP core schedule. Also, if the OAP deployed as cluster, then this method will be called concurrently. For metrics value, you need follow #type to choose #longValue or #doubleValue.\n","excerpt":"Metrics Exporter SkyWalking provides basic and most important metrics aggregation, alarm and …","ref":"/docs/main/v8.2.0/en/setup/backend/metrics-exporter/","title":"Metrics Exporter"},{"body":"Namespace Background SkyWalking is a monitoring tool, which collects metrics from a distributed system. In the real world, a very large distributed system includes hundreds of services, thousands of service instances. In that case, most likely, more than one group, even more than one company are maintaining and monitoring the distributed system. Each one of them takes charge of different parts, don\u0026rsquo;t want or shouldn\u0026rsquo;t share there metrics.\nNamespace is the proposal from this.It is used for tracing and monitoring isolation.\nSet the namespace Set agent.namespace in agent config # The agent namespace # agent.namespace=default-namespace The default value of agent.namespace is empty.\nInfluence The default header key of SkyWalking is sw8, more in this document. After agent.namespace is set, the key changes to namespace-sw8.\nThe across process propagation chain breaks, when the two sides are using different namespace.\n","excerpt":"Namespace Background SkyWalking is a monitoring tool, which collects metrics from a distributed …","ref":"/docs/main/v8.2.0/en/setup/service-agent/java-agent/namespace/","title":"Namespace"},{"body":"Observability Analysis Language Provide OAL(Observability Analysis Language) to analysis incoming data in streaming mode.\nOAL focuses on metrics in Service, Service Instance and Endpoint. Because of that, the language is easy to learn and use.\nSince 6.3, the OAL engine is embedded in OAP server runtime, as oal-rt(OAL Runtime). OAL scripts now locate in /config folder, user could simply change and reboot the server to make it effective. But still, OAL script is compile language, OAL Runtime generates java codes dynamically.\nYou could open set SW_OAL_ENGINE_DEBUG=Y at system env, to see which classes generated.\nGrammar Scripts should be named as *.oal\n// Declare the metrics. METRICS_NAME = from(SCOPE.(* | [FIELD][,FIELD ...])) [.filter(FIELD OP [INT | STRING])] .FUNCTION([PARAM][, PARAM ...]) // Disable hard code disable(METRICS_NAME); Scope Primary SCOPEs are All, Service, ServiceInstance, Endpoint, ServiceRelation, ServiceInstanceRelation, EndpointRelation. Also there are some secondary scopes, which belongs to one primary scope.\nRead Scope Definitions, you can find all existing Scopes and Fields.\nFilter Use filter to build the conditions for the value of fields, by using field name and expression.\nThe expressions support to link by and, or and (...). The OPs support ==, !=, \u0026gt;, \u0026lt;, \u0026gt;=, \u0026lt;=, in [...] ,like %..., like ...% and like %...%, with type detection based of field type. Trigger compile or code generation error if incompatible.\nAggregation Function The default functions are provided by SkyWalking OAP core, and could implement more.\nProvided functions\n longAvg. The avg of all input per scope entity. The input field must be a long.   instance_jvm_memory_max = from(ServiceInstanceJVMMemory.max).longAvg();\n In this case, input are request of each ServiceInstanceJVMMemory scope, avg is based on field max.\n doubleAvg. The avg of all input per scope entity. The input field must be a double.   instance_jvm_cpu = from(ServiceInstanceJVMCPU.usePercent).doubleAvg();\n In this case, input are request of each ServiceInstanceJVMCPU scope, avg is based on field usePercent.\n percent. The number or ratio expressed as a fraction of 100, for the condition matched input.   endpoint_percent = from(Endpoint.*).percent(status == true);\n In this case, all input are requests of each endpoint, condition is endpoint.status == true.\n rate. The rate expressed as a fraction of 100, for the condition matched input.   browser_app_error_rate = from(BrowserAppTraffic.*).rate(trafficCategory == BrowserAppTrafficCategory.FIRST_ERROR, trafficCategory == BrowserAppTrafficCategory.NORMAL);\n In this case, all input are requests of each browser app traffic, numerator condition is trafficCategory == BrowserAppTrafficCategory.FIRST_ERROR and denominator condition is trafficCategory == BrowserAppTrafficCategory.NORMAL. The parameter (1) is the numerator condition. The parameter (2) is the denominator condition.\n count. The sum calls per scope entity.   service_calls_sum = from(Service.*).count();\n In this case, calls of each service.\n histogram. Read Heatmap in WIKI   all_heatmap = from(All.latency).histogram(100, 20);\n In this case, thermodynamic heatmap of all incoming requests. The parameter (1) is the precision of latency calculation, such as in above case, 113ms and 193ms are considered same in the 101-200ms group. The parameter (2) is the group amount. In above case, 21(param value + 1) groups are 0-100ms, 101-200ms, \u0026hellip; 1901-2000ms, 2000+ms\n apdex. Read Apdex in WIKI   service_apdex = from(Service.latency).apdex(name, status);\n In this case, apdex score of each service. The parameter (1) is the service name, which effects the Apdex threshold value loaded from service-apdex-threshold.yml in the config folder. The parameter (2) is the status of this request. The status(success/failure) effects the Apdex calculation.\n p99, p95, p90, p75, p50. Read percentile in WIKI   all_percentile = from(All.latency).percentile(10);\n percentile is the first multiple value metrics, introduced since 7.0.0. As having multiple values, it could be query through getMultipleLinearIntValues GraphQL query. In this case, p99, p95, p90, p75, p50 of all incoming request. The parameter is the precision of p99 latency calculation, such as in above case, 120ms and 124 are considered same. Before 7.0.0, use p99, p95, p90, p75, p50 func(s) to calculate metrics separately. Still supported in 7.x, but don\u0026rsquo;t be recommended, and don\u0026rsquo;t be included in official OAL script.\n all_p99 = from(All.latency).p99(10);\n In this case, p99 value of all incoming requests. The parameter is the precision of p99 latency calculation, such as in above case, 120ms and 124 are considered same.\nMetrics name The metrics name for storage implementor, alarm and query modules. The type inference supported by core.\nGroup All metrics data will be grouped by Scope.ID and min-level TimeBucket.\n In Endpoint scope, the Scope.ID = Endpoint id (the unique id based on service and its Endpoint)  Disable Disable is an advanced statement in OAL, which is only used in certain case. Some of the aggregation and metrics are defined through core hard codes, this disable statement is designed for make them de-active, such as segment, top_n_database_statement. In default, no one is being disable.\nExamples // Caculate p99 of both Endpoint1 and Endpoint2 endpoint_p99 = from(Endpoint.latency).filter(name in (\u0026quot;Endpoint1\u0026quot;, \u0026quot;Endpoint2\u0026quot;)).summary(0.99) // Caculate p99 of Endpoint name started with `serv` serv_Endpoint_p99 = from(Endpoint.latency).filter(name like \u0026quot;serv%\u0026quot;).summary(0.99) // Caculate the avg response time of each Endpoint endpoint_avg = from(Endpoint.latency).avg() // Caculate the p50, p75, p90, p95 and p99 of each Endpoint by 50 ms steps. endpoint_percentile = from(Endpoint.latency).percentile(10) // Caculate the percent of response status is true, for each service. endpoint_success = from(Endpoint.*).filter(status == true).percent() // Caculate the sum of response code in [404, 500, 503], for each service. endpoint_abnormal = from(Endpoint.*).filter(responseCode in [404, 500, 503]).count() // Caculate the sum of request type in [RequestType.PRC, RequestType.gRPC], for each service. endpoint_rpc_calls_sum = from(Endpoint.*).filter(type in [RequestType.PRC, RequestType.gRPC]).count() // Caculate the sum of endpoint name in [\u0026quot;/v1\u0026quot;, \u0026quot;/v2\u0026quot;], for each service. endpoint_url_sum = from(Endpoint.*).filter(name in [\u0026quot;/v1\u0026quot;, \u0026quot;/v2\u0026quot;]).count() // Caculate the sum of calls for each service. endpoint_calls = from(Endpoint.*).sum() disable(segment); disable(endpoint_relation_server_side); disable(top_n_database_statement); ","excerpt":"Observability Analysis Language Provide OAL(Observability Analysis Language) to analysis incoming …","ref":"/docs/main/v8.2.0/en/concepts-and-designs/oal/","title":"Observability Analysis Language"},{"body":"Observability Analysis Platform OAP(Observability Analysis Platform) is a new concept, which starts in SkyWalking 6.x. OAP replaces the old SkyWalking whole backend. The capabilities of the platform are following.\nOAP capabilities OAP accepts data from more sources, which belongs two groups: Tracing and Metrics.\n Tracing. Including, SkyWalking native data formats. Zipkin v1,v2 data formats and Jaeger data formats. Metrics. SkyWalking integrates with Service Mesh platforms, such as Istio, Envoy, Linkerd, to provide observability from data panel or control panel. Also, SkyWalking native agents can run in metrics mode, which highly improve the performance.  At the same time by using any integration solution provided, such as SkyWalking log plugin or toolkits, SkyWalking provides visualization integration for binding tracing and logging together by using the trace id and span id.\nAs usual, all services provided by gRPC and HTTP protocol to make integration easier for unsupported ecosystem.\nTracing in OAP Tracing in OAP has two ways to process.\n Traditional way in SkyWalking 5 series. Format tracing data in SkyWalking trace segment and span formats, even for Zipkin data format. The OAP analysis the segments to get metrics, and push the metrics data into the streaming aggregation. Consider tracing as some kinds of logging only. Just provide save and visualization capabilities for trace.  Also, SkyWalking accepts trace formats from other project, such as Zipkin, Jaeger, OpenCensus. These formats could be processed in the two ways too.\nMetrics in OAP Metrics in OAP is totally new feature in 6 series. Build observability for a distributed system based on metrics of connected nodes. No tracing data is required.\nMetrics data are aggregated inside OAP cluster in streaming mode. See about Observability Analysis Language, which provides the easy way to do aggregation and analysis in script style.\n","excerpt":"Observability Analysis Platform OAP(Observability Analysis Platform) is a new concept, which starts …","ref":"/docs/main/v8.2.0/en/concepts-and-designs/backend-overview/","title":"Observability Analysis Platform"},{"body":"Observe service mesh through ALS Envoy ALS(access log service) provides full logs about RPC routed, including HTTP and TCP.\nThe solution is initialized and firstly implemented by Sheng Wu, Hongtao Gao, Lizan Zhou, and Dhi Aurrahman at 17 May. 2019, and presented on KubeCon China 2019. Here is the recorded Video.\nSkyWalking is the first open source project introducing this ALS based solution to the world. This provides a new way with very low payload to service mesh, but the same observability.\nYou need three steps to open ALS.\n  Open envoyAccessLogService in istio by enabling envoyAccessLogService in ProxyConfig.\nUpper istio 1.6.0, if istio installed by demo profile, you can open ALS ues command:\nistioctl manifest apply --set profile=demo --set meshConfig.defaultConfig.envoyAccessLogService.address=skywalking-oap.skywalking.svc:11800 --set meshConfig.enableEnvoyAccessLogService=true Note: SkyWalking OAP service is at skywalking namespace, and the port of gRPC service is 11800\n  (Default is ACTIVATED) Activate SkyWalking envoy receiver.\n  Active ALS k8s-mesh analysis, set system env variable SW_ENVOY_METRIC_ALS_HTTP_ANALYSIS=k8s-mesh\n  envoy-metric: selector: ${SW_ENVOY_METRIC:default} default: acceptMetricsService: ${SW_ENVOY_METRIC_SERVICE:true} alsHTTPAnalysis: ${SW_ENVOY_METRIC_ALS_HTTP_ANALYSIS:\u0026#34;\u0026#34;} # Setting the system env variable would override this.  For multiple values，please use , symbol to concatenate.\nHere\u0026rsquo;s an example to deploy SkyWalking by Helm chart.\nistioctl install --set profile=demo --set meshConfig.defaultConfig.envoyAccessLogService.address=skywalking-oap.istio-system:11800 --set meshConfig.enableEnvoyAccessLogService=true git clone https://github.com/apache/skywalking-kubernetes.git cd skywalking-kubernetes/chart helm repo add elastic https://helm.elastic.co helm dep up skywalking helm install 8.1.0 skywalking -n istio-system --set oap.env.SW_ENVOY_METRIC_ALS_HTTP_ANALYSIS=k8s-mesh --set fullnameOverride=skywalking --set oap.envoy.als.enabled=true Notice, only use this when Envoy is under Istio\u0026rsquo;s control, and they are in k8s environment. The OAP requires the read right to k8s API server for all pods IPs.\nYou can use kubectl -n istio-system logs -l app=skywalking | grep \u0026quot;K8sALSServiceMeshHTTPAnalysis\u0026quot; to ensure OAP ALS k8s-mesh analysis has been active.\n","excerpt":"Observe service mesh through ALS Envoy ALS(access log service) provides full logs about RPC routed, …","ref":"/docs/main/v8.2.0/en/setup/envoy/als_setting/","title":"Observe service mesh through ALS"},{"body":"Official OAL script First, read OAL introduction.\nFind OAL script at the /config/oal/*.oal of SkyWalking dist, since 8.0.0. You could change it(such as adding filter condition, or add new metrics) and reboot the OAP server, then it will affect.\nAll metrics named in this script could be used in alarm and UI query.\nNotice,\nIf you try to add or remove some metrics, UI may break, we only recommend you to do this when you plan to build your own UI based on the customization analysis core.\n","excerpt":"Official OAL script First, read OAL introduction.\nFind OAL script at the /config/oal/*.oal of …","ref":"/docs/main/v8.2.0/en/guides/backend-oal-scripts/","title":"Official OAL script"},{"body":"Open Fetcher Fetcher is a concept in SkyWalking backend. It uses pulling mode rather than receiver, which read the data from the target systems. This mode is typically in some metrics SDKs, such as Prometheus.\nPrometheus Fetcher prometheus-fetcher: selector: ${SW_PROMETHEUS_FETCHER:default} default: active: ${SW_PROMETHEUS_FETCHER_ACTIVE:false} Configuration file Prometheus fetcher is configured via a configuration file. The configuration file defines everything related to fetching services and their instances, as well as which rule files to load.\nOAP can load the configuration at bootstrap. If the new configuration is not well-formed, OAP fails to start up. The files are located at $CLASSPATH/fetcher-prom-rules.\nThe file is written in YAML format, defined by the scheme described below. Brackets indicate that a parameter is optional.\nA full example can be found here\nGeneric placeholders are defined as follows:\n \u0026lt;duration\u0026gt;: a duration This will parse a textual representation of a duration. The formats accepted are based on the ISO-8601 duration format PnDTnHnMn.nS with days considered to be exactly 24 hours. \u0026lt;labelname\u0026gt;: a string matching the regular expression [a-zA-Z_][a-zA-Z0-9_]* \u0026lt;labelvalue\u0026gt;: a string of unicode characters \u0026lt;host\u0026gt;: a valid string consisting of a hostname or IP followed by an optional port number \u0026lt;path\u0026gt;: a valid URL path \u0026lt;string\u0026gt;: a regular string  # How frequently to fetch targets. fetcherInterval: \u0026lt;duration\u0026gt;  # Per-fetch timeout when fetching this target. fetcherTimeout: \u0026lt;duration\u0026gt; # The HTTP resource path on which to fetch metrics from targets. metricsPath: \u0026lt;path\u0026gt; #Statically configured targets. staticConfig: # The targets specified by the static config. targets: [ - \u0026lt;target\u0026gt; ] # Labels assigned to all metrics fetched from the targets. labels: [ \u0026lt;labelname\u0026gt;: \u0026lt;labelvalue\u0026gt; ... ] # default metric level function appends to all expression in this file. defaultMetricLevel: \u0026lt;exp\u0026gt; # Metrics rule allow you to recompute queries. metricsRules: [ - \u0026lt;metric_rules\u0026gt; ]  # The url of target exporter. the format should be complied with \u0026#34;java.net.URI\u0026#34; url: \u0026lt;string\u0026gt; # The path of root CA file. sslCaFilePath: \u0026lt;string\u0026gt; \u0026lt;metric_rules\u0026gt; # The name of rule, which combinates with a prefix \u0026#39;meter_\u0026#39; as the index/table name in storage. name: \u0026lt;string\u0026gt; # MAL expression. exp: \u0026lt;string\u0026gt; More about MAL, please refer to mal.md\nKafka Fetcher Kafka Fetcher pulls messages from Kafka Broker(s) what is the Agent delivered. Check the agent documentation about the details. Typically Tracing Segments, Service/Instance properties, JVM Metrics, and Meter system data are supported. Kafka Fetcher can work with gRPC/HTTP Receivers at the same time for adopting different transport protocols.\nKafka Fetcher is disabled in default, and we configure as following to enable.\nkafka-fetcher: selector: ${SW_KAFKA_FETCHER:default} default: bootstrapServers: ${SW_KAFKA_FETCHER_SERVERS:localhost:9092} skywalking-segments, skywalking-metrics, skywalking-profile, skywalking-managements and skywalking-meters topics are required by kafka-fetcher. If they do not exist, Kafka Fetcher will create them in default. Also, you can create them by yourself before the OAP server started.\nWhen using the OAP server automatical creation mechanism, you could modify the number of partitions and replications of the topics through the following configurations:\nkafka-fetcher: selector: ${SW_KAFKA_FETCHER:default} default: bootstrapServers: ${SW_KAFKA_FETCHER_SERVERS:localhost:9092} partitions: ${SW_KAFKA_FETCHER_PARTITIONS:3} replicationFactor: ${SW_KAFKA_FETCHER_PARTITIONS_FACTOR:2} enableMeterSystem: ${SW_KAFKA_FETCHER_ENABLE_METER_SYSTEM:false} isSharding: ${SW_KAFKA_FETCHER_IS_SHARDING:false} consumePartitions: ${SW_KAFKA_FETCHER_CONSUME_PARTITIONS:\u0026#34;\u0026#34;} In cluster mode, all topics have the same number of partitions. Then we have to set \u0026quot;isSharding\u0026quot; to \u0026quot;true\u0026quot; and assign the partitions to consume for OAP server. The OAP server can use commas to separate multiple partitions.\nKafka Fetcher allows to configure all the Kafka producers listed here in property kafkaConsumerConfig. Such as:\nkafka-fetcher: selector: ${SW_KAFKA_FETCHER:default} default: bootstrapServers: ${SW_KAFKA_FETCHER_SERVERS:localhost:9092} partitions: ${SW_KAFKA_FETCHER_PARTITIONS:3} replicationFactor: ${SW_KAFKA_FETCHER_PARTITIONS_FACTOR:2} enableMeterSystem: ${SW_KAFKA_FETCHER_ENABLE_METER_SYSTEM:false} isSharding: ${SW_KAFKA_FETCHER_IS_SHARDING:true} consumePartitions: ${SW_KAFKA_FETCHER_CONSUME_PARTITIONS:1,3,5} kafkaConsumerConfig: enable.auto.commit: true ... ","excerpt":"Open Fetcher Fetcher is a concept in SkyWalking backend. It uses pulling mode rather than receiver, …","ref":"/docs/main/v8.2.0/en/setup/backend/backend-fetcher/","title":"Open Fetcher"},{"body":"Oracle and Resin plugins These plugins can\u0026rsquo;t be provided in Apache release because of Oracle and Resin Licenses. If you want to know details, please read Apache license legal document\nDue to license incompatibilities/restrictions these plugins are hosted and released in 3rd part repository, go to OpenSkywalking java plugin extension repository to get these.\n","excerpt":"Oracle and Resin plugins These plugins can\u0026rsquo;t be provided in Apache release because of Oracle …","ref":"/docs/main/v8.2.0/en/setup/service-agent/java-agent/agent-optional-plugins/oracle-resin-plugins/","title":"Oracle and Resin plugins"},{"body":"Overview SkyWalking: an open source observability platform used to collect, analyze, aggregate and visualize data from services and cloud native infrastructures. SkyWalking provides an easy way to maintain a clear view of your distributed systems, even across Clouds. It is a modern APM, specially designed for cloud native, container based distributed systems.\nWhy use SkyWalking? SkyWalking provides solutions for observing and monitoring distributed systems, in many different scenarios. First of all, like traditional approaches, SkyWalking provides auto instrument agents for services, such as Java, C#, Node.js, Go, PHP and Nginx LUA. (with calls out for Python and C++ SDK contributions). In multilanguage, continuously deployed environments, cloud native infrastructures grow more powerful but also more complex. SkyWalking\u0026rsquo;s service mesh receiver allows SkyWalking to receive telemetry data from service mesh frameworks such as Istio/Envoy and Linkerd, allowing users to understanding the entire distributed system.\nSkyWalking provides observability capabilities for service(s), service instance(s), endpoint(s). The terms Service, Instance and Endpoint are used everywhere today, so it is worth defining their specific meanings in the context of SkyWalking:\n Service. Represents a set/group of workloads which provide the same behaviours for incoming requests. You can define the service name when you are using instrument agents or SDKs. SkyWalking can also use the name you define in platforms such as Istio. Service Instance. Each individual workload in the Service group is known as an instance. Like pods in Kubernetes, it doesn\u0026rsquo;t need to be a single OS process, however, if you are using instrument agents, an instance is actually a real OS process. Endpoint. A path in a service for incoming requests, such as an HTTP URI path or a gRPC service class + method signature.  SkyWalking allows users to understand the topology relationship between Services and Endpoints, to view the metrics of every Service/Service Instance/Endpoint and to set alarm rules.\nIn addition, you can integrate\n Other distributed tracing useing SkyWalking native agents and SDKs with Zipkin, Jaeger and OpenCensus. Other metrics systems, such as Prometheus, Sleuth(Micrometer).  Architecture SkyWalking is logically split into four parts: Probes, Platform backend, Storage and UI.\n Probes collect data and reformat them for SkyWalking requirements (different probes support different sources). Platform backend, supports data aggregation, analysis and drives process flow from probes to the UI. The analysis includes SkyWalking natives traces and metrics, 3rd party, including Istio and Envoy telemetry, Zipkin trace format, etc. You even can customize aggregation and analysis by using Observability Analysis Language for native metrics and Meter System for extension metrics. Storage houses SkyWalking data through an open/plugable interface. You can choose an existing implementation, such as ElasticSearch, H2 or a MySQL cluster managed by Sharding-Sphere, or implement your own. Patches for new storage implementors welcome! UI a highly customizale web based interface allowing SkyWalking end users to visualize and manage SkyWalking data.  What is next?  Learn SkyWalking\u0026rsquo;s Project Goals FAQ, Why doesn\u0026rsquo;t SkyWalking involve MQ in the architecture?  ","excerpt":"Overview SkyWalking: an open source observability platform used to collect, analyze, aggregate and …","ref":"/docs/main/v8.2.0/en/concepts-and-designs/overview/","title":"Overview"},{"body":"Plugin automatic test framework Plugin test framework is designed for verifying the plugins' function and compatible status. As there are dozens of plugins and hundreds of versions need to be verified, it is impossible to do manually. The test framework uses container based tech stack, requires a set of real services with agent installed, then the test mock OAP backend is running to check the segments data sent from agents.\nEvery plugin maintained in the main repo requires corresponding test cases, also matching the versions in the supported list doc.\nEnvironment Requirements  MacOS/Linux JDK 8+ Docker Docker Compose  Case Base Image Introduction The test framework provides JVM-container and Tomcat-container base images including JDK8, JDK14. You could choose the suitable one for your test case, if both are suitable, JVM-container is preferred.\nJVM-container Image Introduction JVM-container uses openjdk:8 as the base image. JVM-container has supported JDK14, which inherits openjdk:14. The test case project is required to be packaged as project-name.zip, including startup.sh and uber jar, by using mvn clean package.\nTake the following test projects as good examples\n sofarpc-scenario as a single project case. webflux-scenario as a case including multiple projects. jdk14-with-gson-scenario as a single project case with JDK14.  Tomcat-container Image Introduction Tomcat-container uses tomcat:8.5.57-jdk8-openjdk or tomcat:8.5.57-jdk14-openjdk as the base image. The test case project is required to be packaged as project-name.war by using mvn package.\nTake the following test project as a good example\n spring-4.3.x-scenario  Test project hierarchical structure The test case is an independent maven project, and it is required to be packaged as a war tar ball or zip file, depends on the chosen base image. Also, two external accessible endpoints, mostly two URLs, are required.\nAll test case codes should be in org.apache.skywalking.apm.testcase.* package, unless there are some codes expected being instrumented, then the classes could be in test.org.apache.skywalking.apm.testcase.* package.\nJVM-container test project hierarchical structure\n[plugin-scenario] |- [bin] |- startup.sh |- [config] |- expectedData.yaml |- [src] |- [main] |- ... |- [resource] |- log4j2.xml |- pom.xml |- configuration.yaml |- support-version.list [] = directory Tomcat-container test project hierarchical structure\n[plugin-scenario] |- [config] |- expectedData.yaml |- [src] |- [main] |- ... |- [resource] |- log4j2.xml |- [webapp] |- [WEB-INF] |- web.xml |- pom.xml |- configuration.yaml |- support-version.list [] = directory Test case configuration files The following files are required in every test case.\n   File Name Descriptions     configuration.yml Declare the basic case inform, including, case name, entrance endpoints, mode, dependencies.   expectedData.yaml Describe the expected segmentItems.   support-version.list List the target versions for this case   startup.sh JVM-container only, don\u0026rsquo;t need this when useTomcat-container    * support-version.list format requires every line for a single version(Contains only the last version number of each minor version). Could use # to comment out this version.\nconfiguration.yml    Field description     type Image type, options, jvm or tomcat. Required.   entryService The entrance endpoint(URL) for test case access. Required. (HTTP Method: GET)   healthCheck The health check endpoint(URL) for test case access. Required. (HTTP Method: HEAD)   startScript Path of start up script. Required in type: jvm only.   framework Case name.   runningMode Running mode whether with the optional plugin, options, default(default), with_optional, with_bootstrap   withPlugins Plugin selector rule. eg:apm-spring-annotation-plugin-*.jar. Required when runningMode=with_optional or runningMode=with_bootstrap.   environment Same as docker-compose#environment.   depends_on Same as docker-compose#depends_on.   dependencies Same as docker-compose#services, image、links、hostname、environment、depends_on are supported.    Notice:, docker-compose active only when dependencies is only blank.\nrunningMode option description.\n   Option description     default Active all plugins in plugin folder like the official distribution agent.   with_optional Active default and plugins in optional-plugin by the give selector.   with_bootstrap Active default and plugins in bootstrap-plugin by the give selector.    with_optional/with_bootstrap supports multiple selectors, separated by ;.\nFile Format\ntype: entryService: healthCheck: startScript: framework: runningMode: withPlugins: environment: ... depends_on: ... dependencies: service1: image: hostname: expose: ... environment: ... depends_on: ... links: ... entrypoint: ... healthcheck: ...  dependencies supports docker compose healthcheck. But the format is a little difference. We need - as the start of every config item, and describe it as a string line.  Such as in official doc, the health check is\nhealthcheck: test: [\u0026#34;CMD\u0026#34;, \u0026#34;curl\u0026#34;, \u0026#34;-f\u0026#34;, \u0026#34;http://localhost\u0026#34;] interval: 1m30s timeout: 10s retries: 3 start_period: 40s The here, you should write as\nhealthcheck: - \u0026#39;test: [\u0026#34;CMD\u0026#34;, \u0026#34;curl\u0026#34;, \u0026#34;-f\u0026#34;, \u0026#34;http://localhost\u0026#34;]\u0026#39; - \u0026#34;interval: 1m30s\u0026#34; - \u0026#34;timeout: 10s\u0026#34; - \u0026#34;retries: 3\u0026#34; - \u0026#34;start_period: 40s\u0026#34; In some cases, the dependency service, mostly 3rd party server like SolrJ server, is required to keep the same version as client lib version, which defined as ${test.framework.version} in pom. Could use ${CASE_SERVER_IMAGE_VERSION} as the version number, it will be changed in the test for every version.\n Don\u0026rsquo;t support resource related configurations, such as volumes, ports and ulimits. Because in test scenarios, don\u0026rsquo;t need mapping any port to the host VM, or mount any folder.\n Take following test cases as examples\n dubbo-2.7.x with JVM-container jetty with JVM-container gateway with runningMode canal with docker-compose  expectedData.yaml Operator for number\n   Operator Description     nq Not equal   eq Equal(default)   ge Greater than or equal   gt Greater than    Operator for String\n   Operator Description     not null Not null   null Null or empty String   eq Equal(default)    Expected Data Format Of The Segment\nsegmentItems: - serviceName: SERVICE_NAME(string) segmentSize: SEGMENT_SIZE(int) segments: - segmentId: SEGMENT_ID(string) spans: ...    Field Description     serviceName Service Name.   segmentSize The number of segments is expected.   segmentId trace ID.   spans segment span list. Follow the next section to see how to describe every span.    Expected Data Format Of The Span\nNotice: The order of span list should follow the order of the span finish time.\noperationName: OPERATION_NAME(string) parentSpanId: PARENT_SPAN_ID(int) spanId: SPAN_ID(int) startTime: START_TIME(int) endTime: END_TIME(int) isError: IS_ERROR(string: true, false) spanLayer: SPAN_LAYER(string: DB, RPC_FRAMEWORK, HTTP, MQ, CACHE) spanType: SPAN_TYPE(string: Exit, Entry, Local) componentId: COMPONENT_ID(int) tags: - {key: TAG_KEY(string), value: TAG_VALUE(string)} ... logs: - {key: LOG_KEY(string), value: LOG_VALUE(string)} ... peer: PEER(string) refs: - { traceId: TRACE_ID(string), parentTraceSegmentId: PARENT_TRACE_SEGMENT_ID(string), parentSpanId: PARENT_SPAN_ID(int), parentService: PARENT_SERVICE(string), parentServiceInstance: PARENT_SERVICE_INSTANCE(string), parentEndpoint: PARENT_ENDPOINT_NAME(string), networkAddress: NETWORK_ADDRESS(string), refType: REF_TYPE(string: CrossProcess, CrossThread) } ...    Field Description     operationName Span Operation Name.   parentSpanId Parent span id. Notice: The parent span id of the first span should be -1.   spanId Span Id. Notice, start from 0.   startTime Span start time. It is impossible to get the accurate time, not 0 should be enough.   endTime Span finish time. It is impossible to get the accurate time, not 0 should be enough.   isError Span status, true or false.   componentId Component id for your plugin.   tags Span tag list. Notice, Keep in the same order as the plugin coded.   logs Span log list. Notice, Keep in the same order as the plugin coded.   SpanLayer Options, DB, RPC_FRAMEWORK, HTTP, MQ, CACHE.   SpanType Span type, options, Exit, Entry or Local.   peer Remote network address, IP + port mostly. For exit span, this should be required.    The verify description for SegmentRef\n   Field Description     traceId    parentTraceSegmentId Parent SegmentId, pointing to the segment id in the parent segment.   parentSpanId Parent SpanID, pointing to the span id in the parent segment.   parentService The service of parent/downstream service name.   parentServiceInstance The instance of parent/downstream service instance name.   parentEndpoint The endpoint of parent/downstream service.   networkAddress The peer value of parent exit span.   refType Ref type, options, CrossProcess or CrossThread.    Expected Data Format Of The Meter Items\nmeterItems: - serviceName: SERVICE_NAME(string) meterSize: METER_SIZE(int) meters: - ...    Field Description     serviceName Service Name.   meterSize The number of meters is expected.   meters meter list. Follow the next section to see how to describe every meter.    Expected Data Format Of The Meter\nmeterId: name: NAME(string) tags: - {name: TAG_NAME(string), value: TAG_VALUE(string)} singleValue: SINGLE_VALUE(double) histogramBuckets: - HISTOGRAM_BUCKET(double) ... The verify description for MeterId\n   Field Description     name meter name.   tags meter tags.   tags.name tag name.   tags.value tag value.   singleValue counter or gauge value. Using condition operate of the number to validate, such as gt, ge. If current meter is histogram, don\u0026rsquo;t need to write this field.   histogramBuckets histogram bucket. The bucket list must be ordered. The tool assert at least one bucket of the histogram having nonzero count. If current meter is counter or gauge, don\u0026rsquo;t need to write this field.    startup.sh This script provide a start point to JVM based service, most of them starts by a java -jar, with some variables. The following system environment variables are available in the shell.\n   Variable Description     agent_opts Agent plugin opts, check the detail in plugin doc or the same opt added in this PR.   SCENARIO_NAME Service name. Default same as the case folder name   SCENARIO_VERSION Version   SCENARIO_ENTRY_SERVICE Entrance URL to access this service   SCENARIO_HEALTH_CHECK_URL Health check URL     ${agent_opts} is required to add into your java -jar command, which including the parameter injected by test framework, and make agent installed. All other parameters should be added after ${agent_opts}.\n The test framework will set the service name as the test case folder name by default, but in some cases, there are more than one test projects are required to run in different service codes, could set it explicitly like the following example.\nExample\nhome=\u0026#34;$(cd \u0026#34;$(dirname $0)\u0026#34;; pwd)\u0026#34; java -jar ${agent_opts} \u0026#34;-Dskywalking.agent.service_name=jettyserver-scenario\u0026#34; ${home}/../libs/jettyserver-scenario.jar \u0026amp; sleep 1 java -jar ${agent_opts} \u0026#34;-Dskywalking.agent.service_name=jettyclient-scenario\u0026#34; ${home}/../libs/jettyclient-scenario.jar \u0026amp;  Only set this or use other skywalking options when it is really necessary.\n Take the following test cases as examples\n undertow webflux  Best Practices How To Use The Archetype To Create A Test Case Project We provided archetypes and a script to make creating a project easier. It creates a completed project of a test case. So that we only need to focus on cases. First, we can use followed command to get usage about the script.\nbash ${SKYWALKING_HOME}/test/plugin/generator.sh\nThen, runs and generates a project, named by scenario_name, in ./scenarios.\nRecommendations for pom \u0026lt;properties\u0026gt; \u0026lt;!-- Provide and use this property in the pom. --\u0026gt; \u0026lt;!-- This version should match the library version, --\u0026gt; \u0026lt;!-- in this case, http components lib version 4.3. --\u0026gt; \u0026lt;test.framework.version\u0026gt;4.3\u0026lt;/test.framework.version\u0026gt; \u0026lt;/properties\u0026gt; \u0026lt;dependencies\u0026gt; \u0026lt;dependency\u0026gt; \u0026lt;groupId\u0026gt;org.apache.httpcomponents\u0026lt;/groupId\u0026gt; \u0026lt;artifactId\u0026gt;httpclient\u0026lt;/artifactId\u0026gt; \u0026lt;version\u0026gt;${test.framework.version}\u0026lt;/version\u0026gt; \u0026lt;/dependency\u0026gt; ... \u0026lt;/dependencies\u0026gt; \u0026lt;build\u0026gt; \u0026lt;!-- Set the package final name as same as the test case folder case. --\u0026gt; \u0026lt;finalName\u0026gt;httpclient-4.3.x-scenario\u0026lt;/finalName\u0026gt; .... \u0026lt;/build\u0026gt; How To Implement Heartbeat Service Heartbeat service is designed for checking the service available status. This service is a simple HTTP service, returning 200 means the target service is ready. Then the traffic generator will access the entry service and verify the expected data. User should consider to use this service to detect such as whether the dependent services are ready, especially when dependent services are database or cluster.\nNotice, because heartbeat service could be traced fully or partially, so, segmentSize in expectedData.yaml should use ge as the operator, and don\u0026rsquo;t include the segments of heartbeat service in the expected segment data.\nThe example Process of Writing Tracing Expected Data Expected data file, expectedData.yaml, include SegmentItems part.\nWe are using the HttpClient plugin to show how to write the expected data.\nThere are two key points of testing\n Whether is HttpClient span created. Whether the ContextCarrier created correctly, and propagates across processes.  +-------------+ +------------------+ +-------------------------+ | Browser | | Case Servlet | | ContextPropagateServlet | | | | | | | +-----|-------+ +---------|--------+ +------------|------------+ | | | | | | | WebHttp +-+ | +------------------------\u0026gt; |-| HttpClient +-+ | |--------------------------------\u0026gt; |-| | |-| |-| | |-| |-| | |-| \u0026lt;--------------------------------| | |-| +-+ | \u0026lt;--------------------------| | | +-+ | | | | | | | | | | | | | + + + segmentItems By following the flow of HttpClient case, there should be two segments created.\n Segment represents the CaseServlet access. Let\u0026rsquo;s name it as SegmentA. Segment represents the ContextPropagateServlet access. Let\u0026rsquo;s name it as SegmentB.  segmentItems: - serviceName: httpclient-case segmentSize: ge 2 # Could have more than one health check segments, because, the dependency is not standby. Because Tomcat plugin is a default plugin of SkyWalking, so, in SegmentA, there are two spans\n Tomcat entry span HttpClient exit span  SegmentA span list should like following\n- segmentId: not null spans: - operationName: /httpclient-case/case/context-propagate parentSpanId: 0 spanId: 1 startTime: nq 0 endTime: nq 0 isError: false spanLayer: Http spanType: Exit componentId: eq 2 tags: - {key: url, value: \u0026#39;http://127.0.0.1:8080/httpclient-case/case/context-propagate\u0026#39;} - {key: http.method, value: GET} logs: [] peer: 127.0.0.1:8080 - operationName: /httpclient-case/case/httpclient parentSpanId: -1 spanId: 0 startTime: nq 0 endTime: nq 0 spanLayer: Http isError: false spanType: Entry componentId: 1 tags: - {key: url, value: \u0026#39;http://localhost:{SERVER_OUTPUT_PORT}/httpclient-case/case/httpclient\u0026#39;} - {key: http.method, value: GET} logs: [] peer: null SegmentB should only have one Tomcat entry span, but includes the Ref pointing to SegmentA.\nSegmentB span list should like following\n- segmentId: not null spans: - operationName: /httpclient-case/case/context-propagate parentSpanId: -1 spanId: 0 tags: - {key: url, value: \u0026#39;http://127.0.0.1:8080/httpclient-case/case/context-propagate\u0026#39;} - {key: http.method, value: GET} logs: [] startTime: nq 0 endTime: nq 0 spanLayer: Http isError: false spanType: Entry componentId: 1 peer: null refs: - {parentEndpoint: /httpclient-case/case/httpclient, networkAddress: \u0026#39;localhost:8080\u0026#39;, refType: CrossProcess, parentSpanId: 1, parentTraceSegmentId: not null, parentServiceInstance: not null, parentService: not null, traceId: not null} The example Process of Writing Meter Expected Data Expected data file, expectedData.yaml, include MeterItems part.\nWe are using the toolkit plugin to demonstrate how to write the expected data. When write the meter plugin, the expected data file keeps the same.\nThere is one key point of testing\n Build a meter and operate it.  Such as Counter:\nMeterFactory.counter(\u0026#34;test_counter\u0026#34;).tag(\u0026#34;ck1\u0026#34;, \u0026#34;cv1\u0026#34;).build().increment(1d); MeterFactory.histogram(\u0026#34;test_histogram\u0026#34;).tag(\u0026#34;hk1\u0026#34;, \u0026#34;hv1\u0026#34;).steps(1d, 5d, 10d).build().addValue(2d); +-------------+ +------------------+ | Plugin | | Agent core | | | | | +-----|-------+ +---------|--------+ | | | | | Build or operate +-+ +------------------------\u0026gt; |-| | |-] | |-| | |-| | |-| | |-| | \u0026lt;--------------------------| | +-+ | | | | | | | | + + meterItems By following the flow of the toolkit case, there should be two meters created.\n Meter test_counter created from MeterFactory#counter. Let\u0026rsquo;s name it as MeterA. Meter test_histogram created from MeterFactory#histogram. Let\u0026rsquo;s name it as MeterB.  meterItems: - serviceName: toolkit-case meterSize: 2 They\u0026rsquo;re showing two kinds of meter, MeterA has a single value, MeterB has a histogram value.\nMeterA should like following, counter and gauge use the same data format.\n- meterId: name: test_counter tags: - {name: ck1, value: cv1} singleValue: gt 0 MeterB should like following.\n- meterId: name: test_histogram tags: - {name: hk1, value: hv1} histogramBuckets: - 0.0 - 1.0 - 5.0 - 10.0 Local Test and Pull Request To The Upstream First of all, the test case project could be compiled successfully, with right project structure and be able to deploy. The developer should test the start script could run in Linux/MacOS, and entryService/health services are able to provide the response.\nYou could run test by using following commands\ncd ${SKYWALKING_HOME} bash ./test/plugin/run.sh -f ${scenario_name} Notice，if codes in ./apm-sniffer have been changed, no matter because your change or git update， please recompile the skywalking-agent. Because the test framework will use the existing skywalking-agent folder, rather than recompiling it every time.\nUse ${SKYWALKING_HOME}/test/plugin/run.sh -h to know more command options.\nIf the local test passed, then you could add it to .github/workflows/plugins-test.\u0026lt;n\u0026gt;.yaml file, which will drive the tests running on the GitHub Actions of official SkyWalking repository. Based on your plugin\u0026rsquo;s name, please add the test case into file .github/workflows/plugins-test.\u0026lt;n\u0026gt;.yaml, by alphabetical orders.\nEvery test case is a GitHub Actions Job. Please use the scenario directory name as the case name, mostly you\u0026rsquo;ll just need to decide which file (plugins-test.\u0026lt;n\u0026gt;.yaml) to add your test case, and simply put one line (as follows) in it, take the existed cases as examples. You can run python3 tools/select-group.py to see which file contains the least cases and add your cases into it, in order to balance the running time of each group.\nIf a test case required to run in JDK 14 environment, please add you test case into file plugins-jdk14-test.\u0026lt;n\u0026gt;.yaml.\njobs: PluginsTest: name: Plugin runs-on: ubuntu-18.04 timeout-minutes: 90 strategy: fail-fast: true matrix: case: # ... - \u0026lt;your scenario test directory name\u0026gt; # ... ","excerpt":"Plugin automatic test framework Plugin test framework is designed for verifying the plugins' …","ref":"/docs/main/v8.2.0/en/guides/plugin-test/","title":"Plugin automatic test framework"},{"body":"Plugin Development Guide This document describe how to understand, develop and contribute plugin.\nThere are 2 kinds of plugin\n Tracing plugin. Follow the distributed tracing concept to collect spans with tags and logs. Meter plugin. Collect numeric metrics in Counter, Guage, and Histogram formats.  We also provide the plugin test tool to verify the data collected and reported by the plugin. If you plan to contribute any plugin to our main repo, the data would be verified by this tool too.\nTracing plugin Concepts Span Span is an important and common concept in distributed tracing system. Learn Span from Google Dapper Paper and OpenTracing\nSkyWalking supports OpenTracing and OpenTracing-Java API from 2017. Our Span concepts are similar with the paper and OpenTracing. Also we extend the Span.\nThere are three types of Span\n1.1 EntrySpan EntrySpan represents a service provider, also the endpoint of server side. As an APM system, we are targeting the application servers. So almost all the services and MQ-consumer are EntrySpan(s).\n1.2 LocalSpan LocalSpan represents a normal Java method, which does not relate to remote service, neither a MQ producer/consumer nor a service(e.g. HTTP service) provider/consumer.\n1.3 ExitSpan ExitSpan represents a client of service or MQ-producer, as named as LeafSpan at early age of SkyWalking. e.g. accessing DB by JDBC, reading Redis/Memcached are cataloged an ExitSpan.\nContextCarrier In order to implement distributed tracing, the trace across process need to be bind, and the context should propagate across the process. That is ContextCarrier\u0026rsquo;s duty.\nHere are the steps about how to use ContextCarrier in a A-\u0026gt;B distributed call.\n Create a new and empty ContextCarrier at client side. Create an ExitSpan by ContextManager#createExitSpan or use ContextManager#inject to init the ContextCarrier. Put all items of ContextCarrier into heads(e.g. HTTP HEAD), attachments(e.g. Dubbo RPC framework) or messages(e.g. Kafka) The ContextCarrier propagates to server side by the service call. At server side, get all items from heads, attachments or messages. Create an EntrySpan by ContextManager#createEntrySpan or use ContextManager#extract to bind the client and server.  Let\u0026rsquo;s demonstrate the steps by Apache HTTPComponent client plugin and Tomcat 7 server plugin\n Client side steps by Apache HTTPComponent client plugin  span = ContextManager.createExitSpan(\u0026#34;/span/operation/name\u0026#34;, contextCarrier, \u0026#34;ip:port\u0026#34;); CarrierItem next = contextCarrier.items(); while (next.hasNext()) { next = next.next(); httpRequest.setHeader(next.getHeadKey(), next.getHeadValue()); } Server side steps by Tomcat 7 server plugin  ContextCarrier contextCarrier = new ContextCarrier(); CarrierItem next = contextCarrier.items(); while (next.hasNext()) { next = next.next(); next.setHeadValue(request.getHeader(next.getHeadKey())); } span = ContextManager.createEntrySpan(“/span/operation/name”, contextCarrier); ContextSnapshot Besides across process, across thread but in a process need to be supported, because async process(In-memory MQ) and batch process are common in Java. Across process and across thread are similar, because they are both about propagating context. The only difference is that, don\u0026rsquo;t need to serialize for across thread.\nHere are the three steps about across thread propagation:\n Use ContextManager#capture to get the ContextSnapshot object. Let the sub-thread access the ContextSnapshot by any way, through method arguments or carried by an existed arguments Use ContextManager#continued in sub-thread.  Core APIs ContextManager ContextManager provides all major and primary APIs.\n Create EntrySpan  public static AbstractSpan createEntrySpan(String endpointName, ContextCarrier carrier) Create EntrySpan by operation name(e.g. service name, uri) and ContextCarrier.\nCreate LocalSpan  public static AbstractSpan createLocalSpan(String endpointName) Create LocalSpan by operation name(e.g. full method signature)\nCreate ExitSpan  public static AbstractSpan createExitSpan(String endpointName, ContextCarrier carrier, String remotePeer) Create ExitSpan by operation name(e.g. service name, uri) and new ContextCarrier and peer address (e.g. ip+port, hostname+port)\nAbstractSpan /** * Set the component id, which defines in {@link ComponentsDefine} * * @param component * @return the span for chaining. */ AbstractSpan setComponent(Component component); AbstractSpan setLayer(SpanLayer layer); /** * Set a key:value tag on the Span. * * @return this Span instance, for chaining */ AbstractSpan tag(String key, String value); /** * Record an exception event of the current walltime timestamp. * * @param t any subclass of {@link Throwable}, which occurs in this span. * @return the Span, for chaining */ AbstractSpan log(Throwable t); AbstractSpan errorOccurred(); /** * Record an event at a specific timestamp. * * @param timestamp The explicit timestamp for the log record. * @param event the events * @return the Span, for chaining */ AbstractSpan log(long timestamp, Map\u0026lt;String, ?\u0026gt; event); /** * Sets the string name for the logical operation this span represents. * * @return this Span instance, for chaining */ AbstractSpan setOperationName(String endpointName); Besides setting operation name, tags and logs, two attributes should be set, which are component and layer, especially for EntrySpan and ExitSpan\nSpanLayer is the catalog of span. Here are 5 values:\n UNKNOWN (default) DB RPC_FRAMEWORK, for a RPC framework, not an ordinary HTTP HTTP MQ  Component IDs are defined and reserved by SkyWalking project. For component name/ID extension, please follow Component library definition and extension document.\nSpecial Span Tags All tags are available in the trace view, meanwhile, in the OAP backend analysis, some special tag or tag combination could provide other advanced features.\nTag key status_code The value should be an integer. The response code of OAL entities is according to this.\nTag key db.statement and db.type. The value of db.statement should be a String, representing the Database statement, such as SQL, or [No statement]/+span#operationName if value is empty. When exit span has this tag, OAP samples the slow statements based on agent-analyzer/default/maxSlowSQLLength. The threshold of slow statement is defined by following agent-analyzer/default/slowDBAccessThreshold\nExtension logic endpoint. Tag key x-le Logic endpoint is a concept, which doesn\u0026rsquo;t represent a real RPC call, but requires the statistic. The value of x-le should be JSON format, with two options.\n Define a separated logic endpoint. Provide its own endpoint name, latency and status. Suitable for entry and local span.  { \u0026#34;name\u0026#34;: \u0026#34;GraphQL-service\u0026#34;, \u0026#34;latency\u0026#34;: 100, \u0026#34;status\u0026#34;: true } Declare the current local span representing a logic endpoint.  { \u0026#34;logic-span\u0026#34;: true } Advanced APIs Async Span APIs There is a set of advanced APIs in Span, which work specific for async scenario. When tags, logs, attributes(including end time) of the span needs to set in another thread, you should use these APIs.\n/** * The span finish at current tracing context, but the current span is still alive, until {@link #asyncFinish} * called. * * This method must be called\u0026lt;br/\u0026gt; * 1. In original thread(tracing context). * 2. Current span is active span. * * During alive, tags, logs and attributes of the span could be changed, in any thread. * * The execution times of {@link #prepareForAsync} and {@link #asyncFinish()} must match. * * @return the current span */ AbstractSpan prepareForAsync(); /** * Notify the span, it could be finished. * * The execution times of {@link #prepareForAsync} and {@link #asyncFinish()} must match. * * @return the current span */ AbstractSpan asyncFinish();  Call #prepareForAsync in original context. Do ContextManager#stopSpan in original context when your job in current thread is done. Propagate the span to any other thread. After all set, call #asyncFinish in any thread. Tracing context will be finished and report to backend when all spans\u0026rsquo;s #prepareForAsync finished(Judged by count of API execution).  Develop a plugin Abstract The basic method to trace is intercepting a Java method, by using byte code manipulation tech and AOP concept. SkyWalking boxed the byte code manipulation tech and tracing context propagation, so you just need to define the intercept point(a.k.a. aspect pointcut in Spring)\nIntercept SkyWalking provide two common defines to intercept Contructor, instance method and class method.\n Extend ClassInstanceMethodsEnhancePluginDefine defines Contructor intercept points and instance method intercept points. Extend ClassStaticMethodsEnhancePluginDefine defines class method intercept points.  Of course, you can extend ClassEnhancePluginDefine to set all intercept points. But it is unusual.\nImplement plugin I will demonstrate about how to implement a plugin by extending ClassInstanceMethodsEnhancePluginDefine\n Define the target class name  protected abstract ClassMatch enhanceClass(); ClassMatch represents how to match the target classes, there are 4 ways:\n byName, through the full class name(package name + . + class name) byClassAnnotationMatch, through the class existed certain annotations. byMethodAnnotationMatch, through the class\u0026rsquo;s method existed certain annotations. byHierarchyMatch, through the class\u0026rsquo;s parent classes or interfaces  Attentions:\n Never use ThirdPartyClass.class in the instrumentation definitions, such as takesArguments(ThirdPartyClass.class), or byName(ThirdPartyClass.class.getName()), because of the fact that ThirdPartyClass dose not necessarily exist in the target application and this will break the agent; we have import checks to help on checking this in CI, but it doesn\u0026rsquo;t cover all scenarios of this limitation, so never try to work around this limitation by something like using full-qualified-class-name (FQCN), i.e. takesArguments(full.qualified.ThirdPartyClass.class) and byName(full.qualified.ThirdPartyClass.class.getName()) will pass the CI check, but are still invalid in the agent codes, Use Full Qualified Class Name String Literature Instead. Even you are perfectly sure that the class to be intercepted exists in the target application (such as JDK classes), still, don\u0026rsquo;t use *.class.getName() to get the class String name. Recommend you to use literal String. This is for avoiding ClassLoader issues. by*AnnotationMatch doesn\u0026rsquo;t support the inherited annotations. Don\u0026rsquo;t recommend to use byHierarchyMatch, unless it is really necessary. Because using it may trigger intercepting many unexcepted methods, which causes performance issues and concerns.  Example：\n@Override protected ClassMatch enhanceClassName() { return byName(\u0026#34;org.apache.catalina.core.StandardEngineValve\u0026#34;);\t}\tDefine an instance method intercept point  public InstanceMethodsInterceptPoint[] getInstanceMethodsInterceptPoints(); public interface InstanceMethodsInterceptPoint { /** * class instance methods matcher. * * @return methods matcher */ ElementMatcher\u0026lt;MethodDescription\u0026gt; getMethodsMatcher(); /** * @return represents a class name, the class instance must instanceof InstanceMethodsAroundInterceptor. */ String getMethodsInterceptor(); boolean isOverrideArgs(); } Also use Matcher to set the target methods. Return true in isOverrideArgs, if you want to change the argument ref in interceptor.\nThe following sections will tell you how to implement the interceptor.\nAdd plugin define into skywalking-plugin.def file  tomcat-7.x/8.x=TomcatInstrumentation Implement an interceptor As an interceptor for an instance method, the interceptor implements org.apache.skywalking.apm.agent.core.plugin.interceptor.enhance.InstanceMethodsAroundInterceptor\n/** * A interceptor, which intercept method\u0026#39;s invocation. The target methods will be defined in {@link * ClassEnhancePluginDefine}\u0026#39;s subclass, most likely in {@link ClassInstanceMethodsEnhancePluginDefine} */ public interface InstanceMethodsAroundInterceptor { /** * called before target method invocation. * * @param result change this result, if you want to truncate the method. * @throws Throwable */ void beforeMethod(EnhancedInstance objInst, Method method, Object[] allArguments, Class\u0026lt;?\u0026gt;[] argumentsTypes, MethodInterceptResult result) throws Throwable; /** * called after target method invocation. Even method\u0026#39;s invocation triggers an exception. * * @param ret the method\u0026#39;s original return value. * @return the method\u0026#39;s actual return value. * @throws Throwable */ Object afterMethod(EnhancedInstance objInst, Method method, Object[] allArguments, Class\u0026lt;?\u0026gt;[] argumentsTypes, Object ret) throws Throwable; /** * called when occur exception. * * @param t the exception occur. */ void handleMethodException(EnhancedInstance objInst, Method method, Object[] allArguments, Class\u0026lt;?\u0026gt;[] argumentsTypes, Throwable t); } Use the core APIs in before, after and exception handle stages.\nDo bootstrap class instrumentation. SkyWalking has packaged the bootstrap instrumentation in the agent core. It is easy to open by declaring it in the Instrumentation definition.\nOverride the public boolean isBootstrapInstrumentation() and return true. Such as\npublic class URLInstrumentation extends ClassEnhancePluginDefine { private static String CLASS_NAME = \u0026#34;java.net.URL\u0026#34;; @Override protected ClassMatch enhanceClass() { return byName(CLASS_NAME); } @Override public ConstructorInterceptPoint[] getConstructorsInterceptPoints() { return new ConstructorInterceptPoint[] { new ConstructorInterceptPoint() { @Override public ElementMatcher\u0026lt;MethodDescription\u0026gt; getConstructorMatcher() { return any(); } @Override public String getConstructorInterceptor() { return \u0026#34;org.apache.skywalking.apm.plugin.jre.httpurlconnection.Interceptor2\u0026#34;; } } }; } @Override public InstanceMethodsInterceptPoint[] getInstanceMethodsInterceptPoints() { return new InstanceMethodsInterceptPoint[0]; } @Override public StaticMethodsInterceptPoint[] getStaticMethodsInterceptPoints() { return new StaticMethodsInterceptPoint[0]; } @Override public boolean isBootstrapInstrumentation() { return true; } } NOTICE, doing bootstrap instrumentation should only happen in necessary, but mostly it effect the JRE core(rt.jar), and could make very unexpected result or side effect.\nProvide Customization Config for the Plugin The config could provide different behaviours based on the configurations. SkyWalking plugin mechanism provides the configuration injection and initialization system in the agent core.\nEvery plugin could declare one or more classes to represent the config by using @PluginConfig annotation. The agent core could initialize this class' static field though System environments, System properties, and agent.config static file.\nThe #root() method in the @PluginConfig annotation requires to declare the root class for the initialization process. Typically, SkyWalking prefers to use nested inner static classes for the hierarchy of the configuration. Recommend using Plugin/plugin-name/config-key as the nested classes structure of the Config class.\nNOTE, because of the Java ClassLoader mechanism, the @PluginConfig annotation should be added on the real class used in the interceptor codes.\nSuch as, in the following example, @PluginConfig(root = SpringMVCPluginConfig.class) represents the initialization should start with using SpringMVCPluginConfig as the root. Then the config key of the attribute USE_QUALIFIED_NAME_AS_ENDPOINT_NAME, should be plugin.springmvc.use_qualified_name_as_endpoint_name.\npublic class SpringMVCPluginConfig { public static class Plugin { // NOTE, if move this annotation on the `Plugin` or `SpringMVCPluginConfig` class, it no longer has any effect.  @PluginConfig(root = SpringMVCPluginConfig.class) public static class SpringMVC { /** * If true, the fully qualified method name will be used as the endpoint name instead of the request URL, * default is false. */ public static boolean USE_QUALIFIED_NAME_AS_ENDPOINT_NAME = false; /** * This config item controls that whether the SpringMVC plugin should collect the parameters of the * request. */ public static boolean COLLECT_HTTP_PARAMS = false; } @PluginConfig(root = SpringMVCPluginConfig.class) public static class Http { /** * When either {@link Plugin.SpringMVC#COLLECT_HTTP_PARAMS} is enabled, how many characters to keep and send * to the OAP backend, use negative values to keep and send the complete parameters, NB. this config item is * added for the sake of performance */ public static int HTTP_PARAMS_LENGTH_THRESHOLD = 1024; } } } Meter Plugin Java agent plugin could use meter APIs to collect the metrics for backend analysis.\n Counter API represents a single monotonically increasing counter, automatic collect data and report to backend.  import org.apache.skywalking.apm.agent.core.meter.MeterFactory; Counter counter = MeterFactory.counter(meterName).tag(\u0026#34;tagKey\u0026#34;, \u0026#34;tagValue\u0026#34;).mode(Counter.Mode.INCREMENT).build(); counter.increment(1d);  MeterFactory.counter Create a new counter builder with the meter name. Counter.Builder.tag(String key, String value) Mark a tag key/value pair. Counter.Builder.mode(Counter.Mode mode) Change the counter mode, RATE mode means reporting rate to the backend. Counter.Builder.build() Build a new Counter which is collected and reported to the backend. Counter.increment(double count) Increment count to the Counter, It could be a positive value.   Gauge API represents a single numerical value.  import org.apache.skywalking.apm.agent.core.meter.MeterFactory; ThreadPoolExecutor threadPool = ...; Gauge gauge = MeterFactory.gauge(meterName, () -\u0026gt; threadPool.getActiveCount()).tag(\u0026#34;tagKey\u0026#34;, \u0026#34;tagValue\u0026#34;).build();  MeterFactory.gauge(String name, Supplier\u0026lt;Double\u0026gt; getter) Create a new gauge builder with the meter name and supplier function, this function need to return a double value. Gauge.Builder.tag(String key, String value) Mark a tag key/value pair. Gauge.Builder.build() Build a new Gauge which is collected and reported to the backend.   Histogram API represents a summary sample observations with customize buckets.  import org.apache.skywalking.apm.agent.core.meter.MeterFactory; Histogram histogram = MeterFactory.histogram(\u0026#34;test\u0026#34;).tag(\u0026#34;tagKey\u0026#34;, \u0026#34;tagValue\u0026#34;).steps(Arrays.asList(1, 5, 10)).minValue(0).build(); histogram.addValue(3);  MeterFactory.histogram(String name) Create a new histogram builder with the meter name. Histogram.Builder.tag(String key, String value) Mark a tag key/value pair. Histogram.Builder.steps(List\u0026lt;Double\u0026gt; steps) Set up the max values of every histogram buckets. Histogram.Builder.minValue(double value) Set up the minimal value of this histogram, default is 0. Histogram.Builder.build() Build a new Histogram which is collected and reported to the backend. Histogram.addValue(double value) Add value into the histogram, automatically analyze what bucket count needs to be increment. rule: count into [step1, step2).  Plugin Test Tool Apache SkyWalking Agent Test Tool Suite a tremendously useful test tools suite in a wide variety of languages of Agent. Includes mock collector and validator. The mock collector is a SkyWalking receiver, like OAP server.\nYou could learn how to use this tool to test the plugin in this doc. If you want to contribute plugins to SkyWalking official repo, this is required.\nContribute plugins into Apache SkyWalking repository We are welcome everyone to contribute plugins.\nPlease follow there steps:\n Submit an issue about which plugins you are going to contribute, including supported version. Create sub modules under apm-sniffer/apm-sdk-plugin or apm-sniffer/optional-plugins, and the name should include supported library name and versions Follow this guide to develop. Make sure comments and test cases are provided. Develop and test. Provide the automatic test cases. Learn how to write the plugin test case from this doc Send the pull request and ask for review. The plugin committers approve your plugins, plugin CI-with-IT, e2e and plugin tests passed. The plugin accepted by SkyWalking.  ","excerpt":"Plugin Development Guide This document describe how to understand, develop and contribute plugin. …","ref":"/docs/main/v8.2.0/en/guides/java-plugin-development-guide/","title":"Plugin Development Guide"},{"body":"Powered by Apache SkyWalking This page documents an alphabetical list of institutions that are using Apache SkyWalking for research and production, or providing commercial products including Apache SkyWalking.\n 100tal.cn 北京世纪好未来教育科技有限公司 http://www.100tal.com/ 17173.com https://www.17173.com/ 300.cn 中企动力科技股份有限公司 http://www.300.cn/ 360jinrong.net 360金融 https://www.360jinrong.net/ 4399.com 四三九九网络股份有限公司. http://www.4399.com/ 51mydao.com 买道传感科技（上海）有限公司 https://www.51mydao.com/ 58 Daojia Inc. 58到家 https://www.daojia.com 5i5j. 上海我爱我家房地产经纪有限公司 https://sh.5i5j.com/about/ Anheuser-Busch InBev 百威英博 Agricultural Bank of China 中国农业银行 Aihuishou.com 爱回收网 http://www.aihuishou.com/ Alibaba Cloud, 阿里云, http://aliyun.com Anxin Insurance. 安心财产保险有限责任公司 https://www.95303.com APM Star 北京天空漫步科技有限公司 http://www.apmstar.com AsiaInfo Inc. http://www.asiainfo.com.cn/ Autohome. 汽车之家. http://www.autohome.com.cn baidu 百度 https://www.baidu.com/ Baixing.com 百姓网 http://www.baixing.com/ bitauto 易车 http://bitauto.com hellobanma 斑马网络 https://www.hellobanma.com/ bestsign. 上上签. https://www.bestsign.cn/page/ Beike Finance 贝壳金服 https://www.bkjk.com/ Bizsaas.cn 北京商云科技发展有限公司. http://www.bizsaas.cn/ BoCloud 苏州博纳讯动软件有限公司. http://www.bocloud.com.cn/ Cdlhyj.com 六合远教（成都）科技有限公司 http://www.cdlhyj.com Chehejia Automotive. 北京车和家信息技术有限责任公司. https://www.chehejia.com/ China Eastern Airlines 中国东方航空 http://www.ceair.com/ China Express Airlines 华夏航空 http://www.chinaexpressair.com/ Chinadaas. 北京中数智汇科技股份有限公司. https://www.chinadaas.com/ Chinasoft International 中软国际 China Merchants Bank. 中国招商银行. http://www.cmbchina.com/ China National Software 中软 China Mobile 中国移动 China Unicom 中国联通 China Tower 中国铁塔 China Telecom 中国电信 Chinese Academy of Sciences Chtwm.com. 恒天财富投资管理股份有限公司. https://www.chtwm.com/ Cmft.com. 招商局金融科技. https://www.cmft.com/ CXIST.com 上海程析智能科技有限公司 https://www.cxist.com/ Dangdang.com. 当当网. http://www.dangdang.com/ DaoCloud. https://www.daocloud.io/ deepblueai.com 深兰科技上海有限公司 https://www.deepblueai.com/ Deppon Logistics Co Ltd 德邦物流 https://www.deppon.com/ Deyoushenghuo in WeChat app. 河南有态度信息科技有限公司，微信小程序：得有生活 Dianfubao.com 垫富宝 https://www.dianfubao.com/ DiDi 滴滴出行 dxy.cn 丁香园 http://www.dxy.cn/ Echplus.com 北京易诚互动网络技术有限公司 http://www.echplus.com/ Enmonster 怪兽充电 http://www.enmonster.com/ Eqxiu.com. 北京中网易企秀科技有限公司 http://www.eqxiu.com/ essence.com.cn 安信证券股份有限公司 http://www.essence.com.cn/ fangdd.com 房多多 https://www.fangdd.com fullgoal.com.cn 富国基金管理有限公司 https://www.fullgoal.com.cn/ GTrace System. (No company provided) GSX Techedu Inc. 跟谁学 https://www.genshuixue.com Gdeng.cn 深圳谷登科技有限公司 http://www.gdeng.cn/ GOME 国美 https://www.gome.com.cn/ Guazi.com 瓜子二手车直卖网. https://www.guazi.com/ guohuaitech.com 北京国槐信息科技有限公司. http://www.guohuaitech.com/ GrowingIO 北京易数科技有限公司 https://www.growingio.com/ Haier. 海尔消费金融 https://www.haiercash.com/ Haoyunhu. 上海好运虎供应链管理有限公司 http://www.haoyunhu56.com/ helijia.com 河狸家 http://www.helijia.com/ Huawei Hundun YUNRONG Fintech. 杭州恒生云融网络科技有限公司 https://www.hsjry.com/ hunliji.com 婚礼纪 https://www.hunliji.com/ hydee.cn 海典软件 http://www.hydee.cn/ iBoxChain 盒子科技 https://www.iboxpay.com/ iFLYTEK. 科大讯飞股份有限公司-消费者BG http://www.iflytek.com/ Inspur 浪潮集团 iQIYI.COM. 爱奇艺 https://www.iqiyi.com/ juhaokan 聚好看科技股份有限公司 https://www.juhaokan.org/ Ke.com. 贝壳找房. https://www.ke.com Keking.cn 凯京集团. http://www.keking.cn KubeSphere https://kubesphere.io JoinTown. 九州通医药集团 http://www.jztey.com/ Lagou.com. 拉勾. https://www.lagou.com/ laocaibao. 上海证大爱特金融信息服务有限公司 https://www.laocaibao.com/ Lenovo 联想 liaofan168.com 了凡科技 http://www.liaofan168.com lianzhongyouche.com.cn 联众优车 https://www.lianzhongyouche.com.cn/ Lima 北京力码科技有限公司 https://www.zhongbaounion.com/ Lifesense.com 广东乐心医疗电子股份有限公司 http://www.lifesense.com/ lizhi.fm 荔枝FM https://www.lizhi.fm/ Lixiang.com 理想汽车 https://www.lixiang.com/ Madecare. 北京美德远健科技有限公司. http://www.madecare.com/ Maodou.com 毛豆新车网. https://www.maodou.com/ Mobanker.com 上海前隆信息科技有限公司 http://www.mobanker.com/ Mxnavi. 沈阳美行科技有限公司 http://www.mxnavi.com/ Moji 墨叽（深圳）科技有限公司 https://www.mojivip.com Minsheng FinTech / China Minsheng Bank 民生科技有限责任公司 http://www.mskj.com/ Migu Digital Media Co.Ltd. 咪咕数字传媒有限公司 http://www.migu.cn/ Mypharma.com 北京融贯电子商务有限公司 https://www.mypharma.com NetEase 网易 https://www.163.com/ Osacart in WeChat app 广州美克曼尼电子商务有限公司 Oriente. https://oriente.com/ Peking University 北京大学 Ping An Technology / Ping An Insurance 平安科技 Primeton.com 普元信息技术股份有限公司 http://www.primeton.com qiniu.com 七牛云 http://qiniu.com Qingyidai.com 轻易贷 https://www.qingyidai.com/ Qsdjf.com 浙江钱宝网络科技有限公司 https://www.qsdjf.com/index.html Qk365.com 上海青客电子商务有限公司 https://www.qk365.com Qudian 趣店 http://ir.qudian.com/ Renren Network 人人网 Rong Data. 荣数数据 http://www.rong-data.com/ Rongjinbao. 深圳融金宝互联网金融服务有限公司. http://www.rjb777.com Safedog. 安全狗. http://www.safedog.cn/ servingcloud.com 盈佳云创科技(深圳)有限公司 http://www.servingcloud.com/ SF Express 顺丰速运 https://www.sf-express.com/ Shouqi Limousine \u0026amp; chauffeur Group 首约科技(北京)有限公司. https://www.01zhuanche.com/ shuaibaoshop.com 宁波鲸灵网络科技有限公司 http://www.shuaibaoshop.com/ shuyun.com 杭州数云信息技术有限公司 http://www.shuyun.com/ Sijibao.com 司机宝 https://www.sijibao.com/ Sina 新浪 Sinolink Securities Co.,Ltd. 国金证券佣金宝 http://www.yongjinbao.com.cn/ Source++ https://sourceplusplus.com SPD Bank 浦发银行 StartDT 奇点云 https://www.startdt.com/ State Grid Corporation of China 国家电网有限公司 Successchannel 苏州渠成易销网络科技有限公司. http://www.successchannel.com SuperMap 北京超图软件 syswin.com 北京思源集团 http://www.syswin.com/ szhittech.com 深圳和而泰智能控制股份有限公司. http://www.szhittech.com/ Tencent Tetrate.io https://www.tetrate.io/ Thomas Cook 托迈酷客 https://www.thomascook.com.cn Three Squirrels 三只松鼠 Today36524.com Today便利店 Tongcheng. 同城金服. https://jr.ly.com/ Tools information technology co. LTD 杭州图尔兹信息技术有限公司 http://bintools.cn/ TravelSky 中国航信 http://www.travelsky.net/ Tsfinance.com 重庆宜迅联供应链科技有限公司 https://www.tsfinance.com.cn/ tuhu.cn 途虎养车 https://www.tuhu.cn Tuya. 涂鸦智能. https://www.tuya.com Tydic 天源迪科 https://www.tydic.com/ VBill Payment Co., LTD. 随行付. https://www.vbill.cn/ Wahaha Group 娃哈哈 http://www.wahaha.com.cn/ WeBank. 微众银行 http://www.webank.com Weier. 广州文尔软件科技有限公司. https://www.site0.cn Wochu. 我厨买菜. https://www.wochu.cn Xiaomi. 小米. https://www.mi.com/en/ xin.com 优信集团 http://www.xin.com Xinyebang.com 重庆欣业邦网络技术有限公司 http://www.xinyebang.com xueqiu.com 雪球财经. https://xueqiu.com/ yibainetwork.com 深圳易佰网络有限公司 http://www.yibainetwork.com/ Yiguo. 易果生鲜. http://www.yiguo.com/ Yinji(shenzhen)Network Technology Co.,Ltd. 印记. http://www.yinjiyun.cn/ Yonghui Superstores Co., Ltd. 永辉超市 http://www.yonghui.com.cn Yonyou 用友 Youzan.com 杭州有赞科技有限公司 http://www.youzan.com/ Yunda Express 韵达快运 http://www.yunda56.com/ Yunnan Airport Group Co.,Ltd. 云南机场集团 yxt 云学堂 http://www.yxt.com/ zbj.com 猪八戒 https://www.zbj.com/ zhaopin.com 智联招聘 https://www.zhaopin.com/ zjs.com.cn 北京宅急送快运股份有限公司 http://www.zjs.com.cn/  Use Cases Alibaba and Alibaba Cloud Alibaba products including Cloud DevOps product are under SkyWalking monitoring.\nAlibaba Cloud supports SkyWalking agents and formats in Tracing Analysis cloud service.\nChina Eastern Airlines Integrated in the microservices architecture support platform.\nChina Merchants Bank Use SkyWalking and SkyAPM .net agent in the CMBChina Mall project.\nChina Mobile China Mobile Suzhou Research Center, CMSS, integrated SkyWalking as the APM component in China Mobile PAAS.\nke.com Deploy SkyWalking in production environments.\n Three CentOs Machines(32 CPUs, 64G RAM, 1.3T Disk) for Collector Server Three ElasticSearch(Version 6.4.2, 40 CPUs, 96G RAM, 7T Disk) Nodes for Storage  Support 60+ Instances, Over 300k Calls Per Minute, Over 50k Spans Per Second\nguazi.com Guazi.com uses SkyWalking monitoring 270+ services, including topology + metrics analysis, and collecting 1.1+ billion traces per day with 100% sampling.\nPlan is 1k+ services and 5 billion traces per day.\nOscart Use multiple language agents from SkyWalking and its ecosystem, including SkyWalking Javaagent and SkyAPM nodejs agent. SkyWalking OAP platform acts as backend and visualization.\nPrimeton Integrated in Primeton EOS PLATFORM 8, which is a commercial micro-service platform.\nQiniu Cloud Provide a customized version SkyWalking agent. It could provide distributed tracing and integrated in its intelligence log management platform.\nSource++ An open-source observant programming assistant which aims to bridge APM tools with the developer\u0026rsquo;s IDE to enable tighter feedback loops. Source++ uses SkyWalking as the defacto APM for JVM-based applications.\nTetrate Tetrate provides enterprise level service mesh. SkyWalking acts as the core observability platform for hybrid enterprise service mesh environment.\nlagou.com Lagou.com use Skywalking for JVM-based applications, deployed in production. Custom and optimize muiti collector functions, such as alarm, sql metric, circle operation metric, thread monitor, detail mode. Support 200+ Instances, over 4500k Segments Per Minute.\nYonghui Superstores Yonghui Superstores Co., Ltd. use SkyWalking as primary APM system, to monitor 1k+ instances clusters, which supports 150k+ tps/qps payload. SkyWalking collect, analysis and save 10 billions trace segments(cost 3T disk) each day in 100% sampling strategy. SkyWalking backend cluster is built with 15 nodes OAP and 20 nodes ElasticSearch.\n","excerpt":"Powered by Apache SkyWalking This page documents an alphabetical list of institutions that are using …","ref":"/docs/main/v8.2.0/powered-by/","title":"Powered by Apache SkyWalking"},{"body":"Probe Introduction In SkyWalking, probe means an agent or SDK library integrated into target system, which take charge of collecting telemetry data including tracing and metrics. Based on the target system tech stack, probe could use very different ways to do so. But ultimately they are same, just collect and reformat data, then send to backend.\nIn high level, there are three typical groups in all SkyWalking probes.\n  Language based native agent. This kind of agents runs in target service user space, like a part of user codes. Such as SkyWalking Java agent, use -javaagent command line argument to manipulate codes in runtime, manipulate means change and inject user\u0026rsquo;s codes. Another kind of agents is using some hook or intercept mechanism provided by target libraries. So you can see, these kinds of agents based on languages and libraries.\n  Service Mesh probe. Service Mesh probe collects data from sidecar, control panel in service mesh or proxy. In old days, proxy is only used as ingress of the whole cluster, but with the Service Mesh and sidecar, now we can do observe based on that.\n  3rd-party instrument library. SkyWalking accepts other popular used instrument libraries data format. It analysis the data, transfer it to SkyWalking formats of trace, metrics or both. This feature starts with accepting Zipkin span data. See Receiver for other tracers to know more.\n  You don\u0026rsquo;t need to use Language based native agent and Service Mesh probe at the same time, because they both collect metrics data. As a result of that, your system suffers twice payloads, and the analytic numbers are doubled.\nThere are several recommend ways in using these probes:\n Use Language based native agent only. Use 3rd-party instrument library only, like Zipkin instrument ecosystem. Use Service Mesh probe only. Use Service Mesh probe with Language based native agent or 3rd-party instrument library in tracing status. (Advanced usage)  In addition, let\u0026rsquo;s example what is the meaning of in tracing status?\nIn default, Language based native agent and 3rd-party instrument library both send distributed traces to backend, which do analysis and aggregate on those traces. In tracing status means, backend considers these traces as something like logs, just save them, and build the links between traces and metrics, like which endpoint and service does the trace belong?.\nWhat is next?  Learn the SkyWalking supported probes in Service auto instrument agent, Manual instrument SDK, Service Mesh probe and Zipkin receiver. After understand the probe, read backend overview for understanding analysis and persistence.  ","excerpt":"Probe Introduction In SkyWalking, probe means an agent or SDK library integrated into target system, …","ref":"/docs/main/v8.2.0/en/concepts-and-designs/probe-introduction/","title":"Probe Introduction"},{"body":"Problem when you start your application with skywalking agent,if you find this exception in your agent log which mean EnhanceRequireObjectCache can not be casted to EnhanceRequireObjectCache.eg:\nERROR 2018-05-07 21:31:24 InstMethodsInter : class[class org.springframework.web.method.HandlerMethod] after method[getBean] intercept failure java.lang.ClassCastException: org.apache.skywalking.apm.plugin.spring.mvc.commons.EnhanceRequireObjectCache cannot be cast to org.apache.skywalking.apm.plugin.spring.mvc.commons.EnhanceRequireObjectCache at org.apache.skywalking.apm.plugin.spring.mvc.commons.interceptor.GetBeanInterceptor.afterMethod(GetBeanInterceptor.java:45) at org.apache.skywalking.apm.agent.core.plugin.interceptor.enhance.InstMethodsInter.intercept(InstMethodsInter.java:105) at org.springframework.web.method.HandlerMethod.getBean(HandlerMethod.java) at org.springframework.web.servlet.handler.AbstractHandlerMethodExceptionResolver.shouldApplyTo(AbstractHandlerMethodExceptionResolver.java:47) at org.springframework.web.servlet.handler.AbstractHandlerExceptionResolver.resolveException(AbstractHandlerExceptionResolver.java:131) at org.springframework.web.servlet.handler.HandlerExceptionResolverComposite.resolveException(HandlerExceptionResolverComposite.java:76) ... Reason this exception may caused by some hot deployment tools(spring-boot-devtool) or some else which may change the classloader in runtime.\nResolve  Production environment does not appear this error because developer tools are automatically disabled,look spring-boot-devtools If you want to debug in your development environment normally,you should remove such hot deployment package in your lib path temporarily.  ","excerpt":"Problem when you start your application with skywalking agent,if you find this exception in your …","ref":"/docs/main/v8.2.0/en/faq/enhancerequireobjectcache-cast-exception/","title":"Problem"},{"body":"Problem  Import skywalking project to Eclipse,Occur following errors:   Software being installed: Checkstyle configuration plugin for M2Eclipse 1.0.0.201705301746 (com.basistech.m2e.code.quality.checkstyle.feature.feature.group 1.0.0.201705301746) Missing requirement: Checkstyle configuration plugin for M2Eclipse 1.0.0.201705301746 (com.basistech.m2e.code.quality.checkstyle.feature.feature.group 1.0.0.201705301746) requires \u0026lsquo;net.sf.eclipsecs.core 5.2.0\u0026rsquo; but it could not be found\n Reason Haven\u0026rsquo;t installed Eclipse Checkstyle Plug-in\nResolve Download the plugin through the link:https://sourceforge.net/projects/eclipse-cs/?source=typ_redirect，Eclipse Checkstyle Plug-in version:8.7.0.201801131309 plugin required. plugin notification: The Eclipse Checkstyle plug-in integrates the Checkstyle Java code auditor into the Eclipse IDE. The plug-in provides real-time feedback to the user about violations of rules that check for coding style and possible error prone code constructs.\n","excerpt":"Problem  Import skywalking project to Eclipse,Occur following errors:   Software being installed: …","ref":"/docs/main/v8.2.0/en/faq/import-project-eclipse-requireitems-exception/","title":"Problem"},{"body":"Problem The trace doesn\u0026rsquo;t continue in kafka consumer side.\nReason The kafka client is pulling message from server, the plugin also just traces the pull action. As that, you need to do the manual instrument before the pull action, and include the further data process.\nResolve Use Application Toolkit libraries to do manual instrumentation. such as @KafkaPollAndInvoke annotation at apm-toolkit-kafka or OpenTracing API, Or if you\u0026rsquo;re using spring-kafka 2.2.x or above, you can track the Consumer side without any code change.\n","excerpt":"Problem The trace doesn\u0026rsquo;t continue in kafka consumer side.\nReason The kafka client is pulling …","ref":"/docs/main/v8.2.0/en/faq/kafka-plugin/","title":"Problem"},{"body":"Problem When using a thread pool, TraceSegment data in a thread cannot be reported and there are memory data that cannot be recycled (memory leaks)\nExample ThreadPoolTaskExecutor executor = new ThreadPoolTaskExecutor(); executor.setThreadFactory(r -\u0026gt; new Thread(RunnableWrapper.of(r))); Reason  Worker threads are enhanced, when using thread pool. According to the SkyWalking Java Agent design, when you want to trace cross thread, you need to enhance the task thread.  Resolve   When using Thread Schedule Framework Checked SkyWalking Thread Schedule Framework at SkyWalking Java agent supported list, such as Spring FrameWork @Async, which can implement tracing without any modification.\n  When using Custom Thread Pool Enhance the task thread with the following usage.\n  ExecutorService executorService = Executors.newFixedThreadPool(1); executorService.execute(RunnableWrapper.of(new Runnable() { @Override public void run() { //your code  } })); See across thread solution APIs for more usage\n","excerpt":"Problem When using a thread pool, TraceSegment data in a thread cannot be reported and there are …","ref":"/docs/main/v8.2.0/en/faq/memory-leak-enhance-worker-thread/","title":"Problem"},{"body":"Problem  In maven build, the protoc-plugin occurs error:  [ERROR] Failed to execute goal org.xolstice.maven.plugins:protobuf-maven-plugin:0.5.0:compile-custom (default) on project apm-network: Unable to copy the file to \\skywalking\\apm-network\\target\\protoc-plugins: \\skywalking\\apm-network\\target\\protoc-plugins\\protoc-3.3.0-linux-x86_64.exe (The process cannot access the file because it is being used by another process) -\u0026gt; [Help 1] Reason  Protobuf compiler is dependent on the glibc, but it is not-installed or installed old version in the system.  Resolve  Install or upgrade to the latest version of the glibc library. In container env, recommend using the latest glibc version of the alpine system. Please refer to http://www.gnu.org/software/libc/documentation.html  ","excerpt":"Problem  In maven build, the protoc-plugin occurs error:  [ERROR] Failed to execute goal …","ref":"/docs/main/v8.2.0/en/faq/protoc-plugin-fails-when-build/","title":"Problem"},{"body":"Problem The message with Field ID, 8888, must be revered.\nReason Because Thrift cannot carry metadata to transport Trace Header in the original API, we transport those by wrapping TProtocolFactory to do that.\nThrift allows us to append any additional field in the Message even if the receiver doesn\u0026rsquo;t deal with them. This data is going to be skipped while no one reads. Base on this, we take the 8888th field of Message to store Trace Header(or metadata) and to transport. That means the message with Field ID, 8888, must be revered.\nResolve Avoiding to use the Field(ID is 8888) in your application.\n","excerpt":"Problem The message with Field ID, 8888, must be revered.\nReason Because Thrift cannot carry …","ref":"/docs/main/v8.2.0/en/faq/thrift-plugin/","title":"Problem"},{"body":"Problem  There is no abnormal log in Agent log and Collector log, The traces show, but no other info in UI.  Reason The operating system where the monitored system is located is not set as the current time zone, causing statistics collection time points to deviate.\nResolve Make sure the time is sync in collector servers and monitored application servers.\n","excerpt":"Problem  There is no abnormal log in Agent log and Collector log, The traces show, but no other info …","ref":"/docs/main/v8.2.0/en/faq/why-have-traces-no-others/","title":"Problem"},{"body":"Problem： Maven compilation failure with error like Error: not found: python2 When you compile the project via maven, it failed at module apm-webapp and the following error occured.\nPay attention to key words such as node-sass and Error: not found: python2.\n[INFO] \u0026gt; node-sass@4.11.0 postinstall C:\\XXX\\skywalking\\skywalking-ui\\node_modules\\node-sass [INFO] \u0026gt; node scripts/build.js [ERROR] gyp verb check python checking for Python executable \u0026quot;python2\u0026quot; in the PATH [ERROR] gyp verb `which` failed Error: not found: python2 [ERROR] gyp verb `which` failed at getNotFoundError (C:\\XXX\\skywalking\\skywalking-ui\\node_modules\\which\\which.js:13:12) [ERROR] gyp verb `which` failed at F (C:\\XXX\\skywalking\\skywalking-ui\\node_modules\\which\\which.js:68:19) [ERROR] gyp verb `which` failed at E (C:\\XXX\\skywalking\\skywalking-ui\\node_modules\\which\\which.js:80:29) [ERROR] gyp verb `which` failed at C:\\XXX\\skywalking\\skywalking-ui\\node_modules\\which\\which.js:89:16 [ERROR] gyp verb `which` failed at C:\\XXX\\skywalking\\skywalking-ui\\node_modules\\isexe\\index.js:42:5 [ERROR] gyp verb `which` failed at C:\\XXX\\skywalking\\skywalking-ui\\node_modules\\isexe\\windows.js:36:5 [ERROR] gyp verb `which` failed at FSReqWrap.oncomplete (fs.js:152:21) [ERROR] gyp verb `which` failed code: 'ENOENT' } [ERROR] gyp verb check python checking for Python executable \u0026quot;python\u0026quot; in the PATH [ERROR] gyp verb `which` succeeded python C:\\Users\\XXX\\AppData\\Local\\Programs\\Python\\Python37\\python.EXE [ERROR] gyp ERR! configure error [ERROR] gyp ERR! stack Error: Command failed: C:\\Users\\XXX\\AppData\\Local\\Programs\\Python\\Python37\\python.EXE -c import sys; print \u0026quot;%s.%s.%s\u0026quot; % sys.version_info[:3]; [ERROR] gyp ERR! stack File \u0026quot;\u0026lt;string\u0026gt;\u0026quot;, line 1 [ERROR] gyp ERR! stack import sys; print \u0026quot;%s.%s.%s\u0026quot; % sys.version_info[:3]; [ERROR] gyp ERR! stack ^ [ERROR] gyp ERR! stack SyntaxError: invalid syntax [ERROR] gyp ERR! stack [ERROR] gyp ERR! stack at ChildProcess.exithandler (child_process.js:275:12) [ERROR] gyp ERR! stack at emitTwo (events.js:126:13) [ERROR] gyp ERR! stack at ChildProcess.emit (events.js:214:7) [ERROR] gyp ERR! stack at maybeClose (internal/child_process.js:925:16) [ERROR] gyp ERR! stack at Process.ChildProcess._handle.onexit (internal/child_process.js:209:5) [ERROR] gyp ERR! System Windows_NT 10.0.17134 ...... [INFO] server-starter-es7 ................................. SUCCESS [ 11.657 s] [INFO] apm-webapp ......................................... FAILURE [ 25.857 s] [INFO] apache-skywalking-apm .............................. SKIPPED [INFO] apache-skywalking-apm-es7 .......................... SKIPPED Reason It has nothing to do with SkyWalking.\nAccording to https://github.com/sass/node-sass/issues/1176, if you live in countries where requesting resources from GitHub and npmjs.org is very slowly, some precompiled binaries for dependency node-sass will fail to be downloaded during npm install, then npm will try to compile them itself. That\u0026rsquo;s why python2 is needed.\nResolve 1. Use mirror. Such as in China, please edit skywalking\\apm-webapp\\pom.xml Find\n\u0026lt;configuration\u0026gt; \u0026lt;arguments\u0026gt;install --registry=https://registry.npmjs.org/\u0026lt;/arguments\u0026gt; \u0026lt;/configuration\u0026gt; Replace it with\n\u0026lt;configuration\u0026gt; \u0026lt;arguments\u0026gt;install --registry=https://registry.npm.taobao.org/ --sass_binary_site=https://npm.taobao.org/mirrors/node-sass/\u0026lt;/arguments\u0026gt; \u0026lt;/configuration\u0026gt; 2. Get an enough powerful VPN ","excerpt":"Problem： Maven compilation failure with error like Error: not found: python2 When you compile the …","ref":"/docs/main/v8.2.0/en/faq/maven-compile-npm-failure/","title":"Problem： Maven compilation failure with error like `Error： not found： python2`"},{"body":"Protocols There are two types of protocols list here.\n  Probe Protocol. Include the descriptions and definitions about how agent send collected metrics data and traces, also the formats of each entities.\n  Query Protocol. The backend provide query capability to SkyWalking own UI and others. These queries are based on GraphQL.\n  Probe Protocols They also related to the probe group, for understand that, look Concepts and Designs document. These groups are Language based native agent protocol, Service Mesh protocol and 3rd-party instrument protocol.\nLanguage based native agent protocol There are two types of protocols to make language agents work in distributed environments.\n Cross Process Propagation Headers Protocol and Cross Process Correlation Headers Protocol are in wire data format, agent/SDK usually uses HTTP/MQ/HTTP2 headers to carry the data with rpc request. The remote agent will receive this in the request handler, and bind the context with this specific request. Trace Data Protocol is out of wire data, agent/SDK uses this to send traces and metrics to skywalking or other compatible backend.  Cross Process Propagation Headers Protocol v3 is the new protocol for in-wire context propagation, started in 8.0.0 release.\nCross Process Correlation Headers Protocol v1 is a new in-wire context propagation additional and optional protocols. Please read SkyWalking language agents documentations to see whether it is supported. This protocol defines the data format of transporting custom data with Cross Process Propagation Headers Protocol. SkyWalking javaagent begins to support this since 8.0.0.\nSkyWalking Trace Data Protocol v3 defines the communication way and format between agent and backend.\nBrowser probe protocol The browser probe, such as skywalking-client-js could use this protocol to send to backend. This service provided by gRPC.\nSkyWalking Browser Protocol define the communication way and format between skywalking-client-js and backend.\nService Mesh probe protocol The probe in sidecar or proxy could use this protocol to send data to backendEnd. This service provided by gRPC, requires the following key info:\n Service Name or ID at both sides. Service Instance Name or ID at both sides. Endpoint. URI in HTTP, service method full signature in gRPC. Latency. In milliseconds. Response code in HTTP Status. Success or fail. Protocol. HTTP, gRPC DetectPoint. In Service Mesh sidecar, client or server. In normal L7 proxy, value is proxy.  3rd-party instrument protocol 3rd-party instrument protocols are not defined by SkyWalking. They are just protocols/formats, which SkyWalking is compatible and could receive from their existed libraries. SkyWalking starts with supporting Zipkin v1, v2 data formats.\nBackend is based on modularization principle, so very easy to extend a new receiver to support new protocol/format.\nQuery Protocol Query protocol follows GraphQL grammar, provides data query capabilities, which depends on your analysis metrics. Read query protocol doc for more details.\n","excerpt":"Protocols There are two types of protocols list here.\n  Probe Protocol. Include the descriptions and …","ref":"/docs/main/v8.2.0/en/protocols/readme/","title":"Protocols"},{"body":"Query Protocol Query Protocol defines a set of APIs in GraphQL grammar to provide data query and interactive capabilities with SkyWalking native visualization tool or 3rd party system, including Web UI, CLI or private system.\nQuery protocol official repository, https://github.com/apache/skywalking-query-protocol.\nMetadata Metadata includes the brief info of the whole under monitoring services and their instances, endpoints, etc. Use multiple ways to query this meta data.\nextend type Query { getGlobalBrief(duration: Duration!): ClusterBrief # Normal service related metainfo  getAllServices(duration: Duration!): [Service!]! searchServices(duration: Duration!, keyword: String!): [Service!]! searchService(serviceCode: String!): Service # Fetch all services of Browser type getAllBrowserServices(duration: Duration!): [Service!]! # Service intance query getServiceInstances(duration: Duration!, serviceId: ID!): [ServiceInstance!]! # Endpoint query # Consider there are huge numbers of endpoint, # must use endpoint owner\u0026#39;s service id, keyword and limit filter to do query. searchEndpoint(keyword: String!, serviceId: ID!, limit: Int!): [Endpoint!]! getEndpointInfo(endpointId: ID!): EndpointInfo # Database related meta info. getAllDatabases(duration: Duration!): [Database!]! getTimeInfo: TimeInfo } Topology Show the topology and dependency graph of services or endpoints. Including direct relationship or global map.\nextend type Query { # Query the global topology getGlobalTopology(duration: Duration!): Topology # Query the topology, based on the given service getServiceTopology(serviceId: ID!, duration: Duration!): Topology # Query the instance topology, based on the given clientServiceId and serverServiceId getServiceInstanceTopology(clientServiceId: ID!, serverServiceId: ID!, duration: Duration!): ServiceInstanceTopology # Query the topology, based on the given endpoint getEndpointTopology(endpointId: ID!, duration: Duration!): Topology } Metrics Metrics query targets all the objects defined in OAL script. You could get the metrics data in linear or thermodynamic matrix formats based on the aggregation functions in script.\n3 types of metrics could be query\n Single value. The type of most default metrics is single value, consider this as default. getValues and getLinearIntValues are suitable for this. Multiple value. One metrics defined in OAL include multiple value calculations. Use getMultipleLinearIntValues to get all values. percentile is a typical multiple value func in OAL. Heatmap value. Read Heatmap in WIKI for detail. thermodynamic is the only OAL func. Use getThermodynamic to get the values.  extend type Query { getValues(metric: BatchMetricConditions!, duration: Duration!): IntValues getLinearIntValues(metric: MetricCondition!, duration: Duration!): IntValues # Query the type of metrics including multiple values, and format them as multiple linears. # The seq of these multiple lines base on the calculation func in OAL # Such as, should us this to query the result of func percentile(50,75,90,95,99) in OAL, # then five lines will be responsed, p50 is the first element of return value. getMultipleLinearIntValues(metric: MetricCondition!, numOfLinear: Int!, duration: Duration!): [IntValues!]! getThermodynamic(metric: MetricCondition!, duration: Duration!): Thermodynamic } Metrics are defined in the config/oal/*.oal files.\nAggregation Aggregation query means the metrics data need a secondary aggregation in query stage, which makes the query interfaces have some different arguments. Such as, TopN list of services is a very typical aggregation query, metrics stream aggregation just calculates the metrics values of each service, but the expected list needs ordering metrics data by the values.\nAggregation query is for single value metrics only.\n# The aggregation query is different with the metric query. # All aggregation queries require backend or/and storage do aggregation in query time. extend type Query { # TopN is an aggregation query. getServiceTopN(name: String!, topN: Int!, duration: Duration!, order: Order!): [TopNEntity!]! getAllServiceInstanceTopN(name: String!, topN: Int!, duration: Duration!, order: Order!): [TopNEntity!]! getServiceInstanceTopN(serviceId: ID!, name: String!, topN: Int!, duration: Duration!, order: Order!): [TopNEntity!]! getAllEndpointTopN(name: String!, topN: Int!, duration: Duration!, order: Order!): [TopNEntity!]! getEndpointTopN(serviceId: ID!, name: String!, topN: Int!, duration: Duration!, order: Order!): [TopNEntity!]! } Others The following query(s) are for specific features, including trace, alarm or profile.\n Trace. Query distributed traces by this. Alarm. Through alarm query, you can have alarm trend and details.  The actual query GraphQL scrips could be found inside query-protocol folder in here.\nCondition Duration Duration is a widely used parameter type as the APM data is time related. The explanations are as following. Step is related the precision.\n# The Duration defines the start and end time for each query operation. # Fields: `start` and `end` # represents the time span. And each of them matches the step. # ref https://www.ietf.org/rfc/rfc3339.txt # The time formats are # `SECOND` step: yyyy-MM-dd HHmmss # `MINUTE` step: yyyy-MM-dd HHmm # `HOUR` step: yyyy-MM-dd HH # `DAY` step: yyyy-MM-dd # `MONTH` step: yyyy-MM # Field: `step` # represents the accurate time point. # e.g. # if step==HOUR , start=2017-11-08 09, end=2017-11-08 19 # then # metrics from the following time points expected # 2017-11-08 9:00 -\u0026gt; 2017-11-08 19:00 # there are 11 time points (hours) in the time span. input Duration { start: String! end: String! step: Step! } enum Step { MONTH DAY HOUR MINUTE SECOND } ","excerpt":"Query Protocol Query Protocol defines a set of APIs in GraphQL grammar to provide data query and …","ref":"/docs/main/v8.2.0/en/protocols/query-protocol/","title":"Query Protocol"},{"body":"Scopes and Fields By using Aggregation Function, the requests will group by time and Group Key(s) in each scope.\nSCOPE All    Name Remarks Group Key Type     name Represent the service name of each request.  string   serviceInstanceName Represent the name of the service instance id referred.  string   endpoint Represent the endpoint path of each request.  string   latency Represent how much time of each request.  int(in ms)   status Represent whether success or fail of the request.  bool(true for success)   responseCode Represent the response code of HTTP response, if this request is the HTTP call. e.g. 200, 404, 302  int   type Represent the type of each request. Such as: Database, HTTP, RPC, gRPC.  enum    SCOPE Service Calculate the metrics data from each request of the service.\n   Name Remarks Group Key Type     name Represent the name of the service  string   nodeType Represent which kind of node of Service or Network address represents to, Such as: Normal, Database, MQ, Cache.  enum   serviceInstanceName Represent the name of the service instance id referred  string   endpointName Represent the name of the endpoint, such a full path of HTTP URI  string   latency Represent how much time of each request.  int   status Represent whether success or fail of the request.  bool(true for success)   responseCode Represent the response code of HTTP response, if this request is the HTTP call  int   type Represent the type of each request. Such as: Database, HTTP, RPC, gRPC.  enum    SCOPE ServiceInstance Calculate the metrics data from each request of the service instance.\n   Name Remarks Group Key Type     name Represent the name of the service instance. Such as ip:port@Service Name. Notice: current native agent uses uuid@ipv4 as instance name, which is useless when you want to setup a filter in aggregation.  string   serviceName Represent the name of the service.  string   nodeType Represent which kind of node of Service or Network address represents to, Such as: Normal, Database, MQ, Cache.  enum   endpointName Represent the name of the endpoint, such a full path of HTTP URI.  string   latency Represent how much time of each request.  int   status Represent whether success or fail of the request.  bool(true for success)   responseCode Represent the response code of HTTP response, if this request is the HTTP call.  int   type Represent the type of each request. Such as: Database, HTTP, RPC, gRPC.  enum    Secondary scopes of ServiceInstance Calculate the metrics data if the service instance is a JVM and collected by javaagent.\n SCOPE ServiceInstanceJVMCPU     Name Remarks Group Key Type     name Represent the name of the service instance. Such as ip:port@Service Name. Notice: current native agent uses uuid@ipv4 as instance name, which is useless when you want to setup a filter in aggregation.  string   serviceName Represent the name of the service.  string   usePercent Represent how much percent of cpu time cost  double    SCOPE ServiceInstanceJVMMemory     Name Remarks Group Key Type     name Represent the name of the service instance. Such as ip:port@Service Name. Notice: current native agent uses uuid@ipv4 as instance name, which is useless when you want to setup a filter in aggregation.  string   serviceName Represent the name of the service.  string   heapStatus Represent this value the memory metrics values are heap or not  bool   init See JVM document  long   max See JVM document  long   used See JVM document  long   committed See JVM document  long    SCOPE ServiceInstanceJVMMemoryPool     Name Remarks Group Key Type     name Represent the name of the service instance. Such as ip:port@Service Name. Notice: current native agent uses uuid@ipv4 as instance name, which is useless when you want to setup a filter in aggregation.  string   serviceName Represent the name of the service.  string   poolType Include CODE_CACHE_USAGE, NEWGEN_USAGE, OLDGEN_USAGE, SURVIVOR_USAGE, PERMGEN_USAGE, METASPACE_USAGE based on different version of JVM.  enum   init See JVM document  long   max See JVM document  long   used See JVM document  long   committed See JVM document  long    SCOPE ServiceInstanceJVMGC     Name Remarks Group Key Type     name Represent the name of the service instance. Such as ip:port@Service Name. Notice: current native agent uses uuid@ipv4 as instance name, which is useless when you want to setup a filter in aggregation.  string   serviceName Represent the name of the service.  string   phrase Include NEW and OLD  Enum   time GC time cost  long   count Count of GC op  long    SCOPE ServiceInstanceJVMThread     Name Remarks Group Key Type     name Represent the name of the service instance. Such as ip:port@Service Name. Notice: current native agent uses uuid@ipv4 as instance name, which is useless when you want to setup a filter in aggregation.  string   serviceName Represent the name of the service.  string   liveCount Represent Current number of live threads  int   daemonCount Represent Current number of daemon threads  int   peakCount Represent Current number of peak threads  int    SCOPE Endpoint Calculate the metrics data from each request of the endpoint in the service.\n   Name Remarks Group Key Type     name Represent the name of the endpoint, such a full path of HTTP URI.  string   serviceName Represent the name of the service.  string   serviceNodeType Represent which kind of node of Service or Network address represents to, Such as: Normal, Database, MQ, Cache.  enum   serviceInstanceName Represent the name of the service instance id referred.  string   latency Represent how much time of each request.  int   status Represent whether success or fail of the request.  bool(true for success)   responseCode Represent the response code of HTTP response, if this request is the HTTP call.  int   type Represent the type of each request. Such as: Database, HTTP, RPC, gRPC.  enum    SCOPE ServiceRelation Calculate the metrics data from each request between one service and the other service\n   Name Remarks Group Key Type     sourceServiceName Represent the name of the source service.  string   sourceServiceNodeType Represent which kind of node of Service or Network address represents to, Such as: Normal, Database, MQ, Cache.  enum   sourceServiceInstanceName Represent the name of the source service instance.  string   destServiceName Represent the name of the destination service.  string   destServiceNodeType Represent which kind of node of Service or Network address represents to.  enum   destServiceInstanceName Represent the name of the destination service instance.  string   endpoint Represent the endpoint used in this call.  string   componentId Represent the id of component used in this call. yes string   latency Represent how much time of each request.  int   status Represent whether success or fail of the request.  bool(true for success)   responseCode Represent the response code of HTTP response, if this request is the HTTP call.  int   type Represent the type of each request. Such as: Database, HTTP, RPC, gRPC.  enum   detectPoint Represent where is the relation detected. Values: client, server, proxy. yes enum   tlsMode Represent TLS mode between source and destination services. For example service_relation_mtls_cpm = from(ServiceRelation.*).filter(tlsMode == \u0026quot;mTLS\u0026quot;).cpm()  string    SCOPE ServiceInstanceRelation Calculate the metrics data from each request between one service instance and the other service instance\n   Name Remarks Group Key Type     sourceServiceName Represent the name of the source service.  string   sourceServiceNodeType Represent which kind of node of Service or Network address represents to, Such as: Normal, Database, MQ, Cache.  enum   sourceServiceInstanceName Represent the name of the source service instance.  string   destServiceName Represent the name of the destination service.     destServiceNodeType Represent which kind of node of Service or Network address represents to, Such as: Normal, Database, MQ, Cache.  string   destServiceInstanceName Represent the name of the destination service instance.  string   endpoint Represent the endpoint used in this call.  string   componentId Represent the id of component used in this call. yes string   latency Represent how much time of each request.  int   status Represent whether success or fail of the request.  bool(true for success)   responseCode Represent the response code of HTTP response, if this request is the HTTP call.  int   type Represent the type of each request. Such as: Database, HTTP, RPC, gRPC.  enum   detectPoint Represent where is the relation detected. Values: client, server, proxy. yes enum   tlsMode Represent TLS mode between source and destination service instances. For example, service_instance_relation_mtls_cpm = from(ServiceInstanceRelation.*).filter(tlsMode == \u0026quot;mTLS\u0026quot;).cpm()  string    SCOPE EndpointRelation Calculate the metrics data of the dependency between one endpoint and the other endpoint. This relation is hard to detect, also depends on tracing lib to propagate the prev endpoint. So EndpointRelation scope aggregation effects only in service under tracing by SkyWalking native agents, including auto instrument agents(like Java, .NET), OpenCensus SkyWalking exporter implementation or others propagate tracing context in SkyWalking spec.\n   Name Remarks Group Key Type     endpoint Represent the endpoint as parent in the dependency.  string   serviceName Represent the name of the service.  string   serviceNodeType Represent which kind of node of Service or Network address represents to, Such as: Normal, Database, MQ, Cache.  enum   childEndpoint Represent the endpoint being used by the parent endpoint in row(1)  string   childServiceName Represent the endpoint being used by the parent service in row(1)  string   childServiceNodeType Represent which kind of node of Service or Network address represents to, Such as: Normal, Database, MQ, Cache.  string   childServiceInstanceName Represent the endpoint being used by the parent service instance in row(1)  string   rpcLatency Represent the latency of the RPC from some codes in the endpoint to the childEndpoint. Exclude the latency caused by the endpoint(1) itself.     componentId Represent the id of component used in this call. yes string   status Represent whether success or fail of the request.  bool(true for success)   responseCode Represent the response code of HTTP response, if this request is the HTTP call.  int   type Represent the type of each request. Such as: Database, HTTP, RPC, gRPC.  enum   detectPoint Represent where is the relation detected. Values: client, server, proxy. yes enum    SCOPE BrowserAppTraffic Calculate the metrics data form each request of the browser app (only browser).\n   Name Remarks Group Key Type     name Represent the browser app name of each request.  string   count Represents the number of request, fixed at 1.  int   trafficCategory Represents traffic category, Values: NORMAL, FIRST_ERROR, ERROR  enum   errorCategory Represents error category, Values: AJAX, RESOURCE, VUE, PROMISE, UNKNOWN  enum    SCOPE BrowserAppSingleVersionTraffic Calculate the metrics data form each request of the browser single version in the browser app (only browser).\n   Name Remarks Group Key Type     name Represent the single version name of each request.  string   serviceName Represent the name of the browser app.  string   count Represents the number of request, fixed at 1.  int   trafficCategory Represents traffic category, Values: NORMAL, FIRST_ERROR, ERROR  enum   errorCategory Represents error category, Values: AJAX, RESOURCE, VUE, PROMISE, UNKNOWN  enum    SCOPE BrowserAppPageTraffic Calculate the metrics data form each request of the page in the browser app (only browser).\n   Name Remarks Group Key Type     name Represent the page name of each request.  string   serviceName Represent the name of the browser app.  string   count Represents the number of request, fixed at 1.  int   trafficCategory Represents the traffic category, Values: NORMAL, FIRST_ERROR, ERROR  enum   errorCategory Represents the error category, Values: AJAX, RESOURCE, VUE, PROMISE, UNKNOWN  enum    SCOPE BrowserAppPagePerf Calculate the metrics data form each request of the page in the browser app (only browser).\n   Name Remarks Group Key Type     name Represent the page name of each request.  string   serviceName Represent the name of the browser app.  string   redirectTime Represents the time of redirection.  int(in ms)   dnsTime Represents the DNS query time.  int(in ms)   ttfbTime Time to first Byte.  int(in ms)   tcpTime TCP connection time.  int(in ms)   transTime Content transfer time.  int(in ms)   domAnalysisTime Dom parsing time.  int(in ms)   fptTime First paint time or blank screen time.  int(in ms)   domReadyTime Dom ready time.  int(in ms)   loadPageTime Page full load time.  int(in ms)   resTime Synchronous load resources in the page.  int(in ms)   sslTime Only valid for HTTPS.  int(in ms)   ttlTime Time to interact.  int(in ms)   firstPackTime First pack time.  int(in ms)   fmpTime First Meaningful Paint.  int(in ms)    ","excerpt":"Scopes and Fields By using Aggregation Function, the requests will group by time and Group Key(s) in …","ref":"/docs/main/v8.2.0/en/concepts-and-designs/scope-definitions/","title":"Scopes and Fields"},{"body":"Sending Envoy Metrics to SkyWalking OAP Server Example This is an example of sending Envoy Stats to SkyWalking OAP server through Metric Service.\nRunning the example The example requires docker and docker-compose to be installed in your local. It fetches images from Docker Hub.\nNote that in ths setup, we override the log4j2.xml config to set the org.apache.skywalking.oap.server.receiver.envoy logger level to DEBUG. This enables us to see the messages sent by Envoy to SkyWalking OAP server.\n$ make up $ docker-compose logs -f skywalking $ # Please wait for a moment until SkyWalking is ready and Envoy starts sending the stats. You will see similar messages like the following: skywalking_1 | 2019-08-31 23:57:40,672 - org.apache.skywalking.oap.server.receiver.envoy.MetricServiceGRPCHandler -26870 [grpc-default-executor-0] DEBUG [] - Received msg identifier { skywalking_1 | node { skywalking_1 | id: \u0026quot;ingress\u0026quot; skywalking_1 | cluster: \u0026quot;envoy-proxy\u0026quot; skywalking_1 | metadata { skywalking_1 | fields { skywalking_1 | key: \u0026quot;skywalking\u0026quot; skywalking_1 | value { skywalking_1 | string_value: \u0026quot;iscool\u0026quot; skywalking_1 | } skywalking_1 | } skywalking_1 | fields { skywalking_1 | key: \u0026quot;envoy\u0026quot; skywalking_1 | value { skywalking_1 | string_value: \u0026quot;isawesome\u0026quot; skywalking_1 | } skywalking_1 | } skywalking_1 | } skywalking_1 | locality { skywalking_1 | region: \u0026quot;ap-southeast-1\u0026quot; skywalking_1 | zone: \u0026quot;zone1\u0026quot; skywalking_1 | sub_zone: \u0026quot;subzone1\u0026quot; skywalking_1 | } skywalking_1 | build_version: \u0026quot;e349fb6139e4b7a59a9a359be0ea45dd61e589c5/1.11.1/Clean/RELEASE/BoringSSL\u0026quot; skywalking_1 | } skywalking_1 | } skywalking_1 | envoy_metrics { skywalking_1 | name: \u0026quot;cluster.service_skywalking.update_success\u0026quot; skywalking_1 | type: COUNTER skywalking_1 | metric { skywalking_1 | counter { skywalking_1 | value: 2.0 skywalking_1 | } skywalking_1 | timestamp_ms: 1567295859556 skywalking_1 | } skywalking_1 | } ... $ # To tear down: $ make down ","excerpt":"Sending Envoy Metrics to SkyWalking OAP Server Example This is an example of sending Envoy Stats to …","ref":"/docs/main/v8.2.0/en/setup/envoy/examples/metrics/readme/","title":"Sending Envoy Metrics to SkyWalking OAP Server Example"},{"body":"Service Auto Instrument Agent Service auto instrument agent is a subset of Language based native agents. In this kind of agent, it is based on some language specific features, usually a VM based languages.\nWhat does Auto Instrument mean? Many users know these agents from hearing They say don't need to change any single line of codes, SkyWalking used to put these words in our readme page too. But actually, it is right and wrong. For end user, YES, they don\u0026rsquo;t need to change codes, at least for most cases. But also NO, the codes are still changed by agent, usually called manipulate codes at runtime. Underlying, it is just auto instrument agent including codes about how to change codes through VM interface, such as change class in Java through javaagent premain.\nAlso, we said that the most auto instrument agents are VM based, but actually, you can build a tool at compiling time, rather than runtime.\nWhat are limits? Auto instrument is so cool, also you can create those in compiling time, that you don\u0026rsquo;t depend on VM features, then is there any limit?\nThe answer definitely YES. And they are:\n  In process propagation possible in most cases. In many high level languages, they are used to build business system, such as Java and .NET. Most codes of business logic are running in the same thread for per request, which make the propagation could be based on thread Id, and stack module to make sure the context is safe.\n  Just effect frameworks or libraries. Because of the changing codes by agents, it also means the codes are already known by agent plugin developers. So, there is always a supported list in this kind of probes. Like SkyWalking Java agent supported list.\n  Across thread can\u0026rsquo;t be supported all the time. Like we said about in process propagation, most codes run in a single thread per request, especially business codes. But in some other scenarios, they do things in different threads, such as job assignment, task pool or batch process. Or some languages provide coroutine or similar thing like Goroutine, then developer could run async process with low payload, even been encouraged. In those cases, auto instrument will face problems.\n  So, no mystery for auto instrument, in short words, agent developers write an activation to make instrument codes work you. That is all.\nWhat is next? If you want to learn about manual instrument libs in SkyWalking, see Manual instrument SDK section.\n","excerpt":"Service Auto Instrument Agent Service auto instrument agent is a subset of Language based native …","ref":"/docs/main/v8.2.0/en/concepts-and-designs/service-agent/","title":"Service Auto Instrument Agent"},{"body":"Service Mesh Probe Service Mesh probes use the extendable mechanism provided in Service Mesh implementor, like Istio.\nWhat is Service Mesh? The following explanation came from Istio documents.\n The term service mesh is often used to describe the network of microservices that make up such applications and the interactions between them. As a service mesh grows in size and complexity, it can become harder to understand and manage. Its requirements can include discovery, load balancing, failure recovery, metrics, and monitoring, and often more complex operational requirements such as A/B testing, canary releases, rate limiting, access control, and end-to-end authentication.\n Where does the probe collect data from? Istio is a very typical Service Mesh design and implementor. It defines Control Panel and Data Panel, which are wide used. Here is Istio Architecture:\nService Mesh probe can choose to collect data from Control Panel or Data Panel. In Istio, it means collecting telemetry data from Mixer(Control Panel) or Envoy sidecar(Data Panel). Underlying they are same data, the probe collects two telemetry entities from client side and server side per request.\nHow does Service Mesh make backend work? From the probe, you can see there must have no trace related in this kind of probe, so why SkyWalking platform still works?\nService Mesh probes collects telemetry data from each request, so it knows the source, destination, endpoint, latency and status. By those, backend can tell the whole topology map by combining these call as lines, and also the metrics of each nodes through their incoming request. Backend asked for the same metrics data from parsing tracing data. So, the right expression is: Service Mesh metrics are exact the metrics, what the traces parsers generate. They are same.\nWhat is Next?  If you want to use the service mesh probe, read set SkyWalking on Service Mesh document.  ","excerpt":"Service Mesh Probe Service Mesh probes use the extendable mechanism provided in Service Mesh …","ref":"/docs/main/v8.2.0/en/concepts-and-designs/service-mesh-probe/","title":"Service Mesh Probe"},{"body":"Setting Override SkyWalking backend supports setting overrides by system properties and system environment variables. You could override the settings in application.yml\nSystem properties key rule ModuleName.ProviderName.SettingKey.\n  Example\nOverride restHost in this setting segment\n  core: default: restHost: ${SW_CORE_REST_HOST:0.0.0.0} restPort: ${SW_CORE_REST_PORT:12800} restContextPath: ${SW_CORE_REST_CONTEXT_PATH:/} gRPCHost: ${SW_CORE_GRPC_HOST:0.0.0.0} gRPCPort: ${SW_CORE_GRPC_PORT:11800} Use command arg\n-Dcore.default.restHost=172.0.4.12 System environment variables   Example\nOverride restHost in this setting segment through environment variables\n  core: default: restHost: ${REST_HOST:0.0.0.0} restPort: ${SW_CORE_REST_PORT:12800} restContextPath: ${SW_CORE_REST_CONTEXT_PATH:/} gRPCHost: ${SW_CORE_GRPC_HOST:0.0.0.0} gRPCPort: ${SW_CORE_GRPC_PORT:11800} If the REST_HOST  environment variable exists in your operating system and its value is 172.0.4.12, then the value of restHost here will be overwritten to 172.0.4.12, otherwise, it will be set to 0.0.0.0.\nBy the way, Placeholder nesting is also supported, like ${REST_HOST:${ANOTHER_REST_HOST:127.0.0.1}}. In this case, if the REST_HOST  environment variable not exists, but the REST_ANOTHER_REST_HOSTHOST environment variable exists and its value is 172.0.4.12, then the value of restHost here will be overwritten to 172.0.4.12, otherwise, it will be set to 127.0.0.1.\n","excerpt":"Setting Override SkyWalking backend supports setting overrides by system properties and system …","ref":"/docs/main/v8.2.0/en/setup/backend/backend-setting-override/","title":"Setting Override"},{"body":"Setting Override In default, SkyWalking provide agent.config for agent.\nSetting override means end user can override the settings in these config file, through using system properties or agent options.\nSystem properties Use skywalking. + key in config file as system properties key, to override the value.\n  Why need this prefix?\nThe agent system properties and env share with target application, this prefix can avoid variable conflict.\n  Example\nOverride agent.application_code by this.\n  -Dskywalking.agent.application_code=31200 Agent options Add the properties after the agent path in JVM arguments.\n-javaagent:/path/to/skywalking-agent.jar=[option1]=[value1],[option2]=[value2]   Example\nOverride agent.application_code and logging.level by this.\n  -javaagent:/path/to/skywalking-agent.jar=agent.application_code=31200,logging.level=debug   Special characters\nIf a separator(, or =) in the option or value, it should be wrapped in quotes.\n  -javaagent:/path/to/skywalking-agent.jar=agent.ignore_suffix='.jpg,.jpeg' System environment variables   Example\nOverride agent.application_code and logging.level by this.\n  # The service name in UI agent.service_name=${SW_AGENT_NAME:Your_ApplicationName} # Logging level logging.level=${SW_LOGGING_LEVEL:INFO} If the SW_AGENT_NAME  environment variable exists in your operating system and its value is skywalking-agent-demo, then the value of agent.service_name here will be overwritten to skywalking-agent-demo, otherwise, it will be set to Your_ApplicationName.\nBy the way, Placeholder nesting is also supported, like ${SW_AGENT_NAME:${ANOTHER_AGENT_NAME:Your_ApplicationName}}. In this case, if the SW_AGENT_NAME  environment variable not exists, but the ANOTHER_AGENT_NAME environment variable exists and its value is skywalking-agent-demo, then the value of agent.service_name here will be overwritten to skywalking-agent-demo,otherwise, it will be set to Your_ApplicationName.\nOverride priority Agent Options \u0026gt; System.Properties(-D) \u0026gt; System environment variables \u0026gt; Config file\n","excerpt":"Setting Override In default, SkyWalking provide agent.config for agent.\nSetting override means end …","ref":"/docs/main/v8.2.0/en/setup/service-agent/java-agent/setting-override/","title":"Setting Override"},{"body":"Setup The document explains how to install Skywalking based on the kind of probes you are going to use. If you don\u0026rsquo;t understand, please read Concepts and Designs first.\nImportant: Don\u0026rsquo;t forget to configure the timezone on your UI, and you also need to be sure your OAP backend servers are also using the same timezone.\nIf you have any issues, please check that your issue is not already described in the FAQ.\nDownload official releases  Backend, UI and Java agent are Apache official release, you can find them on the Apache SkyWalking download page.  Language agents in Service   Java agent. Introduces how to install java agent to your service, without any impact in your code.\n  LUA agent. Introduce how to install the lua agent in Nginx + LUA module or OpenResty.\n  Python Agent. Introduce how to install the Python Agent in a Python service.\n  Node.js agent. Introduce how to install the NodeJS Agent in a NodeJS service.\n  The following agents and SDKs are compatible with the SkyWalking\u0026rsquo;s data formats and network protocols, but are maintained by 3rd-parties. You can go to their project repositories for additional info about guides and releases.\n  SkyAPM .NET Core agent. See .NET Core agent project document for more details.\n  SkyAPM Node.js agent. See Node.js server side agent project document for more details.\n  SkyAPM PHP SDK. See PHP agent project document for more details.\n  SkyAPM GO2Sky. See GO2Sky project document for more details.\n  Browser Monitoring Apache SkyWalking Client JS. Support collecting metrics and error logs for the Browser or JavaScript based mobile app.\nNote, make sure the receiver-browser has been opened, default is ON since 8.2.0.\nService Mesh  Istio  SkyWalking on Istio. Introduces how to use Istio Mixer bypass adapter to work with SkyWalking.   Envoy  Use ALS (access log service) to observe service mesh, without Mixer. Follow document for guides.    Proxy  Envoy Proxy  Sending metrics to Skywalking from Envoy. How to send metrics from Envoy to SkyWalking using Metrics service.    Setup backend Follow backend and UI setup document to understand how the backend and UI configuration works. Different scenarios and advanced features are also explained.\nChanges log Backend, UI and Java agent changes are available here.\nUpgrade FAQ 6.x version upgrade FAQ introduces the recommendation ways to do SkyWalking upgrade.\n","excerpt":"Setup The document explains how to install Skywalking based on the kind of probes you are going to …","ref":"/docs/main/v8.2.0/en/setup/readme/","title":"Setup"},{"body":"Setup java agent  Agent is available for JDK 8 - 14 in 7.x releases. JDK 1.6 - JDK 12 are supported in all 6.x releases NOTICE¹ Find agent folder in SkyWalking release package Set agent.service_name in config/agent.config. Could be any String in English. Set collector.backend_service in config/agent.config. Default point to 127.0.0.1:11800, only works for local backend. Add -javaagent:/path/to/skywalking-package/agent/skywalking-agent.jar to JVM argument. And make sure to add it before the -jar argument.  The agent release dist is included in Apache official release. New agent package looks like this.\n+-- agent +-- activations apm-toolkit-log4j-1.x-activation.jar apm-toolkit-log4j-2.x-activation.jar apm-toolkit-logback-1.x-activation.jar ... +-- config agent.config +-- plugins apm-dubbo-plugin.jar apm-feign-default-http-9.x.jar apm-httpClient-4.x-plugin.jar ..... +-- optional-plugins apm-gson-2.x-plugin.jar ..... +-- bootstrap-plugins jdk-http-plugin.jar ..... +-- logs skywalking-agent.jar  Start your application.  Supported middleware, framework and library SkyWalking agent has supported various middlewares, frameworks and libraries. Read supported list to get them and supported version. If the plugin is in Optional² catalog, go to optional plugins section to learn how to active it.\nAdvanced features  All plugins are in /plugins folder. The plugin jar is active when it is in there. Remove the plugin jar, it disabled. The default logging output folder is /logs.  Install javaagent FAQs  Linux Tomcat 7, Tomcat 8, Tomcat 9\nChange the first line of tomcat/bin/catalina.sh.  CATALINA_OPTS=\u0026#34;$CATALINA_OPTS-javaagent:/path/to/skywalking-agent/skywalking-agent.jar\u0026#34;; export CATALINA_OPTS  Windows Tomcat 7, Tomcat 8, Tomcat 9\nChange the first line of tomcat/bin/catalina.bat.  set \u0026#34;CATALINA_OPTS=-javaagent:/path/to/skywalking-agent/skywalking-agent.jar\u0026#34;  JAR file\nAdd -javaagent argument to command line in which you start your app. eg:  java -javaagent:/path/to/skywalking-agent/skywalking-agent.jar -jar yourApp.jar  Jetty\nModify jetty.sh, add -javaagent argument to command line in which you start your app. eg:  export JAVA_OPTIONS=\u0026#34;${JAVA_OPTIONS}-javaagent:/path/to/skywalking-agent/skywalking-agent.jar\u0026#34; Table of Agent Configuration Properties This is the properties list supported in agent/config/agent.config.\n   property key Description Default     agent.namespace Namespace isolates headers in cross process propagation. The HEADER name will be HeaderName:Namespace. Not set   agent.service_name The service name to represent a logic group providing the same capabilities/logic. Suggestion: set a unique name for every logic service group, service instance nodes share the same code, Max length is 50(UTF-8 char) Your_ApplicationName   agent.sample_n_per_3_secs Negative or zero means off, by default.SAMPLE_N_PER_3_SECS means sampling N TraceSegment in 3 seconds tops. Not set   agent.authentication Authentication active is based on backend setting, see application.yml for more details.For most scenarios, this needs backend extensions, only basic match auth provided in default implementation. Not set   agent.span_limit_per_segment The max number of spans in a single segment. Through this config item, SkyWalking keep your application memory cost estimated. 300   agent.ignore_suffix If the operation name of the first span is included in this set, this segment should be ignored. Not set   agent.is_open_debugging_class If true, skywalking agent will save all instrumented classes files in /debugging folder. SkyWalking team may ask for these files in order to resolve compatible problem. Not set   agent.is_cache_enhanced_class If true, SkyWalking agent will cache all instrumented classes files to memory or disk files (decided by class cache mode), allow another java agent to enhance those classes that enhanced by SkyWalking agent. To use some Java diagnostic tools (such as BTrace, Arthas) to diagnose applications or add a custom java agent to enhance classes, you need to enable this feature. Read this FAQ for more details false   agent.class_cache_mode The instrumented classes cache mode: MEMORY or FILE. MEMORY: cache class bytes to memory, if instrumented classes is too many or too large, it may take up more memory. FILE: cache class bytes in /class-cache folder, automatically clean up cached class files when the application exits. MEMORY   agent.instance_name Instance name is the identity of an instance, should be unique in the service. If empty, SkyWalking agent will generate an 32-bit uuid. Default, use UUID@hostname as the instance name. Max length is 50(UTF-8 char) \u0026quot;\u0026quot;   agent.instance_properties[key]=value Add service instance custom properties. Not set   agent.cause_exception_depth How depth the agent goes, when log all cause exceptions. 5   agent.force_reconnection_period  Force reconnection period of grpc, based on grpc_channel_check_interval. 1   agent.operation_name_threshold  The operationName max length, setting this value \u0026gt; 190 is not recommended. 150   agent.keep_tracing Keep tracing even the backend is not available if this value is true. false   osinfo.ipv4_list_size Limit the length of the ipv4 list size. 10   collector.grpc_channel_check_interval grpc channel status check interval. 30   collector.heartbeat_period agent heartbeat report period. Unit, second. 30   collector.properties_report_period_factor The agent sends the instance properties to the backend every collector.heartbeat_period * collector.properties_report_period_factor seconds 10   collector.backend_service Collector SkyWalking trace receiver service addresses. 127.0.0.1:11800   collector.grpc_upstream_timeout How long grpc client will timeout in sending data to upstream. Unit is second. 30 seconds   collector.get_profile_task_interval Sniffer get profile task list interval. 20   logging.level Log level: TRACE, DEBUG, INFO, WARN, ERROR, OFF. Default is info. INFO   logging.file_name Log file name. skywalking-api.log   logging.output Log output. Default is FILE. Use CONSOLE means output to stdout. FILE   logging.dir Log files directory. Default is blank string, means, use \u0026ldquo;{theSkywalkingAgentJarDir}/logs \u0026quot; to output logs. {theSkywalkingAgentJarDir} is the directory where the skywalking agent jar file is located \u0026quot;\u0026quot;   logging.resolver Logger resolver: PATTERN or JSON. The default is PATTERN, which uses logging.pattern to print traditional text logs. JSON resolver prints logs in JSON format. PATTERN   logging.pattern  Logging format. There are all conversion specifiers: * %level means log level. * %timestamp means now of time with format yyyy-MM-dd HH:mm:ss:SSS.\n* %thread means name of current thread.\n* %msg means some message which user logged. * %class means SimpleName of TargetClass. * %throwable means a throwable which user called. * %agent_name means agent.service_name. Only apply to the PatternLogger. %level %timestamp %thread %class : %msg %throwable   logging.max_file_size The max size of log file. If the size is bigger than this, archive the current file, and write into a new file. 300 * 1024 * 1024   logging.max_history_files The max history log files. When rollover happened, if log files exceed this number,then the oldest file will be delete. Negative or zero means off, by default. -1   statuscheck.ignored_exceptions Listed exceptions would not be treated as an error. Because in some codes, the exception is being used as a way of controlling business flow. \u0026quot;\u0026quot;   statuscheck.max_recursive_depth The max recursive depth when checking the exception traced by the agent. Typically, we don\u0026rsquo;t recommend setting this more than 10, which could cause a performance issue. Negative value and 0 would be ignored, which means all exceptions would make the span tagged in error status. 1   jvm.buffer_size The buffer size of collected JVM info. 60 * 10   buffer.channel_size The buffer channel size. 5   buffer.buffer_size The buffer size. 300   profile.active If true, skywalking agent will enable profile when user create a new profile task. Otherwise disable profile. true   profile.max_parallel Parallel monitor segment count 5   profile.duration Max monitor segment time(minutes), if current segment monitor time out of limit, then stop it. 10   profile.dump_max_stack_depth Max dump thread stack depth 500   profile.snapshot_transport_buffer_size Snapshot transport to backend buffer size 50   meter.active If true, the agent collects and reports metrics to the backend. true   meter.report_interval Report meters interval. The unit is second 20   meter.max_meter_size Max size of the meter pool 500   plugin.mount Mount the specific folders of the plugins. Plugins in mounted folders would work. plugins,activations   plugin.peer_max_length  Peer maximum description limit. 200   plugin.exclude_plugins  Exclude some plugins define in plugins dir.Plugin names is defined in Agent plugin list \u0026quot;\u0026quot;   plugin.mongodb.trace_param If true, trace all the parameters in MongoDB access, default is false. Only trace the operation, not include parameters. false   plugin.mongodb.filter_length_limit If set to positive number, the WriteRequest.params would be truncated to this length, otherwise it would be completely saved, which may cause performance problem. 256   plugin.elasticsearch.trace_dsl If true, trace all the DSL(Domain Specific Language) in ElasticSearch access, default is false. false   plugin.springmvc.use_qualified_name_as_endpoint_name If true, the fully qualified method name will be used as the endpoint name instead of the request URL, default is false. false   plugin.toolit.use_qualified_name_as_operation_name If true, the fully qualified method name will be used as the operation name instead of the given operation name, default is false. false   plugin.jdbc.trace_sql_parameters If set to true, the parameters of the sql (typically java.sql.PreparedStatement) would be collected. false   plugin.jdbc.sql_parameters_max_length If set to positive number, the db.sql.parameters would be truncated to this length, otherwise it would be completely saved, which may cause performance problem. 512   plugin.jdbc.sql_body_max_length If set to positive number, the db.statement would be truncated to this length, otherwise it would be completely saved, which may cause performance problem. 2048   plugin.solrj.trace_statement If true, trace all the query parameters(include deleteByIds and deleteByQuery) in Solr query request, default is false. false   plugin.solrj.trace_ops_params If true, trace all the operation parameters in Solr request, default is false. false   plugin.light4j.trace_handler_chain If true, trace all middleware/business handlers that are part of the Light4J handler chain for a request. false   plugin.opgroup.* Support operation name customize group rules in different plugins. Read Group rule supported plugins Not set   plugin.springtransaction.simplify_transaction_definition_name If true, the transaction definition name will be simplified. false   plugin.jdkthreading.threading_class_prefixes Threading classes (java.lang.Runnable and java.util.concurrent.Callable) and their subclasses, including anonymous inner classes whose name match any one of the THREADING_CLASS_PREFIXES (splitted by ,) will be instrumented, make sure to only specify as narrow prefixes as what you\u0026rsquo;re expecting to instrument, (java. and javax. will be ignored due to safety issues) Not set   plugin.tomcat.collect_http_params This config item controls that whether the Tomcat plugin should collect the parameters of the request. Also, activate implicitly in the profiled trace. false   plugin.springmvc.collect_http_params This config item controls that whether the SpringMVC plugin should collect the parameters of the request, when your Spring application is based on Tomcat, consider only setting either plugin.tomcat.collect_http_params or plugin.springmvc.collect_http_params. Also, activate implicitly in the profiled trace. false   plugin.http.http_params_length_threshold When COLLECT_HTTP_PARAMS is enabled, how many characters to keep and send to the OAP backend, use negative values to keep and send the complete parameters, NB. this config item is added for the sake of performance. 1024   plugin.http.http_headers_length_threshold When include_http_headers declares header names, this threshold controls the length limitation of all header values. use negative values to keep and send the complete headers. Note. this config item is added for the sake of performance. 2048   plugin.http.include_http_headers Set the header names, which should be collected by the plugin. Header name must follow javax.servlet.http definition. Multiple names should be split by comma. ``(No header would be collected) |   plugin.feign.collect_request_body This config item controls that whether the Feign plugin should collect the http body of the request. false   plugin.feign.filter_length_limit When COLLECT_REQUEST_BODY is enabled, how many characters to keep and send to the OAP backend, use negative values to keep and send the complete body. 1024   plugin.feign.supported_content_types_prefix When COLLECT_REQUEST_BODY is enabled and content-type start with SUPPORTED_CONTENT_TYPES_PREFIX, collect the body of the request , multiple paths should be separated by , application/json,text/   plugin.influxdb.trace_influxql If true, trace all the influxql(query and write) in InfluxDB access, default is true. true   correlation.element_max_number Max element count of the correlation context. 3   correlation.value_max_length Max value length of correlation context element. 128   plugin.dubbo.collect_consumer_arguments Apache Dubbo consumer collect arguments in RPC call, use Object#toString to collect arguments. false   plugin.dubbo.consumer_arguments_length_threshold When plugin.dubbo.collect_consumer_arguments is true, Arguments of length from the front will to the OAP backend 256   plugin.dubbo.collect_provider_arguments Apache Dubbo provider collect arguments in RPC call, use Object#toString to collect arguments. false   plugin.dubbo.consumer_provider_length_threshold When plugin.dubbo.provider_consumer_arguments is true, Arguments of length from the front will to the OAP backend 256   plugin.kafka.bootstrap_servers A list of host/port pairs to use for establishing the initial connection to the Kafka cluster. localhost:9092   plugin.kafka.get_topic_timeout Timeout period of reading topics from the Kafka server, the unit is second. 10   plugin.kafka.consumer_config Kafka producer configuration.    plugin.kafka.producer_config Kafka producer configuration. Read producer configure to get more details. Check Kafka report doc for more details and examples.    plugin.kafka.topic_meter Specify which Kafka topic name for Meter System data to report to. skywalking_meters   plugin.kafka.topic_metrics Specify which Kafka topic name for JVM metrics data to report to. skywalking_metrics   plugin.kafka.topic_segment Specify which Kafka topic name for traces data to report to. skywalking_segments   plugin.kafka.topic_profilings Specify which Kafka topic name for Thread Profiling snapshot to report to. skywalking_profilings   plugin.kafka.topic_management Specify which Kafka topic name for the register or heartbeat data of Service Instance to report to. skywalking_managements   plugin.springannotation.classname_match_regex Match spring beans with regular expression for the class name. Multiple expressions could be separated by a comma. This only works when Spring annotation plugin has been activated. All the spring beans tagged with @Bean,@Service,@Dao, or @Repository.    Optional Plugins Java agent plugins are all pluggable. Optional plugins could be provided in optional-plugins folder under agent or 3rd party repositories. For using these plugins, you need to put the target plugin jar file into /plugins.\nNow, we have the following known optional plugins.\n Plugin of tracing Spring annotation beans Plugin of tracing Oracle and Resin Filter traces through specified endpoint name patterns Plugin of Gson serialization lib in optional plugin folder. Plugin of Zookeeper 3.4.x in optional plugin folder. The reason of being optional plugin is, many business irrelevant traces are generated, which cause extra payload to agents and backends. At the same time, those traces may be just heartbeat(s). Customize enhance Trace methods based on description files, rather than write plugin or change source codes. Plugin of Spring Cloud Gateway 2.1.x in optional plugin folder. Please only active this plugin when you install agent in Spring Gateway. spring-cloud-gateway-2.x-plugin and spring-webflux-5.x-plugin are both required. Plugin of Spring Transaction in optional plugin folder. The reason of being optional plugin is, many local span are generated, which also spend more CPU, memory and network. Plugin of Kotlin coroutine provides the tracing across coroutines automatically. As it will add local spans to all across routines scenarios, Please assess the performance impact. Plugin of quartz-scheduler-2.x in the optional plugin folder. The reason for being an optional plugin is, many task scheduling systems are based on quartz-scheduler, this will cause duplicate tracing and link different sub-tasks as they share the same quartz level trigger, such as ElasticJob. Plugin of spring-webflux-5.x in the optional plugin folder. Please only activate this plugin when you use webflux alone as a web container. If you are using SpringMVC 5 or Spring Gateway, you don\u0026rsquo;t need this plugin.  Bootstrap class plugins All bootstrap plugins are optional, due to unexpected risk. Bootstrap plugins are provided in bootstrap-plugins folder. For using these plugins, you need to put the target plugin jar file into /plugins.\nNow, we have the following known bootstrap plugins.\n Plugin of JDK HttpURLConnection. Agent is compatible with JDK 1.6+ Plugin of JDK Callable and Runnable. Agent is compatible with JDK 1.6+  Advanced Features  Set the settings through system properties for config file override. Read setting override. Use gRPC TLS to link backend. See open TLS Monitor a big cluster by different SkyWalking services. Use Namespace to isolate the context propagation. Set client token if backend open token authentication. Application Toolkit, are a collection of libraries, provided by SkyWalking APM. Using them, you have a bridge between your application and SkyWalking APM agent.  If you want your codes to interact with SkyWalking agent, including getting trace id, setting tags, propagating custom data etc.. Try SkyWalking manual APIs. If you require customized metrics, try SkyWalking Meter System Toolkit. If you want to print trace context(e.g. traceId) in your logs, choose the log frameworks, log4j, log4j2, logback If you want to continue traces across thread manually, use across thread solution APIs. If you want to forward MicroMeter/Spring Sleuth metrics to Meter System, use SkyWalking MicroMeter Register. If you want to use OpenTracing Java APIs, try SkyWalking OpenTracing compatible tracer. More details you could find at http://opentracing.io If you want to tolerate some exceptions, read tolerate custom exception doc.   If you want to specify the path of your agent.config file. Read set config file through system properties  Advanced Reporters The advanced report provides an alternative way to submit the agent collected data to the backend. All of them are in the optional-reporter-plugins folder, move the one you needed into the reporter-plugins folder for the activation. Notice, don\u0026rsquo;t try to activate multiple reporters, that could cause unexpected fatal errors.\n Use Kafka to transport the traces, JVM metrics, instance properties, and profiled snapshots to the backend. Read the How to enable Kafka Reporter for more details.  Plugin Development Guide SkyWalking java agent supports plugin to extend the supported list. Please follow our Plugin Development Guide.\nTest If you are interested in plugin compatible tests or agent performance, see the following reports.\n Plugin Test in every Pull Request Java Agent Performance Test  Notice ¹ Due to gRPC didn\u0026rsquo;t support JDK 1.6 since 2018, SkyWalking abandoned the JDK 6/7 supports in all 7.x releases. But, with gRPC back forward compatibility(at least for now), all SkyWalking 6.x agents could work with 7.x, including the agent and backend.\n","excerpt":"Setup java agent  Agent is available for JDK 8 - 14 in 7.x releases. JDK 1.6 - JDK 12 are supported …","ref":"/docs/main/v8.2.0/en/setup/service-agent/java-agent/readme/","title":"Setup java agent"},{"body":"Skywalking Agent List  activemq-5.x armeria-063-084 armeria-085 armeria-086 armeria-098 avro-1.x brpc-java canal-1.x cassandra-java-driver-3.x dubbo ehcache-2.x elastic-job-2.x elastic-job-3.x elasticsearch-5.x elasticsearch-6.x feign-default-http-9.x feign-pathvar-9.x finagle graphql grpc-1.x gson-2.8.x h2-1.x hbase-1.x httpasyncclient-4.x httpclient-3.x httpclient-4.x hystrix-1.x influxdb-2.x jdk-http-plugin jdk-threading-plugin jedis-2.x jetty-client-9.0 jetty-client-9.x jetty-server-9.x kafka-0.11.x/1.x/2.x kotlin-coroutine lettuce-5.x light4j mariadb-2.x memcache-2.x mongodb-2.x mongodb-3.x mongodb-4.x motan-0.x mysql-5.x mysql-6.x mysql-8.x netty-socketio nutz-http-1.x nutz-mvc-annotation-1.x okhttp-3.x play-2.x postgresql-8.x pulsar quasar quartz-scheduler-2.x rabbitmq-5.x redisson-3.x resteasy-server-3.x rocketMQ-3.x rocketMQ-4.x servicecomb-0.x servicecomb-1.x sharding-jdbc-1.5.x sharding-sphere-3.x sharding-sphere-4.0.0 sharding-sphere-4.1.0 sharding-sphere-4.x sharding-sphere-4.x-rc3 sofarpc solrj-7.x spring-annotation spring-async-annotation-5.x spring-cloud-feign-1.x spring-cloud-feign-2.x spring-cloud-gateway-2.0.x spring-cloud-gateway-2.1.x spring-concurrent-util-4.x spring-core-patch spring-kafka-2.x spring-mvc-annotation spring-mvc-annotation-3.x spring-mvc-annotation-4.x spring-mvc-annotation-5.x spring-resttemplate-4.x spring-scheduled-annotation spring-tx spring-webflux-5.x spring-webflux-5.x-webclient spymemcached-2.x struts2-2.x thrift tomcat-7.x/8.x toolkit-counter toolkit-gauge toolkit-histogram toolkit-kafka toolkit-log4j toolkit-log4j2 toolkit-logback toolkit-opentracing toolkit-tag toolkit-trace toolkit-exception undertow-2.x-plugin vertx-core-3.x xxl-job-2.x zookeeper-3.4.x  ","excerpt":"Skywalking Agent List  activemq-5.x armeria-063-084 armeria-085 armeria-086 armeria-098 avro-1.x …","ref":"/docs/main/v8.2.0/en/setup/service-agent/java-agent/plugin-list/","title":"Skywalking Agent List"},{"body":"SkyWalking Cross Process Correlation Headers Protocol  Version 1.0  The Cross Process Correlation Headers Protocol is used to transport custom data by leveraging the capability of Cross Process Propagation Headers Protocol.\nThis is an optional and additional protocol for language tracer implementation. All tracer implementation could consider to implement this. Cross Process Correlation Header key is sw8-correlation. The value is the encoded(key):encoded(value) list with elements splitted by , such as base64(string key):base64(string value),base64(string key2):base64(string value2).\nRecommendations of language APIs Recommended implementation in different language API.\n TraceContext#putCorrelation and TraceContext#getCorrelation are recommended to write and read the correlation context, with key/value string. The key should be added if it is absent. The later writes should override the previous value. The total number of all keys should be less than 3, and the length of each value should be less than 128 bytes. The context should be propagated as well when tracing context is propagated across threads and processes.  ","excerpt":"SkyWalking Cross Process Correlation Headers Protocol  Version 1.0  The Cross Process Correlation …","ref":"/docs/main/v8.2.0/en/protocols/skywalking-cross-process-correlation-headers-protocol-v1/","title":"SkyWalking Cross Process Correlation Headers Protocol"},{"body":"SkyWalking Cross Process Propagation Headers Protocol  Version 3.0  SkyWalking is more likely an APM system, rather than the common distributed tracing system. The Headers are much more complex than them in order to improving analysis performance of OAP. You can find many similar mechanism in other commercial APM systems. (Some are even much more complex than our\u0026rsquo;s)\nAbstract SkyWalking Cross Process Propagation Headers Protocol v3 is also named as sw8 protocol, which is for context propagation.\nStandard Header Item The standard header should be the minimal requirement for the context propagation.\n Header Name: sw8. Header Value: 8 fields split by -. The length of header value should be less than 2k(default).  Value format example, XXXXX-XXXXX-XXXX-XXXX\nValues Values include the following segments, all String type values are in BASE64 encoding.\n Required(s)   Sample. 0 or 1. 0 means context exists, but could(most likely will) ignore. 1 means this trace need to be sampled and send to backend. Trace Id. String(BASE64 encoded). Literal String and unique globally. Parent trace segment Id. String(BASE64 encoded). Literal String and unique globally. Parent span Id. Integer. Begin with 0. This span id points to the parent span in parent trace segment. Parent service. String(BASE64 encoded). The length should not be less or equal than 50 UTF-8 characters. Parent service instance. String(BASE64 encoded). The length should be less or equal than 50 UTF-8 characters. Parent endpoint. String(BASE64 encoded). Operation Name of the first entry span in the parent segment. The length should be less than 150 UTF-8 characters. Target address used at client side of this request. String(BASE64 encoded). The network address(not must be IP + port) used at client side to access this target service.   Sample values, 1-TRACEID-SEGMENTID-3-PARENT_SERVICE-PARENT_INSTANCE-PARENT_ENDPOINT-IPPORT  Extension Header Item Extension header item is designed for the advanced features. It provides the interaction capabilities between the agents deployed in upstream and downstream services.\n Header Name: sw8-x Header Value: Split by -. The fields are extendable.  Values The current value includes fields.\n Tracing Mode. empty, 0 or 1. empty or 0 is default. 1 represents all spans generated in this context should skip analysis, spanObject#skipAnalysis=true. This context should be propagated to upstream in the default, unless it is changed in the tracing process.  ","excerpt":"SkyWalking Cross Process Propagation Headers Protocol  Version 3.0  SkyWalking is more likely an APM …","ref":"/docs/main/v8.2.0/en/protocols/skywalking-cross-process-propagation-headers-protocol-v3/","title":"SkyWalking Cross Process Propagation Headers Protocol"},{"body":"Apache SkyWalking release guide This document guides every committer to release SkyWalking in Apache Way, and also help committers to check the release for vote.\nSetup your development environment Follow Apache maven deployment environment document to set gpg tool and encrypt passwords\nUse the following block as a template and place it in ~/.m2/settings.xml\n\u0026lt;settings\u0026gt; ... \u0026lt;servers\u0026gt; \u0026lt;!-- To publish a snapshot of some part of Maven --\u0026gt; \u0026lt;server\u0026gt; \u0026lt;id\u0026gt;apache.snapshots.https\u0026lt;/id\u0026gt; \u0026lt;username\u0026gt; \u0026lt;!-- YOUR APACHE LDAP USERNAME --\u0026gt; \u0026lt;/username\u0026gt; \u0026lt;password\u0026gt; \u0026lt;!-- YOUR APACHE LDAP PASSWORD (encrypted) --\u0026gt; \u0026lt;/password\u0026gt; \u0026lt;/server\u0026gt; \u0026lt;!-- To stage a release of some part of Maven --\u0026gt; \u0026lt;server\u0026gt; \u0026lt;id\u0026gt;apache.releases.https\u0026lt;/id\u0026gt; \u0026lt;username\u0026gt; \u0026lt;!-- YOUR APACHE LDAP USERNAME --\u0026gt; \u0026lt;/username\u0026gt; \u0026lt;password\u0026gt; \u0026lt;!-- YOUR APACHE LDAP PASSWORD (encrypted) --\u0026gt; \u0026lt;/password\u0026gt; \u0026lt;/server\u0026gt; ... \u0026lt;/servers\u0026gt; \u0026lt;/settings\u0026gt; Add your GPG public key  Add your GPG public key into SkyWalking GPG KEYS file, only if you are a committer, use your Apache id and password login this svn, and update file. Don\u0026rsquo;t override the existing file. Upload your GPG public key to public GPG site. Such as MIT\u0026rsquo;s site. This site should be in Apache maven staging repository check list.  Test your settings This step is only for test, if your env is set right, don\u0026rsquo;t need to check every time.\n./mvnw clean install -Pall (this will build artifacts, sources and sign) Prepare the release ./mvnw release:clean ./mvnw release:prepare -DautoVersionSubmodules=true -Pall  Set version number as x.y.z, and tag as vx.y.z (version tag must start with v, you will find the purpose in next step.)  You could do a GPG sign before doing release, if you need input the password to sign, and the maven don\u0026rsquo;t give the chance, but just failure. Run gpg --sign xxx to any file could remember the password for enough time to do release.\nStage the release ./mvnw release:perform -DskipTests -Pall  The release will automatically be inserted into a temporary staging repository for you.  Build and sign the source code package export RELEASE_VERSION=x.y.z (example: RELEASE_VERSION=5.0.0-alpha) cd tools/releasing sh create_source_release.sh NOTICE, create_source_release.sh is just suitable for MacOS. Welcome anyone to contribute Windows bat and Linux shell.\nThis scripts should do following things\n Use v + RELEASE_VERSION as tag to clone the codes. Make git submodule init/update done. Exclude all unnecessary files in the target source tar, such as .git, .github, .gitmodules. See the script for the details. Do gpg and shasum 512.  The apache-skywalking-apm-x.y.z-src.tgz should be found in tools/releasing folder, with .asc, .sha512.\nFind and download distribution in Apache Nexus Staging repositories  Use ApacheId to login https://repository.apache.org/ Go to https://repository.apache.org/#stagingRepositories Search skywalking and find your staging repository Close the repository and wait for all checks pass. In this step, your GPG KEYS will be checked. See set PGP document, if you haven\u0026rsquo;t done it before. Go to {REPO_URL}/org/apache/skywalking/apache-skywalking-apm/x.y.z Download .tar.gz and .zip with .asc and .sha1  Upload to Apache svn  Use ApacheId to login https://dist.apache.org/repos/dist/dev/skywalking/ Create folder, named by release version and round, such as: x.y.z Upload Source code package to the folder with .asc, .sha512  Package name: apache-skywalking-x.y.z-src.tar.gz See Section \u0026ldquo;Build and sign the source code package\u0026rdquo; for more details   Upload distribution package to the folder with .asc, .sha512  Package name: apache-skywalking-bin-x.y.z.tar.gz, apache-skywalking-bin-x.y.z.zip See Section \u0026ldquo;Find and download distribution in Apache Nexus Staging repositories\u0026rdquo; for more details Create .sha512 package: shasum -a 512 file \u0026gt; file.sha512    Make the internal announcements Send an announcement mail in dev mail list.\nMail title: [ANNOUNCE] SkyWalking x.y.z test build available Mail content: The test build of x.y.z is available. We welcome any comments you may have, and will take all feedback into account if a quality vote is called for this build. Release notes: * https://github.com/apache/skywalking/blob/master/CHANGES.md Release Candidate: * https://dist.apache.org/repos/dist/dev/skywalking/xxxx * sha512 checksums - sha512xxxxyyyzzz apache-skywalking-apm-x.x.x-src.tgz - sha512xxxxyyyzzz apache-skywalking-apm-bin-x.x.x.tar.gz - sha512xxxxyyyzzz apache-skywalking-apm-bin-x.x.x.zip Maven 2 staging repository: * https://repository.apache.org/content/repositories/xxxx/org/apache/skywalking/ Release Tag : * (Git Tag) x.y.z Release CommitID : * https://github.com/apache/skywalking/tree/(Git Commit ID) * Git submodule * skywalking-ui: https://github.com/apache/skywalking-rocketbot-ui/tree/(Git Commit ID) * apm-protocol/apm-network/src/main/proto: https://github.com/apache/skywalking-data-collect-protocol/tree/(Git Commit ID) * oap-server/server-query-plugin/query-graphql-plugin/src/main/resources/query-protocol https://github.com/apache/skywalking-query-protocol/tree/(Git Commit ID) Keys to verify the Release Candidate : * https://dist.apache.org/repos/dist/release/skywalking/KEYS Guide to build the release from source : * https://github.com/apache/skywalking/blob/x.y.z/docs/en/guides/How-to-build.md A vote regarding the quality of this test build will be initiated within the next couple of days. Wait at least 48 hours for test responses Any PMC, committer or contributor can test features for releasing, and feedback. Based on that, PMC will decide whether start a vote.\nCall a vote in dev Call a vote in dev@skywalking.apache.org\nMail title: [VOTE] Release Apache SkyWalking version x.y.z Mail content: Hi All, This is a call for vote to release Apache SkyWalking version x.y.z. Release notes: * https://github.com/apache/skywalking/blob/x.y.z/CHANGES.md Release Candidate: * https://dist.apache.org/repos/dist/dev/skywalking/xxxx * sha512 checksums - sha512xxxxyyyzzz apache-skywalking-apm-x.x.x-src.tgz - sha512xxxxyyyzzz apache-skywalking-apm-bin-x.x.x.tar.gz - sha512xxxxyyyzzz apache-skywalking-apm-bin-x.x.x.zip Maven 2 staging repository: * https://repository.apache.org/content/repositories/xxxx/org/apache/skywalking/ Release Tag : * (Git Tag) x.y.z Release CommitID : * https://github.com/apache/skywalking/tree/(Git Commit ID) * Git submodule * skywalking-ui: https://github.com/apache/skywalking-rocketbot-ui/tree/(Git Commit ID) * apm-protocol/apm-network/src/main/proto: https://github.com/apache/skywalking-data-collect-protocol/tree/(Git Commit ID) * oap-server/server-query-plugin/query-graphql-plugin/src/main/resources/query-protocol https://github.com/apache/skywalking-query-protocol/tree/(Git Commit ID) Keys to verify the Release Candidate : * https://dist.apache.org/repos/dist/release/skywalking/KEYS Guide to build the release from source : * https://github.com/apache/skywalking/blob/x.y.z/docs/en/guides/How-to-build.md Voting will start now (xxxx date) and will remain open for at least 72 hours, Request all PMC members to give their vote. [ ] +1 Release this package. [ ] +0 No opinion. [ ] -1 Do not release this package because.... Vote Check All PMC members and committers should check these before vote +1.\n Features test. All artifacts in staging repository are published with .asc, .md5, *sha1 files Source code and distribution package (apache-skywalking-x.y.z-src.tar.gz, apache-skywalking-bin-x.y.z.tar.gz, apache-skywalking-bin-x.y.z.zip) are in https://dist.apache.org/repos/dist/dev/skywalking/x.y.z with .asc, .sha512 LICENSE and NOTICE are in Source code and distribution package. Check shasum -c apache-skywalking-apm-x.y.z-src.tgz.sha512 Check gpg --verify apache-skywalking-apm-x.y.z-src.tgz.asc apache-skywalking-apm-x.y.z-src.tgz Build distribution from source code package (apache-skywalking-x.y.z-src.tar.gz) by following this doc. Apache RAT check. Run ./mvnw apache-rat:check. (No binary in source codes)  Vote result should follow these.\n PMC vote is +1 binding, all others is +1 no binding. In 72 hours, you get at least 3 (+1 binding), and have more +1 than -1. Vote pass.  Publish release  Move source codes tar balls and distributions to https://dist.apache.org/repos/dist/release/skywalking/.  \u0026gt; export SVN_EDITOR=vim \u0026gt; svn mv https://dist.apache.org/repos/dist/dev/skywalking/x.y.z https://dist.apache.org/repos/dist/release/skywalking .... enter your apache password .... Do release in nexus staging repo. Public download source and distribution tar/zip locate in http://www.apache.org/dyn/closer.cgi/skywalking/x.y.z/xxx. We only publish Apache mirror path as release info. Public asc and sha512 locate in https://www.apache.org/dist/skywalking/x.y.z/xxx Public KEYS pointing to https://www.apache.org/dist/skywalking/KEYS Update website download page. http://skywalking.apache.org/downloads/ . Include new download source, distribution, sha512, asc and document links. Links could be found by following above rules(3)-(6). Add a release event on website homepage and event page. Announce the public release with changelog or key features. Send ANNOUNCE email to dev@skywalking.apache.org, announce@apache.org, the sender should use Apache email account.  Mail title: [ANNOUNCE] Apache SkyWalking x.y.z released Mail content: Hi all, Apache SkyWalking Team is glad to announce the first release of Apache SkyWalking x.y.z. SkyWalking: APM (application performance monitor) tool for distributed systems, especially designed for microservices, cloud native and container-based (Docker, Kubernetes, Mesos) architectures. This release contains a number of new features, bug fixes and improvements compared to version a.b.c(last release). The notable changes since x.y.z include: (Highlight key changes) 1. ... 2. ... 3. ... Please refer to the change log for the complete list of changes: https://github.com/apache/skywalking/blob/vx.y.z/CHANGES.md Apache SkyWalking website: http://skywalking.apache.org/ Downloads: http://skywalking.apache.org/downloads/ Twitter: https://twitter.com/ASFSkyWalking SkyWalking Resources: - GitHub: https://github.com/apache/skywalking - Issue: https://github.com/apache/skywalking/issues - Mailing list: dev@skywalkiing.apache.org - Apache SkyWalking Team ","excerpt":"Apache SkyWalking release guide This document guides every committer to release SkyWalking in Apache …","ref":"/docs/main/v8.2.0/en/guides/how-to-release/","title":"SkyWalking release guide"},{"body":"Skywalking with Kotlin coroutine This Plugin provides an auto instrument support plugin for Kotlin coroutine based on context snapshot.\nDescription SkyWalking provide tracing context propagation inside thread. In order to support Kotlin Coroutine, we provide this additional plugin.\nImplementation principle As we know, Kotlin coroutine switches the execution thread by CoroutineDispatcher.\n Create a snapshot of the current context before dispatch the continuation. Then create a coroutine span after thread switched, mark the span continued with the snapshot. Every new span which created in the new thread will be a child of this coroutine span. So we can link those span together in a tracing. After the original runnable executed, we need to stop the coroutine span for cleaning thread state.  Some screenshots Run without the plugin We run a Kotlin coroutine based gRPC server without this coroutine plugin.\nYou can find, the one call (client -\u0026gt; server1 -\u0026gt; server2) has been split two tracing paths.\n Server1 without exit span and server2 tracing path.  Server2 tracing path.   Run with the plugin Without changing codes manually, just install the plugin. We can find the spans be connected together. We can get all info of one client call.\n","excerpt":"Skywalking with Kotlin coroutine This Plugin provides an auto instrument support plugin for Kotlin …","ref":"/docs/main/v8.2.0/en/setup/service-agent/java-agent/agent-optional-plugins/kotlin-coroutine-plugin/","title":"Skywalking with Kotlin coroutine"},{"body":"Slow Database Statement Slow Database statements are significant important to find out the bottleneck of the system, which relied on Database.\nSlow DB statements are based on sampling, right now, the core samples top 50 slowest in every 10 minutes. But duration of those statements must be slower than threshold.\nThe setting format is following, unit is millisecond.\n database-type:thresholdValue,database-type2:thresholdValue2\n Default setting is default:200,mongodb:100. Reserved DB type is default, which be as default threshold for all database types, except set explicitly.\nNotice, the threshold should not be too small, like 1ms. Functionally, it works, but would cost OAP performance issue, if your system statement access time are mostly more than 1ms.\n","excerpt":"Slow Database Statement Slow Database statements are significant important to find out the …","ref":"/docs/main/v8.2.0/en/setup/backend/slow-db-statement/","title":"Slow Database Statement"},{"body":"Source and Scope extension for new metrics From OAL scope introduction, you should already have understood what the scope is. At here, as you want to do more extension, you need understand deeper, which is the Source.\nSource and Scope are binding concepts. Scope declare the id(int) and name, Source declare the attributes. Please follow these steps to create a new Source and Scope.\n In the OAP core module, it provide SourceReceiver internal service.  public interface SourceReceiver extends Service { void receive(Source source); } All analysis data must be a org.apache.skywalking.oap.server.core.source.Source sub class, tagged by @SourceType annotation, and in org.apache.skywalking package. Then it could be supported by OAL script and OAP core.  Such as existed source, Service.\n@ScopeDeclaration(id = SERVICE_INSTANCE, name = \u0026#34;ServiceInstance\u0026#34;, catalog = SERVICE_INSTANCE_CATALOG_NAME) @ScopeDefaultColumn.VirtualColumnDefinition(fieldName = \u0026#34;entityId\u0026#34;, columnName = \u0026#34;entity_id\u0026#34;, isID = true, type = String.class) public class ServiceInstance extends Source { @Override public int scope() { return DefaultScopeDefine.SERVICE_INSTANCE; } @Override public String getEntityId() { return String.valueOf(id); } @Getter @Setter private int id; @Getter @Setter @ScopeDefaultColumn.DefinedByField(columnName = \u0026#34;service_id\u0026#34;) private int serviceId; @Getter @Setter private String name; @Getter @Setter private String serviceName; @Getter @Setter private String endpointName; @Getter @Setter private int latency; @Getter @Setter private boolean status; @Getter @Setter private int responseCode; @Getter @Setter private RequestType type; }  The scope() method in Source, returns an ID, which is not a random number. This ID need to be declared through @ScopeDeclaration annotation too. The ID in @ScopeDeclaration and ID in scope() method should be same for this Source.\n  The String getEntityId() method in Source, requests the return value representing unique entity which the scope related. Such as, in this Service scope, the id is service id, representing a particular service, like Order service. This value is used in OAL group mechanism.\n  @ScopeDefaultColumn.VirtualColumnDefinition and @ScopeDefaultColumn.DefinedByField are required, all declared fields(virtual/byField) are going to be pushed into persistent entity, mapping to such as ElasticSearch index and Database table column. Such as, include entity id mostly, and service id for endpoint and service instance level scope. Take a reference to all existing scopes. All these fields are detected by OAL Runtime, and required in query stage.\n  Add scope name as keyword to oal grammar definition file, OALLexer.g4, which is at antlr4 folder of generate-tool-grammar module.\n  Add scope name keyword as source in parser definition file, OALParser.g4, which is at same fold of OALLexer.g4.\n   After you done all of these, you could build a receiver, which do\n Get the original data of the metrics, Build the source, send into SourceReceiver. Write your whole OAL scripts. Repackage the project.  ","excerpt":"Source and Scope extension for new metrics From OAL scope introduction, you should already have …","ref":"/docs/main/v8.2.0/en/guides/source-extension/","title":"Source and Scope extension for new metrics"},{"body":"Spring annotation plugin This plugin allows to trace all methods of beans in Spring context, which are annotated with @Bean, @Service, @Component and @Repository.\n Why does this plugin optional?  Tracing all methods in Spring context all creates a lot of spans, which also spend more CPU, memory and network. Of course you want to have spans as many as possible, but please make sure your system payload can support these.\n","excerpt":"Spring annotation plugin This plugin allows to trace all methods of beans in Spring context, which …","ref":"/docs/main/v8.2.0/en/setup/service-agent/java-agent/agent-optional-plugins/spring-annotation-plugin/","title":"Spring annotation plugin"},{"body":"Spring sleuth setup Spring Sleuth provides Spring Boot auto-configuration for distributed tracing. Skywalking integrates it\u0026rsquo;s micrometer part, and it can send metrics to the Skywalking Meter System.\nSet up agent  Add the Micrometer and Skywalking meter registry dependency into project pom.xml file. Also you could found more detail at Toolkit micrometer.  \u0026lt;dependency\u0026gt; \u0026lt;groupId\u0026gt;org.springframework.boot\u0026lt;/groupId\u0026gt; \u0026lt;artifactId\u0026gt;spring-boot-starter-actuator\u0026lt;/artifactId\u0026gt; \u0026lt;/dependency\u0026gt; \u0026lt;dependency\u0026gt; \u0026lt;groupId\u0026gt;org.apache.skywalking\u0026lt;/groupId\u0026gt; \u0026lt;artifactId\u0026gt;apm-toolkit-micrometer-registry\u0026lt;/artifactId\u0026gt; \u0026lt;version\u0026gt;${skywalking.version}\u0026lt;/version\u0026gt; \u0026lt;/dependency\u0026gt; Create the Skywalking meter resgitry into spring bean management.  @Bean SkywalkingMeterRegistry skywalkingMeterRegistry() { // Add rate configs If you need, otherwise using none args construct  SkywalkingConfig config = new SkywalkingConfig(Arrays.asList(\u0026#34;\u0026#34;)); return new SkywalkingMeterRegistry(config); } Set up backend receiver  Enable meter receiver in the applicaiton.yml.  receiver-meter: selector: ${SW_RECEIVER_METER:default} default: Configure the meter config file, It already has the spring sleuth meter config. If you also has some customized meter at the agent side, please read meter document to configure meter.  Add UI dashboard   Open the dashboard view, click edit button to edit the templates.\n  Create a new template. Template type: Standard -\u0026gt; Template Configuration: Spring -\u0026gt; Input the Template Name.\n  Click view button, Finally get the spring sleuth dashboard.\n  Supported meter Supported 3 types information: Application, System, JVM.\n Application: HTTP request count and duration, JDBC max/idle/active connection count, Tomcat session active/reject count. System: CPU system/process usage, OS System load, OS Process file count. JVM: GC pause count and duration, Memory max/used/committed size, Thread peak/live/daemon count, Classes loaded/unloaded count.  ","excerpt":"Spring sleuth setup Spring Sleuth provides Spring Boot auto-configuration for distributed tracing. …","ref":"/docs/main/v8.2.0/en/setup/backend/spring-sleuth-setup/","title":"Spring sleuth setup"},{"body":"Start up mode In different deployment tool, such as k8s, you may need different startup mode. We provide another two optional startup modes.\nDefault mode Default mode. Do initialization works if necessary, start listen and provide service.\nRun /bin/oapService.sh(.bat) to start in this mode. Also when use startup.sh(.bat) to start.\nInit mode In this mode, oap server starts up to do initialization works, then exit. You could use this mode to init your storage, such as ElasticSearch indexes, MySQL and TiDB tables, and init data.\nRun /bin/oapServiceInit.sh(.bat) to start in this mode.\nNo-init mode In this mode, oap server starts up without initialization works, but it waits for ElasticSearch indexes, MySQL and TiDB tables existed, start listen and provide service. Meaning, this oap server expect another oap server to do the initialization.\nRun /bin/oapServiceNoInit.sh(.bat) to start in this mode.\n","excerpt":"Start up mode In different deployment tool, such as k8s, you may need different startup mode. We …","ref":"/docs/main/v8.2.0/en/setup/backend/backend-start-up-mode/","title":"Start up mode"},{"body":"Support custom enhance Here is an optional plugin apm-customize-enhance-plugin\nIntroduce SkyWalking has provided Java agent plugin development guide to help developers to build new plugin.\nThis plugin is not designed for replacement but for user convenience. The behaviour is very similar with @Trace toolkit, but without code change requirement, and more powerful, such as provide tag and log.\nHow to configure Implementing enhancements to custom classes requires two steps.\n Active the plugin, move the optional-plugins/apm-customize-enhance-plugin.jar to plugin/apm-customize-enhance-plugin.jar. Set plugin.customize.enhance_file in agent.config, which targets to rule file, such as /absolute/path/to/customize_enhance.xml. Set enhancement rules in customize_enhance.xml. \u0026lt;?xml version=\u0026#34;1.0\u0026#34; encoding=\u0026#34;UTF-8\u0026#34;?\u0026gt; \u0026lt;enhanced\u0026gt; \u0026lt;class class_name=\u0026#34;test.apache.skywalking.testcase.customize.service.TestService1\u0026#34;\u0026gt; \u0026lt;method method=\u0026#34;staticMethod()\u0026#34; operation_name=\u0026#34;/is_static_method\u0026#34; static=\u0026#34;true\u0026#34;/\u0026gt; \u0026lt;method method=\u0026#34;staticMethod(java.lang.String,int.class,java.util.Map,java.util.List,[Ljava.lang.Object;)\u0026#34; operation_name=\u0026#34;/is_static_method_args\u0026#34; static=\u0026#34;true\u0026#34;\u0026gt; \u0026lt;operation_name_suffix\u0026gt;arg[0]\u0026lt;/operation_name_suffix\u0026gt; \u0026lt;operation_name_suffix\u0026gt;arg[1]\u0026lt;/operation_name_suffix\u0026gt; \u0026lt;operation_name_suffix\u0026gt;arg[3].[0]\u0026lt;/operation_name_suffix\u0026gt; \u0026lt;tag key=\u0026#34;tag_1\u0026#34;\u0026gt;arg[2].[\u0026#39;k1\u0026#39;]\u0026lt;/tag\u0026gt; \u0026lt;tag key=\u0026#34;tag_2\u0026#34;\u0026gt;arg[4].[1]\u0026lt;/tag\u0026gt; \u0026lt;log key=\u0026#34;log_1\u0026#34;\u0026gt;arg[4].[2]\u0026lt;/log\u0026gt; \u0026lt;/method\u0026gt; \u0026lt;method method=\u0026#34;method()\u0026#34; static=\u0026#34;false\u0026#34;/\u0026gt; \u0026lt;method method=\u0026#34;method(java.lang.String,int.class)\u0026#34; operation_name=\u0026#34;/method_2\u0026#34; static=\u0026#34;false\u0026#34;\u0026gt; \u0026lt;operation_name_suffix\u0026gt;arg[0]\u0026lt;/operation_name_suffix\u0026gt; \u0026lt;tag key=\u0026#34;tag_1\u0026#34;\u0026gt;arg[0]\u0026lt;/tag\u0026gt; \u0026lt;log key=\u0026#34;log_1\u0026#34;\u0026gt;arg[1]\u0026lt;/log\u0026gt; \u0026lt;/method\u0026gt; \u0026lt;method method=\u0026#34;method(test.apache.skywalking.testcase.customize.model.Model0,java.lang.String,int.class)\u0026#34; operation_name=\u0026#34;/method_3\u0026#34; static=\u0026#34;false\u0026#34;\u0026gt; \u0026lt;operation_name_suffix\u0026gt;arg[0].id\u0026lt;/operation_name_suffix\u0026gt; \u0026lt;operation_name_suffix\u0026gt;arg[0].model1.name\u0026lt;/operation_name_suffix\u0026gt; \u0026lt;operation_name_suffix\u0026gt;arg[0].model1.getId()\u0026lt;/operation_name_suffix\u0026gt; \u0026lt;tag key=\u0026#34;tag_os\u0026#34;\u0026gt;arg[0].os.[1]\u0026lt;/tag\u0026gt; \u0026lt;log key=\u0026#34;log_map\u0026#34;\u0026gt;arg[0].getM().[\u0026#39;k1\u0026#39;]\u0026lt;/log\u0026gt; \u0026lt;/method\u0026gt; \u0026lt;/class\u0026gt; \u0026lt;class class_name=\u0026#34;test.apache.skywalking.testcase.customize.service.TestService2\u0026#34;\u0026gt; \u0026lt;method method=\u0026#34;staticMethod(java.lang.String,int.class)\u0026#34; operation_name=\u0026#34;/is_2_static_method\u0026#34; static=\u0026#34;true\u0026#34;\u0026gt; \u0026lt;tag key=\u0026#34;tag_2_1\u0026#34;\u0026gt;arg[0]\u0026lt;/tag\u0026gt; \u0026lt;log key=\u0026#34;log_1_1\u0026#34;\u0026gt;arg[1]\u0026lt;/log\u0026gt; \u0026lt;/method\u0026gt; \u0026lt;method method=\u0026#34;method([Ljava.lang.Object;)\u0026#34; operation_name=\u0026#34;/method_4\u0026#34; static=\u0026#34;false\u0026#34;\u0026gt; \u0026lt;tag key=\u0026#34;tag_4_1\u0026#34;\u0026gt;arg[0].[0]\u0026lt;/tag\u0026gt; \u0026lt;/method\u0026gt; \u0026lt;method method=\u0026#34;method(java.util.List,int.class)\u0026#34; operation_name=\u0026#34;/method_5\u0026#34; static=\u0026#34;false\u0026#34;\u0026gt; \u0026lt;tag key=\u0026#34;tag_5_1\u0026#34;\u0026gt;arg[0].[0]\u0026lt;/tag\u0026gt; \u0026lt;log key=\u0026#34;log_5_1\u0026#34;\u0026gt;arg[1]\u0026lt;/log\u0026gt; \u0026lt;/method\u0026gt; \u0026lt;/class\u0026gt; \u0026lt;/enhanced\u0026gt; ``\n   Explanation of the configuration in the file    configuration explanation     class_name The enhanced class   method The interceptor method of the class   operation_name If fill it out, will use it instead of the default operation_name.   operation_name_suffix What it means adding dynamic data after the operation_name.   static Is this method static.   tag Will add a tag in local span. The value of key needs to be represented on the XML node.   log Will add a log in local span. The value of key needs to be represented on the XML node.   arg[x] What it means is to get the input arguments. such as arg[0] is means get first arguments.   .[x] When the parsing object is Array or List, you can use it to get the object at the specified index.   .[\u0026lsquo;key\u0026rsquo;] When the parsing object is Map, you can get the map \u0026lsquo;key\u0026rsquo; through it.      ","excerpt":"Support custom enhance Here is an optional plugin apm-customize-enhance-plugin\nIntroduce SkyWalking …","ref":"/docs/main/v8.2.0/en/setup/service-agent/java-agent/customize-enhance-trace/","title":"Support custom enhance"},{"body":"Support custom trace ignore Here is an optional plugin apm-trace-ignore-plugin\nIntroduce  The purpose of this plugin is to filter endpoint which are expected to be ignored by the tracing system. You can setup multiple URL path patterns, The endpoints match these patterns wouldn\u0026rsquo;t be traced. The current matching rules follow Ant Path match style , like /path/*, /path/**, /path/?. Copy apm-trace-ignore-plugin-x.jar to agent/plugins, restarting the agent can effect the plugin.  How to configure There are two ways to configure ignore patterns. Settings through system env has higher priority.\n Set through the system environment variable,you need to add skywalking.trace.ignore_path to the system variables, the value is the path that you need to ignore, multiple paths should be separated by , Copy/agent/optional-plugins/apm-trace-ignore-plugin/apm-trace-ignore-plugin.config to /agent/config/ dir, and add rules to filter traces  trace.ignore_path=/your/path/1/**,/your/path/2/** ","excerpt":"Support custom trace ignore Here is an optional plugin apm-trace-ignore-plugin\nIntroduce  The …","ref":"/docs/main/v8.2.0/en/setup/service-agent/java-agent/agent-optional-plugins/trace-ignore-plugin/","title":"Support custom trace ignore"},{"body":"Support gRPC SSL transportation for OAP server For OAP communication we are currently using gRPC, a multi-platform RPC framework that uses protocol buffers for message serialization. The nice part about gRPC is that it promotes the use of SSL/TLS to authenticate and encrypt exchanges. Now OAP supports to enable SSL transportation for gRPC receivers.\nYou can follow below steps to enable this feature\nCreating SSL/TLS Certificates It seems like step one is to generate certificates and key files for encrypting communication. I thought this would be fairly straightforward using openssl from the command line.\nUse this script if you are not familiar with how to generate key files.\nWe need below files:\n server.pem a private RSA key to sign and authenticate the public key. It\u0026rsquo;s either a PKCS#8(PEM) or PKCS#1(DER). server.crt self-signed X.509 public keys for distribution. ca.crt a certificate authority public key for a client to validate the server\u0026rsquo;s certificate.  Config OAP server You can enable gRPC SSL by add following lines to application.yml/core/default.\ngRPCSslEnabled: true gRPCSslKeyPath: /path/to/server.pem gRPCSslCertChainPath: /path/to/server.crt gRPCSslTrustedCAPath: /path/to/ca.crt gRPCSslKeyPath and gRPCSslCertChainPath are loaded by OAP server to encrypt the communication. gRPCSslTrustedCAPath helps gRPC client to verify server certificates in cluster mode.\nWhen new files are in place, they can be load dynamically instead of restarting OAP instance.\nIf you enable sharding-server to ingest data from external, add following lines to application.yml/receiver-sharing-server/default:\ngRPCSslEnabled: true gRPCSslKeyPath: /path/to/server.pem gRPCSslCertChainPath: /path/to/server.crt Because sharding-server only receives data from external, so it doesn\u0026rsquo;t need CA at all.\nIf you port to java agent, refer to TLS.md to config java agent to enable TLS.\n","excerpt":"Support gRPC SSL transportation for OAP server For OAP communication we are currently using gRPC, a …","ref":"/docs/main/v8.2.0/en/setup/backend/grpc-ssl/","title":"Support gRPC SSL transportation for OAP server"},{"body":"Support Transport Layer Security (TLS) Transport Layer Security (TLS) is a very common security way when transport data through Internet. In some use cases, end users report the background:\n Target(under monitoring) applications are in a region, which also named VPC, at the same time, the SkyWalking backend is in another region (VPC).\nBecause of that, security requirement is very obvious.\n Authentication Mode Only support no mutual auth.\n Use this script if you are not familiar with how to generate key files. Find ca.crt, and use it at client side Find server.crt ,server.pem and ca.crt. Use them at server side. Please refer to gRPC SSL for more details.  Open and config TLS Agent config  Place ca.crt into /ca folder in agent package. Notice, /ca is not created in distribution, please create it by yourself.  Agent open TLS automatically after the /ca/ca.crt file detected.\no make sure can\u0026rsquo;t access other ports out of region (VPC), such as firewall, proxy.\n","excerpt":"Support Transport Layer Security (TLS) Transport Layer Security (TLS) is a very common security way …","ref":"/docs/main/v8.2.0/en/setup/service-agent/java-agent/tls/","title":"Support Transport Layer Security (TLS)"},{"body":"Telemetry for backend By default, the telemetry is disabled by setting selector to none, like this\ntelemetry: selector: ${SW_TELEMETRY:none} none: prometheus: host: ${SW_TELEMETRY_PROMETHEUS_HOST:0.0.0.0} port: ${SW_TELEMETRY_PROMETHEUS_PORT:1234} sslEnabled: ${SW_TELEMETRY_PROMETHEUS_SSL_ENABLED:false} sslKeyPath: ${SW_TELEMETRY_PROMETHEUS_SSL_KEY_PATH:\u0026#34;\u0026#34;} sslCertChainPath: ${SW_TELEMETRY_PROMETHEUS_SSL_CERT_CHAIN_PATH:\u0026#34;\u0026#34;} but you can set one of prometheus to enable them, for more information, refer to the details below.\nPrometheus Prometheus is supported as telemetry implementor. By using this, prometheus collects metrics from SkyWalking backend.\nSet prometheus to provider. The endpoint open at http://0.0.0.0:1234/ and http://0.0.0.0:1234/metrics.\ntelemetry: selector: ${SW_TELEMETRY:prometheus} prometheus: Set host and port if needed.\ntelemetry: selector: ${SW_TELEMETRY:prometheus} prometheus: host: 127.0.0.1 port: 1543 Set SSL relevant settings to expose a secure endpoint. Notice private key file and cert chain file could be uploaded once changes are applied to them.\ntelemetry: selector: ${SW_TELEMETRY:prometheus} prometheus: host: 127.0.0.1 port: 1543 sslEnabled: true sslKeyPath: /etc/ssl/key.pem sslCertChainPath: /etc/ssl/cert-chain.pem Grafana Visualization Provide the grafana dashboard settings. Check SkyWalking Telemetry dashboard config.\nSelf Observability SkyWalking supports to collect telemetry data into OAP backend directly. Users could check them out through UI or GraphQL API then.\nAdding following configuration to enable self-observability related modules.\n Setting up prometheus telemetry.  telemetry: selector: ${SW_TELEMETRY:prometheus} prometheus: host: 127.0.0.1 port: 1543 Setting up prometheus fetcher  prometheus-fetcher: selector: ${SW_PROMETHEUS_FETCHER:default} default: active: ${SW_PROMETHEUS_FETCHER_ACTIVE:true} Make sure config/fetcher-prom-rules/self.yaml exists.  Once you deploy an oap-server cluster, the target host should be replaced with a dedicated IP or hostname. For instances, there are three oap server in your cluster, their host is service1, service2 and service3 respectively. You should update each self.yaml to twist target host.\nservice1:\nfetcherInterval: PT15S fetcherTimeout: PT10S metricsPath: /metrics staticConfig: # targets will be labeled as \u0026#34;instance\u0026#34; targets: - service1:1234 labels: service: oap-server ... service2:\nfetcherInterval: PT15S fetcherTimeout: PT10S metricsPath: /metrics staticConfig: # targets will be labeled as \u0026#34;instance\u0026#34; targets: - service2:1234 labels: service: oap-server ... service3:\nfetcherInterval: PT15S fetcherTimeout: PT10S metricsPath: /metrics staticConfig: # targets will be labeled as \u0026#34;instance\u0026#34; targets: - service3:1234 labels: service: oap-server ... ","excerpt":"Telemetry for backend By default, the telemetry is disabled by setting selector to none, like this …","ref":"/docs/main/v8.2.0/en/setup/backend/backend-telemetry/","title":"Telemetry for backend"},{"body":"Thread dump merging mechanism The performance profile is an enhancement feature in the APM system. We are using the thread dump to estimate the method execution time, rather than adding many local spans. In this way, the resource cost would be much less than using distributed tracing to locate slow method. This feature is suitable in the production environment. This document introduces how thread dumps are merged into the final report as a stack tree(s).\nThread analyst Read data and transform Read data from the database and convert it to a data structure in gRPC.\nst=\u0026gt;start: Start e=\u0026gt;end: End op1=\u0026gt;operation: Load data using paging op2=\u0026gt;operation: Transform data using parallel st(right)-\u0026gt;op1(right)-\u0026gt;op2 op2(right)-\u0026gt;e Copy code and paste it into this link to generate flow chart.\n Use the stream to read data by page (50 records per page). Convert data into gRPC data structures in the form of parallel streams. Merge into a list of data.  Data analyze Use the group by and collector modes in the Java parallel stream to group according to the first stack element in the database records, and use the collector to perform data aggregation. Generate a multi-root tree.\nst=\u0026gt;start: Start e=\u0026gt;end: End op1=\u0026gt;operation: Group by first stack element sup=\u0026gt;operation: Generate empty stack tree acc=\u0026gt;operation: Accumulator data to stack tree com=\u0026gt;operation: Combine stack trees fin=\u0026gt;operation: Calculate durations and build result st(right)-\u0026gt;op1-\u0026gt;sup(right)-\u0026gt;acc acc(right)-\u0026gt;com(right)-\u0026gt;fin-\u0026gt;e Copy code and paste it into this link to generate flow chart.\n Group by first stack element: Use the first level element in each stack to group, ensuring that the stacks have the same root node. Generate empty stack tree: Generate multiple top-level empty trees for preparation of the following steps, The reason for generating multiple top-level trees is that original data can be add in parallel without generating locks. Accumulator data to stack tree: Add every thread dump into the generated trees.  Iterate through each element in the thread dump to find if there is any child element with the same code signature and same stack depth in the parent element. If not, then add this element. Keep the dump sequences and timestamps in each nodes from the source.   Combine stack trees: Combine all trees structures into one by using the rules as same as Accumulator.  Use LDR to traversal tree node. Use the Stack data structure to avoid recursive calls, each stack element represents the node that needs to be merged. The task of merging two nodes is to merge the list of children nodes. If they have the same code signature and same parents, save the dump sequences and timestamps in this node. Otherwise, the node needs to be added into the target node as a new child.   Calculate durations and build result: Calculate relevant statistics and generate response.  Use the same traversal node logic as in the Combine stack trees step. Convert to a GraphQL data structure, and put all nodes into a list for subsequent duration calculations. Calculate each node\u0026rsquo;s duration in parallel. For each node, sort the sequences, if there are two continuous sequences, the duration should add the duration of these two seq\u0026rsquo;s timestamp. Calculate each node execution in parallel. For each node, the duration of the current node should minus the time consumed by all children.    Profile data debug Please follow the exporter tool to package profile data. Unzip the profile data and using analyzer main function to run it.\n","excerpt":"Thread dump merging mechanism The performance profile is an enhancement feature in the APM system. …","ref":"/docs/main/v8.2.0/en/guides/backend-profile/","title":"Thread dump merging mechanism"},{"body":"Token Authentication Supported version 7.0.0+\nWhy need token authentication after we have TLS? TLS is about transport security, which makes sure the network can be trusted. The token authentication is about monitoring application data can be trusted.\nToken In current version, Token is considered as a simple string.\nSet Token  Set token in agent.config file  # Authentication active is based on backend setting, see application.yml for more details. agent.authentication = ${SW_AGENT_AUTHENTICATION:xxxx} Set token in application.yml file  ······ receiver-sharing-server: default: authentication: ${SW_AUTHENTICATION:\u0026#34;\u0026#34;} ······ Authentication fails The Skywalking OAP verifies every request from agent, only allows requests whose token matches the one configured in application.yml.\nIf the token is not right, you will see the following log in agent\norg.apache.skywalking.apm.dependencies.io.grpc.StatusRuntimeException: PERMISSION_DENIED FAQ Can I use token authentication instead of TLS? No, you shouldn\u0026rsquo;t. In tech way, you can of course, but token and TLS are used for untrusted network env. In that circumstance, TLS has higher priority than this. Token can be trusted only under TLS protection.Token can be stolen easily if you send it through a non-TLS network.\nDo you support other authentication mechanisms? Such as ak/sk? For now, no. But we appreciate someone contributes this feature.\n","excerpt":"Token Authentication Supported version 7.0.0+\nWhy need token authentication after we have TLS? TLS …","ref":"/docs/main/v8.2.0/en/setup/backend/backend-token-auth/","title":"Token Authentication"},{"body":"Token Authentication Token In current version, Token is considered as a simple string.\nSet Token Set token in agent.config file\n# Authentication active is based on backend setting, see application.yml for more details. agent.authentication = xxxx Meanwhile, open the backend token authentication.\nAuthentication fails The Collector verifies every request from agent, allowed only the token match.\nIf the token is not right, you will see the following log in agent\norg.apache.skywalking.apm.dependencies.io.grpc.StatusRuntimeException: PERMISSION_DENIED FAQ Can I use token authentication instead of TLS? No, you shouldn\u0026rsquo;t. In tech way, you can of course, but token and TLS are used for untrusted network env. In that circumstance, TLS has higher priority than this. Token can be trusted only under TLS protection.Token can be stolen easily if you send it through a non-TLS network.\nDo you support other authentication mechanisms? Such as ak/sk? For now, no. But we appreciate someone contributes this feature.\n","excerpt":"Token Authentication Token In current version, Token is considered as a simple string.\nSet Token Set …","ref":"/docs/main/v8.2.0/en/setup/service-agent/java-agent/token-auth/","title":"Token Authentication"},{"body":"trace cross thread  Dependency the toolkit, such as using maven or gradle  \u0026lt;dependency\u0026gt; \u0026lt;groupId\u0026gt;org.apache.skywalking\u0026lt;/groupId\u0026gt; \u0026lt;artifactId\u0026gt;apm-toolkit-trace\u0026lt;/artifactId\u0026gt; \u0026lt;version\u0026gt;${skywalking.version}\u0026lt;/version\u0026gt; \u0026lt;/dependency\u0026gt;  usage 1.  @TraceCrossThread public static class MyCallable\u0026lt;String\u0026gt; implements Callable\u0026lt;String\u0026gt; { @Override public String call() throws Exception { return null; } } ... ExecutorService executorService = Executors.newFixedThreadPool(1); executorService.submit(new MyCallable());  usage 2.  ExecutorService executorService = Executors.newFixedThreadPool(1); executorService.submit(CallableWrapper.of(new Callable\u0026lt;String\u0026gt;() { @Override public String call() throws Exception { return null; } })); or\nExecutorService executorService = Executors.newFixedThreadPool(1); executorService.execute(RunnableWrapper.of(new Runnable() { @Override public void run() { //your code  } }));  usage 3.  @TraceCrossThread public class MySupplier\u0026lt;String\u0026gt; implements Supplier\u0026lt;String\u0026gt; { @Override public String get() { return null; } } ... CompletableFuture.supplyAsync(new MySupplier\u0026lt;String\u0026gt;()); or\nCompletableFuture.supplyAsync(SupplierWrapper.of(()-\u0026gt;{ return \u0026#34;SupplierWrapper\u0026#34;; })).thenAccept(System.out::println); Sample codes only\n","excerpt":"trace cross thread  Dependency the toolkit, such as using maven or gradle  \u0026lt;dependency\u0026gt; …","ref":"/docs/main/v8.2.0/en/setup/service-agent/java-agent/application-toolkit-trace-cross-thread/","title":"trace cross thread"},{"body":"Trace Data Protocol v3 Trace Data Protocol describes the data format between SkyWalking agent/sniffer and backend.\nOverview Trace data protocol is defined and provided in gRPC format, also implemented in HTTP 1.1\nReport service instance status   Service Instance Properties Service instance has more information than a name, once the agent wants to report this, use ManagementService#reportInstanceProperties service providing a string-key/string-value pair list as the parameter. language of target instance is expected at least.\n  Service Ping Service instance should keep alive with the backend. The agent should set a scheduler using ManagementService#keepAlive service in every minute.\n  Send trace and metrics After you have service id and service instance id, you could send traces and metrics. Now we have\n TraceSegmentReportService#collect for skywalking native trace format JVMMetricReportService#collect for skywalking native jvm format  For trace format, there are some notices\n Segment is a concept in SkyWalking, it should include all span for per request in a single OS process, usually single thread based on language. Span has 3 different groups.    EntrySpan EntrySpan represents a service provider, also the endpoint of server side. As an APM system, we are targeting the application servers. So almost all the services and MQ-consumer are EntrySpan(s).\n  LocalSpan LocalSpan represents a normal Java method, which don\u0026rsquo;t relate with remote service, neither a MQ producer/consumer nor a service(e.g. HTTP service) provider/consumer.\n  ExitSpan ExitSpan represents a client of service or MQ-producer, as named as LeafSpan at early age of SkyWalking. e.g. accessing DB by JDBC, reading Redis/Memcached are cataloged an ExitSpan.\n   Span across thread or process parent info is called Reference. Reference carries trace id, segment id, span id, service name, service instance name, endpoint name and target address used at client side(not required in across thread) of this request in the parent. Follow Cross Process Propagation Headers Protocol v3 to get more details.\n  Span#skipAnalysis could be TRUE, if this span doesn\u0026rsquo;t require backend analysis.\n  ","excerpt":"Trace Data Protocol v3 Trace Data Protocol describes the data format between SkyWalking …","ref":"/docs/main/v8.2.0/en/protocols/trace-data-protocol-v3/","title":"Trace Data Protocol v3"},{"body":"Trace Sampling at server side When we run a distributed tracing system, the trace bring us detailed info, but cost a lot at storage. Open server side trace sampling mechanism, the metrics of service, service instance, endpoint and topology are all accurate as before, but only don\u0026rsquo;t save all the traces into storage.\nOf course, even you open sampling, the traces will be kept as consistent as possible. Consistent means, once the trace segments have been collected and reported by agents, the backend would do their best to don\u0026rsquo;t break the trace. See Recommendation to understand why we called it as consistent as possible and do their best to don't break the trace.\nSet the sample rate In agent-analyzer module, you will find sampleRate setting.\nagent-analyzer: default: ... sampleRate: ${SW_TRACE_SAMPLE_RATE:1000} # The sample rate precision is 1/10000. 10000 means 100% sample in default. forceSampleErrorSegment: ${SW_FORCE_SAMPLE_ERROR_SEGMENT:true} # When sampling mechanism activated, this config would make the error status segment sampled, ignoring the sampling rate. sampleRate is for you to set sample rate to this backend. The sample rate precision is 1/10000. 10000 means 100% sample in default.\nforceSampleErrorSegment is for you to open force save some error segment when sampling mechanism active. When sampling mechanism activated, this config would make the error status segment sampled, ignoring the sampling rate.\nRecommendation You could set different backend instances with different sampleRate values, but we recommend you to set the same.\nWhen you set the rate different, let\u0026rsquo;s say\n Backend-InstanceA.sampleRate = 35 Backend-InstanceB.sampleRate = 55  And we assume the agents reported all trace segments to backend, Then the 35% traces in the global will be collected and saved in storage consistent/complete, with all spans. 20% trace segments, which reported to Backend-InstanceB, will saved in storage, maybe miss some trace segments, because they are reported to Backend-InstanceA and ignored.\nNote When you open sampling, the actual sample rate could be over sampleRate. Because currently, all error segments will be saved, meanwhile, the upstream and downstream may not be sampled. This feature is going to make sure you could have the error stacks and segments, but don\u0026rsquo;t guarantee you would have the whole trace.\nAlso, the side effect would be, if most of the accesses are fail, the sampling rate would be closing to 100%, which could crash the backend or storage clusters.\n","excerpt":"Trace Sampling at server side When we run a distributed tracing system, the trace bring us detailed …","ref":"/docs/main/v8.2.0/en/setup/backend/trace-sampling/","title":"Trace Sampling at server side"},{"body":"TTL In SkyWalking, there are two types of observability data, besides metadata.\n Record, including trace and alarm. Maybe log in the future. Metric, including such as percentile, heat map, success rate, cpm(rpm) etc.  You have following settings for different types.\n# Set a timeout on metrics data. After the timeout has expired, the metrics data will automatically be deleted. recordDataTTL: ${SW_CORE_RECORD_DATA_TTL:3} # Unit is day metricsDataTTL: ${SW_CORE_METRICS_DATA_TTL:7} # Unit is day  recordDataTTL affects Record data, including tracing and alarm. metricsDataTTL affects all metrics, including service, instance, endpoint metrics and topology map metrics.  ","excerpt":"TTL In SkyWalking, there are two types of observability data, besides metadata.\n Record, including …","ref":"/docs/main/v8.2.0/en/setup/backend/ttl/","title":"TTL"},{"body":"UI SkyWalking UI distribution is already included in our Apache official release.\nStartup Startup script is also in /bin/webappService.sh(.bat). UI runs as an OS Java process, powered-by Zuul.\nSettings Setting file of UI is webapp/webapp.yml in distribution package. It is constituted by three parts.\n Listening port. Backend connect info.  server: port: 8080 collector: path: /graphql ribbon: ReadTimeout: 10000 # Point to all backend\u0026#39;s restHost:restPort, split by ,  listOfServers: 10.2.34.1:12800,10.2.34.2:12800 ","excerpt":"UI SkyWalking UI distribution is already included in our Apache official release.\nStartup Startup …","ref":"/docs/main/v8.2.0/en/setup/backend/ui-setup/","title":"UI"},{"body":"UI Introduction SkyWalking official UI provides the default and powerful visualization capabilities for SkyWalking observing distributed cluster.\nThe latest introduction video could be found on the Youtube\n\nSkyWalking dashboard includes the following part.\n Feature Tab Selector Zone. The key features are list there. The more details will be introduced below. Reload Zone. Control the reload mechanism, including reload periodically or manually. Time Selector Zone. Control the timezone and time range. And a Chinese/English switch button here, default, the UI uses the browser language setting. We also welcome to contribute more languages.  Dashboard Dashboard provide metrics of service, service instance and endpoint. There are a few metrics terms you need to understand\n Throughput CPM , represents calls per minute. Apdex score, Read Apdex in WIKI Response Time Percentile, including p99, p95, p90, p75, p50. Read percentile in WIKI SLA, represents the successful rate. For HTTP, it means the rate of 200 response code.  Service, Instance and Dashboard selector could reload manually rather than reload the whole page. NOTICE, the Reload Zone wouldn\u0026rsquo;t reload these selectors.\nTwo default dashboards are provided to visualize the metrics of service and database.\nUser could click the lock button left aside the Service/Instance/Endpoint Reload button to custom your own dashboard.\nCustom Dashboard Users could customize the dashboard. The default dashboards are provided through the default templates located in /ui-initialized-templates folders.\nThe template file follows this format.\ntemplates: - name: template name # The unique name # The type includes DASHBOARD, TOPOLOGY_INSTANCE, TOPOLOGY_ENDPOINT. # DASHBOARD type templates could have multiple definitions, by using different names. # TOPOLOGY_INSTANCE, TOPOLOGY_ENDPOINT type templates should be defined once,  # as they are used in the topology page only. type: \u0026#34;DASHBOARD\u0026#34; # Custom the dashboard or create a new one on the UI, set the metrics as you like in the edit mode. # Then, you could export this configuration through the page and add it here. configuration: |-[ { \u0026#34;name\u0026#34;:\u0026#34;Spring Sleuth\u0026#34;, \u0026#34;type\u0026#34;:\u0026#34;service\u0026#34;, \u0026#34;children\u0026#34;:[ { \u0026#34;name\u0026#34;:\u0026#34;Sleuth\u0026#34;, \u0026#34;children\u0026#34;: [{ \u0026#34;width\u0026#34;: \u0026#34;3\u0026#34;, \u0026#34;title\u0026#34;: \u0026#34;HTTP Request\u0026#34;, \u0026#34;height\u0026#34;: \u0026#34;200\u0026#34;, \u0026#34;entityType\u0026#34;: \u0026#34;ServiceInstance\u0026#34;, \u0026#34;independentSelector\u0026#34;: false, \u0026#34;metricType\u0026#34;: \u0026#34;REGULAR_VALUE\u0026#34;, \u0026#34;metricName\u0026#34;: \u0026#34;meter_http_server_requests_count\u0026#34;, \u0026#34;queryMetricType\u0026#34;: \u0026#34;readMetricsValues\u0026#34;, \u0026#34;chartType\u0026#34;: \u0026#34;ChartLine\u0026#34;, \u0026#34;unit\u0026#34;: \u0026#34;Count\u0026#34; } ... ] } ] } ] # Activated means this templates added into the UI page automatically. # False means providing a basic template, user needs to add it manually on the page. activated: false # True means wouldn\u0026#39;t show up on the dashboard. Only keeps the definition in the storage. disabled: false NOTE, UI initialized templates would only be initialized if there is no template in the storage has the same name. Check the entity named as ui_template in your storage.\nTopology Topology map shows the relationship among the services and instances with metrics.\n Topology shows the default global topology including all services. Service Selector supports to show direct relationships including upstream and downstream. Custom Group provides the any sub topology capability of service group. Service Deep Dive opens when you click any service. The honeycomb could do metrics, trace and alarm query of the selected service. Service Relationship Metrics gives the metrics of service RPC interactions and instances of these two services.  Trace Query Trace query is a typical feature as SkyWalking provided distributed agents.\n Trace Segment List is not the trace list. Every trace has several segments belonging to different services. If\nquery by all services or by trace id, different segments with same trace id could be list there. Span is clickable, the detail of each span will pop up at the left side. Trace Views provides 3 typical and different usage views to visualize the trace.  Profile Profile is an interaction feature. It provides the method level performance diagnosis.\nTo start the profile analysis, user need to create the profile task\n Select the specific service. Set the endpoint name. This endpoint name typically is the operation name of the first span. Find this on the trace segment list view. Monitor time could start right now or from any given future time. Monitor duration defines the observation time window to find the suitable request to do performance analysis. Even the profile add a very limited performance impact to the target system, but it is still an additional load. This duration make the impact controllable. Min duration threshold provides a filter mechanism, if a request of the given endpoint response quickly, it wouldn\u0026rsquo;t be profiled. This could make sure, the profiled data is the expected one. Max sampling count gives the max dataset of agent will collect. It helps to reduce the memory and network load. One implicit condition, in any moment, SkyWalking only accept one profile task for each service. Agent could have different settings to control or limit this feature, read document setup for more details. Not all SkyWalking ecosystem agent supports this feature, java agent from 7.0.0 supports this in default.  Once the profile done, the profiled trace segments would show up. And you could request for analysis for any span. Typically, we analysis spans having long self duration, if the span and its children both have long duration, you could choose include children or exclude childrend to set the analysis boundaries.\nAfter choose the right span, and click the analysis button, you will see the stack based analysis result. The slowest methods have been highlighted.\nAdvanced features  Since 7.1.0, the profiled trace collects the HTTP request parameters for Tomcat and SpringMVC Controller automatically.  Alarm Alarm page lists all triggered alarm. Read the backend setup documentation to know how to set up the alarm rule or integrate with 3rd party system.\n","excerpt":"UI Introduction SkyWalking official UI provides the default and powerful visualization capabilities …","ref":"/docs/main/v8.2.0/en/ui/readme/","title":"UI Introduction"},{"body":"Uninstrumented Gateways/Proxies The uninstrumented gateways are not instrumented by SkyWalking agent plugin when they are started, but they can be configured in gateways.yml file or via Dynamic Configuration. The reason why they can\u0026rsquo;t register to backend automatically is that there\u0026rsquo;re no suitable agent plugins, for example, there is no agent plugins for Nginx, haproxy, etc. So in order to visualize the real topology, we provide a way to configure the gateways/proxies manually.\nConfiguration Format The configuration content includes the gateways' names and their instances:\ngateways: - name: proxy0 # the name is not used for now instances: - host: 127.0.0.1 # the host/ip of this gateway instance port: 9099 # the port of this gateway instance, defaults to 80 Note that the host of the instance must be the one that is actually used in client side, for example, if the instance proxyA has 2 IPs, say 192.168.1.110 and 192.168.1.111, both of which delegates the target service, and the client connects to 192.168.1.110, then configuring 192.168.1.111 as the host won\u0026rsquo;t work properly.\n","excerpt":"Uninstrumented Gateways/Proxies The uninstrumented gateways are not instrumented by SkyWalking agent …","ref":"/docs/main/v8.2.0/en/setup/backend/uninstrumented-gateways/","title":"Uninstrumented Gateways/Proxies"},{"body":"Using E2E local remote debugging The E2E remote debugging port of service containers is 5005. If the developer wants to use remote debugging, he needs to add remote debugging parameters to the start service command, and then expose the port 5005.\nFor example, this is the configuration of a container in the skywalking/test/e2e/e2e-test/docker/base-compose.yml. JAVA_OPTS is a preset variable for passing additional parameters in the AOP service startup command, so we only need to add the JAVA remote debugging parameters agentlib:jdwp=transport=dt_socket,server=y,suspend=n,address=5005 to the configuration and exposes the port 5005.\noap: image: skywalking/oap:latest expose: ... - 5005 ... environment: ... JAVA_OPTS: \u0026gt;-... -agentlib:jdwp=transport=dt_socket,server=y,suspend=n,address=5005 ... At last, if the E2E test failed and is retrying, the developer can get the ports mapping in the file skywalking/test/e2e/e2e-test/remote_real_port and selects the host port of the corresponding service for remote debugging. For example,\n#remote_real_port #The remote debugging port on the host is 32783 oap-localhost:32783 #The remote debugging port on the host is 32782 provider-localhost:32782 ","excerpt":"Using E2E local remote debugging The E2E remote debugging port of service containers is 5005. If the …","ref":"/docs/main/v8.2.0/en/guides/e2e-local-remote-debug/","title":"Using E2E local remote debugging"},{"body":"V6 upgrade SkyWalking v6 is widely used in many production environments. Users may wants to upgrade to an old release to new. This is a guidance to tell users how to do that.\nNOTICE, the following ways are not the only ways to do upgrade.\nUse Canary Release Like all applications, SkyWalking could use canary release method to upgrade by following these steps\n Deploy a new cluster by using the latest(or new) version of SkyWalking OAP cluster with new database cluster. Once the target(being monitored) service has chance to upgrade the agent.jar(or just simply reboot), change the collector.backend_service pointing to the new OAP backend, and use/add a new namespace(agent.namespace in Table of Agent Configuration Properties). The namespace will avoid the conflict between different versions. When all target services have been rebooted, the old OAP clusters could be discarded.  Canary Release methods works for any version upgrade.\nOnline Hot Reboot Upgrade The reason we required Canary Release is, SkyWalking agent has cache mechanisms, switching to a new cluster makes the cache unavailable for new OAP cluster. In the 6.5.0+(especially for agent version), we have Agent hot reboot trigger mechanism. By using that, we could do upgrade an easier way, deploy a new cluster by using the latest(or new) version of SkyWalking OAP cluster with new database cluster, and shift the traffic to the new cluster once for all. Based on the mechanism, all agents will go into cool_down mode, then back online. More detail, read the backend setup document.\nNOTICE, as a known bug in 6.4.0, its agent could have re-connection issue, so, even this bot reboot mechanism included in 6.4.0, it may not work in some network scenarios, especially in k8s.\nAgent Compatibility All versions of SkyWalking 6.x(even 7.x) are compatible with each others, so users could only upgrade the OAP servers first. The agent is also enhanced from version to version, so from SkyWalking team\u0026rsquo;s recommendations, upgrade the agent once you have the chance.\n","excerpt":"V6 upgrade SkyWalking v6 is widely used in many production environments. Users may wants to upgrade …","ref":"/docs/main/v8.2.0/en/faq/v6-version-upgrade/","title":"V6 upgrade"},{"body":"V8 upgrade SkyWalking v8 begins to use v3 protocol, so, it is incompatible with previous releases. Users who intend to upgrade in v8 series releases could follow this guidance.\nRegister in v6 and v7 has been removed in v8 for better scaling out performance, please upgrade in the following ways.\n Use a different storage or a new namespace. Also, could consider erasing the whole storage index/table(s) related to SkyWalking. Deploy the whole SkyWalking cluster, and expose in a new network address. If you are using the language agents, upgrade the new agents too, meanwhile, make sure the agent has supported the different language. And set up the backend address to the new SkyWalking OAP cluster.  ","excerpt":"V8 upgrade SkyWalking v8 begins to use v3 protocol, so, it is incompatible with previous releases. …","ref":"/docs/main/v8.2.0/en/faq/v8-version-upgrade/","title":"V8 upgrade"},{"body":"Version 3.x -\u0026gt; 5.0.0-alpha Upgrade FAQs Collector Problem There is no information showing in the UI.\nCause In upgrate from 3.2.6 to 5.0.0, Elasticsearch indexes aren\u0026rsquo;t recreated, because not indexes exist, but aren\u0026rsquo;t compatible with 5.0.0-alpha. When service name registered, the es will create this column by default type string, which is wrong.\nSolution Clean the data folder in ElasticSearch and restart ElasticSearch, collector and your under monitoring application.\n","excerpt":"Version 3.x -\u0026gt; 5.0.0-alpha Upgrade FAQs Collector Problem There is no information showing in the …","ref":"/docs/main/v8.2.0/en/faq/v3-version-upgrade/","title":"Version 3.x -\u003e 5.0.0-alpha Upgrade FAQs"},{"body":"Visualization SkyWalking native UI provides the default visualization solution. It provides observability related graphs about overview, service, service instance, endpoint, trace and alarm, including topology, dependency graph, heatmap, etc.\nAlso, we have already known, many of our users have integrated SkyWalking into their products. If you want to do that too, please use SkyWalking query protocol.\n","excerpt":"Visualization SkyWalking native UI provides the default visualization solution. It provides …","ref":"/docs/main/v8.2.0/en/concepts-and-designs/ui-overview/","title":"Visualization"},{"body":"Welcome Here are SkyWalking 8 official documents. You\u0026rsquo;re welcome to join us.\nFrom here you can learn all about SkyWalking’s architecture, how to deploy and use SkyWalking, and develop based on SkyWalking contributions guidelines.\nNOTICE, SkyWalking 8 uses brand new tracing APIs, it is incompatible with all previous releases.\n  Concepts and Designs. You\u0026rsquo;ll find the the most important core ideas about SkyWalking. You can learn from here if you want to understand what is going on under our cool features and visualization.\n  Setup. Guides for installing SkyWalking in different scenarios. As a platform, it provides several ways of the observability.\n  UI Introduction. Introduce the UI usage and features.\n  Contributing Guides. Guides are for PMC member, committer or new contributor. Here, you can find how to start contributing.\n  Protocols. Protocols show the communication ways between agents/probes and backend. Anyone interested in uplink telemetry data should definitely read this.\n  FAQs. A manifest of already known setup problems, secondary developments experiments. When you are facing a problem, check here first.\n  In addition, you might find these links interesting:\n  The latest and old releases are all available at Apache SkyWalking release page. The change logs are here.\n  SkyWalking WIKI hosts the context of some changes and events.\n  Up-to-date overview of SkyWalking module call flow and hierarchy including ability to analize each module individually.\n  You can find the speaking schedules at Conf, online videos and articles about SkyWalking in Community resource catalog.\n  We\u0026rsquo;re always looking for help improving our documentation and codes, so please don’t hesitate to file an issue if you see any problem. Or better yet, submit your own contributions through pull request to help make them better.\n","excerpt":"Welcome Here are SkyWalking 8 official documents. You\u0026rsquo;re welcome to join us.\nFrom here you can …","ref":"/docs/main/v8.2.0/readme/","title":"Welcome"},{"body":"Why can\u0026rsquo;t I see any data in the UI? There are three main reasons no data can be shown by the UI:\n No traces have been sent to the collector. Traces have been sent, but the timezone of your containers is incorrect. Traces are in the collector, but you\u0026rsquo;re not watching the correct timeframe in the UI.  No traces Be sure to check the logs of your agents to see if they are connected to the collector and traces are being sent.\nIncorrect timezone in containers Be sure to check the time in your containers.\nThe UI isn\u0026rsquo;t showing any data Be sure to configure the timeframe shown by the UI.\n","excerpt":"Why can\u0026rsquo;t I see any data in the UI? There are three main reasons no data can be shown by the …","ref":"/docs/main/v8.2.0/en/faq/time-and-timezone/","title":"Why can't I see any data in the UI?"},{"body":"Why doesn\u0026rsquo;t SkyWalking involve MQ in the architecture? People usually ask about these questions when they know SkyWalking at the first time. They think MQ should be better in the performance and supporting high throughput, like the following\nHere are the reasons the SkyWalking\u0026rsquo;s opinions.\nIs MQ a good or right way to communicate with OAP backend? This question comes out when people think about what happens when the OAP cluster is not powerful enough or offline. But I want to ask the questions before answer this.\n Why do you think OAP should be not powerful enough? As it is not, the speed of data analysis wouldn\u0026rsquo;t catch up with producers(agents). Then what is the point of adding new deployment requirement? Maybe you will argue says, the payload is sometimes higher than usual as there is hot business time. But, my question is how much higher? If less than 40%, how many resources will you use for the new MQ cluster? How about moving them to new OAP and ES nodes? If higher than 40%, such as 70%-2x times? Then, I could say, your MQ wastes more resources than it saves. Your MQ would support 2x-3x payload, and with 10%-20% cost in general time. Furthermore, in this case, if the payload/throughput are so high, how long the OAP cluster could catch up. I would say never before it catches up, the next hot time event is coming.  Besides all this analysis, why do you want the traces still 100%, as you are costing so many resources? Better than this, we could consider adding a better dynamic trace sampling mechanism at the backend, when throughput goes over the threshold, active the sampling rate to 100%-\u0026gt;10% step by step, which means you could get the OAP and ES 3 times more powerful than usual, just ignore the traces at hot time.\nIs MQ transport acceptable even there are several side effects? Even MQ transport is not recommended from the production perspective, SkyWalking still has optional plugins named kafka-reporter and kafka-fetcher for this feature since 8.1.0.\nHow about MQ metrics data exporter? I would say, it is already available there. Exporter module with gRPC default mechanism is there. It is easy to provide a new implementor of that module.\n","excerpt":"Why doesn\u0026rsquo;t SkyWalking involve MQ in the architecture? People usually ask about these …","ref":"/docs/main/v8.2.0/en/faq/why_mq_not_involved/","title":"Why doesn't SkyWalking involve MQ in the architecture?"},{"body":"Why metrics indexes in Hour and Day precisions stop update after upgrade to 7.x? This is an expected case when 6.x-\u0026gt;7.x upgrade. Read Downsampling Data Packing feature of the ElasticSearch storage.\nThe users could simply delete all expired *-day_xxxxx and *-hour_xxxxx(xxxxx is a timestamp) indexes. SkyWalking is using metrics name-xxxxx and metrics name-month_xxxxx indexes only.\n","excerpt":"Why metrics indexes in Hour and Day precisions stop update after upgrade to 7.x? This is an expected …","ref":"/docs/main/v8.2.0/en/faq/hour-day-metrics-stopping/","title":"Why metrics indexes in Hour and Day precisions stop update after upgrade to 7.x?"},{"body":"Work with Istio Instructions for transport Istio\u0026rsquo;s metrics to SkyWalking OAP server.\nPrerequisites Istio should be installed in kubernetes cluster. Follow Istio getting start to finish it.\nDeploy Skywalking backend Follow the deploying backend in kubernetes to install oap server in kubernetes cluster.\nSetup Istio to send metrics to oap Our scripts are wrote based on Istio 1.3.3.\n Install Istio metric template  kubectl apply -f https://raw.githubusercontent.com/istio/istio/1.3.3/mixer/template/metric/template.yaml\nInstall SkyWalking adapter  kubectl apply -f skywalkingadapter.yml\nFind the skywalkingadapter.yml at here.\nNOTICE, due to Istio Mixer is default OFF, we recommend you to consider our ALS solution\n","excerpt":"Work with Istio Instructions for transport Istio\u0026rsquo;s metrics to SkyWalking OAP server. …","ref":"/docs/main/v8.2.0/en/setup/istio/readme/","title":"Work with Istio"},{"body":"SkyWalking 8.6.0 is released. Go to downloads page to find release tars. Changes by Version\nProject  Add OpenSearch as storage option. Upgrade Kubernetes Java client dependency to 11.0. Fix plugin test script error in macOS.  Java Agent  Add trace_segment_ref_limit_per_span configuration mechanism to avoid OOM. Improve GlobalIdGenerator performance. Add an agent plugin to support elasticsearch7. Add jsonrpc4j agent plugin. new options to support multi skywalking cluster use same kafka cluster(plugin.kafka.namespace) resolve agent has no retries if connect kafka cluster failed when bootstrap Add Seata in the component definition. Seata plugin hosts on Seata project. Extended Kafka plugin to properly trace consumers that have topic partitions directly assigned. Support Kafka consumer 2.8.0. Support print SkyWalking context to logs. Add MessageListener enhancement in pulsar plugin. fix a bug that spring-mvc set an error endpoint name if the controller class annotation implements an interface. Add an optional agent plugin to support mybatis. Add spring-cloud-gateway-3.x optional plugin. Add okhttp-4.x plugin. Fix NPE when thrift field is nested in plugin thrift Fix possible NullPointerException in agent\u0026rsquo;s ES plugin. Fix the conversion problem of float type in ConfigInitializer. Fixed part of the dynamic configuration of ConfigurationDiscoveryService that does not take effect under certain circumstances. Introduce method interceptor API v2 Fix ClassCast issue for RequestHolder/ResponseHolder. fixed jdk-threading-plugin memory leak. Optimize multiple field reflection operation in Feign plugin. Fix trace-ignore-plugin TraceIgnorePathPatterns can\u0026rsquo;t set empty value  OAP-Backend  BugFix: filter invalid Envoy access logs whose socket address is empty. Fix K8s monitoring the incorrect metrics calculate. Loop alarm into event system. Support alarm tags. Support WeLink as a channel of alarm notification. Fix: Some defensive codes didn\u0026rsquo;t work in PercentileFunction combine. CVE: fix Jetty vulnerability. https://nvd.nist.gov/vuln/detail/CVE-2019-17638 Fix: MAL function would miss samples name after creating new samples. perf: use iterator.remove() to remove modulesWithoutProvider Support analyzing Envoy TCP access logs and persist error TCP logs. Fix: Envoy error logs are not persisted when no metrics are generated Fix: Memory leakage of low version etcd client. fix-issue Allow multiple definitions as fallback in metadata-service-mapping.yaml file and k8sServiceNameRule. Fix: NPE when configmap has no data. Fix: Dynamic Configuration key slowTraceSegmentThreshold not work Fix: != is not supported in oal when parameters are numbers. Include events of the entity(s) in the alarm. Support native-json format log in kafka-fetcher-plugin. Fix counter misuse in the alarm core. Alarm can\u0026rsquo;t be triggered in time. Events can be configured as alarm source. Make the number of core worker in meter converter thread pool configurable. Add HTTP implementation of logs reporting protocol. Make metrics exporter still work even when storage layer failed. Fix Jetty HTTP TRACE issue, disable HTTP methods except POST. CVE: upgrade snakeyaml to prevent billion laughs attack in dynamic configuration. polish debug logging avoids null value when the segment ignored.  UI  Add logo for kong plugin. Add apisix logo. Refactor js to ts for browser logs and style change. When creating service groups in the topology, it is better if the service names are sorted. Add tooltip for dashboard component. Fix style of endpoint dependency. Support search and visualize alarms with tags. Fix configurations on dashboard. Support to configure the maximum number of displayed items. After changing the durationTime, the topology shows the originally selected group or service. remove the no use maxItemNum for labeled-value metric, etc. Add Azure Functions logo. Support search Endpoint use keyword params in trace view. Add a function which show the statistics infomation during the trace query. Remove the sort button at the column of Type in the trace statistics page. Optimize the APISIX icon in the topology. Implement metrics templates in the topology. Visualize Events on the alarm page. Update duration steps in graphs for Trace and Log.  Documentation  Polish k8s monitoring otel-collector configuration example. Print SkyWalking context to logs configuration example. Update doc about metrics v2 APIs.  All issues and pull requests are here\n","excerpt":"SkyWalking 8.6.0 is released. Go to downloads page to find release tars. Changes by Version\nProject …","ref":"/events/release-apache-skywalking-apm-8-6-0/","title":"Release Apache SkyWalking APM 8.6.0"},{"body":"Apache SkyWalking Satellite Release Guide This documentation guides the release manager to release the SkyWalking Satellite in the Apache Way, and also helps people to check the release for vote.\nPrerequisites  Close(if finished, or move to next milestone otherwise) all issues in the current milestone from skywalking-satellite and skywalking, create a new milestone if needed. Update CHANGES.md.  Add your GPG public key to Apache svn   Upload your GPG public key to a public GPG site, such as MIT\u0026rsquo;s site.\n  Log in id.apache.org and submit your key fingerprint.\n  Add your GPG public key into SkyWalking GPG KEYS file, you can do this only if you are a PMC member. You can ask a PMC member for help. DO NOT override the existed KEYS file content, only append your key at the end of the file.\n  Build and sign the source code package export VERSION=\u0026lt;the version to release\u0026gt; git clone git@github.com:apache/skywalking-satellite \u0026amp;\u0026amp; cd skywalking-satellite git tag -a \u0026#34;$VERSION\u0026#34; -m \u0026#34;Release Apache SkyWalking-Satellite $VERSION\u0026#34; git push --tags make release In total, six files should be automatically generated in the directory: skywalking-satellite-${VERSION}-bin.tgz, skywalking-satellite-${VERSION}-src.tgz, and their corresponding asc, sha512 files.\nUpload to Apache svn svn co https://dist.apache.org/repos/dist/dev/skywalking/ mkdir -p skywalking/satellite/\u0026#34;$VERSION\u0026#34; cp skywalking-satellite/skywalking*.tgz skywalking/satellite/\u0026#34;$VERSION\u0026#34; cp skywalking-satellite/skywalking*.tgz.asc skywalking/satellite/\u0026#34;$VERSION\u0026#34; cp skywalking-satellite/skywalking-satellite*.tgz.sha512 skywalking/satellite/\u0026#34;$VERSION\u0026#34; cd skywalking/satellite \u0026amp;\u0026amp; svn add \u0026#34;$VERSION\u0026#34; \u0026amp;\u0026amp; svn commit -m \u0026#34;Draft Apache SkyWalking-Satellite release $VERSION\u0026#34; Make the internal announcement Send an announcement email to dev@ mailing list, please check all links before sending the email.\nSubject: [ANNOUNCEMENT] SkyWalking Satellite $VERSION test build available Content: The test build of SkyWalking Satellite $VERSION is now available. We welcome any comments you may have, and will take all feedback into account if a quality vote is called for this build. Release notes: * https://github.com/apache/skywalking-satellite/blob/$VERSION/CHANGES.md Release Candidate: * https://dist.apache.org/repos/dist/dev/skywalking/satellite/$VERSION * sha512 checksums - sha512xxxxyyyzzz apache-skywalking-satellite-bin-x.x.x.tgz - sha512xxxxyyyzzz apache-skywalking-satellite-src-x.x.x.tgz Release Tag : * (Git Tag) v$VERSION Release Commit Hash : * https://github.com/apache/skywalking-satellite/tree/\u0026lt;Git Commit Hash\u0026gt; Keys to verify the Release Candidate : * http://pgp.mit.edu:11371/pks/lookup?op=get\u0026amp;search=0x8BD99F552D9F33D7 corresponding to kezhenxu94@apache.org Guide to build the release from source : * https://github.com/apache/skywalking-satellite/blob/$VERSION/docs/en/guides/contribuation/How-to-release.md A vote regarding the quality of this test build will be initiated within the next couple of days. Wait at least 48 hours for test responses Any PMC, committer or contributor can test features for releasing, and feedback. Based on that, PMC will decide whether to start a vote or not.\nCall for vote in dev@ mailing list Call for vote in dev@skywalking.apache.org, please check all links before sending the email.\nSubject: [VOTE] Release Apache SkyWalking Satellite version $VERSION Content: Hi the SkyWalking Community: This is a call for vote to release Apache SkyWalking Satellite version $VERSION. Release notes: * https://github.com/apache/skywalking-satellite/blob/$VERSION/CHANGES.md Release Candidate: * https://dist.apache.org/repos/dist/dev/skywalking/satellite/$VERSION * sha512 checksums - sha512xxxxyyyzzz skywalking-satellite-x.x.x-src.tgz - sha512xxxxyyyzzz skywalking-satellite-x.x.x-bin.tgz Release Tag : * (Git Tag) v$VERSION Release Commit Hash : * https://github.com/apache/skywalking-satellite/tree/\u0026lt;Git Commit Hash\u0026gt; Keys to verify the Release Candidate : * https://dist.apache.org/repos/dist/release/skywalking/KEYS Guide to build the release from source : * https://github.com/apache/skywalking-satellite/blob/$VERSION/docs/en/guides/contribuation/How-to-release.md Voting will start now and will remain open for at least 72 hours, all PMC members are required to give their votes. [ ] +1 Release this package. [ ] +0 No opinion. [ ] -1 Do not release this package because.... Thanks. [1] https://github.com/apache/skywalking/blob/master/docs/en/guides/How-to-release.md#vote-check Vote Check All PMC members and committers should check these before voting +1:\n Features test. All artifacts in staging repository are published with .asc, .md5, and sha files. Source codes and distribution packages (skywalking-satellite-$VERSION-{src,bin}.tgz) are in https://dist.apache.org/repos/dist/dev/skywalking/satellite/$VERSION with .asc, .sha512. LICENSE and NOTICE are in source codes and distribution package. Check shasum -c skywalking-satellite-$VERSION-{src,bin}.tgz.sha512. Check gpg --verify skywalking-satellite-$VERSION-{src,bin}.tgz.asc skywalking-satellite-$VERSION-{src,bin}.tgz. Build distribution from source code package by following this command, make build. Licenses check, make license.  Vote result should follow these:\n  PMC vote is +1 binding, all others is +1 no binding.\n  Within 72 hours, you get at least 3 (+1 binding), and have more +1 than -1. Vote pass.\n  Send the closing vote mail to announce the result. When count the binding and no binding votes, please list the names of voters. An example like this:\n[RESULT][VOTE] Release Apache SkyWalking Satellite version $VERSION 3 days passed, we’ve got ($NUMBER) +1 bindings (and ... +1 non-bindings): (list names) +1 bindings: xxx ... +1 non-bindings: xxx ... Thank you for voting, I’ll continue the release process.   Publish release   Move source codes tar balls and distributions to https://dist.apache.org/repos/dist/release/skywalking/, you can do this only if you are a PMC member.\nexport SVN_EDITOR=vim svn mv https://dist.apache.org/repos/dist/dev/skywalking/satellite/$VERSION https://dist.apache.org/repos/dist/release/skywalking/satellite   Refer to the previous PR, update the event and download links on the website.\n  Update Github release page, follow the previous convention.\n  Send ANNOUNCE email to dev@skywalking.apache.org and announce@apache.org, the sender should use his/her Apache email account, please check all links before sending the email.\nSubject: [ANNOUNCEMENT] Apache SkyWalking Satellite $VERSION Released Content: Hi the SkyWalking Community On behalf of the SkyWalking Team, I’m glad to announce that SkyWalking Satellite $VERSION is now released. SkyWalking Satellite: A lightweight collector/sidecar could be deployed closing to the target monitored system, to collect metrics, traces, and logs. SkyWalking: APM (application performance monitor) tool for distributed systems, especially designed for microservices, cloud native and container-based (Docker, Kubernetes, Mesos) architectures. Download Links: http://skywalking.apache.org/downloads/ Release Notes : https://github.com/apache/skywalking-satellite/blob/$VERSION/CHANGES.md Website: http://skywalking.apache.org/ SkyWalking Satellite Resources: - Issue: https://github.com/apache/skywalking/issues - Mailing list: dev@skywalking.apache.org - Documents: https://github.com/apache/skywalking-satellite/blob/$VERSION/README.md The Apache SkyWalking Team   Remove Unnecessary Releases Please remember to remove all unnecessary releases in the mirror svn (https://dist.apache.org/repos/dist/release/skywalking/), if you don\u0026rsquo;t recommend users to choose those version. For example, you have removed the download and documentation links from the website. If they want old ones, the Archive repository has all of them.\n","excerpt":"Apache SkyWalking Satellite Release Guide This documentation guides the release manager to release …","ref":"/docs/skywalking-satellite/latest/en/guides/contribution/how-to-release/","title":"Apache SkyWalking Satellite Release Guide"},{"body":"Client/grpc-client Description The gRPC client is a sharing plugin to keep connection with the gRPC server and delivery the data to it.\nDefaultConfig # The gRPC server address (default localhost:11800).  server_addr: localhost:11800 # The TLS switch (default false). enable_TLS: false # The file path of client.pem. The config only works when opening the TLS switch. client_pem_path: \u0026#34;\u0026#34; # The file path of client.key. The config only works when opening the TLS switch. client_key_path: \u0026#34;\u0026#34; # The file path oca.pem. The config only works when opening the TLS switch. ca_pem_path: \u0026#34;\u0026#34; # InsecureSkipVerify controls whether a client verifies the server\u0026#39;s certificate chain and host name. insecure_skip_verify: true # The auth value when send request authentication: \u0026#34;\u0026#34; # How frequently to check the connection check_period: 5 ","excerpt":"Client/grpc-client Description The gRPC client is a sharing plugin to keep connection with the gRPC …","ref":"/docs/skywalking-satellite/latest/en/setup/plugins/client_grpc-client/","title":"Client/grpc-client"},{"body":"Client/kafka-client Description The Kafka client is a sharing plugin to keep connection with the Kafka brokers and delivery the data to it.\nDefaultConfig # The Kafka broker addresses (default localhost:9092). Multiple values are separated by commas. brokers: localhost:9092 # The Kakfa version should follow this pattern, which is major_minor_veryMinor_patch (default 1.0.0.0). version: 1.0.0.0 # The TLS switch (default false). enable_TLS: false # The file path of client.pem. The config only works when opening the TLS switch. client_pem_path: \u0026#34;\u0026#34; # The file path of client.key. The config only works when opening the TLS switch. client_key_path: \u0026#34;\u0026#34; # The file path oca.pem. The config only works when opening the TLS switch. ca_pem_path: \u0026#34;\u0026#34; # 0 means NoResponse, 1 means WaitForLocal and -1 means WaitForAll (default 1). required_acks: 1 # The producer max retry times (default 3). producer_max_retry: 3 # The meta max retry times (default 3). meta_max_retry: 3 # How long to wait for the cluster to settle between retries (default 100ms). Time unit is ms. retry_backoff: 100 # The max message bytes. max_message_bytes: 1000000 # If enabled, the producer will ensure that exactly one copy of each message is written (default false). idempotent_writes: false # A user-provided string sent with every request to the brokers for logging, debugging, and auditing purposes (default Satellite). client_id: Satellite # Compression codec represents the various compression codecs recognized by Kafka in messages. 0 : None, 1 : Gzip, 2 : Snappy, 3 : LZ4, 4 : ZSTD compression_codec: 0 # How frequently to refresh the cluster metadata in the background. Defaults to 10 minutes. The unit is minute. refresh_period: 10 # InsecureSkipVerify controls whether a client verifies the server\u0026#39;s certificate chain and host name. insecure_skip_verify: true ","excerpt":"Client/kafka-client Description The Kafka client is a sharing plugin to keep connection with the …","ref":"/docs/skywalking-satellite/latest/en/setup/plugins/client_kafka-client/","title":"Client/kafka-client"},{"body":"Common configuration The common configuration has 2 parts, which are logger configuration and the telemetry configuration.\nLogger    Config Default Description     log_pattern %time [%level][%field] - %msg The log format pattern configuration.   time_pattern 2006-01-02 15:04:05.000 The time format pattern configuration.   level info The lowest level of printing allowed.    Self Telemetry    Config Default Description     cluster default-cluster The space concept for the deployment, such as the namespace concept in the Kubernetes.   service default-service The group concept for the deployment, such as the service resource concept in the Kubernetes.   instance default-instance The minimum running unit, such as the pod concept in the Kubernetes.    ","excerpt":"Common configuration The common configuration has 2 parts, which are logger configuration and the …","ref":"/docs/skywalking-satellite/latest/en/setup/configuration/common/","title":"Common configuration"},{"body":"Compiling Platform Linux, MacOS and Windows are supported in SkyWalking Satellite. However, some components don\u0026rsquo;t fit the Windows platform, including:\n mmap-queue  Command git clone https://github.com/apache/skywalking-satellite cd skywalking-satellite make build ","excerpt":"Compiling Platform Linux, MacOS and Windows are supported in SkyWalking Satellite. However, some …","ref":"/docs/skywalking-satellite/latest/en/guides/compile/how-to-compile/","title":"Compiling"},{"body":"Concepts and Designs Concepts and Designs help you to learn and understand the SkyWalking Satellite and the landscape.\n What is SkyWalking Satellite?  Overview and Core concepts. Provides a high-level description and introduction, including the problems the project solves. Project Goals. Provides the goals, which SkyWalking Satellite is trying to focus and provide features about them.    After you read the above documents, you should understand basic goals of the SkyWalking Satellite. Now, you can choose which following parts you are interested, then dive in.\n Module Design Plugin mechanism Project Structure The design of the memory mapped queue  ","excerpt":"Concepts and Designs Concepts and Designs help you to learn and understand the SkyWalking Satellite …","ref":"/docs/skywalking-satellite/latest/en/concepts-and-designs/readme/","title":"Concepts and Designs"},{"body":"Design The mmap-queue is a big, fast, and persistent queue based on the memory-mapped files. One mmap-queue has a directory to store the whole data. The queue directory is made up of many segments and 1 metafile. This is originally implemented by bigqueue project, we changed it a little for fitting the Satellite project requirements.\n Segment: Segment is the real data store center, that provides large-space storage and does not reduce read and write performance as much as possible by using mmap. And we will avoid deleting files by reusing them. Meta: The purpose of meta is to find the data that the consumer needs.  Meta Metadata only needs 80B to store the Metadata for the pipe. But for memory alignment, it takes at least one memory page size, which is generally 4K.\n[ 8Bit ][ 8Bit ][ 8Bit ][ 8Bit ][ 8Bit ][ 8Bit ][ 8Bit ][ 8Bit ][ 8Bit ][ 8Bit ] [metaVersion][ ID ][ offset][ ID ][ offset][ ID ][ offset][ ID ][ offset][capacity] [metaVersion][writing offset][watermark offset][committed offset][reading offset][capacity] Transforming BenchmarkTest Test machine: macbook pro 2018\nModel Name:\tMacBook Pro Model Identifier:\tMacBookPro15,1 Processor Name:\t6-Core Intel Core i7 Processor Speed:\t2.2 GHz Number of Processors:\t1 Total Number of Cores:\t6 L2 Cache (per Core):\t256 KB L3 Cache:\t9 MB Hyper-Threading Technology:\tEnabled Memory:\t16 GB System Firmware Version:\t1554.60.15.0.0 (iBridge: 18.16.13030.0.0,0 push operation goos: darwin goarch: amd64 pkg: github.com/apache/skywalking-satellite/plugins/queue/mmap BenchmarkEnqueue BenchmarkEnqueue/segmentSize:_128KB_maxInMemSegments:18_message:8KB_queueCapacity:10000 27585\t43559 ns/op\t9889 B/op\t9 allocs/op BenchmarkEnqueue/segmentSize:_256KB_maxInMemSegments:10_message:8KB_queueCapacity:10000 39326\t31773 ns/op\t9840 B/op\t9 allocs/op BenchmarkEnqueue/segmentSize:_512KB_maxInMemSegments:6_message:8KB_queueCapacity:10000 56770\t22990 ns/op\t9816 B/op\t9 allocs/op BenchmarkEnqueue/segmentSize:_256KB_maxInMemSegments:20_message:8KB_queueCapacity:10000 43803\t29778 ns/op\t9840 B/op\t9 allocs/op BenchmarkEnqueue/segmentSize:_128KB_maxInMemSegments:10_message:16KB_queueCapacity:10000 16870\t80576 ns/op\t18944 B/op\t10 allocs/op BenchmarkEnqueue/segmentSize:_128KB_maxInMemSegments:10_message:8KB_queueCapacity:100000 36922\t39085 ns/op\t9889 B/op\t9 allocs/op PASS push and pop operation goos: darwin goarch: amd64 pkg: github.com/apache/skywalking-satellite/plugins/queue/mmap BenchmarkEnqueueAndDequeue BenchmarkEnqueueAndDequeue/segmentSize:_128KB_maxInMemSegments:18_message:8KB_queueCapacity:10000 21030\t60728 ns/op\t28774 B/op\t42 allocs/op BenchmarkEnqueueAndDequeue/segmentSize:_256KB_maxInMemSegments:10_message:8KB_queueCapacity:10000 30327\t41274 ns/op\t28726 B/op\t42 allocs/op BenchmarkEnqueueAndDequeue/segmentSize:_512KB_maxInMemSegments:6_message:8KB_queueCapacity:10000 32738\t37923 ns/op\t28700 B/op\t42 allocs/op BenchmarkEnqueueAndDequeue/segmentSize:_256KB_maxInMemSegments:20_message:8KB_queueCapacity:10000 28209\t41169 ns/op\t28726 B/op\t42 allocs/op BenchmarkEnqueueAndDequeue/segmentSize:_128KB_maxInMemSegments:10_message:16KB_queueCapacity:10000 14677\t89637 ns/op\t54981 B/op\t43 allocs/op BenchmarkEnqueueAndDequeue/segmentSize:_128KB_maxInMemSegments:10_message:8KB_queueCapacity:100000 22228\t54963 ns/op\t28774 B/op\t42 allocs/op PASS ","excerpt":"Design The mmap-queue is a big, fast, and persistent queue based on the memory-mapped files. One …","ref":"/docs/skywalking-satellite/latest/en/concepts-and-designs/mmap-queue/","title":"Design"},{"body":"Design Goals The document outlines the core design goals for SkyWalking Satellite project.\n  Light Weight. SkyWalking Satellite has a limited cost for resources and high-performance because of the requirements of the sidecar deployment model.\n  Pluggability. SkyWalking Satellite core team provides many default implementations, but definitely it is not enough, and also don\u0026rsquo;t fit every scenario. So, we provide a lot of features for being pluggable.\n  Portability. SkyWalking Satellite can run in multiple environments, including:\n Use traditional deployment as a daemon process to collect data. Use cloud services as a sidecar, such as in the Kubernetes platform.    Interoperability. Observability is a big landscape, SkyWalking is impossible to support all, even by its community. So SkyWalking Satellite is compatible with many protocols, including:\n SkyWalking protocol (WIP) Prometheus protocol.    ","excerpt":"Design Goals The document outlines the core design goals for SkyWalking Satellite project.\n  Light …","ref":"/docs/skywalking-satellite/latest/en/concepts-and-designs/project-goals/","title":"Design Goals"},{"body":"Fallbacker/none-fallbacker Description The fallbacker would do nothing when facing failure data.\nDefaultConfig yaml\n","excerpt":"Fallbacker/none-fallbacker Description The fallbacker would do nothing when facing failure data. …","ref":"/docs/skywalking-satellite/latest/en/setup/plugins/fallbacker_none-fallbacker/","title":"Fallbacker/none-fallbacker"},{"body":"Fallbacker/timer-fallbacker Description This is a timer fallback trigger to process the forward failure data.\nDefaultConfig # The forwarder max attempt times. max_attempts: 3 # The exponential_backoff is the standard retry duration, and the time for each retry is expanded # by 2 times until the number of retries reaches the maximum.(Time unit is millisecond.) exponential_backoff: 2000 # The max backoff time used in retrying, which would override the latency time when the latency time # with exponential increasing larger than it.(Time unit is millisecond.) max_backoff: 5000 ","excerpt":"Fallbacker/timer-fallbacker Description This is a timer fallback trigger to process the forward …","ref":"/docs/skywalking-satellite/latest/en/setup/plugins/fallbacker_timer-fallbacker/","title":"Fallbacker/timer-fallbacker"},{"body":"FAQ  What is the performance of the Satellite?  ","excerpt":"FAQ  What is the performance of the Satellite?  ","ref":"/docs/skywalking-satellite/latest/en/faq/readme/","title":"FAQ"},{"body":"Fetcher/prometheus-fetcher Description This is a Prometheus fetcher for SkyWalking meter format, which is defined at https://github.com/apache/skywalking-data-collect-protocol/blob/master/language-agent/Meter.proto.\nDefaultConfig ## Prometheus scrape configure scrape_configs: - job_name: \u0026#39;prometheus\u0026#39; metrics_path: \u0026#39;/metrics\u0026#39; scrape_interval: 10s static_configs: - targets: [\u0026#39;127.0.0.1:2020\u0026#39;] ","excerpt":"Fetcher/prometheus-fetcher Description This is a Prometheus fetcher for SkyWalking meter format, …","ref":"/docs/skywalking-satellite/latest/en/setup/plugins/fetcher_prometheus_fetcher/","title":"Fetcher/prometheus-fetcher"},{"body":"Forwarder/meter-grpc-forwarder Description This is a synchronization meter grpc forwarder with the SkyWalking meter protocol.\nDefaultConfig yaml\n","excerpt":"Forwarder/meter-grpc-forwarder Description This is a synchronization meter grpc forwarder with the …","ref":"/docs/skywalking-satellite/latest/en/setup/plugins/forwarder_meter-grpc-forwarder/","title":"Forwarder/meter-grpc-forwarder"},{"body":"Forwarder/nativelog-grpc-forwarder Description This is a synchronization grpc forwarder with the SkyWalking native log protocol.\nDefaultConfig yaml\n","excerpt":"Forwarder/nativelog-grpc-forwarder Description This is a synchronization grpc forwarder with the …","ref":"/docs/skywalking-satellite/latest/en/setup/plugins/forwarder_nativelog-grpc-forwarder/","title":"Forwarder/nativelog-grpc-forwarder"},{"body":"Forwarder/nativelog-kafka-forwarder Description This is a synchronization Kafka forwarder with the SkyWalking native log protocol.\nDefaultConfig # The remote topic.  topic: \u0026#34;log-topic\u0026#34; ","excerpt":"Forwarder/nativelog-kafka-forwarder Description This is a synchronization Kafka forwarder with the …","ref":"/docs/skywalking-satellite/latest/en/setup/plugins/forwarder_nativelog-kafka-forwarder/","title":"Forwarder/nativelog-kafka-forwarder"},{"body":"Guides If you want to debug or develop SkyWalking Satellite, The following documentations would guide you.\n Contribution  How to contribute a plugin? How to release SkyWalking Satellite?   Compile  How to compile SkyWalking Satellite?   Test  How to add unit test for a plugin?    ","excerpt":"Guides If you want to debug or develop SkyWalking Satellite, The following documentations would …","ref":"/docs/skywalking-satellite/latest/en/guides/readme/","title":"Guides"},{"body":"How to write a new plugin? If you want to add a custom plugin in SkyWalking Satellite, the following contents would guide you. Let\u0026rsquo;s use memory-queue as an example of how to write a plugin.\n  Choose the plugin category. As the memory-queue is a queue, the plugin should be written in the skywalking-satellite/plugins/queue directory. So we create a new directory called memory as the plugin codes space.\n  Implement the interface in the skywalking-satellite/plugins/queue/api. Each plugin has 3 common methods, which are Name(), Description(), DefaultConfig().\n Name() returns the unique name in the plugin category. Description() returns the description of the plugin, which would be used to generate the plugin documentation. DefaultConfig() returns the default plugin config with yaml pattern, which would be used as the default value in the plugin struct and to generate the plugin documentation.  type Queue struct { config.CommonFields // config  EventBufferSize int `mapstructure:\u0026#34;event_buffer_size\u0026#34;` // The maximum buffer event size.  // components  buffer *goconcurrentqueue.FixedFIFO } func (q *Queue) Name() string { return Name } func (q *Queue) Description() string { return \u0026#34;this is a memory queue to buffer the input event.\u0026#34; } func (q *Queue) DefaultConfig() string { return ` # The maximum buffer event size. event_buffer_size: 5000   Add unit test.\n  Generate the plugin docs.\n  make gen-docs ","excerpt":"How to write a new plugin? If you want to add a custom plugin in SkyWalking Satellite, the following …","ref":"/docs/skywalking-satellite/latest/en/guides/contribution/how-to-write-plugin/","title":"How to write a new plugin?"},{"body":"Module Design Pipe The pipe is an isolation concept in Satellite. Each pipe has one pipeline to process the telemetry data(metrics/traces/logs). Two pipes are not sharing data.\n Satellite --------------------------------------------------------------------- | ------------------------------------------- | | | Pipe | | | ------------------------------------------- | | ------------------------------------------- | | | Pipe | | | ------------------------------------------- | | ------------------------------------------- | | | Pipe | | | ------------------------------------------- | --------------------------------------------------------------------- Modules Module is the core workers in Satellite. Module is constituted by the specific extension plugins. There are 3 modules in one namespace, which are Gatherer, Processor, and Sender.\n The Gatherer module is responsible for fetching or receiving data and pushing the data to Queue. So there are 2 kinds of Gatherer, which are ReceiverGatherer and FetcherGatherer. The Processor module is responsible for reading data from the queue and processing data by a series of filter chains. The Sender module is responsible for async processing and forwarding the data to the external services in the batch mode. After sending success, Sender would also acknowledge the offset of Queue in Gatherer.   Pipe -------------------------------------------------------------------- | ---------- ----------- -------- | | | Gatherer | =\u0026gt; | Processor | =\u0026gt; | Sender | | | ---------- ----------- -------- | -------------------------------------------------------------------- LifeCycle\n Prepare: Prepare phase is to do some preparation works, such as register the client status listener to the client in ReceiverGatherer. Boot: Boot phase is to start the current module until receives a close signal. ShutDown: ShutDown phase is to close the used resources.  Plugins Plugin is the minimal components in the module. Sateliite has 2 plugin catalogs, which are sharing plugins and normal plugins.\n a sharing plugin instance could be sharing with multiple modules in the different pipes. a normal plugin instance is only be used in a fixed module of the fixed pipes.  Sharing plugin Nowadays, there are 2 kinds of sharing plugins in Satellite, which are server plugins and client plugins. The reason why they are sharing plugins is to reduce the resource cost in connection. Server plugins are sharing with the ReceiverGatherer modules in the different pipes to receive the external requests. And the client plugins is sharing with the Sender modules in the different pipes to connect with external services, such as Kafka and OAP.\n Sharing Server Sharing Client -------------------------------------------------------------------- | ------------------ ----------- -------- | | | ReceiverGatherer | =\u0026gt; | Processor | =\u0026gt; | Sender | | | ------------------ ----------- -------- | -------------------------------------------------------------------- -------------------------------------------------------------------- | ------------------ ----------- -------- | | | ReceiverGatherer | =\u0026gt; | Processor | =\u0026gt; | Sender | | | ------------------ ----------- -------- | -------------------------------------------------------------------- -------------------------------------------------------------------- | ------------------ ----------- -------- | | | ReceiverGatherer | =\u0026gt; | Processor | =\u0026gt; | Sender | | | ------------------ ----------- -------- | -------------------------------------------------------------------- Normal plugin There are 7 kinds of normal plugins in Satellite, which are Receiver, Fetcher, Queue, Parser, Filter, Forwarder, and Fallbacker.\n Receiver: receives the input APM data from the request. Fetcher: fetch the APM data by fetching. Queue: store the APM data to ensure the data stability. Parser: supports some ways to parse data, such parse a csv file. Filter: processes the APM data. Forwarder: forwards the APM data to the external receiver, such as Kafka and OAP. Fallbacker: supports some fallback strategies, such as timer retry strategy.   Gatherer Processor ------------------------------- ------------------------------------------- | ----------- --------- | | ----------- ----------- | | | Receiver | ==\u0026gt; | Queue | |==\u0026gt;| | Filter | ==\u0026gt; ... ==\u0026gt; | Filter | | | | /Fetcher | | Mem/File | | | ----------- ----------- | | ----------- ---------- | | || || | -------------------------------- | \\/\t\\/ | | --------------------------------------- | | | OutputEventContext | | | --------------------------------------- | ------------------------------------------- || \\/ Sender ------------------------------------------ | --- --- | | | B | | D | ----------------- | | | A | | I | |Segment Forwarder| | | | T | | S | | (Fallbacker) | | | | C | | P | ----------------- | | | H | =\u0026gt; | A | | ===\u0026gt; Kakfa/OAP | | B | | T | =\u0026gt; ...... | | | U | | C | | | | F | | H | ----------------- | | | F | | E | | Meter Forwarder| | | | E | | R | | (Fallbacker | | | | R | | | ----------------- | | --- --- | ------------------------------------------ 1. The Fetcher/Receiver plugin would fetch or receive the input data. 2. The Parser plugin would parse the input data to SerializableEvent that is supported to be stored in Queue. 3. The Queue plugin stores the SerializableEvent. However, whether serializing depends on the Queue implements. For example, the serialization is unnecessary when using a Memory Queue. Once an event is pulled by the consumer of Queue, the event will be processed by the filters in Processor. 4. The Filter plugin would process the event to create a new event. Next, the event is passed to the next filter to do the same things until the whole filters are performed. All created events would be stored in the OutputEventContext. However, only the events labeled with RemoteEvent type would be forwarded by Forwarder. 5. After processing, the events in OutputEventContext would be stored in the BatchBuffer. When the timer is triggered or the capacity limit is reached, the events in BatchBuffer would be partitioned by EventType and sent to the different Forwarders, such as Segment Forwarder and Meter Forwarder. 6. The Follower in different Senders would share with the remote client to avoid make duplicate connections and have the same Fallbacker(FallBack strategy) to process data. When all forwarders send success or process success in Fallbacker, the dispatcher would also ack the batch is a success. ============================================================================================ ","excerpt":"Module Design Pipe The pipe is an isolation concept in Satellite. Each pipe has one pipeline to …","ref":"/docs/skywalking-satellite/latest/en/concepts-and-designs/module_design/","title":"Module Design"},{"body":"Overview SkyWalking Satellite: an open-source agent designed for the cloud-native infrastructures, which provides a low-cost, high-efficient, and more secure way to collect telemetry data, such that Trace Segments, Logs, or Metrics.\nWhy use SkyWalking Satellite? Observability is the solution to the complex scenario of cloud-native services. However, we may encounter different telemetry data scenarios, different language services, big data analysis, etc. Satellite provides a unified data collection layer for cloud-native services. You can easily use it to connect to the SkyWalking ecosystem and enhance the capacity of SkyWalking. There are some enhance features on the following when using Satellite.\n Provide a unified data collection layer to collect logs, traces, and metrics. Provide a safer local cache to reduce the memory cost of the service. Provide the unified transfer way shields the functional differences in the different language libs, such as MQ. Provides the preprocessing functions to ensure accuracy of the metrics, such as sampling.  Architecture SkyWalking Satellite is logically split into three parts: Gatherer, Processor, and Sender.\n Gatherer collect data and reformat them for SkyWalking requirements. Processor processes the input data to generate the new data for Observability. Sender would transfer the downstream data to the SkyWalking OAP with different protocols.  ","excerpt":"Overview SkyWalking Satellite: an open-source agent designed for the cloud-native infrastructures, …","ref":"/docs/skywalking-satellite/latest/en/concepts-and-designs/overview/","title":"Overview"},{"body":"Pipe Plugins The pipe plugin configurations contain a series of pipe configuration. Each pipe configuration has 5 parts, which are common_config, gatherer, processor and the sender.\ncommon_config    Config Description     pipe_name The unique collect space name.    Gatherer The gatherer has 2 roles, which are the receiver and fetcher.\nReceiver Role    Config Description     server_name The server name in the sharing pipe, which would be used in the receiver plugin.   receiver The receiver configuration. Please read the doc to find all receiver plugins.   queue The queue buffers the input telemetry data. Please read the doc to find all queue plugins.    Fetcher Role    Config Description     fetch_interval The time interval between two fetch operations. The time unit is millisecond.   fetcher The fetcher configuration. Please read the doc to find all fetcher plugins.   queue The queue buffers the input telemetry data. Please read the doc to find all queue plugins.    processor The filter configuration. Please read the doc to find all filter plugins.\nsender    Config Description     flush_time The time interval between two flush operations. And the time unit is millisecond.   max_buffer_size The maximum buffer elements.   min_flush_events The minimum flush elements.   client_name The client name used in the forwarders of the sharing pipe.   forwarders The forwarder plugin list. Please read the doc to find all forwarders plugins.   fallbacker The fallbacker plugin. Please read the doc to find all fallbacker plugins.    Example pipes: - common_config: pipe_name: pipe1 gatherer: server_name: \u0026#34;grpc-server\u0026#34; receiver: plugin_name: \u0026#34;grpc-nativelog-receiver\u0026#34; queue: plugin_name: \u0026#34;mmap-queue\u0026#34; segment_size: ${SATELLITE_MMAP_QUEUE_SIZE:524288} max_in_mem_segments: ${SATELLITE_MMAP_QUEUE_MAX_IN_MEM_SEGMENTS:6} queue_dir: \u0026#34;pipe1-log-grpc-receiver-queue\u0026#34; processor: filters: sender: fallbacker: plugin_name: none-fallbacker flush_time: ${SATELLITE_PIPE1_SENDER_FLUSH_TIME:1000} max_buffer_size: ${SATELLITE_PIPE1_SENDER_MAX_BUFFER_SIZE:200} min_flush_events: ${SATELLITE_PIPE1_SENDER_MIN_FLUSH_EVENTS:100} client_name: kafka-client forwarders: - plugin_name: nativelog-kafka-forwarder topic: ${SATELLITE_NATIVELOG-TOPIC:log-topic} ","excerpt":"Pipe Plugins The pipe plugin configurations contain a series of pipe configuration. Each pipe …","ref":"/docs/skywalking-satellite/latest/en/setup/configuration/pipe-plugins/","title":"Pipe Plugins"},{"body":"Plugin List  Client  grpc-client kafka-client   Fallbacker  none-fallbacker timer-fallbacker   Fetcher  prometheus-fetcher   Filter Forwarder  meter-grpc-forwarder nativelog-grpc-forwarder nativelog-kafka-forwarder   Parser Queue  memory-queue mmap-queue   Receiver  grpc-nativelog-receiver http-nativelog-receiver   Server  grpc-server http-server prometheus-server    ","excerpt":"Plugin List  Client  grpc-client kafka-client   Fallbacker  none-fallbacker timer-fallbacker …","ref":"/docs/skywalking-satellite/latest/en/setup/plugins/plugin-list/","title":"Plugin List"},{"body":"plugin structure Plugin is a common concept for Satellite, which is in all externsion plugins.\nRegistration mechanism The Plugin registration mechanism in Satellite is similar to the SPI registration mechanism of Java. Plugin registration mechanism supports to register an interface and its implementation, that means different interfaces have different registration spaces. We can easily find the type of a specific plugin according to the interface and the plugin name and initialize it according to the type.\nstructure:\n code: map[reflect.Type]map[string]reflect.Value meaning: map[interface type]map[plugin name] plugin type  Initialization mechanism Users can easily find a plugin type and initialize an empty plugin instance according to the previous registration mechanism. For setting up the configuration of the extension convenience, we define the initialization mechanism in Plugin structure.\nIn the initialization mechanism, the plugin category(interface) and the init config is required.\nInitialize processing is like the following.\n Find the plugin name in the input config according to the fixed key plugin_name. Find plugin type according to the plugin category(interface) and the plugin name. Create an empty plugin. Initialize the plugin according to the merged config, which is created by the input config and the default config.  Plugin usage in Satellite Nowadays, the numbers of the Plugin categories is 2. One is the sharing Plugin, and another is the other normal Plugin.\n Extension Plugins:  sharing plugins  Server Plugin Client Plugin   normal plugins  Receiver Plugin Fetcher Plugin Parser Plugin Queue Plugin Filter Plugin Fallbacker Plugin Forwarder Plugin      ","excerpt":"plugin structure Plugin is a common concept for Satellite, which is in all externsion plugins. …","ref":"/docs/skywalking-satellite/latest/en/concepts-and-designs/plugin_mechanism/","title":"plugin structure"},{"body":"Project Structure  cmd: The starter of Satellite. configs: Satellite configs. internal: Core, API, and common utils.  internal/pkg: Sharing with Core and Plugins, such as api and utils. internal/satellite: The core of Satellite.   plugins: Contains all plugins.  plugins/{type}: Contains the plugins of this {type}. Satellite has 9 plugin types. plugins/{type}/api: Contains the plugin definition and initializer. plugins/{type}/{plugin-name}: Contains the specific plugin. init.go: Register the plugins to the plugin registry.    . ├── CHANGES.md ├── cmd ├── configs ├── docs ├── go.sum ├── internal │ ├── pkg │ └── satellite ├── plugins │ ├── client │ ├── fallbacker │ ├── fetcher │ ├── filter │ ├── forwarder │ ├── init.go │ ├── parser │ ├── queue │ ├── receiver │ └── server ","excerpt":"Project Structure  cmd: The starter of Satellite. configs: Satellite configs. internal: Core, API, …","ref":"/docs/skywalking-satellite/latest/en/concepts-and-designs/project_structue/","title":"Project Structure"},{"body":"Queue/memory-queue Description This is a memory queue to buffer the input event.\nDefaultConfig # The maximum buffer event size. event_buffer_size: 5000 ","excerpt":"Queue/memory-queue Description This is a memory queue to buffer the input event.\nDefaultConfig # The …","ref":"/docs/skywalking-satellite/latest/en/setup/plugins/queue_memory-queue/","title":"Queue/memory-queue"},{"body":"Queue/mmap-queue Description This is a memory mapped queue to provide the persistent storage for the input event. Please note that this plugin does not support Windows platform.\nDefaultConfig # The size of each segment. Default value is 256K. The unit is Byte. segment_size: 262114 # The max num of segments in memory. Default value is 10. max_in_mem_segments: 10 # The capacity of Queue = segment_size * queue_capacity_segments. queue_capacity_segments: 2000 # The period flush time. The unit is ms. Default value is 1 second. flush_period: 1000 # The max number in one flush time. Default value is 10000. flush_ceiling_num: 10000 # The max size of the input event. Default value is 20k. max_event_size: 20480 ","excerpt":"Queue/mmap-queue Description This is a memory mapped queue to provide the persistent storage for the …","ref":"/docs/skywalking-satellite/latest/en/setup/plugins/queue_mmap-queue/","title":"Queue/mmap-queue"},{"body":"Receiver/grpc-nativelog-receiver Description This is a receiver for SkyWalking native logging format, which is defined at https://github.com/apache/skywalking-data-collect-protocol/blob/master/logging/Logging.proto.\nDefaultConfig yaml\n","excerpt":"Receiver/grpc-nativelog-receiver Description This is a receiver for SkyWalking native logging …","ref":"/docs/skywalking-satellite/latest/en/setup/plugins/receiver_grpc-nativelog-receiver/","title":"Receiver/grpc-nativelog-receiver"},{"body":"Receiver/http-nativelog-receiver Description This is a receiver for SkyWalking http logging format, which is defined at https://github.com/apache/skywalking-data-collect-protocol/blob/master/logging/Logging.proto.\nDefaultConfig # The native log request URI. uri: \u0026#34;/logging\u0026#34; # The request timeout seconds. timeout: 5 ","excerpt":"Receiver/http-nativelog-receiver Description This is a receiver for SkyWalking http logging format, …","ref":"/docs/skywalking-satellite/latest/en/setup/plugins/receiver_http-nativelog-receiver/","title":"Receiver/http-nativelog-receiver"},{"body":"Server/grpc-server Description This is a sharing plugin, which would start a gRPC server.\nDefaultConfig # The address of grpc server. Default value is :11800 address: :11800 # The network of grpc. Default value is :tcp network: tcp # The max size of receiving log. Default value is 2M. The unit is Byte. max_recv_msg_size: 2097152 # The max concurrent stream channels. max_concurrent_streams: 32 # The TLS cert file path. tls_cert_file: \u0026#34;\u0026#34; # The TLS key file path. tls_key_file: \u0026#34;\u0026#34; ","excerpt":"Server/grpc-server Description This is a sharing plugin, which would start a gRPC server. …","ref":"/docs/skywalking-satellite/latest/en/setup/plugins/server_grpc-server/","title":"Server/grpc-server"},{"body":"Server/http-server Description This is a sharing plugin, which would start a http server.\nDefaultConfig # The http server address. address: \u0026#34;:12800\u0026#34; ","excerpt":"Server/http-server Description This is a sharing plugin, which would start a http server. …","ref":"/docs/skywalking-satellite/latest/en/setup/plugins/server_http-server/","title":"Server/http-server"},{"body":"Server/prometheus-server Description This is a prometheus server to export the metrics in Satellite.\nDefaultConfig # The prometheus server address. address: \u0026#34;:1234\u0026#34; # The prometheus server metrics endpoint. endpoint: \u0026#34;/metrics\u0026#34; ","excerpt":"Server/prometheus-server Description This is a prometheus server to export the metrics in Satellite. …","ref":"/docs/skywalking-satellite/latest/en/setup/plugins/server_prometheus-server/","title":"Server/prometheus-server"},{"body":"Setting Override SkyWalking Satellite supports setting overrides by system environment variables. You could override the settings in satellite_config.yaml\nSystem environment variables   Example\nOverride log_pattern in this setting segment through environment variables\n  logger: log_pattern: ${SATELLITE_LOGGER_LOG_PATTERN:%time [%level][%field] - %msg} time_pattern: ${SATELLITE_LOGGER_TIME_PATTERN:2006-01-02 15:04:05.000} level: ${SATELLITE_LOGGER_LEVEL:info} If the SATELLITE_LOGGER_LOG_PATTERN  environment variable exists in your operating system and its value is %msg, then the value of log_pattern here will be overwritten to %msg, otherwise, it will be set to %time [%level][%field] - %msg.\n","excerpt":"Setting Override SkyWalking Satellite supports setting overrides by system environment variables. …","ref":"/docs/skywalking-satellite/latest/en/setup/configuration/override-settings/","title":"Setting Override"},{"body":"Setup First and most important thing is, SkyWalking Satellite startup behaviours are driven by configs/satellite_config.yaml. Understanding the setting file will help you to read this document.\nStartup script Startup Script\nbin/startup.sh satellite_config.yaml The core concept behind this setting file is, SkyWalking Satellite is based on pure modularization design. End user can switch or assemble the collector features by their own requirements.\nSo, in satellite_config.yaml, there are three parts.\n The common configurations. The sharing plugin configurations. The pipe plugin configurations.  Advanced feature document link list  Overriding settings in satellite_config.yaml is supported  ","excerpt":"Setup First and most important thing is, SkyWalking Satellite startup behaviours are driven by …","ref":"/docs/skywalking-satellite/latest/en/setup/readme/","title":"Setup"},{"body":"Sharing Plugins Sharing plugin configurations has three 3 parts, which are common_config, clients and servers.\nCommon Configuration    Config Default Description     pipe_name sharing The group name of sharing plugins    Clients Clients have a series of client plugins, which would be sharing with the plugins of the other pipes. Please read the doc to find all client plugin configurations.\nServers Servers have a series of server plugins, which would be sharing with the plugins of the other pipes. Please read the doc to find all server plugin configurations.\nExample # The sharing plugins referenced by the specific plugins in the different pipes. sharing: common_config: pipe_name: sharing clients: - plugin_name: \u0026#34;kafka-client\u0026#34; brokers: ${SATELLITE_KAFKA_CLIENT_BROKERS:127.0.0.1:9092} version: ${SATELLITE_KAFKA_VERSION:\u0026#34;2.1.1\u0026#34;} servers: - plugin_name: \u0026#34;grpc-server\u0026#34; - plugin_name: \u0026#34;prometheus-server\u0026#34; address: ${SATELLITE_PROMETHEUS_ADDRESS:\u0026#34;:8090\u0026#34;} ","excerpt":"Sharing Plugins Sharing plugin configurations has three 3 parts, which are common_config, clients …","ref":"/docs/skywalking-satellite/latest/en/setup/configuration/sharing-plugins/","title":"Sharing Plugins"},{"body":"Unit Test For Satellite, the specific plugin may have some common dependencies. So we provide a global test initializer to init the dependencies.\nimport ( _ \u0026quot;github.com/apache/skywalking-satellite/internal/satellite/test\u0026quot; ) ","excerpt":"Unit Test For Satellite, the specific plugin may have some common dependencies. So we provide a …","ref":"/docs/skywalking-satellite/latest/en/guides/test/how-to-unit-test/","title":"Unit Test"},{"body":"Welcome Here are SkyWalking Satellite official documentations. You\u0026rsquo;re welcome to join us.\nFrom here you can learn all about SkyWalking Satellite\u0026rsquo;s architecture, how to deploy and use SkyWalking Satellite.\n  Concepts and Designs. The most important core ideas about SkyWalking Satellite. You can learn from here if you want to understand what is going on under our cool features.\n  Setup. Introduce how to set up the SkyWalking Satellite.\n  Guides. Guide users to develop or debug SkyWalking Satellite.\n  Protocols. Protocols show the communication ways between agents/probes, Satellite and SkyWalking. Anyone interested in uplink telemetry data should definitely read this.\n  Change logs. The feature records of the different versions.\n  FAQs. A manifest of already known setup problems, secondary developments experiments. When you are facing a problem, check here first.\n  We\u0026rsquo;re always looking for help improve our documentation and codes, so please don’t hesitate to file an issue if you see any problem. Or better yet, submit your own contributions through pull request to help make them better.\n","excerpt":"Welcome Here are SkyWalking Satellite official documentations. You\u0026rsquo;re welcome to join us.\nFrom …","ref":"/docs/skywalking-satellite/latest/readme/","title":"Welcome"},{"body":"What is the performance of the Satellite? Performance The performance reduction of the mmap-queue is mainly due to the file persistent operation to ensure data stability. However, the queue is used to collect some core telemetry data. We will continue to optimize the performance of this queue.\n 0.5 core supported 3000 ops throughput with memory queue. 0.5 core supported 1500 ops throughput with the memory mapped queue(Ensure data stability).  Details Testing environment  machine:  cpu: INTEL Xeon E5-2650 V4 12C 2.2GHZ * 2 memory: INVENTEC PC4-19200 * 8 harddisk: INVENTEC SATA 4T 7.2K * 8   Kafka:  region: the same region with the test machine in Baidu Cloud. version.: 0.1.1.0   The input plugin: grpc-nativelog-receiver resource limit:  cpu: 500m(0.5 core) memory: 100M    Performance Test With Memory Queue    Qps stack memory in use heap memory in use no-heap memory in use     400 2.13M 11M 83K   800 2.49M 13.4M 83K   1200 2.72M 13.4M 83K   1600 2.85M 16.2M 83K   2000 2.92M 17.6M 83K   2400 2.98M 18.3M 83K   2800 3.54M 26.8M 83K   3000 3.34M 28M 83K    Performance Test With Mmap Queue    Qps stack memory in use heap memory in use no-heap memory in use     400 2.39M 9.5M 83K   800 2.43M 12.1M 83K   1200 2.49M 12M 83K   1600 2.62M 13.3M 83K    ","excerpt":"What is the performance of the Satellite? Performance The performance reduction of the mmap-queue is …","ref":"/docs/skywalking-satellite/latest/en/faq/performance/","title":"What is the performance of the Satellite?"},{"body":"Abstract Apache SkyWalking hosts SkyWalkingDay Conference 2021 in June 26th, jointly with Tencent and Tetrate.\nWe are going to share SkyWalking\u0026rsquo;s roadmap, features, product experiences and open source culture.\nWelcome to join us.\nVenue Addr./地址 北京市海淀区西格玛大厦B1多功能厅\nDate June 26th.\nRegistration For Free Register for onsite or online\nSessions 10:00 - 10:20 Apache SkyWalking Landscape  吴晟 Sheng Wu. Tetrate Founding Engineer, Apache Software Foundation board director. SkyWalking founder.  SkyWalking 2020-2021年发展和后续计划\n10:20 - 10:50 微服务可观测性分析平台的探索与实践  凌若川 腾讯高级工程师  可观测性分析平台作为云原生时代微服务系统基础组件，开放性与性能是决定平台价值的核心要素。 复杂微服务应用场景与海量多维链路数据，对可观测性分析平台在开放性设计和各环节高性能实现带来诸多挑战。 本次分享中将重点梳理腾讯云微服务团队在构建云原生可观测性分析平台过程中遇到的挑战，介绍我们在架构设计与实现方面的探索与实践。\n 云原生时代微服务可观测性平台面临的性能与可用性挑战 腾讯云在构建高性能微服务可观测性分析平台的探索与实践 微服务可观测性分析平台架构的下一阶段演进方向展望  10:50 - 11:20 BanyanDB数据模型背后的逻辑  高洪涛 Hongtao Gao. Tetrate SRE, SkyWalking PMC, Apache ShardingSphere PMC.  BanyanDB作为为处理Apache SkyWalking产生的trace，log和metric的数据而特别设计的数据库，其背后数据模型的抉择是非常与众不同的。 在本次分享中，我将根据RUM猜想来讨论为什么BanyanDB使用的数据模型对于APM数据而言是更加高效和可靠的。\n通过本次分享，观众可以：\n 理解数据库设计的取舍 了解BanyanDB的数据模型 认识到该模型对于APM类数据有特定的优势  11:20 - 11:50 Apache SkyWalking 如何做前端监控  范秋霞 Qiuxia Fan，Tetrate FE SRE，SkyWalking PMC.  Apache SkyWalking对前端进行了监控与跟踪，分别有Metric, Log, Trace三部分。本次分享我会介绍页面性能指标的收集与计算，同时用案列进行分析，也会讲解Log的采集方法以及Source Map错误定位的实施。最后介绍浏览器端Requets的跟踪方法。\n通过本次分享，观众可以：\n 了解页面的性能指标以及收集计算方法 了解前端如何做错误日志收集 如何对页面请求进行跟踪以及跟踪的好处  午休 13:30 - 14:00 一名普通工程师，该如何正确的理解开源精神？  王晔倞 Yeliang Wang. API7 Partner / Product VP.  开源精神，那也许是一种给于和获取的平衡，有给于才能有获取，有获取才会有给于的动力。无需指责别人只会获取，我们应该懂得开源是一种创造方式，一个没有创造欲和创造力的人加入开源也是无用的。\n通过本次分享，观众可以：\n 为什么国内一些程序员会对开源产生误解？ 了解 “开源≠自由≠非商业” 的来龙去脉。 一名普通工程师，如何高效地向开源社区做贡献？  14:00 - 14:30 可观测性技术生态和OpenTelemetry原理及实践  陈一枭 腾讯. OpenTelemetry docs-cn maintainer、Tencent OpenTelemetry OTeam创始人  综述云原生可观测性技术生态，介绍OpenTracing，OpenMetrics，OpenTelemetry等标准演进。介绍OpenTelemetry存在价值意义，介绍OpenTelemetry原理及其整体生态规划。介绍腾讯在OpenTelemetry方面的实践。\n本次分享内容如下：\n 云原生可观测性技术简介 OpenTelemetry及其它规范简介 OpenTelemetry原理 OpenTelemetry在腾讯的应用及实践  14:30 - 15:10 利用 Apache SkyWalking 事件采集系统更快定位故障  柯振旭 Zhenxu Ke，Tetrate SRE, Apache SkyWalking PMC. Apache Incubator PMC. Apache Dubbo committer.  通过本次分享，听众可以：\n 了解 SkyWalking 的事件采集系统； 了解上报事件至 SkyWalking 的多种方式； 学习如何利用 SkyWalking 采集的事件结合 metrics，分析目标系统的性能问题；  15:10 - 15:30 茶歇 15:30 - 16:00 可观测性自动注入技术原理探索与实践  詹启新 Tencnet OpenTelemetry Oteam PMC  在可观测领域中自动注入已经成为重要的组成部分之一，其优异简便的使用方式并且可同时覆盖到链路、指标、日志，大大降低了接入成本及运维成本，属于友好的一种接入方式； 本次分享将介绍Java中的字节码注入技术原理，及在可观测领域的应用实践\n 常用的自动注入技术原理简介 介绍可观测性在Java落地的要点 opentelemetry-java-instrumentation的核心原理及实现 opentelemetry自动注入的应用实践  16:00 - 16:30 如何利用 Apache APISIX 提升 Nginx 的可观测性  金卫 Wei Jin, API7 Engineer Apache SkyWalking committer. Apache apisix-ingress-controller Founder. Apache APISIX PMC.  在云原生时代，动态和可观测性是 API 网关的标准特性。Apache APISIX 不仅覆盖了 Nginx 的传统功能，在可观测性上也和 SkyWalking 深度合作，大大提升了服务治理能力。本次分享会介绍如何无痛的提升 Nginx 的可观测性和 APISIX 在未来可观测性方面的规划。\n通过本次分享，观众可以：\n 通过 Apache APISIX 实现观测性的几种手段. 了解 Apache APISIX 高效且易用的秘诀. 结合 Apache skywalking 进一步提升可观测性.  16:35 抽奖，结束 Sponsors  Tencent Tetrate SegmentFault 思否  Anti-harassment policy SkyWalkingDay is dedicated to providing a harassment-free experience for everyone. We do not tolerate harassment of participants in any form. Sexual language and imagery will also not be tolerated in any event venue. Participants violating these rules may be sanctioned or expelled without a refund, at the discretion of the event organizers. Our anti-harassment policy can be found at Apache website.\nContact Us Send mail to dev@skywalking.apache.org.\n","excerpt":"Abstract Apache SkyWalking hosts SkyWalkingDay Conference 2021 in June 26th, jointly with Tencent …","ref":"/events/skywalkingday-2021/","title":"SkyWalkingDay Conference 2021, relocating at Beijing"},{"body":"SkyWalking NodeJS 0.3.0 is released. Go to downloads page to find release tars.\n Add ioredis plugin. (#53) Endpoint cold start detection and marking. (#52) Add mysql2 plugin. (#54) Add AzureHttpTriggerPlugin. (#51) Add Node 15 into test matrix. (#45) Segment reference and reporting overhaul. (#50) Add http ignore by method. (#49) Add secure connection option. (#48) BugFix: wrong context during many async spans. (#46) Add Node Mongoose Plugin. (#44)  ","excerpt":"SkyWalking NodeJS 0.3.0 is released. Go to downloads page to find release tars.\n Add ioredis plugin. …","ref":"/events/release-apache-skywalking-nodejs-0-3-0/","title":"Release Apache SkyWalking for NodeJS 0.3.0"},{"body":"SkyWalking Client JS 0.5.1 is released. Go to downloads page to find release tars.\n Add noTraceOrigins option. Fix wrong URL when using relative path. Catch frames errors. Get response.body as a stream with the fetch API. Support reporting multiple logs. Support typescript project.  ","excerpt":"SkyWalking Client JS 0.5.1 is released. Go to downloads page to find release tars.\n Add …","ref":"/events/release-apache-skywalking-client-js-0-5-1/","title":"Release Apache SkyWalking Client JS 0.5.1"},{"body":"SkyWalking Kong Agent 0.1.1 is released. Go to downloads page to find release tars.\n Establish the SkyWalking Kong Agent.  ","excerpt":"SkyWalking Kong Agent 0.1.1 is released. Go to downloads page to find release tars.\n Establish the …","ref":"/events/release-apache-skywalking-kong-0-1-1/","title":"Release Apache SkyWalking Kong 0.1.1"},{"body":"  ","excerpt":"  ","ref":"/zh/2021-05-09-summer-2021-asf20/","title":"[视频] 大咖说开源 第二季 第4期 | Apache软件基金会20年"},{"body":"","excerpt":"","ref":"/tags/license/","title":"License"},{"body":"We posted our Response to Elastic 2021 License Change blog 4 months ago. It doesn\u0026rsquo;t have a big impact in the short term, but because of the incompatibility between SSPL and Apache 2.0, we lost the chance of upgrading the storage server, which concerns the community and our users. So, we have to keep looking for a new option as a replacement.\nThere was an open source project, Open Distro for Elasticsearch, maintained by the AWS team. It is an Apache 2.0-licensed distribution of Elasticsearch enhanced with enterprise security, alerting, SQL, and more. After Elastic relicensed its projects, we talked with their team, and they have an agenda to take over the community leadship and keep maintaining Elasticsearch, as it was licensed by Apache 2.0. So, they are good to fork and continue.\nOn April 12th, 2021, AWS announced the new project, OpenSearch, driven by the community, which is initialized from people of AWS, Red Hat, SAP, Capital One, and Logz.io. Read this Introducing OpenSearch blog for more detail.\nOnce we had this news in public, we begin to plan the process of evaluating and testing OpenSearch as SkyWalking\u0026rsquo;s storage option. Read our issue.\nToday, we are glad to ANNOUNCE, OpenSearch could replace ElastcSearch as the storage, and it is still licensed under Apache 2.0.\nThis has been merged in the main stream, and you can find it in the dev doc already.\nOpenSearch OpenSearch storage shares the same configurations as Elasticsearch 7. In order to activate Elasticsearch 7 as storage, set storage provider to elasticsearch7. Please download the apache-skywalking-bin-es7.tar.gz if you want to use OpenSearch as storage.\nSkyWalking community will keep our eyes on the OpenSearch project, and look forward to their first GA release.\n NOTE: we have to add a warning NOTICE to the Elasticsearch storage doc:\nNOTICE: Elastic announced through their blog that Elasticsearch will be moving over to a Server Side Public License (SSPL), which is incompatible with Apache License 2.0. This license change is effective from Elasticsearch version 7.11. So please choose the suitable Elasticsearch version according to your usage.\n","excerpt":"We posted our Response to Elastic 2021 License Change blog 4 months ago. It doesn\u0026rsquo;t have a big …","ref":"/blog/2021-05-09-opensearch-supported/","title":"OpenSearch, a new storage option to avoid ElasticSearch's SSPL"},{"body":"","excerpt":"","ref":"/tags/","title":"Tags"},{"body":"","excerpt":"","ref":"/zh_tags/video/","title":"Video"},{"body":"","excerpt":"","ref":"/zh_tags/","title":"Zh_tags"},{"body":"Hailin Wang(GitHub ID, hailin0) began his SkyWalking journey since Aug 23rd, 2020.\nHe is very active on the code contributions and brought several important features into the SkyWalking ecosystem.\nHe is on the 33rd of the contributor in the main repository[1], focuses on plugin contributions, and logs ecosystem integration, see his code contributions[2]. And also, he started a new and better way to make other open-source projects integrating with SkyWalking.\nHe used over 2 months to make the SkyWalking agent and its plugins as a part of Apache DolphinScheduler\u0026rsquo;s default binary distribution[3], see this PR[4]. This kind of example has affected further community development. Our PMC member, Yuguang Zhao, is using this way to ship our agent and plugins into the Seata project[5]. With SkyWalking\u0026rsquo;s growing, I would not doubt that this kind of integration would be more.\nThe SkyWalking accepts him as a new committer.\nWelcome Hailin Wang join the committer team.\n[1] https://github.com/apache/skywalking/graphs/contributors [2] https://github.com/apache/skywalking/commits?author=hailin0 [3] https://github.com/apache/dolphinscheduler/tree/1.3.6-prepare/ext/skywalking [4] https://github.com/apache/incubator-dolphinscheduler/pull/4852 [5] https://github.com/seata/seata/pull/3652\n","excerpt":"Hailin Wang(GitHub ID, hailin0) began his SkyWalking journey since Aug 23rd, 2020.\nHe is very active …","ref":"/events/welcome-hailin-wang-as-new-committer/","title":"Welcome Hailin Wang as new committer"},{"body":"SkyWalking LUA Nginx 0.5.0 is released. Go to downloads page to find release tars.\n Adapt to Kong agent. Correct the version format luarock.  ","excerpt":"SkyWalking LUA Nginx 0.5.0 is released. Go to downloads page to find release tars.\n Adapt to Kong …","ref":"/events/release-apache-skywalking-lua-nginx-0.5.0/","title":"Release Apache SkyWalking LUA Nginx 0.5.0"},{"body":"SkyWalking 8.5.0 is released. Go to downloads page to find release tars. Changes by Version\nProject  Incompatible Change. Indices and templates of ElasticSearch(6/7, including zipkin-elasticsearch7) storage option have been changed. Update frontend-maven-plugin to 1.11.0, for Download node x64 binary on Apple Silicon. Add E2E test for VM monitoring that metrics from Prometheus node-exporter. Upgrade lombok to 1.18.16. Add Java agent Dockerfile to build Docker image for Java agent.  Java Agent  Remove invalid mysql configuration in agent.config. Add net.bytebuddy.agent.builder.AgentBuilder.RedefinitionStrategy.Listener to show detail message when redefine errors occur. Fix ClassCastException of log4j gRPC reporter. Fix NPE when Kafka reporter activated. Enhance gRPC log appender to allow layout pattern. Fix apm-dubbo-2.7.x-plugin memory leak due to some Dubbo RpcExceptions. Fix lettuce-5.x-plugin get null host in redis sentinel mode. Fix ClassCastException by making CallbackAdapterInterceptor to implement EnhancedInstance interface in the spring-kafka plugin. Fix NullPointerException with KafkaProducer.send(record). Support config agent.span_limit_per_segment can be changed in the runtime. Collect and report agent starting / shutdown events. Support jedis pipeline in jedis-2.x-plugin. Fix apm-toolkit-log4j-2.x-activation no trace Id in async log. Replace hbase-1.x-plugin with hbase-1.x-2.x-plugin to adapt hbase client 2.x Remove the close_before_method and close_after_method parameters of custom-enhance-plugin to avoid memory leaks. Fix bug that springmvc-annotation-4.x-plugin, witness class does not exist in some versions. Add Redis command parameters to \u0026lsquo;db.statement\u0026rsquo; field on Lettuce span UI for displaying more info. Fix NullPointerException with ReactiveRequestHolder.getHeaders. Fix springmvc reactive api can\u0026rsquo;t collect HTTP statusCode. Fix bug that asynchttpclient plugin does not record the response status code. Fix spanLayer is null in optional plugin(gateway-2.0.x-plugin gateway-2.1.x-plugin). Support @Trace, @Tag and @Tags work for static methods.  OAP-Backend  Allow user-defined JAVA_OPTS in the startup script. Metrics combination API supports abandoning results. Add a new concept \u0026ldquo;Event\u0026rdquo; and its implementations to collect events. Add some defensive codes for NPE and bump up Kubernetes client version to expose exception stack trace. Update the timestamp field type for LogQuery. Support Zabbix protocol to receive agent metrics. Update the Apdex metric combine calculator. Enhance MeterSystem to allow creating metrics with same metricName / function / scope. Storage plugin supports postgresql. Fix kubernetes.client.openapi.ApiException. Remove filename suffix in the meter active file config. Introduce log analysis language (LAL). Fix alarm httpclient connection leak. Add sum function in meter system. Remove Jaeger receiver. Remove the experimental Zipkin span analyzer. Upgrade the Zipkin Elasticsearch storage from 6 to 7. Require Zipkin receiver must work with zipkin-elasticsearch7 storage option. Fix DatabaseSlowStatementBuilder statement maybe null. Remove fields of parent entity in the relation sources. Save Envoy http access logs when error occurs. Fix wrong service_instance_sla setting in the topology-instance.yml. Fix wrong metrics name setting in the self-observability.yml. Add telemetry data about metrics in, metrics scraping, mesh error and trace in metrics to zipkin receiver. Fix tags store of log and trace on h2/mysql/pg storage. Merge indices by Metrics Function and Meter Function in Elasticsearch Storage. Fix receiver don\u0026rsquo;t need to get itself when healthCheck Remove group concept from AvgHistogramFunction. Heatmap(function result) doesn\u0026rsquo;t support labels. Support metrics grouped by scope labelValue in MAL, no need global same labelValue as before. Add functions in MAL to filter metrics according to the metric value. Optimize the self monitoring grafana dashboard. Enhance the export service. Add function retagByK8sMeta and opt type K8sRetagType.Pod2Service in MAL for k8s to relate pods and services. Using \u0026ldquo;service.istio.io/canonical-name\u0026rdquo; to replace \u0026ldquo;app\u0026rdquo; label to resolve Envoy ALS service name. Support k8s monitoring. Make the flushing metrics operation concurrent. Fix ALS K8SServiceRegistry didn\u0026rsquo;t remove the correct entry. Using \u0026ldquo;service.istio.io/canonical-name\u0026rdquo; to replace \u0026ldquo;app\u0026rdquo; label to resolve Envoy ALS service name. Append the root slash(/) to getIndex and getTemplate requests in ES(6 and 7) client. Fix disable statement not working. This bug exists since 8.0.0. Remove the useless metric in vm.yaml.  UI  Update selector scroller to show in all pages. Implement searching logs with date. Add nodejs 14 compiling. Fix trace id by clear search conditions. Search endpoints with keywords. Fix pageSize on logs page. Update echarts version to 5.0.2. Fix instance dependency on the topology page. Fix resolved url for vue-property-decorator. Show instance attributes. Copywriting grammar fix. Fix log pages tags column not updated. Fix the problem that the footer and topology group is shaded when the topology radiation is displayed. When the topology radiation chart is displayed, the corresponding button should be highlighted. Refactor the route mapping, Dynamically import routing components, Improve first page loading performance. Support topology of two mutually calling services. Implement a type of table chart in the dashboard. Support event in the dashboard. Show instance name in the trace view. Fix groups of services in the topography.  Documentation  Polish documentation due to we have covered all tracing, logging, and metrics fields. Adjust documentation about Zipkin receiver. Add backend-infrastructure-monitoring doc.  All issues and pull requests are here\n","excerpt":"SkyWalking 8.5.0 is released. Go to downloads page to find release tars. Changes by Version\nProject …","ref":"/events/release-apache-skywalking-apm-8-5-0/","title":"Release Apache SkyWalking APM 8.5.0"},{"body":"SkyWalking Cloud on Kubernetes 0.3.0 is released. Go to downloads page to find release tars.\n Support special characters in the metric selector of HPA metric adapter. Add the namespace to HPA metric name.  ","excerpt":"SkyWalking Cloud on Kubernetes 0.3.0 is released. Go to downloads page to find release tars. …","ref":"/events/release-apache-skywalking-cloud-on-kubernetes-0-3-0/","title":"Release Apache SkyWalking Cloud on Kubernetes 0.3.0"},{"body":"SkyWalking NodeJS 0.2.0 is released. Go to downloads page to find release tars.\n Add AMQPLib plugin (RabbitMQ). (#34) Add MongoDB plugin. (#33) Add PgPlugin - PosgreSQL. (#31) Add MySQLPlugin to plugins. (#30) Add http protocol of host to http plugins. (#28) Add tag http.method to plugins. (#26) Bugfix: child spans created on immediate cb from op. (#41) Bugfix: async and preparing child entry/exit. (#36) Bugfix: tsc error of dist lib. (#24) Bugfix: AxiosPlugin async() / resync(). (#21) Bugfix: some requests of express / axios are not close correctly. (#20) Express plugin uses http wrap explicitly if http plugin disabled. (#42)  ","excerpt":"SkyWalking NodeJS 0.2.0 is released. Go to downloads page to find release tars.\n Add AMQPLib plugin …","ref":"/events/release-apache-skywalking-nodejs-0-2-0/","title":"Release Apache SkyWalking for NodeJS 0.2.0"},{"body":"SkyWalking Python 0.6.0 is released. Go to downloads page to find release tars.\n Fixes:  Segment data loss when gRPC timing out. (#116) sw_tornado plugin async handler status set correctly. (#115) sw_pymysql error when connection haven\u0026rsquo;t db. (#113)    ","excerpt":"SkyWalking Python 0.6.0 is released. Go to downloads page to find release tars.\n Fixes:  Segment …","ref":"/events/release-apache-skywalking-python-0-6-0/","title":"Release Apache SkyWalking Python 0.6.0"},{"body":"","excerpt":"","ref":"/tags/apm/","title":"APM"},{"body":" Origin: End-User Tracing in a SkyWalking-Observed Browser - The New Stack\n Apache SkyWalking: an APM (application performance monitor) system, especially designed for microservices, cloud native, and container-based (Docker, Kubernetes, Mesos) architectures.\nskywalking-client-js: a lightweight client-side JavaScript exception, performance, and tracing library. It provides metrics and error collection to the SkyWalking backend. It also makes the browser the starting point for distributed tracing.\nBackground Web application performance affects the retention rate of users. If a page load time is too long, the user will give up. So we need to monitor the web application to understand performance and ensure that servers are stable, available and healthy. SkyWalking is an APM tool and the skywalking-client-js extends its monitoring to include the browser, providing performance metrics and error collection to the SkyWalking backend.\nPerformance Metrics The skywalking-client-js uses [window.performance] (https://developer.mozilla.org/en-US/docs/Web/API/Window/performance) for performance data collection. From the MDN doc, the performance interface provides access to performance-related information for the current page. It\u0026rsquo;s part of the High Resolution Time API, but is enhanced by the Performance Timeline API, the Navigation Timing API, the User Timing API, and the Resource Timing API. In skywalking-client-js, all performance metrics are calculated according to the Navigation Timing API defined in the W3C specification. We can get a PerformanceTiming object describing our page using the window.performance.timing property. The PerformanceTiming interface contains properties that offer performance timing information for various events that occur during the loading and use of the current page.\nWe can better understand these attributes when we see them together in the figure below from W3C:\nThe following table contains performance metrics in skywalking-client-js.\n   Metrics Name Describe Calculating Formulae Note     redirectTime Page redirection time redirectEnd - redirectStart If the current document and the document that is redirected to are not from the same origin, set redirectStart, redirectEnd to 0   ttfbTime Time to First Byte responseStart - requestStart According to Google Development   dnsTime Time to DNS query domainLookupEnd - domainLookupStart    tcpTime Time to TCP link connectEnd - connectStart    transTime Time to content transfer responseEnd - responseStart    sslTime Time to SSL secure connection connectEnd - secureConnectionStart Only supports HTTPS   resTime Time to resource loading loadEventStart - domContentLoadedEventEnd Represents a synchronized load resource in pages   fmpTime Time to First Meaningful Paint - Listen for changes in page elements. Traverse each new element, and calculate the total score of these elements. If the element is visible, the score is 1 * weight; if the element is not visible, the score is 0   domAnalysisTime Time to DOM analysis domInteractive - responseEnd    fptTime First Paint Time responseEnd - fetchStart    domReadyTime Time to DOM ready domContentLoadedEventEnd - fetchStart    loadPageTime Page full load time loadEventStart - fetchStart    ttlTime Time to interact domInteractive - fetchStart    firstPackTime Time to first package responseStart - domainLookupStart     Skywalking-client-js collects those performance metrics and sends them to the OAP (Observability Analysis Platform) server , which aggregates data on the back-end side that is then shown in visualizations on the UI side. Users can optimize the page according to these data.\nException Metrics There are five kinds of errors that can be caught in skywalking-client-js:\n The resource loading error is captured by window.addeventlistener ('error ', callback, true) window.onerror catches JS execution errors window.addEventListener('unhandledrejection', callback) is used to catch the promise errors the Vue errors are captured by Vue.config.errorHandler the Ajax errors are captured by addEventListener('error', callback); addEventListener('abort', callback); addEventListener('timeout', callback);  in send callback.  The Skywalking-client-js traces error data to the OAP server, finally visualizing data on the UI side. For an error overview of the App, there are several metrics for basic statistics and trends of errors, including the following metrics.\n App Error Count, the total number of errors in the selected time period. App JS Error Rate, the proportion of PV with JS errors in a selected time period to total PV. All of Apps Error Count, Top N Apps error count ranking. All of Apps JS Error Rate, Top N Apps JS error rate ranking. Error Count of Versions in the Selected App, Top N Error Count of Versions in the Selected App ranking. Error Rate of Versions in the Selected App, Top N JS Error Rate of Versions in the Selected App ranking. Error Count of the Selected App, Top N Error Count of the Selected App ranking. Error Rate of the Selected App, Top N JS Error Rate of the Selected App ranking.  For pages, we use several metrics for basic statistics and trends of errors, including the following metrics:\n Top Unstable Pages / Error Rate, Top N Error Count pages of the Selected version ranking. Top Unstable Pages / Error Count, Top N Error Count pages of the Selected version ranking. Page Error Count Layout, data display of different errors in a period of time.  User Metrics SkyWalking browser monitoring also provides metrics about how the visitors use the monitored websites, such as PV(page views), UV(unique visitors), top N PV(page views), etc.\nIn SPAs (single page applications), the page will be refreshed only once. The traditional method only reports PV once after the page loading, but cannot count the PV of each sub-page, and can\u0026rsquo;t make other types of logs aggregate by sub-page.\nSkyWalking browser monitoring provides two processing methods for SPA pages:\n  Enable SPA automatic parsing. This method is suitable for most single page application scenarios with URL hash as the route. In the initialized configuration item, set enableSPA to true, which will turn on the page\u0026rsquo;s hashchange event listener (trigger re reporting PV), and use URL hash as the page field in other data reporting.\n  Manual reporting. This method can be used in all single page application scenarios. This method can be used if the first method is not usable. The following example provides a set page method to manually update the page name when data is reported. When this method is called, the page PV will be re reported by default:\n  app.on(\u0026#39;routeChange\u0026#39;, function (to) { ClientMonitor.setPerformance({ collector: \u0026#39;http://127.0.0.1:8080\u0026#39;, service: \u0026#39;browser-app\u0026#39;, serviceVersion: \u0026#39;1.0.0\u0026#39;, pagePath: to.path, autoTracePerf: true, enableSPA: true, }); }); Let\u0026rsquo;s take a look at the result found in the following image. It shows the most popular applications and versions, and the changes of PV over a period of time.\nMake the browser the starting point for distributed tracing SkyWalking browser monitoring intercepts HTTP requests to trace segments and spans. It supports tracking these following modes of HTTP requests: XMLHttpRequest and fetch. It also supports tracking libraries and tools based on XMLHttpRequest and fetch - such as Axios, SuperAgent, OpenApi, and so on.\nLet’s see how the SkyWalking browser monitoring intercepts HTTP requests:\nAfter this, use window.addEventListener('xhrReadyStateChange', callback) and set the readyState value tosw8 = xxxx in the request header. At the same time, reporting requests information to the back-end side. Finally, we can view trace data on the trace page. The following graphic is from the trace page:\nTo see how we listen for fetch requests, let’s see the source code of fetch\nAs you can see, it creates a promise and a new XMLHttpRequest object. Because the code of the fetch is built into the browser, it must monitor the code execution first. Therefore, when we add listening events, we can\u0026rsquo;t monitor the code in the fetch. Just after monitoring the code execution, let\u0026rsquo;s rewrite the fetch:\nimport { fetch } from \u0026#39;whatwg-fetch\u0026#39;; window.fetch = fetch; In this way, we can intercept the fetch request through the above method.\nAdditional Resources  End-User Tracing in a SkyWalking-Observed Browser.  ","excerpt":"Origin: End-User Tracing in a SkyWalking-Observed Browser - The New Stack\n Apache SkyWalking: an APM …","ref":"/blog/end-user-tracing-in-a-skywalking-observed-browser/","title":"End-User Tracing in a SkyWalking-Observed Browser"},{"body":"","excerpt":"","ref":"/tags/observability/","title":"Observability"},{"body":"","excerpt":"","ref":"/tags/web-performance/","title":"Web-performance"},{"body":"","excerpt":"","ref":"/tags/design/","title":"Design"},{"body":"","excerpt":"","ref":"/tags/logs/","title":"Logs"},{"body":"SourceMarker is an open-source continuous feedback IDE plugin built on top of Apache SkyWalking, a popular open-source APM system with monitoring, tracing, and diagnosing capabilities for distributed software systems. SkyWalking, a truly holistic system, provides the means for automatically producing, storing, and querying software operation metrics. It requires little to no code changes to implement and is lightweight enough to be used in production. By itself, SkyWalking is a formidable force in the realm of continuous monitoring technology.\nSourceMarker, leveraging the continuous monitoring functionality provided by SkyWalking, creates continuous feedback technology by automatically linking software operation metrics to source code and displaying feedback directly inside of the IDE. While currently only supporting JetBrains-based IDEs and JVM-based programming languages, SourceMarker may be extended to support any number of programming languages and IDEs. Using SourceMarker, software developers can understand and validate software operation inside of their IDE. Instead of charts that indicate the health of the application, software developers can view the health of individual source code components and interpret software operation metrics from a much more familiar perspective. Such capabilities improve productivity as time spent continuously context switching from development to monitoring would be eliminated.\nLogging The benefits of continuous feedback technology are immediately apparent with the ability to view and search logs directly from source code. Instead of tailing log files or viewing logs through the browser, SourceMarker allows software developers to navigate production logs just as easily as they navigate source code. By using the source code as the primary perspective for navigating logs, SourceMarker allows software developers to view logs specific to any package, class, method, or line directly from the context of the source code which resulted in those logs.\nTracing Furthermore, continuous feedback technology offers software developers a deeper understanding of software by explicitly tying the implicit software operation to source code. Instead of visualizing software traces as Gantt charts, SourceMarker allows software developers to step through trace stacks while automatically resolving trace tags and logs. With SourceMarker, software developers can navigate production software traces in much the same way one debugs local applications.\nAlerting Most importantly, continuous feedback technology keeps software developers aware of production software operation. Armed with an APM-powered IDE, every software developer can keep track of the behavior of any method, class, package, and even the entire application itself. Moreover, this allows for source code to be the medium through which production bugs are made evident, thereby creating the feasibility of source code with the ability to self-diagnose and convey its own health.\n Download SourceMarker SourceMarker aims to bridge the theoretical and empirical practices of software development through continuous feedback. The goal is to make developing software with empirical data feel natural and intuitive, creating more complete software developers that understand the entire software development cycle.\n https://github.com/sourceplusplus/sourcemarker  This project is still early in its development, so if you think of any ways to improve SourceMarker, please let us know.\n","excerpt":"SourceMarker is an open-source continuous feedback IDE plugin built on top of Apache SkyWalking, a …","ref":"/blog/2021-03-16-continuous-feedback/","title":"SourceMarker: Continuous Feedback for Developers"},{"body":"","excerpt":"","ref":"/tags/tracing/","title":"Tracing"},{"body":"SkyWalking LUA Nginx 0.4.1 is released. Go to downloads page to find release tars.\n fix: missing constants in the rockspsec.  ","excerpt":"SkyWalking LUA Nginx 0.4.1 is released. Go to downloads page to find release tars.\n fix: missing …","ref":"/events/release-apache-skywalking-lua-nginx-0.4.1/","title":"Release Apache SkyWalking LUA Nginx 0.4.1"},{"body":"SkyWalking LUA Nginx 0.4.0 is released. Go to downloads page to find release tars.\n Add a global field \u0026lsquo;includeHostInEntrySpan\u0026rsquo;, type \u0026lsquo;boolean\u0026rsquo;, mark the entrySpan include host/domain. Add destroyBackendTimer to stop reporting metrics. Doc: set random seed in init_worker phase. Local cache some variables and reuse them in Lua module. Enable local cache and use tablepool to reuse the temporary table.  ","excerpt":"SkyWalking LUA Nginx 0.4.0 is released. Go to downloads page to find release tars.\n Add a global …","ref":"/events/release-apache-skywalking-lua-nginx-0.4.0/","title":"Release Apache SkyWalking LUA Nginx 0.4.0"},{"body":"SkyWalking Client JS 0.4.0 is released. Go to downloads page to find release tars.\n Update stack and message in logs. Fix wrong URL when using relative path in xhr.  ","excerpt":"SkyWalking Client JS 0.4.0 is released. Go to downloads page to find release tars.\n Update stack and …","ref":"/events/release-apache-skywalking-client-js-0-4-0/","title":"Release Apache SkyWalking Client JS 0.4.0"},{"body":"SkyWalking Satellite 0.1.0 is released. Go to downloads page to find release tars.\nFeatures  Build the Satellite core structure. Add prometheus self telemetry. Add kafka client plugin. Add none-fallbacker plugin. Add timer-fallbacker plugin. Add nativelog-kafka-forwarder plugin. Add memory-queue plugin. Add mmap-queue plugin. Add grpc-nativelog-receiver plugin. Add http-nativelog-receiver plugin. Add grpc-server plugin. Add http-server plugin. Add prometheus-server plugin.  Bug Fixes Issues and PR  All issues are here All and pull requests are here  ","excerpt":"SkyWalking Satellite 0.1.0 is released. Go to downloads page to find release tars.\nFeatures  Build …","ref":"/events/release-apache-skwaylking-satellite-0-1-0/","title":"Release Apache SkyWalking Satellite 0.1.0"},{"body":"Juntao Zhang leads and finished the re-build process of the whole skywalking website. Immigrate to the whole automatic website update, super friendly to users. Within the re-building process, he took several months contributions to bring the document of our main repository to host on the SkyWalking website, which is also available for host documentations of other repositories. We were waiting for this for years.\nJust in the website repository, he has 3800 LOC contributions through 26 commits.\nWe are honored to have him on the PMC team.\n","excerpt":"Juntao Zhang leads and finished the re-build process of the whole skywalking website. Immigrate to …","ref":"/events/welcome-juntao-zhang-to-join-the-pmc/","title":"Welcome Juntao Zhang (张峻滔) to join the PMC"},{"body":"Apache SkyWalking Satellite Release Guide This documentation guides the release manager to release the SkyWalking Satellite in the Apache Way, and also helps people to check the release for vote.\nPrerequisites  Close(if finished, or move to next milestone otherwise) all issues in the current milestone from skywalking-satellite and skywalking, create a new milestone if needed. Update CHANGES.md.  Add your GPG public key to Apache svn   Upload your GPG public key to a public GPG site, such as MIT\u0026rsquo;s site.\n  Log in id.apache.org and submit your key fingerprint.\n  Add your GPG public key into SkyWalking GPG KEYS file, you can do this only if you are a PMC member. You can ask a PMC member for help. DO NOT override the existed KEYS file content, only append your key at the end of the file.\n  Build and sign the source code package export VERSION=\u0026lt;the version to release\u0026gt; git clone --recurse-submodules git@github.com:apache/skywalking-satellite \u0026amp;\u0026amp; cd skywalking-satellite git tag -a \u0026#34;$VERSION\u0026#34; -m \u0026#34;Release Apache SkyWalking-Satellite $VERSION\u0026#34; git push --tags make release In total, six files should be automatically generated in the directory: skywalking-satellite-${VERSION}-bin.tgz, skywalking-satellite-${VERSION}-src.tgz, and their corresponding asc, sha512 files.\nUpload to Apache svn svn co https://dist.apache.org/repos/dist/dev/skywalking/ mkdir -p skywalking/satellite/\u0026#34;$VERSION\u0026#34; cp skywalking-satellite/skywalking*.tgz skywalking/satellite/\u0026#34;$VERSION\u0026#34; cp skywalking-satellite/skywalking*.tgz.asc skywalking/satellite/\u0026#34;$VERSION\u0026#34; cp skywalking-satellite/skywalking-satellite*.tgz.sha512 skywalking/satellite/\u0026#34;$VERSION\u0026#34; cd skywalking/satellite \u0026amp;\u0026amp; svn add \u0026#34;$VERSION\u0026#34; \u0026amp;\u0026amp; svn commit -m \u0026#34;Draft Apache SkyWalking-Satellite release $VERSION\u0026#34; Make the internal announcement Send an announcement email to dev@ mailing list, please check all links before sending the email.\nSubject: [ANNOUNCEMENT] SkyWalking Satellite $VERSION test build available Content: The test build of SkyWalking Satellite $VERSION is now available. We welcome any comments you may have, and will take all feedback into account if a quality vote is called for this build. Release notes: * https://github.com/apache/skywalking-satellite/blob/$VERSION/CHANGES.md Release Candidate: * https://dist.apache.org/repos/dist/dev/skywalking/satellite/$VERSION * sha512 checksums - sha512xxxxyyyzzz apache-skywalking-satellite-bin-x.x.x.tgz - sha512xxxxyyyzzz apache-skywalking-satellite-src-x.x.x.tgz Release Tag : * (Git Tag) v$VERSION Release Commit Hash : * https://github.com/apache/skywalking-satellite/tree/\u0026lt;Git Commit Hash\u0026gt; Keys to verify the Release Candidate : * http://pgp.mit.edu:11371/pks/lookup?op=get\u0026amp;search=0x8BD99F552D9F33D7 corresponding to kezhenxu94@apache.org Guide to build the release from source : * https://github.com/apache/skywalking-satellite/blob/$VERSION/docs/en/guides/contribuation/How-to-release.md A vote regarding the quality of this test build will be initiated within the next couple of days. Wait at least 48 hours for test responses Any PMC, committer or contributor can test features for releasing, and feedback. Based on that, PMC will decide whether to start a vote or not.\nCall for vote in dev@ mailing list Call for vote in dev@skywalking.apache.org, please check all links before sending the email.\nSubject: [VOTE] Release Apache SkyWalking Satellite version $VERSION Content: Hi the SkyWalking Community: This is a call for vote to release Apache SkyWalking Satellite version $VERSION. Release notes: * https://github.com/apache/skywalking-satellite/blob/$VERSION/CHANGES.md Release Candidate: * https://dist.apache.org/repos/dist/dev/skywalking/satellite/$VERSION * sha512 checksums - sha512xxxxyyyzzz skywalking-satellite-x.x.x-src.tgz - sha512xxxxyyyzzz skywalking-satellite-x.x.x-bin.tgz Release Tag : * (Git Tag) v$VERSION Release Commit Hash : * https://github.com/apache/skywalking-satellite/tree/\u0026lt;Git Commit Hash\u0026gt; Keys to verify the Release Candidate : * https://dist.apache.org/repos/dist/release/skywalking/KEYS Guide to build the release from source : * https://github.com/apache/skywalking-satellite/blob/$VERSION/docs/en/guides/contribuation/How-to-release.md Voting will start now and will remain open for at least 72 hours, all PMC members are required to give their votes. [ ] +1 Release this package. [ ] +0 No opinion. [ ] -1 Do not release this package because.... Thanks. [1] https://github.com/apache/skywalking/blob/master/docs/en/guides/How-to-release.md#vote-check Vote Check All PMC members and committers should check these before voting +1:\n Features test. All artifacts in staging repository are published with .asc, .md5, and sha files. Source codes and distribution packages (skywalking-satellite-$VERSION-{src,bin}.tgz) are in https://dist.apache.org/repos/dist/dev/skywalking/satellite/$VERSION with .asc, .sha512. LICENSE and NOTICE are in source codes and distribution package. Check shasum -c skywalking-satellite-$VERSION-{src,bin}.tgz.sha512. Check gpg --verify skywalking-satellite-$VERSION-{src,bin}.tgz.asc skywalking-satellite-$VERSION-{src,bin}.tgz. Build distribution from source code package by following this command, make build. Licenses check, make license.  Vote result should follow these:\n  PMC vote is +1 binding, all others is +1 no binding.\n  Within 72 hours, you get at least 3 (+1 binding), and have more +1 than -1. Vote pass.\n  Send the closing vote mail to announce the result. When count the binding and no binding votes, please list the names of voters. An example like this:\n[RESULT][VOTE] Release Apache SkyWalking Satellite version $VERSION 3 days passed, we’ve got ($NUMBER) +1 bindings (and ... +1 non-bindings): (list names) +1 bindings: xxx ... +1 non-bindings: xxx ... Thank you for voting, I’ll continue the release process.   Publish release   Move source codes tar balls and distributions to https://dist.apache.org/repos/dist/release/skywalking/, you can do this only if you are a PMC member.\nexport SVN_EDITOR=vim svn mv https://dist.apache.org/repos/dist/dev/skywalking/satellite/$VERSION https://dist.apache.org/repos/dist/release/skywalking/satellite   Refer to the previous PR, update the event and download links on the website.\n  Update Github release page, follow the previous convention.\n  Send ANNOUNCE email to dev@skywalking.apache.org and announce@apache.org, the sender should use his/her Apache email account, please check all links before sending the email.\nSubject: [ANNOUNCEMENT] Apache SkyWalking Satellite $VERSION Released Content: Hi the SkyWalking Community On behalf of the SkyWalking Team, I’m glad to announce that SkyWalking Satellite $VERSION is now released. SkyWalking Satellite: A lightweight collector/sidecar could be deployed closing to the target monitored system, to collect metrics, traces, and logs. SkyWalking: APM (application performance monitor) tool for distributed systems, especially designed for microservices, cloud native and container-based (Docker, Kubernetes, Mesos) architectures. Download Links: http://skywalking.apache.org/downloads/ Release Notes : https://github.com/apache/skywalking-satellite/blob/$VERSION/CHANGES.md Website: http://skywalking.apache.org/ SkyWalking Satellite Resources: - Issue: https://github.com/apache/skywalking/issues - Mailing list: dev@skywalking.apache.org - Documents: https://github.com/apache/skywalking-satellite/blob/$VERSION/README.md The Apache SkyWalking Team   Remove Unnecessary Releases Please remember to remove all unnecessary releases in the mirror svn (https://dist.apache.org/repos/dist/release/skywalking/), if you don\u0026rsquo;t recommend users to choose those version. For example, you have removed the download and documentation links from the website. If they want old ones, the Archive repository has all of them.\n","excerpt":"Apache SkyWalking Satellite Release Guide This documentation guides the release manager to release …","ref":"/docs/skywalking-satellite/v0.1.0/en/guides/contribution/how-to-release/","title":"Apache SkyWalking Satellite Release Guide"},{"body":"Client/kafka-client Description The Kafka client is a sharing plugin to keep connection with the Kafka brokers and delivery the data to it.\nDefaultConfig # The Kafka broker addresses (default localhost:9092). Multiple values are separated by commas. brokers: localhost:9092 # The Kakfa version should follow this pattern, which is major_minor_veryMinor_patch (default 1.0.0.0). version: 1.0.0.0 # The TLS switch (default false). enable_TLS: false # The file path of client.pem. The config only works when opening the TLS switch. client_pem_path: \u0026#34;\u0026#34; # The file path of client.key. The config only works when opening the TLS switch. client_key_path: \u0026#34;\u0026#34; # The file path oca.pem. The config only works when opening the TLS switch. ca_pem_path: \u0026#34;\u0026#34; # 0 means NoResponse, 1 means WaitForLocal and -1 means WaitForAll (default 1). required_acks: 1 # The producer max retry times (default 3). producer_max_retry: 3 # The meta max retry times (default 3). meta_max_retry: 3 # How long to wait for the cluster to settle between retries (default 100ms). Time unit is ms. retry_backoff: 100 # The max message bytes. max_message_bytes: 1000000 # If enabled, the producer will ensure that exactly one copy of each message is written (default false). idempotent_writes: false # A user-provided string sent with every request to the brokers for logging, debugging, and auditing purposes (default Satellite). client_id: Satellite # Compression codec represents the various compression codecs recognized by Kafka in messages. 0 : None, 1 : Gzip, 2 : Snappy, 3 : LZ4, 4 : ZSTD compression_codec: 0 # How frequently to refresh the cluster metadata in the background. Defaults to 10 minutes. The unit is minute. refresh_period: 10 # InsecureSkipVerify controls whether a client verifies the server\u0026#39;s certificate chain and host name. insecure_skip_verify: true ","excerpt":"Client/kafka-client Description The Kafka client is a sharing plugin to keep connection with the …","ref":"/docs/skywalking-satellite/v0.1.0/en/setup/plugins/client_kafka-client/","title":"Client/kafka-client"},{"body":"Common configuration The common configuration has 2 parts, which are logger configuration and the telemetry configuration.\nLogger    Config Default Description     log_pattern %time [%level][%field] - %msg The log format pattern configuration.   time_pattern 2006-01-02 15:04:05.000 The time format pattern configuration.   level info The lowest level of printing allowed.    Self Telemetry    Config Default Description     cluster default-cluster The space concept for the deployment, such as the namespace concept in the Kubernetes.   service default-service The group concept for the deployment, such as the service resource concept in the Kubernetes.   instance default-instance The minimum running unit, such as the pod concept in the Kubernetes.    ","excerpt":"Common configuration The common configuration has 2 parts, which are logger configuration and the …","ref":"/docs/skywalking-satellite/v0.1.0/en/setup/configuration/common/","title":"Common configuration"},{"body":"Compiling Platform Linux, MacOS and Windows are supported in SkyWalking Satellite. However, some components don\u0026rsquo;t fit the Windows platform, including:\n mmap-queue  Command git clone https://github.com/apache/skywalking-satellite cd skywalking-satellite git submodule init git submodule update make build ","excerpt":"Compiling Platform Linux, MacOS and Windows are supported in SkyWalking Satellite. However, some …","ref":"/docs/skywalking-satellite/v0.1.0/en/guides/compile/how-to-compile/","title":"Compiling"},{"body":"Concepts and Designs Concepts and Designs help you to learn and understand the SkyWalking Satellite and the landscape.\n What is SkyWalking Satellite?  Overview and Core concepts. Provides a high-level description and introduction, including the problems the project solves. Project Goals. Provides the goals, which SkyWalking Satellite is trying to focus and provide features about them.    After you read the above documents, you should understand basic goals of the SkyWalking Satellite. Now, you can choose which following parts you are interested, then dive in.\n Module Design Plugin mechanism Project Structure The design of the memory mapped queue  ","excerpt":"Concepts and Designs Concepts and Designs help you to learn and understand the SkyWalking Satellite …","ref":"/docs/skywalking-satellite/v0.1.0/en/concepts-and-designs/readme/","title":"Concepts and Designs"},{"body":"Design The mmap-queue is a big, fast, and persistent queue based on the memory-mapped files. One mmap-queue has a directory to store the whole data. The queue directory is made up of many segments and 1 metafile. This is originally implemented by bigqueue project, we changed it a little for fitting the Satellite project requirements.\n Segment: Segment is the real data store center, that provides large-space storage and does not reduce read and write performance as much as possible by using mmap. And we will avoid deleting files by reusing them. Meta: The purpose of meta is to find the data that the consumer needs.  Meta Metadata only needs 80B to store the Metadata for the pipe. But for memory alignment, it takes at least one memory page size, which is generally 4K.\n[ 8Bit ][ 8Bit ][ 8Bit ][ 8Bit ][ 8Bit ][ 8Bit ][ 8Bit ][ 8Bit ][ 8Bit ][ 8Bit ] [metaVersion][ ID ][ offset][ ID ][ offset][ ID ][ offset][ ID ][ offset][capacity] [metaVersion][writing offset][watermark offset][committed offset][reading offset][capacity] Transforming BenchmarkTest Test machine: macbook pro 2018\nModel Name:\tMacBook Pro Model Identifier:\tMacBookPro15,1 Processor Name:\t6-Core Intel Core i7 Processor Speed:\t2.2 GHz Number of Processors:\t1 Total Number of Cores:\t6 L2 Cache (per Core):\t256 KB L3 Cache:\t9 MB Hyper-Threading Technology:\tEnabled Memory:\t16 GB System Firmware Version:\t1554.60.15.0.0 (iBridge: 18.16.13030.0.0,0 push operation goos: darwin goarch: amd64 pkg: github.com/apache/skywalking-satellite/plugins/queue/mmap BenchmarkEnqueue BenchmarkEnqueue/segmentSize:_128KB_maxInMemSegments:18_message:8KB_queueCapacity:10000 10000\t106520 ns/op\t9888 B/op\t9 allocs/op BenchmarkEnqueue/segmentSize:_256KB_maxInMemSegments:10_message:8KB_queueCapacity:10000 18536\t54331 ns/op\t9839 B/op\t9 allocs/op BenchmarkEnqueue/segmentSize:_512KB_maxInMemSegments:6_message:8KB_queueCapacity:10000 27859\t43251 ns/op\t9815 B/op\t9 allocs/op BenchmarkEnqueue/segmentSize:_256KB_maxInMemSegments:20_message:8KB_queueCapacity:10000 23673\t45910 ns/op\t9839 B/op\t9 allocs/op BenchmarkEnqueue/segmentSize:_128KB_maxInMemSegments:10_message:16KB_queueCapacity:10000 10000\t131686 ns/op\t18941 B/op\t10 allocs/op BenchmarkEnqueue/segmentSize:_128KB_maxInMemSegments:10_message:8KB_queueCapacity:100000 23011\t47101 ns/op\t9887 B/op\t9 allocs/op PASS push and pop operation goos: darwin goarch: amd64 pkg: github.com/apache/skywalking-satellite/plugins/queue/mmap BenchmarkEnqueueAndDequeue BenchmarkEnqueueAndDequeue/segmentSize:_128KB_maxInMemSegments:18_message:8KB_queueCapacity:10000 18895\t53056 ns/op\t28773 B/op\t42 allocs/op BenchmarkEnqueueAndDequeue/segmentSize:_256KB_maxInMemSegments:10_message:8KB_queueCapacity:10000 24104\t117128 ns/op\t28725 B/op\t42 allocs/op BenchmarkEnqueueAndDequeue/segmentSize:_512KB_maxInMemSegments:6_message:8KB_queueCapacity:10000 23733\t71632 ns/op\t28699 B/op\t41 allocs/op BenchmarkEnqueueAndDequeue/segmentSize:_256KB_maxInMemSegments:20_message:8KB_queueCapacity:10000 26286\t64377 ns/op\t28725 B/op\t42 allocs/op BenchmarkEnqueueAndDequeue/segmentSize:_128KB_maxInMemSegments:10_message:16KB_queueCapacity:10000 10000\t118004 ns/op\t54978 B/op\t43 allocs/op BenchmarkEnqueueAndDequeue/segmentSize:_128KB_maxInMemSegments:10_message:8KB_queueCapacity:100000 16489\t64400 ns/op\t28772 B/op\t42 allocs/op PASS ","excerpt":"Design The mmap-queue is a big, fast, and persistent queue based on the memory-mapped files. One …","ref":"/docs/skywalking-satellite/v0.1.0/en/concepts-and-designs/mmap-queue/","title":"Design"},{"body":"Design Goals The document outlines the core design goals for SkyWalking Satellite project.\n  Light Weight. SkyWalking Satellite has a limited cost for resources and high-performance because of the requirements of the sidecar deployment model.\n  Pluggability. SkyWalking Satellite core team provides many default implementations, but definitely it is not enough, and also don\u0026rsquo;t fit every scenario. So, we provide a lot of features for being pluggable.\n  Portability. SkyWalking Satellite can run in multiple environments, including:\n Use traditional deployment as a daemon process to collect data. Use cloud services as a sidecar, such as in the Kubernetes platform.    Interoperability. Observability is a big landscape, SkyWalking is impossible to support all, even by its community. So SkyWalking Satellite is compatible with many protocols, including:\n SkyWalking protocol (WIP) Prometheus protocol.    ","excerpt":"Design Goals The document outlines the core design goals for SkyWalking Satellite project.\n  Light …","ref":"/docs/skywalking-satellite/v0.1.0/en/concepts-and-designs/project-goals/","title":"Design Goals"},{"body":"Fallbacker/none-fallbacker Description The fallbacker would do nothing when facing failure data.\nDefaultConfig yaml\n","excerpt":"Fallbacker/none-fallbacker Description The fallbacker would do nothing when facing failure data. …","ref":"/docs/skywalking-satellite/v0.1.0/en/setup/plugins/fallbacker_none-fallbacker/","title":"Fallbacker/none-fallbacker"},{"body":"Fallbacker/timer-fallbacker Description This is a timer fallback trigger to process the forward failure data.\nDefaultConfig # The forwarder max attempt times. max_attempts: 3 # The exponential_backoff is the standard retry duration, and the time for each retry is expanded # by 2 times until the number of retries reaches the maximum.(Time unit is millisecond.) exponential_backoff: 2000 # The max backoff time used in retrying, which would override the latency time when the latency time # with exponential increasing larger than it.(Time unit is millisecond.) max_backoff: 5000 ","excerpt":"Fallbacker/timer-fallbacker Description This is a timer fallback trigger to process the forward …","ref":"/docs/skywalking-satellite/v0.1.0/en/setup/plugins/fallbacker_timer-fallbacker/","title":"Fallbacker/timer-fallbacker"},{"body":"FAQ  What is the performance of the Satellite?  ","excerpt":"FAQ  What is the performance of the Satellite?  ","ref":"/docs/skywalking-satellite/v0.1.0/en/faq/readme/","title":"FAQ"},{"body":"Forwarder/nativelog-kafka-forwarder Description This is a synchronization Kafka forwarder with the SkyWalking native log protocol.\nDefaultConfig # The remote topic.  topic: \u0026#34;log-topic\u0026#34; ","excerpt":"Forwarder/nativelog-kafka-forwarder Description This is a synchronization Kafka forwarder with the …","ref":"/docs/skywalking-satellite/v0.1.0/en/setup/plugins/forwarder_nativelog-kafka-forwarder/","title":"Forwarder/nativelog-kafka-forwarder"},{"body":"Guides If you want to debug or develop SkyWalking Satellite, The following documentations would guide you.\n Contribution  How to contribute a plugin? How to release SkyWalking Satellite?   Compile  How to compile SkyWalking Satellite?   Test  How to add unit test for a plugin?    ","excerpt":"Guides If you want to debug or develop SkyWalking Satellite, The following documentations would …","ref":"/docs/skywalking-satellite/v0.1.0/en/guides/readme/","title":"Guides"},{"body":"How to write a new plugin? If you want to add a custom plugin in SkyWalking Satellite, the following contents would guide you. Let\u0026rsquo;s use memory-queue as an example of how to write a plugin.\n  Choose the plugin category. As the memory-queue is a queue, the plugin should be written in the skywalking-satellite/plugins/queue directory. So we create a new directory called memory as the plugin codes space.\n  Implement the interface in the skywalking-satellite/plugins/queue/api. Each plugin has 3 common methods, which are Name(), Description(), DefaultConfig().\n Name() returns the unique name in the plugin category. Description() returns the description of the plugin, which would be used to generate the plugin documentation. DefaultConfig() returns the default plugin config with yaml pattern, which would be used as the default value in the plugin struct and to generate the plugin documentation.  type Queue struct { config.CommonFields // config  EventBufferSize int `mapstructure:\u0026#34;event_buffer_size\u0026#34;` // The maximum buffer event size.  // components  buffer *goconcurrentqueue.FixedFIFO } func (q *Queue) Name() string { return Name } func (q *Queue) Description() string { return \u0026#34;this is a memory queue to buffer the input event.\u0026#34; } func (q *Queue) DefaultConfig() string { return ` # The maximum buffer event size. event_buffer_size: 5000   Add unit test.\n  Generate the plugin docs.\n  make check ","excerpt":"How to write a new plugin? If you want to add a custom plugin in SkyWalking Satellite, the following …","ref":"/docs/skywalking-satellite/v0.1.0/en/guides/contribution/how-to-write-plugin/","title":"How to write a new plugin?"},{"body":"Module Design Pipe The pipe is an isolation concept in Satellite. Each pipe has one pipeline to process the telemetry data(metrics/traces/logs). Two pipes are not sharing data.\n Satellite --------------------------------------------------------------------- | ------------------------------------------- | | | Pipe | | | ------------------------------------------- | | ------------------------------------------- | | | Pipe | | | ------------------------------------------- | | ------------------------------------------- | | | Pipe | | | ------------------------------------------- | --------------------------------------------------------------------- Modules Module is the core workers in Satellite. Module is constituted by the specific extension plugins. There are 3 modules in one namespace, which are Gatherer, Processor, and Sender.\n The Gatherer module is responsible for fetching or receiving data and pushing the data to Queue. So there are 2 kinds of Gatherer, which are ReceiverGatherer and FetcherGatherer. The Processor module is responsible for reading data from the queue and processing data by a series of filter chains. The Sender module is responsible for async processing and forwarding the data to the external services in the batch mode. After sending success, Sender would also acknowledge the offset of Queue in Gatherer.   Pipe -------------------------------------------------------------------- | ---------- ----------- -------- | | | Gatherer | =\u0026gt; | Processor | =\u0026gt; | Sender | | | ---------- ----------- -------- | -------------------------------------------------------------------- LifeCycle\n Prepare: Prepare phase is to do some preparation works, such as register the client status listener to the client in ReceiverGatherer. Boot: Boot phase is to start the current module until receives a close signal. ShutDown: ShutDown phase is to close the used resources.  Plugins Plugin is the minimal components in the module. Sateliite has 2 plugin catalogs, which are sharing plugins and normal plugins.\n a sharing plugin instance could be sharing with multiple modules in the different pipes. a normal plugin instance is only be used in a fixed module of the fixed pipes.  Sharing plugin Nowadays, there are 2 kinds of sharing plugins in Satellite, which are server plugins and client plugins. The reason why they are sharing plugins is to reduce the resource cost in connection. Server plugins are sharing with the ReceiverGatherer modules in the different pipes to receive the external requests. And the client plugins is sharing with the Sender modules in the different pipes to connect with external services, such as Kafka and OAP.\n Sharing Server Sharing Client -------------------------------------------------------------------- | ------------------ ----------- -------- | | | ReceiverGatherer | =\u0026gt; | Processor | =\u0026gt; | Sender | | | ------------------ ----------- -------- | -------------------------------------------------------------------- -------------------------------------------------------------------- | ------------------ ----------- -------- | | | ReceiverGatherer | =\u0026gt; | Processor | =\u0026gt; | Sender | | | ------------------ ----------- -------- | -------------------------------------------------------------------- -------------------------------------------------------------------- | ------------------ ----------- -------- | | | ReceiverGatherer | =\u0026gt; | Processor | =\u0026gt; | Sender | | | ------------------ ----------- -------- | -------------------------------------------------------------------- Normal plugin There are 7 kinds of normal plugins in Satellite, which are Receiver, Fetcher, Queue, Parser, Filter, Forwarder, and Fallbacker.\n Receiver: receives the input APM data from the request. Fetcher: fetch the APM data by fetching. Queue: store the APM data to ensure the data stability. Parser: supports some ways to parse data, such parse a csv file. Filter: processes the APM data. Forwarder: forwards the APM data to the external receiver, such as Kafka and OAP. Fallbacker: supports some fallback strategies, such as timer retry strategy.   Gatherer Processor ------------------------------- ------------------------------------------- | ----------- --------- | | ----------- ----------- | | | Receiver | ==\u0026gt; | Queue | |==\u0026gt;| | Filter | ==\u0026gt; ... ==\u0026gt; | Filter | | | | /Fetcher | | Mem/File | | | ----------- ----------- | | ----------- ---------- | | || || | -------------------------------- | \\/\t\\/ | | --------------------------------------- | | | OutputEventContext | | | --------------------------------------- | ------------------------------------------- || \\/ Sender ------------------------------------------ | --- --- | | | B | | D | ----------------- | | | A | | I | |Segment Forwarder| | | | T | | S | | (Fallbacker) | | | | C | | P | ----------------- | | | H | =\u0026gt; | A | | ===\u0026gt; Kakfa/OAP | | B | | T | =\u0026gt; ...... | | | U | | C | | | | F | | H | ----------------- | | | F | | E | | Meter Forwarder| | | | E | | R | | (Fallbacker | | | | R | | | ----------------- | | --- --- | ------------------------------------------ 1. The Fetcher/Receiver plugin would fetch or receive the input data. 2. The Parser plugin would parse the input data to SerializableEvent that is supported to be stored in Queue. 3. The Queue plugin stores the SerializableEvent. However, whether serializing depends on the Queue implements. For example, the serialization is unnecessary when using a Memory Queue. Once an event is pulled by the consumer of Queue, the event will be processed by the filters in Processor. 4. The Filter plugin would process the event to create a new event. Next, the event is passed to the next filter to do the same things until the whole filters are performed. All created events would be stored in the OutputEventContext. However, only the events labeled with RemoteEvent type would be forwarded by Forwarder. 5. After processing, the events in OutputEventContext would be stored in the BatchBuffer. When the timer is triggered or the capacity limit is reached, the events in BatchBuffer would be partitioned by EventType and sent to the different Forwarders, such as Segment Forwarder and Meter Forwarder. 6. The Follower in different Senders would share with the remote client to avoid make duplicate connections and have the same Fallbacker(FallBack strategy) to process data. When all forwarders send success or process success in Fallbacker, the dispatcher would also ack the batch is a success. ============================================================================================ ","excerpt":"Module Design Pipe The pipe is an isolation concept in Satellite. Each pipe has one pipeline to …","ref":"/docs/skywalking-satellite/v0.1.0/en/concepts-and-designs/module_design/","title":"Module Design"},{"body":"Overview SkyWalking Satellite: an open-source agent designed for the cloud-native infrastructures, which provides a low-cost, high-efficient, and more secure way to collect telemetry data, such that Trace Segments, Logs, or Metrics.\nWhy use SkyWalking Satellite? Observability is the solution to the complex scenario of cloud-native services. However, we may encounter different telemetry data scenarios, different language services, big data analysis, etc. Satellite provides a unified data collection layer for cloud-native services. You can easily use it to connect to the SkyWalking ecosystem and enhance the capacity of SkyWalking. There are some enhance features on the following when using Satellite.\n Provide a unified data collection layer to collect logs, traces, and metrics. Provide a safer local cache to reduce the memory cost of the service. Provide the unified transfer way shields the functional differences in the different language libs, such as MQ. Provides the preprocessing functions to ensure accuracy of the metrics, such as sampling.  Architecture SkyWalking Satellite is logically split into three parts: Gatherer, Processor, and Sender.\n Gatherer collect data and reformat them for SkyWalking requirements. Processor processes the input data to generate the new data for Observability. Sender would transfer the downstream data to the SkyWalking OAP with different protocols.  ","excerpt":"Overview SkyWalking Satellite: an open-source agent designed for the cloud-native infrastructures, …","ref":"/docs/skywalking-satellite/v0.1.0/en/concepts-and-designs/overview/","title":"Overview"},{"body":"Pipe Plugins The pipe plugin configurations contain a series of pipe configuration. Each pipe configuration has 5 parts, which are common_config, gatherer, processor and the sender.\ncommon_config    Config Description     pipe_name The unique collect space name.    Gatherer The gatherer has 2 roles, which are the receiver and fetcher.\nReceiver Role    Config Description     server_name The server name in the sharing pipe, which would be used in the receiver plugin.   receiver The receiver configuration. Please read the doc to find all receiver plugins.   queue The queue buffers the input telemetry data. Please read the doc to find all queue plugins.    Fetcher Role    Config Description     fetch_interval The time interval between two fetch operations. The time unit is millisecond.   fetcher The fetcher configuration. Please read the doc to find all fetcher plugins.   queue The queue buffers the input telemetry data. Please read the doc to find all queue plugins.    processor The filter configuration. Please read the doc to find all filter plugins.\nsender    Config Description     flush_time The time interval between two flush operations. And the time unit is millisecond.   max_buffer_size The maximum buffer elements.   min_flush_events The minimum flush elements.   client_name The client name used in the forwarders of the sharing pipe.   forwarders The forwarder plugin list. Please read the doc to find all forwarders plugins.   fallbacker The fallbacker plugin. Please read the doc to find all fallbacker plugins.    Example pipes: - common_config: pipe_name: pipe1 gatherer: server_name: \u0026#34;grpc-server\u0026#34; receiver: plugin_name: \u0026#34;grpc-nativelog-receiver\u0026#34; queue: plugin_name: \u0026#34;mmap-queue\u0026#34; segment_size: ${SATELLITE_MMAP_QUEUE_SIZE:524288} max_in_mem_segments: ${SATELLITE_MMAP_QUEUE_MAX_IN_MEM_SEGMENTS:6} queue_dir: \u0026#34;pipe1-log-grpc-receiver-queue\u0026#34; processor: filters: sender: fallbacker: plugin_name: none-fallbacker flush_time: ${SATELLITE_PIPE1_SENDER_FLUSH_TIME:1000} max_buffer_size: ${SATELLITE_PIPE1_SENDER_MAX_BUFFER_SIZE:200} min_flush_events: ${SATELLITE_PIPE1_SENDER_MIN_FLUSH_EVENTS:100} client_name: kafka-client forwarders: - plugin_name: nativelog-kafka-forwarder topic: ${SATELLITE_NATIVELOG-TOPIC:log-topic} ","excerpt":"Pipe Plugins The pipe plugin configurations contain a series of pipe configuration. Each pipe …","ref":"/docs/skywalking-satellite/v0.1.0/en/setup/configuration/pipe-plugins/","title":"Pipe Plugins"},{"body":"Plugin List  Client  kafka-client   Fallbacker  none-fallbacker timer-fallbacker   Fetcher Filter Forwarder  nativelog-kafka-forwarder   Parser Queue  memory-queue mmap-queue   Receiver  grpc-nativelog-receiver http-nativelog-receiver   Server  grpc-server http-server prometheus-server    ","excerpt":"Plugin List  Client  kafka-client   Fallbacker  none-fallbacker timer-fallbacker   Fetcher Filter …","ref":"/docs/skywalking-satellite/v0.1.0/en/setup/plugins/plugin-list/","title":"Plugin List"},{"body":"plugin structure Plugin is a common concept for Satellite, which is in all externsion plugins.\nRegistration mechanism The Plugin registration mechanism in Satellite is similar to the SPI registration mechanism of Java. Plugin registration mechanism supports to register an interface and its implementation, that means different interfaces have different registration spaces. We can easily find the type of a specific plugin according to the interface and the plugin name and initialize it according to the type.\nstructure:\n code: map[reflect.Type]map[string]reflect.Value meaning: map[interface type]map[plugin name] plugin type  Initialization mechanism Users can easily find a plugin type and initialize an empty plugin instance according to the previous registration mechanism. For setting up the configuration of the extension convenience, we define the initialization mechanism in Plugin structure.\nIn the initialization mechanism, the plugin category(interface) and the init config is required.\nInitialize processing is like the following.\n Find the plugin name in the input config according to the fixed key plugin_name. Find plugin type according to the plugin category(interface) and the plugin name. Create an empty plugin. Initialize the plugin according to the merged config, which is created by the input config and the default config.  Plugin usage in Satellite Nowadays, the numbers of the Plugin categories is 2. One is the sharing Plugin, and another is the other normal Plugin.\n Extension Plugins:  sharing plugins  Server Plugin Client Plugin   normal plugins  Receiver Plugin Fetcher Plugin Parser Plugin Queue Plugin Filter Plugin Fallbacker Plugin Forwarder Plugin      ","excerpt":"plugin structure Plugin is a common concept for Satellite, which is in all externsion plugins. …","ref":"/docs/skywalking-satellite/v0.1.0/en/concepts-and-designs/plugin_mechanism/","title":"plugin structure"},{"body":"Project Structure  cmd: The starter of Satellite. configs: Satellite configs. internal: Core, API, and common utils.  internal/pkg: Sharing with Core and Plugins, such as api and utils. internal/satellite: The core of Satellite.   plugins: Contains all plugins.  plugins/{type}: Contains the plugins of this {type}. Satellite has 9 plugin types. plugins/{type}/api: Contains the plugin definition and initializer. plugins/{type}/{plugin-name}: Contains the specific plugin. init.go: Register the plugins to the plugin registry.    . ├── CHANGES.md ├── cmd ├── configs ├── docs ├── go.sum ├── internal │ ├── pkg │ └── satellite ├── plugins │ ├── client │ ├── fallbacker │ ├── fetcher │ ├── filter │ ├── forwarder │ ├── init.go │ ├── parser │ ├── queue │ ├── receiver │ └── server ","excerpt":"Project Structure  cmd: The starter of Satellite. configs: Satellite configs. internal: Core, API, …","ref":"/docs/skywalking-satellite/v0.1.0/en/concepts-and-designs/project_structue/","title":"Project Structure"},{"body":"Queue/memory-queue Description This is a memory queue to buffer the input event.\nDefaultConfig # The maximum buffer event size. event_buffer_size: 5000 ","excerpt":"Queue/memory-queue Description This is a memory queue to buffer the input event.\nDefaultConfig # The …","ref":"/docs/skywalking-satellite/v0.1.0/en/setup/plugins/queue_memory-queue/","title":"Queue/memory-queue"},{"body":"Queue/mmap-queue Description This is a memory mapped queue to provide the persistent storage for the input event. Please note that this plugin does not support Windows platform.\nDefaultConfig # The size of each segment. Default value is 256K. The unit is Byte. segment_size: 262114 # The max num of segments in memory. Default value is 10. max_in_mem_segments: 10 # The capacity of Queue = segment_size * queue_capacity_segments. queue_capacity_segments: 2000 # The period flush time. The unit is ms. Default value is 1 second. flush_period: 1000 # The max number in one flush time. Default value is 10000. flush_ceiling_num: 10000 # The max size of the input event. Default value is 20k. max_event_size: 20480 ","excerpt":"Queue/mmap-queue Description This is a memory mapped queue to provide the persistent storage for the …","ref":"/docs/skywalking-satellite/v0.1.0/en/setup/plugins/queue_mmap-queue/","title":"Queue/mmap-queue"},{"body":"Receiver/grpc-nativelog-receiver Description This is a receiver for SkyWalking native logging format, which is defined at https://github.com/apache/skywalking-data-collect-protocol/blob/master/logging/Logging.proto.\nDefaultConfig yaml\n","excerpt":"Receiver/grpc-nativelog-receiver Description This is a receiver for SkyWalking native logging …","ref":"/docs/skywalking-satellite/v0.1.0/en/setup/plugins/receiver_grpc-nativelog-receiver/","title":"Receiver/grpc-nativelog-receiver"},{"body":"Receiver/http-nativelog-receiver Description This is a receiver for SkyWalking http logging format, which is defined at https://github.com/apache/skywalking-data-collect-protocol/blob/master/logging/Logging.proto.\nDefaultConfig # The native log request URI. uri: \u0026#34;/logging\u0026#34; # The request timeout seconds. timeout: 5 ","excerpt":"Receiver/http-nativelog-receiver Description This is a receiver for SkyWalking http logging format, …","ref":"/docs/skywalking-satellite/v0.1.0/en/setup/plugins/receiver_http-nativelog-receiver/","title":"Receiver/http-nativelog-receiver"},{"body":"Server/grpc-server Description This is a sharing plugin, which would start a gRPC server.\nDefaultConfig # The address of grpc server. Default value is :11800 address: :11800 # The network of grpc. Default value is :tcp network: tcp # The max size of receiving log. Default value is 2M. The unit is Byte. max_recv_msg_size: 2097152 # The max concurrent stream channels. max_concurrent_streams: 32 # The TLS cert file path. tls_cert_file: \u0026#34;\u0026#34; # The TLS key file path. tls_key_file: \u0026#34;\u0026#34; ","excerpt":"Server/grpc-server Description This is a sharing plugin, which would start a gRPC server. …","ref":"/docs/skywalking-satellite/v0.1.0/en/setup/plugins/server_grpc-server/","title":"Server/grpc-server"},{"body":"Server/http-server Description This is a sharing plugin, which would start a http server.\nDefaultConfig # The http server address. address: \u0026#34;:12800\u0026#34; ","excerpt":"Server/http-server Description This is a sharing plugin, which would start a http server. …","ref":"/docs/skywalking-satellite/v0.1.0/en/setup/plugins/server_http-server/","title":"Server/http-server"},{"body":"Server/prometheus-server Description This is a prometheus server to export the metrics in Satellite.\nDefaultConfig # The prometheus server address. address: \u0026#34;:1234\u0026#34; # The prometheus server metrics endpoint. endpoint: \u0026#34;/metrics\u0026#34; ","excerpt":"Server/prometheus-server Description This is a prometheus server to export the metrics in Satellite. …","ref":"/docs/skywalking-satellite/v0.1.0/en/setup/plugins/server_prometheus-server/","title":"Server/prometheus-server"},{"body":"Setting Override SkyWalking Satellite supports setting overrides by system environment variables. You could override the settings in satellite_config.yaml\nSystem environment variables   Example\nOverride log_pattern in this setting segment through environment variables\n  logger: log_pattern: ${SATELLITE_LOGGER_LOG_PATTERN:%time [%level][%field] - %msg} time_pattern: ${SATELLITE_LOGGER_TIME_PATTERN:2006-01-02 15:04:05.000} level: ${SATELLITE_LOGGER_LEVEL:info} If the SATELLITE_LOGGER_LOG_PATTERN  environment variable exists in your operating system and its value is %msg, then the value of log_pattern here will be overwritten to %msg, otherwise, it will be set to %time [%level][%field] - %msg.\n","excerpt":"Setting Override SkyWalking Satellite supports setting overrides by system environment variables. …","ref":"/docs/skywalking-satellite/v0.1.0/en/setup/configuration/override-settings/","title":"Setting Override"},{"body":"Setup First and most important thing is, SkyWalking Satellite startup behaviours are driven by configs/satellite_config.yaml. Understanding the setting file will help you to read this document.\nStartup script Startup Script\nbin/startup.sh satellite_config.yaml The core concept behind this setting file is, SkyWalking Satellite is based on pure modularization design. End user can switch or assemble the collector features by their own requirements.\nSo, in satellite_config.yaml, there are three parts.\n The common configurations. The sharing plugin configurations. The pipe plugin configurations.  Advanced feature document link list  Overriding settings in satellite_config.yaml is supported  ","excerpt":"Setup First and most important thing is, SkyWalking Satellite startup behaviours are driven by …","ref":"/docs/skywalking-satellite/v0.1.0/en/setup/readme/","title":"Setup"},{"body":"Sharing Plugins Sharing plugin configurations has three 3 parts, which are common_config, clients and servers.\nCommon Configuration    Config Default Description     pipe_name sharing The group name of sharing plugins    Clients Clients have a series of client plugins, which would be sharing with the plugins of the other pipes. Please read the doc to find all client plugin configurations.\nServers Servers have a series of server plugins, which would be sharing with the plugins of the other pipes. Please read the doc to find all server plugin configurations.\nExample # The sharing plugins referenced by the specific plugins in the different pipes. sharing: common_config: pipe_name: sharing clients: - plugin_name: \u0026#34;kafka-client\u0026#34; brokers: ${SATELLITE_KAFKA_CLIENT_BROKERS:127.0.0.1:9092} version: ${SATELLITE_KAFKA_VERSION:\u0026#34;2.1.1\u0026#34;} servers: - plugin_name: \u0026#34;grpc-server\u0026#34; - plugin_name: \u0026#34;prometheus-server\u0026#34; address: ${SATELLITE_PROMETHEUS_ADDRESS:\u0026#34;:8090\u0026#34;} ","excerpt":"Sharing Plugins Sharing plugin configurations has three 3 parts, which are common_config, clients …","ref":"/docs/skywalking-satellite/v0.1.0/en/setup/configuration/sharing-plugins/","title":"Sharing Plugins"},{"body":"Unit Test For Satellite, the specific plugin may have some common dependencies. So we provide a global test initializer to init the dependencies.\nimport ( _ \u0026quot;github.com/apache/skywalking-satellite/internal/satellite/test\u0026quot; ) ","excerpt":"Unit Test For Satellite, the specific plugin may have some common dependencies. So we provide a …","ref":"/docs/skywalking-satellite/v0.1.0/en/guides/test/how-to-unit-test/","title":"Unit Test"},{"body":"Welcome Here are SkyWalking Satellite official documentations. You\u0026rsquo;re welcome to join us.\nFrom here you can learn all about SkyWalking Satellite\u0026rsquo;s architecture, how to deploy and use SkyWalking Satellite.\n  Concepts and Designs. The most important core ideas about SkyWalking Satellite. You can learn from here if you want to understand what is going on under our cool features.\n  Setup. Introduce how to set up the SkyWalking Satellite.\n  Guides. Guide users to develop or debug SkyWalking Satellite.\n  Protocols. Protocols show the communication ways between agents/probes, Satellite and SkyWalking. Anyone interested in uplink telemetry data should definitely read this.\n  Change logs. The feature records of the different versions.\n  FAQs. A manifest of already known setup problems, secondary developments experiments. When you are facing a problem, check here first.\n  We\u0026rsquo;re always looking for help improve our documentation and codes, so please don’t hesitate to file an issue if you see any problem. Or better yet, submit your own contributions through pull request to help make them better.\n","excerpt":"Welcome Here are SkyWalking Satellite official documentations. You\u0026rsquo;re welcome to join us.\nFrom …","ref":"/docs/skywalking-satellite/v0.1.0/readme/","title":"Welcome"},{"body":"What is the performance of the Satellite? Performance The performance reduction of the mmap-queue is mainly due to the file persistent operation to ensure data stability. However, the queue is used to collect some core telemetry data. We will continue to optimize the performance of this queue.\n 0.5 core supported 3000 ops throughput with memory queue. 0.5 core supported 1500 ops throughput with the memory mapped queue(Ensure data stability).  Details Testing environment  machine:  cpu: INTEL Xeon E5-2650 V4 12C 2.2GHZ * 2 memory: INVENTEC PC4-19200 * 8 harddisk: INVENTEC SATA 4T 7.2K * 8   Kafka:  region: the same region with the test machine in Baidu Cloud. version.: 0.1.1.0   The input plugin: grpc-nativelog-receiver resource limit:  cpu: 500m(0.5 core) memory: 100M    Performance Test With Memory Queue    Qps stack memory in use heap memory in use no-heap memory in use     400 2.13M 11M 83K   800 2.49M 13.4M 83K   1200 2.72M 13.4M 83K   1600 2.85M 16.2M 83K   2000 2.92M 17.6M 83K   2400 2.98M 18.3M 83K   2800 3.54M 26.8M 83K   3000 3.34M 28M 83K    Performance Test With Mmap Queue    Qps stack memory in use heap memory in use no-heap memory in use     400 2.39M 9.5M 83K   800 2.43M 12.1M 83K   1200 2.49M 12M 83K   1600 2.62M 13.3M 83K    ","excerpt":"What is the performance of the Satellite? Performance The performance reduction of the mmap-queue is …","ref":"/docs/skywalking-satellite/v0.1.0/en/faq/performance/","title":"What is the performance of the Satellite?"},{"body":" Origin: Observe VM Service Meshes with Apache SkyWalking and the Envoy Access Log Service - The New Stack\n Apache SkyWalking: an APM (application performance monitor) system, especially designed for microservices, cloud native, and container-based (Docker, Kubernetes, Mesos) architectures.\nEnvoy Access Log Service: Access Log Service (ALS) is an Envoy extension that emits detailed access logs of all requests going through Envoy.\nBackground In the previous post, we talked about the observability of service mesh under Kubernetes environment, and applied it to the bookinfo application in practice. We also mentioned that, in order to map the IP addresses into services, SkyWalking needs access to the service metadata from a Kubernetes cluster, which is not available for services deployed in virtual machines (VMs). In this post, we will introduce a new analyzer in SkyWalking that leverages Envoy’s metadata exchange mechanism to decouple with Kubernetes. The analyzer is designed to work in Kubernetes environments, VM environments, and hybrid environments. If there are virtual machines in your service mesh, you might want to try out this new analyzer for better observability, which we will demonstrate in this tutorial.\nHow it works The mechanism of how the analyzer works is the same as what we discussed in the previous post. What makes VMs different from Kubernetes is that, for VM services, there are no places where we can fetch the metadata to map the IP addresses into services.\nThe basic idea we present in this article is to carry the metadata along with Envoy’s access logs, which is called metadata-exchange mechanism in Envoy. When Istio pilot-agent starts an Envoy proxy as a sidecar of a service, it collects the metadata of that service from the Kubernetes platform, or a file on the VM where that service is deployed, and injects the metadata into the bootstrap configuration of Envoy. Envoy will carry the metadata transparently when emitting access logs to the SkyWalking receiver.\nBut how does Envoy compose a piece of a complete access log that involves the client side and server side? When a request goes out from Envoy, a plugin of istio-proxy named \u0026ldquo;metadata-exchange\u0026rdquo; injects the metadata into the http headers (with a prefix like x-envoy-downstream-), and the metadata is propagated to the server side. The Envoy sidecar of the server side receives the request and parses the headers into metadata, and puts the metadata into the access log, keyed by wasm.downstream_peer. The server side Envoy also puts its own metadata into the access log keyed by wasm.upstream_peer. Hence the two sides of a single request are completed.\nWith the metadata-exchange mechanism, we can use the metadata directly without any extra query.\nExample In this tutorial, we will use another demo application Online Boutique that consists of 10+ services so that we can deploy some of them in VMs and make them communicate with other services deployed in Kubernetes.\nTopology of Online Boutique In order to cover as many cases as possible, we will deploy CheckoutService and PaymentService on VM and all the other services on Kubernetes, so that we can cover the cases like Kubernetes → VM (e.g. Frontend → CheckoutService), VM → Kubernetes (e.g. CheckoutService → ShippingService), and VM → VM ( e.g. CheckoutService → PaymentService).\nNOTE: All the commands used in this tutorial are accessible on GitHub.\ngit clone https://github.com/SkyAPMTest/sw-als-vm-demo-scripts cd sw-als-vm-demo-scripts Make sure to init the gcloud SDK properly before moving on. Modify the GCP_PROJECT in file env.sh to your own project name. Most of the other variables should be OK to work if you keep them intact. If you would like to use ISTIO_VERSION \u0026gt;/= 1.8.0, please make sure this patch is included.\n  Prepare Kubernetes cluster and VM instances 00-create-cluster-and-vms.sh creates a new GKE cluster and 2 VM instances that will be used through the entire tutorial, and sets up some necessary firewall rules for them to communicate with each other.\n  Install Istio and SkyWalking 01a-install-istio.sh installs Istio Operator with spec resources/vmintegration.yaml. In the YAML file, we enable the meshExpansion that supports VM in mesh. We also enable the Envoy access log service and specify the address skywalking-oap.istio-system.svc.cluster.local:11800 to which Envoy emits the access logs. 01b-install-skywalking.sh installs Apache SkyWalking and sets the analyzer to mx-mesh.\n  Create files to initialize the VM 02-create-files-to-transfer-to-vm.sh creates necessary files that will be used to initialize the VMs. 03-copy-work-files-to-vm.sh securely transfers the generated files to the VMs with gcloud scp command. Now use ./ssh.sh checkoutservice and ./ssh.sh paymentservice to log into the two VMs respectively, and cd to the ~/work directory, execute ./prep-checkoutservice.sh on checkoutservice VM instance and ./prep-paymentservice.sh on paymentservice VM instance. The Istio sidecar should be installed and started properly. To verify that, use tail -f /var/logs/istio/istio.log to check the Istio logs. The output should be something like:\n2020-12-12T08:07:07.348329Z\tinfo\tsds\tresource:default new connection 2020-12-12T08:07:07.348401Z\tinfo\tsds\tSkipping waiting for gateway secret 2020-12-12T08:07:07.348401Z\tinfo\tsds\tSkipping waiting for gateway secret 2020-12-12T08:07:07.568676Z\tinfo\tcache\tRoot cert has changed, start rotating root cert for SDS clients 2020-12-12T08:07:07.568718Z\tinfo\tcache\tGenerateSecret default 2020-12-12T08:07:07.569398Z\tinfo\tsds\tresource:default pushed key/cert pair to proxy 2020-12-12T08:07:07.949156Z\tinfo\tcache\tLoaded root cert from certificate ROOTCA 2020-12-12T08:07:07.949348Z\tinfo\tsds\tresource:ROOTCA pushed root cert to proxy 2020-12-12T20:12:07.384782Z\tinfo\tsds\tresource:default pushed key/cert pair to proxy 2020-12-12T20:12:07.384832Z\tinfo\tsds\tDynamic push for secret default The dnsmasq configuration address=/.svc.cluster.local/{ISTIO_SERVICE_IP_STUB} also resolves the domain names ended with .svc.cluster.local to Istio service IP, so that you are able to access the Kubernetes services in the VM by fully qualified domain name (FQDN) such as httpbin.default.svc.cluster.local.\n  Deploy demo application Because we want to deploy CheckoutService and PaymentService manually on VM, resources/google-demo.yaml removes the two services from the original YAML . 04a-deploy-demo-app.sh deploys the other services on Kubernetes. Then log into the 2 VMs, run ~/work/deploy-checkoutservice.sh and ~/work/deploy-paymentservice.sh respectively to deploy CheckoutService and PaymentService.\n  Register VMs to Istio Services on VMs can access the services on Kubernetes by FQDN, but that’s not the case when the Kubernetes services want to talk to the VM services. The mesh has no idea where to forward the requests such as checkoutservice.default.svc.cluster.local because checkoutservice is isolated in the VM. Therefore, we need to register the services to the mesh. 04b-register-vm-with-istio.sh registers the VM services to the mesh by creating a \u0026ldquo;dummy\u0026rdquo; service without running Pods, and a WorkloadEntry to bridge the \u0026ldquo;dummy\u0026rdquo; service with the VM service.\n  Done! The demo application contains a load generator service that performs requests repeatedly. We only need to wait a few seconds, and then open the SkyWalking web UI to check the results.\nexport POD_NAME=$(kubectl get pods --namespace istio-system -l \u0026quot;app=skywalking,release=skywalking,component=ui\u0026quot; -o jsonpath=\u0026quot;{.items[0].metadata.name}\u0026quot;) echo \u0026quot;Visit http://127.0.0.1:8080 to use your application\u0026quot; kubectl port-forward $POD_NAME 8080:8080 --namespace istio-system Navigate the browser to http://localhost:8080 . The metrics, topology should be there.\nTroubleshooting If you face any trouble when walking through the steps, here are some common problems and possible solutions:\n  VM service cannot access Kubernetes services? It’s likely the DNS on the VM doesn’t correctly resolve the fully qualified domain names. Try to verify that with nslookup istiod.istio-system.svc.cluster.local. If it doesn’t resolve to the Kubernetes CIDR address, recheck the step in prep-checkoutservice.sh and prep-paymentservice.sh. If the DNS works correctly, try to verify that Envoy has fetched the upstream clusters from the control plane with curl http://localhost:15000/clusters. If it doesn’t contain the target service, recheck prep-checkoutservice.sh.\n  Services are normal but nothing on SkyWalking WebUI? Check the SkyWalking OAP logs via kubectl -n istio-system logs -f $(kubectl get pod -A -l \u0026quot;app=skywalking,release=skywalking,component=oap\u0026quot; -o name) and WebUI logs via kubectl -n istio-system logs -f $(kubectl get pod -A -l \u0026quot;app=skywalking,release=skywalking,component=ui\u0026quot; -o name) to see whether there are any error logs . Also, make sure the time zone at the bottom-right of the browser is set to UTC +0.\n  Additional Resources  Observe a Service Mesh with Envoy ALS.  ","excerpt":"Origin: Observe VM Service Meshes with Apache SkyWalking and the Envoy Access Log Service - The New …","ref":"/blog/obs-service-mesh-vm-with-sw-and-als/","title":"Observe VM Service Meshes with Apache SkyWalking and the Envoy Access Log Service"},{"body":"","excerpt":"","ref":"/tags/service-mesh/","title":"Service Mesh"},{"body":"","excerpt":"","ref":"/tags/agent/","title":"Agent"},{"body":"When using SkyWalking java agent, people usually propagate context easily. They even do not need to change the business code. However, it becomes harder when you want to propagate context between threads when using ThreadPoolExecutor. You can use the RunnableWrapper in the maven artifact org.apache.skywalking:apm-toolkit-trace. This way you must change your code. The developer manager usually don\u0026rsquo;t like this because there may be lots of projects, or lots of runnable code. If they don\u0026rsquo;t use SkyWalking some day, the code added will be superfluous and inelegant.\nIs there a way to propagate context without changing the business code? Yes.\nSkywalking java agent enhances a class by add a field and implement an interface. The ThreadPoolExecutor is a special class that is used widely. We even don\u0026rsquo;t know when and where it is loaded. Most JVMs do not allow changes in the class file format for classes that have been loaded previously. So SkyWalking should not enhance the ThreadPoolExecutor successfully by retransforming when the ThreadPoolExecutor has been loaded. However, we can apply advice to the ThreadPoolExecutor#execute method and wrap the Runnable param using our own agent, then enhance the wrapper class by SkyWalking java agent. An advice do not change the layout of a class.\nNow we should decide how to do this. You can use the RunnableWrapper in the maven artifact org.apache.skywalking:apm-toolkit-trace to wrap the param, but you need to face another problem. This RunnableWrapper has a plugin whose active condition is checking if there is @TraceCrossThread. Agent core uses net.bytebuddy.pool.TypePool.Default.WithLazyResolution.LazyTypeDescription to find the annotations of a class. The LazyTypeDescription finds annotations by using a URLClassLoader with no urls if the classloader is null(bootstrap classloader). So it can not find the @TraceCrossThread class unless you change the LocationStrategy of SkyWalking java agent builder.\nIn this project, I write my own wrapper class, and simply add a plugin with a name match condition. Next, Let me show you how these two agents work together.\n  Move the plugin to the skywalking \u0026ldquo;plugins\u0026rdquo; directory.\n  Add this agent after the SkyWalking agent since the wrapper class should not be loaded before SkyWalking agent instrumentation have finished. For example,\n java -javaagent:/path/to/skywalking-agent.jar -javaagent:/path/to/skywalking-tool-agent-v1.0.0.jar \u0026hellip;\n   When our application runs\n SkyWalking java agent adds a transformer by parsing the plugin for enhancing the wrapper class in the tool agent. The tool agent loads the wrapper class into bootstrap classloader. This triggers the previous transformer. The tool agent applies an advice to the ThreadPoolExecutor class, wrapping the java.lang.Runnable param of \u0026ldquo;execute\u0026rdquo; method with the wrapper class. Now SkyWalking propagates the context with the wrapper class.    Enjoy tracing with ThreadPoolExecutor in SkyWalking!\n","excerpt":"When using SkyWalking java agent, people usually propagate context easily. They even do not need to …","ref":"/blog/2021-02-09-skywalking-trace-threadpool/","title":"Apache SkyWalking: How to propagate context between threads when using ThreadPoolExecutor"},{"body":"","excerpt":"","ref":"/tags/java/","title":"Java"},{"body":"SkyWalking CLI 0.6.0 is released. Go to downloads page to find release tars.\n  Features\n Support authorization when connecting to the OAP Add install command and manifest sub-command Add event command and report sub-command    Bug Fixes\n Fix the bug that can\u0026rsquo;t query JVM instance metrics    Chores\n Set up a simple test with GitHub Actions Reorganize the project layout Update year in NOTICE Add missing license of swck Use license-eye to check license header    ","excerpt":"SkyWalking CLI 0.6.0 is released. Go to downloads page to find release tars.\n  Features\n Support …","ref":"/events/release-apache-skywalking-cli-0-6-0/","title":"Release Apache SkyWalking CLI 0.6.0"},{"body":"","excerpt":"","ref":"/tags/infrastructure-monitoring/","title":"Infrastructure Monitoring"},{"body":" Origin: Tetrate.io blog\n Background Apache SkyWalking\u0026ndash; the APM tool for distributed systems\u0026ndash; has historically focused on providing observability around tracing and metrics, but service performance is often affected by the host. The newest release, SkyWalking 8.4.0, introduces a new feature for monitoring virtual machines. Users can easily detect possible problems from the dashboard\u0026ndash; for example, when CPU usage is overloaded, when there’s not enough memory or disk space, or when the network status is unhealthy, etc.\nHow it works SkyWalking leverages Prometheus and OpenTelemetry for collecting metrics data as we did for Istio control panel metrics; Prometheus is mature and widely used, and we expect to see increased adoption of the new CNCF project, OpenTelemetry. The SkyWalking OAP Server receives these metrics data of OpenCensus format from OpenTelemetry. The process is as follows:\n Prometheus Node Exporter collects metrics data from the VMs. OpenTelemetry Collector fetches metrics from Node Exporters via Prometheus Receiver, and pushes metrics to SkyWalking OAP Server via the OpenCensus GRPC Exporter. The SkyWalking OAP Server parses the expression with MAL to filter/calculate/aggregate and store the results. The expression rules are in /config/otel-oc-rules/vm.yaml. We can now see the data on the SkyWalking WebUI dashboard.  What to monitor SkyWalking provides default monitoring metrics including:\n CPU Usage (%) Memory RAM Usage (MB) Memory Swap Usage (MB) CPU Average Used CPU Load Memory RAM (total/available/used MB) Memory Swap (total/free MB) File System Mount point Usage (%) Disk R/W (KB/s) Network Bandwidth Usage (receive/transmit KB/s) Network Status (tcp_curr_estab/tcp_tw/tcp_alloc/sockets_used/udp_inuse) File fd Allocated  The following is how it looks when we monitor Linux:\nHow to use To enable this feature, we need to install Prometheus Node Exporter and OpenTelemetry Collector and activate the VM monitoring rules in SkyWalking OAP Server.\nInstall Prometheus Node Exporter wget https://github.com/prometheus/node_exporter/releases/download/v1.0.1/node_exporter-1.0.1.linux-amd64.tar.gz tar xvfz node_exporter-1.0.1.linux-amd64.tar.gz cd node_exporter-1.0.1.linux-amd64 ./node_exporter In linux Node Exporter exposes metrics on port 9100 by default. When it is running, we can get the metrics from the /metrics endpoint. Use a web browser or command curl to verify.\ncurl http://localhost:9100/metrics We should see all the metrics from the output like:\n# HELP go_gc_duration_seconds A summary of the pause duration of garbage collection cycles. # TYPE go_gc_duration_seconds summary go_gc_duration_seconds{quantile=\u0026#34;0\u0026#34;} 7.7777e-05 go_gc_duration_seconds{quantile=\u0026#34;0.25\u0026#34;} 0.000113756 go_gc_duration_seconds{quantile=\u0026#34;0.5\u0026#34;} 0.000127199 go_gc_duration_seconds{quantile=\u0026#34;0.75\u0026#34;} 0.000147778 go_gc_duration_seconds{quantile=\u0026#34;1\u0026#34;} 0.000371894 go_gc_duration_seconds_sum 0.292994058 go_gc_duration_seconds_count 2029 ... Note: We only need to install Node Exporter, rather than Prometheus server. If you want to get more information about Prometheus Node Exporter see: https://prometheus.io/docs/guides/node-exporter/\nInstall OpenTelemetry Collector We can quickly install a OpenTelemetry Collector instance by using docker-compose with the following steps:\n Create a directory to store the configuration files, like /usr/local/otel. Create docker-compose.yaml and otel-collector-config.yaml in this directory represented below:  docker-compose.yaml\nversion: \u0026#34;2\u0026#34; services: # Collector otel-collector: # Specify the image to start the container from image: otel/opentelemetry-collector:0.19.0 # Set the otel-collector configfile  command: [\u0026#34;--config=/etc/otel-collector-config.yaml\u0026#34;] # Mapping the configfile to host directory volumes: - ./otel-collector-config.yaml:/etc/otel-collector-config.yaml ports: - \u0026#34;13133:13133\u0026#34; # health_check extension - \u0026#34;55678\u0026#34; # OpenCensus receiver otel-collector-config.yaml\nextensions: health_check: # A receiver is how data gets into the OpenTelemetry Collector receivers: # Set Prometheus Receiver to collects metrics from targets # It’s supports the full set of Prometheus configuration prometheus: config: scrape_configs: - job_name: \u0026#39;otel-collector\u0026#39; scrape_interval: 10s static_configs: # Replace the IP to your VMs‘s IP which has installed Node Exporter - targets: [ \u0026#39;vm1:9100\u0026#39; ] - targets: [ \u0026#39;vm2:9100\u0026#39; ] - targets: [ ‘vm3:9100\u0026#39; ] processors: batch: # An exporter is how data gets sent to different systems/back-ends exporters: # Exports metrics via gRPC using OpenCensus format opencensus: endpoint: \u0026#34;docker.for.mac.host.internal:11800\u0026#34; # The OAP Server address insecure: true logging: logLevel: debug service: pipelines: metrics: receivers: [prometheus] processors: [batch] exporters: [logging, opencensus] extensions: [health_check] In this directory use command docker-compose to start up the container:  docker-compose up -d After the container is up and running, you should see metrics already exported in the logs:\n... Metric #165 Descriptor: -\u0026gt; Name: node_network_receive_compressed_total -\u0026gt; Description: Network device statistic receive_compressed. -\u0026gt; Unit: -\u0026gt; DataType: DoubleSum -\u0026gt; IsMonotonic: true -\u0026gt; AggregationTemporality: AGGREGATION_TEMPORALITY_CUMULATIVE DoubleDataPoints #0 Data point labels: -\u0026gt; device: ens4 StartTime: 1612234754364000000 Timestamp: 1612235563448000000 Value: 0.000000 DoubleDataPoints #1 Data point labels: -\u0026gt; device: lo StartTime: 1612234754364000000 Timestamp: 1612235563448000000 Value: 0.000000 ... If you want to get more information about OpenTelemetry Collector see: https://opentelemetry.io/docs/collector/\nSet up SkyWalking OAP Server To activate the oc handler and vm relevant rules, set your environment variables:\nSW_OTEL_RECEIVER=default SW_OTEL_RECEIVER_ENABLED_OC_RULES=vm Note: If there are other rules already activated , you can add vm with use , as a separator.\nSW_OTEL_RECEIVER_ENABLED_OC_RULES=vm,oap Start the SkyWalking OAP Server.\nDone! After all of the above steps are completed, check out the SkyWalking WebUI. Dashboard VM provides the default metrics of all observed virtual machines. Note: Clear the browser local cache if you used it to access deployments of previous SkyWalking versions.\nAdditional Resources  Read more about the SkyWalking 8.4 release highlights. Get more SkyWalking updates on Twitter.  ","excerpt":"Origin: Tetrate.io blog\n Background Apache SkyWalking\u0026ndash; the APM tool for distributed …","ref":"/blog/2021-02-07-infrastructure-monitoring/","title":"SkyWalking 8.4 provides infrastructure monitoring"},{"body":" Origin: Tetrate.io blog\n The Apache SkyWalking team today announced the 8.4 release is generally available. This release fills the gap between all previous versions of SkyWalking and the logging domain area. The release also advances SkyWalking’s capabilities for infrastructure observability, starting with virtual machine monitoring.\nBackground SkyWalking has historically focused on the tracing and metrics fields of observability. As its features for tracing, metrics and service level monitoring have become more and more powerful and stable, the SkyWalking team has started to explore new scenarios covered by observability. Because service performance is reflected in the logs, and is highly impacted by the infrastructure on which it runs, SkyWalking brings these two fields into the 8.4 release. This release blog briefly introduces the two new features as well as some other notable changes.\nLogs Metrics, tracing, and logging are considered the three pillars of observability [1]. SkyWalking had the full features of metrics and tracing prior to 8.4; today, as 8.4 is released, the last piece of the jigsaw is now in place.\nFigure 1: Logs Collected By SkyWalking\nFigure 2: Logs Collected By SkyWalking\nThe Java agent firstly provides SDKs to enhance the widely-used logging frameworks, log4j (1.x and 2.x) [2] and logback [3], and send the logs to the SkyWalking backend (OAP). The latter is able to collect logs from wherever the protocol is implemented. This is not a big deal, but when it comes to the correlation between logs and traces, the traditional solution is to print the trace IDs in the logs, and pick the IDs in the error logs to query the related traces. SkyWalking just simplifies the workflow by correlating the logs and traces natively. Navigating between traces and their related logs is as simple as clicking a button.\nFigure 3: Correlation Between Logs and Traces\nInfrastructure Monitoring SkyWalking is known as an application performance monitoring tool. One of the most important factors that impacts the application’s performance is the infrastructure on which the application runs. In the 8.4 release, we added the monitoring metrics of virtual machines into the dashboard.\nFigure 4: VM Metrics\nFundamental metrics such as CPU Used, Memory Used, Disk Read / Write and Network Usage are available on the dashboard. And as usual, those metrics are also available to be configured as alarm triggers when needed.\nDynamic Configurations at Agent Side Dynamic configuration at the backend side has long existed in SkyWalking for several versions. Now, it finally comes to the agent side! Prior to 8.4, you’d have to restart the target services when you modify some configuration items of the agent \u0026ndash; for instance, sampling rate (agent side), ignorable endpoint paths, etc. Now, say goodbye to rebooting. Modifying configurations is not the only usage of the dynamic configuration mechanism. The latter gives countless possibilities to the agent side in terms of dynamic behaviours, e.g. enabling / disabling plugins, enabling / disabling the whole agent, etc. Just imagine!\nGrouped Service Topology This enhancement is from the UI. SkyWalking backend supports grouping the services by user-defined dimensions. In a real world use case, the services are usually grouped by business group or department. When a developer opens the topology map, out of hundreds of services, he or she may just want to focus on the services in charge. The grouped service topology comes to the rescue: one can now choose to display only services belonging to a specified group.\nFigure 5: Grouped Service Topology\nOther Notable Enhancements  Agent: resolves domain names to look up backend service IP addresses. Backend: meter receiver supports meter analysis language (MAL). Backend: several CVE fixes. Backend: supports Envoy {AccessLog,Metrics}Service API V3 and adopts MAL.  Links  [1] https://peter.bourgon.org/blog/2017/02/21/metrics-tracing-and-logging.html [2] https://logging.apache.org/log4j/2.x/ [3] http://logback.qos.ch  Additional Resources  Read more about the SkyWalking 8.4 release highlights. Get more SkyWalking updates on Twitter.  ","excerpt":"Origin: Tetrate.io blog\n The Apache SkyWalking team today announced the 8.4 release is generally …","ref":"/blog/skywalking8-4-release/","title":"Apache SkyWalking 8.4: Logs, VM Monitoring, and Dynamic Configurations at Agent Side"},{"body":"","excerpt":"","ref":"/tags/release-blog/","title":"Release Blog"},{"body":"SkyWalking 8.4.0 is released. Go to downloads page to find release tars. Changes by Version\nProject  Incompatible with previous releases when use H2/MySQL/TiDB storage options, due to support multiple alarm rules triggered for one entity. Chore: adapt create_source_release.sh to make it runnable on Linux. Add package to .proto files, prevent polluting top-level namespace in some languages; The OAP server supports previous agent releases, whereas the previous OAP server (\u0026lt;=8.3.0) won\u0026rsquo;t recognize newer agents since this version (\u0026gt;= 8.4.0). Add ElasticSearch 7.10 to test matrix and verify it works. Replace Apache RAT with skywalking-eyes to check license headers. Set up test of Envoy ALS / MetricsService under Istio 1.8.2 to verify Envoy V3 protocol Test: fix flaky E2E test of Kafka.  Java Agent  The operation name of quartz-scheduler plugin, has been changed as the quartz-scheduler/${className} format. Fix jdk-http and okhttp-3.x plugin did not overwrite the old trace header. Add interceptors of method(analyze, searchScroll, clearScroll, searchTemplate and deleteByQuery) for elasticsearch-6.x-plugin. Fix the unexpected RunningContext recreation in the Tomcat plugin. Fix the potential NPE when trace_sql_parameters is enabled. Update byte-buddy to 1.10.19. Fix thrift plugin trace link broken when intermediate service does not mount agent Fix thrift plugin collects wrong args when the method without parameter. Fix DataCarrier\u0026rsquo;s org.apache.skywalking.apm.commons.datacarrier.buffer.Buffer implementation isn\u0026rsquo;t activated in IF_POSSIBLE mode. Fix ArrayBlockingQueueBuffer\u0026rsquo;s useless IF_POSSIBLE mode list Support building gRPC TLS channel but CA file is not required. Add witness method mechanism in the agent plugin core. Add Dolphinscheduler plugin definition. Make sampling still works when the trace ignores plug-in activation. Fix mssql-plugin occur ClassCastException when call the method of return generate key. The operation name of dubbo and dubbo-2.7.x-plugin, has been changed as the groupValue/className.methodName format Fix bug that rocketmq-plugin set the wrong tag. Fix duplicated EnhancedInstance interface added. Fix thread leaks caused by the elasticsearch-6.x-plugin plugin. Support reading segmentId and spanId with toolkit. Fix RestTemplate plugin recording url tag with wrong port Support collecting logs and forwarding through gRPC. Support config agent.sample_n_per_3_secs can be changed in the runtime. Support config agent.ignore_suffix can be changed in the runtime. Support DNS periodic resolving mechanism to update backend service. Support config agent.trace.ignore_path can be changed in the runtime. Added support for transmitting logback 1.x and log4j 2.x formatted \u0026amp; un-formatted messages via gPRC  OAP-Backend  Make meter receiver support MAL. Support influxDB connection response format option. Fix some error when use JSON as influxDB response format. Support Kafka MirrorMaker 2.0 to replicate topics between Kafka clusters. Add the rule name field to alarm record storage entity as a part of ID, to support multiple alarm rules triggered for one entity. The scope id has been removed from the ID. Fix MAL concurrent execution issues. Fix group name can\u0026rsquo;t be queried in the GraphQL. Fix potential gRPC connection leak(not closed) for the channels among OAP instances. Filter OAP instances(unassigned in booting stage) of the empty IP in KubernetesCoordinator. Add component ID for Python aiohttp plugin requester and server. Fix H2 in-memory database table missing issues Add component ID for Python pyramid plugin server. Add component ID for NodeJS Axios plugin. Fix searchService method error in storage-influxdb-plugin. Add JavaScript component ID. Fix CVE of UninstrumentedGateways in Dynamic Configuration activation. Improve query performance in storage-influxdb-plugin. Fix the uuid field in GRPCConfigWatcherRegister is not updated. Support Envoy {AccessLog,Metrics}Service API V3. Adopt the MAL in Envoy metrics service analyzer. Fix the priority setting doesn\u0026rsquo;t work of the ALS analyzers. Fix bug that endpoint-name-grouping.yml is not customizable in Dockerized case. Fix bug that istio version metric type on UI template mismatches the otel rule. Improve ReadWriteSafeCache concurrency read-write performance Fix bug that if use JSON as InfluxDB.ResponseFormat then NumberFormatException maybe occur. Fix timeBucket not taking effect in EqualsAndHashCode annotation of some relationship metrics. Fix SharingServerConfig\u0026rsquo;s propertie is not correct in the application.yml, contextPath -\u0026gt; restConnextPath. Istio control plane: remove redundant metrics and polish panel layout. Fix bug endpoint name grouping not work due to setting service name and endpoint name out of order. Fix receiver analysis error count metrics. Log collecting and query implementation. Support Alarm to feishu. Add the implementation of ConfigurationDiscovery on the OAP side. Fix bug in parseInternalErrorCode where some error codes are never reached. OAL supports multiple values when as numeric. Add node information from the Openensus proto to the labels of the samples, to support the identification of the source of the Metric data. Fix bug that the same sample name in one MAL expression caused IllegalArgumentException in Analyzer.analyse. Add the text analyzer for querying log in the es storage. Chore: Remove duplicate codes in Envoy ALS handler. Remove the strict rule of OAL disable statement parameter. Fix a legal metric query adoption bug. Don\u0026rsquo;t support global level metric query. Add VM MAL and ui-template configration, support Prometheus node-exporter VM metrics that pushed from OpenTelemetry-collector. Remove unused log query parameters.  UI  Fix un-removed tags in trace query. Fix unexpected metrics name on single value component. Don\u0026rsquo;t allow negative value as the refresh period. Fix style issue in trace table view. Separation Log and Dashboard selector data to avoid conflicts. Fix trace instance selector bug. Fix Unnecessary sidebar in tooltips for charts. Refactor dashboard query in a common script. Implement refreshing data for topology by updating date. Implement group selector in the topology. Fix all as default parameter for services selector. Add icon for Python aiohttp plugin. Add icon for Python pyramid plugin. Fix topology render all services nodes when groups changed. Fix rk-footer utc input\u0026rsquo;s width. Update rk-icon and rewrite rk-header svg tags with rk-icon. Add icon for http type. Fix rk-footer utc without local storage. Sort group names in the topology. Add logo for Dolphinscheduler. Fix dashboard wrong instance. Add a legend for the topology. Update the condition of unhealthy cube. Fix: use icons to replace buttons for task list in profile. Fix: support = in the tag value in the trace query page. Add envoy proxy component logo. Chore: set up license-eye to check license headers and add missing license headers. Fix prop for instances-survey and endpoints-survey. Fix envoy icon in topology. Implement the service logs on UI. Change the flask icon to light version for a better view of topology dark theme. Implement viewing logs on trace page. Fix update props of date component. Fix query conditions for logs. Fix style of selectors to word wrap. Fix logs time. Fix search ui for logs.  Documentation  Update the documents of backend fetcher and self observability about the latest configurations. Add documents about the group name of service. Update docs about the latest UI. Update the document of backend trace sampling with the latest configuration. Update kafka plugin support version to 2.6.1. Add FAQ about Fix compiling on Mac M1 chip.  All issues and pull requests are here\n","excerpt":"SkyWalking 8.4.0 is released. Go to downloads page to find release tars. Changes by Version\nProject …","ref":"/events/release-apache-skywalking-apm-8-4-0/","title":"Release Apache SkyWalking APM 8.4.0"},{"body":"Background The verifier is an important part of the next generation End-to-End Testing framework (NGE2E), which is responsible for verifying whether the actual output satisfies the expected template.\nDesign Thinking We will implement the verifier with Go template, plus some enhancements. Firstly, users need to write a Go template file with provided functions and actions to describe how the expected data looks like. Then the verifer renders the template with the actual data object. Finally, the verifier compares the rendered output with the actual data. If the rendered output is not the same with the actual output, it means the actual data is inconsist with the expected data. Otherwise, it means the actual data match the expected data. On failure, the verifier will also print out what are different between expected and actual data.\nBranches / Actions The verifier inherits all the actions from the standard Go template, such as if, with, range, etc. In addition, we also provide some custom actions to satisfy our own needs.\nList Elements Match contains checks if the actual list contains elements that match the given template.\nExamples:\nmetrics: {{- contains .metrics }} - name: {{ notEmpty .name }} id: {{ notEmpty .id }} value: {{ gt .value 0 }} {{- end }} It means that the list metrics must contain an element whose name and id are not empty, and value is greater than 0.\nmetrics: {{- contains .metrics }} - name: p95 value: {{ gt .value 0 }} - name: p99 value: {{ gt .value 0 }} {{- end }} This means that the list metrics must contain an element named p95 with a value greater than 0, and an element named p95 with a value greater than 0. Besides the two element, the list metrics may or may not have other random elements.\nFunctions Users can use these provided functions in the template to describe the expected data.\nNot Empty notEmpty checks if the string s is empty.\nExample:\nid: {{ notEmpty .id }} Regexp match regexp checks if string s matches the regular expression pattern.\nExamples:\nlabel: {{ regexp .label \u0026#34;ratings.*\u0026#34; }} Base64 b64enc s returns the Base64 encoded string of s.\nExamples:\nid: {{ b64enc \u0026#34;User\u0026#34; }}.static-suffix # this evalutes the base64 encoded string of \u0026#34;User\u0026#34;, concatenated with a static suffix \u0026#34;.static-suffix\u0026#34; Result:\nid: VXNlcg==.static-suffix Full Example Here is an example of expected data:\n# expected.data.yaml nodes: - id: {{ b64enc \u0026#34;User\u0026#34; }}.0 name: User type: USER isReal: false - id: {{ b64enc \u0026#34;Your_ApplicationName\u0026#34; }}.1 name: Your_ApplicationName type: Tomcat isReal: true - id: {{ $h2ID := (index .nodes 2).id }}{{ notEmpty $h2ID }} # We assert that nodes[2].id is not empty and save it to variable `h2ID` for later use name: localhost:-1 type: H2 isReal: false calls: - id: {{ notEmpty (index .calls 0).id }} source: {{ b64enc \u0026#34;Your_ApplicationName\u0026#34; }}.1 target: {{ $h2ID }} # We use the previously assigned variable `h2Id` to asert that the `target` is equal to the `id` of the nodes[2] detectPoints: - CLIENT - id: {{ b64enc \u0026#34;User\u0026#34; }}.0-{{ b64enc \u0026#34;Your_ApplicationName\u0026#34; }}.1 source: {{ b64enc \u0026#34;User\u0026#34; }}.0 target: {{ b64enc \u0026#34;Your_ApplicationName\u0026#34; }}.1 detectPoints: - SERVER will validate this data:\n# actual.data.yaml nodes: - id: VXNlcg==.0 name: User type: USER isReal: false - id: WW91cl9BcHBsaWNhdGlvbk5hbWU=.1 name: Your_ApplicationName type: Tomcat isReal: true - id: bG9jYWxob3N0Oi0x.0 name: localhost:-1 type: H2 isReal: false calls: - id: WW91cl9BcHBsaWNhdGlvbk5hbWU=.1-bG9jYWxob3N0Oi0x.0 source: WW91cl9BcHBsaWNhdGlvbk5hbWU=.1 detectPoints: - CLIENT target: bG9jYWxob3N0Oi0x.0 - id: VXNlcg==.0-WW91cl9BcHBsaWNhdGlvbk5hbWU=.1 source: VXNlcg==.0 detectPoints: - SERVER target: WW91cl9BcHBsaWNhdGlvbk5hbWU=.1 # expected.data.yaml metrics: {{- contains .metrics }} - name: {{ notEmpty .name }} id: {{ notEmpty .id }} value: {{ gt .value 0 }} {{- end }} will validate this data:\n# actual.data.yaml metrics: - name: business-zone::projectA id: YnVzaW5lc3Mtem9uZTo6cHJvamVjdEE=.1 value: 1 - name: system::load balancer1 id: c3lzdGVtOjpsb2FkIGJhbGFuY2VyMQ==.1 value: 0 - name: system::load balancer2 id: c3lzdGVtOjpsb2FkIGJhbGFuY2VyMg==.1 value: 0 and will report an error when validating this data, because there is no element with a value greater than 0:\n# actual.data.yaml metrics: - name: business-zone::projectA id: YnVzaW5lc3Mtem9uZTo6cHJvamVjdEE=.1 value: 0 - name: system::load balancer1 id: c3lzdGVtOjpsb2FkIGJhbGFuY2VyMQ==.1 value: 0 - name: system::load balancer2 id: c3lzdGVtOjpsb2FkIGJhbGFuY2VyMg==.1 value: 0 The contains does an unordered list verification, in order to do list verifications including orders, you can simply use the basic ruls like this:\n# expected.data.yaml metrics: - name: p99 value: {{ gt (index .metrics 0).value 0 }} - name: p95 value: {{ gt (index .metrics 1).value 0 }} which expects the actual metrics list to be exactly ordered, with first element named p99 and value greater 0, second element named p95 and value greater 0.\n","excerpt":"Background The verifier is an important part of the next generation End-to-End Testing framework …","ref":"/blog/2021-02-01-e2e-verifier-design/","title":"[Design] The Verifier of NGE2E"},{"body":"","excerpt":"","ref":"/tags/testing/","title":"Testing"},{"body":"SkyWalking Cloud on Kubernetes 0.2.0 is released. Go to downloads page to find release tars.\n Introduce custom metrics adapter to SkyWalking OAP cluster for Kubernetes HPA autoscaling. Add RBAC files and service account to support Kubernetes coordination. Add default and validation webhooks to operator controllers. Add UI CRD to deploy skywalking UI server. Add Fetcher CRD to fetch metrics from other telemetry system, for example, Prometheus.  ","excerpt":"SkyWalking Cloud on Kubernetes 0.2.0 is released. Go to downloads page to find release tars. …","ref":"/events/release-apache-skywalking-cloud-on-kubernetes-0-2-0/","title":"Release Apache SkyWalking Cloud on Kubernetes 0.2.0"},{"body":"Apache SkyWalking is an open source APM for distributed system, Apache Software Foundation top-level project.\nAt Jan. 11th, 2021, we noticed the Tencent Cloud Service, Tencent Service Watcher - TSW, for first time. Due to the similar short name, which SkyWalking is also called SW in the community, we connected with the service team of Tencent Cloud, and kindly asked.\nThey used to replay, TSW is purely developed by Tencent team itself, which doesn\u0026rsquo;t have any code dependency on SkyWalking.. We didn\u0026rsquo;t push harder.\nBut one week later, Jan 18th, 2021, our V.P., Sheng got the report again from Haoyang SkyWalking PMC member, through WeChat DM(direct message),. He provided complete evidence to prove TSW actually re-distributed the SkyWalking\u0026rsquo;s Java agent. We keep one copy of their agent\u0026rsquo;s distribution(at Jan. 18th), you could be downloaded here.\nSome typically evidences are here\n  ServiceManager is copied and package-name changed in the TSW\u0026rsquo;s agent.   ContextManager is copied and ackage-name changed in the TSW\u0026rsquo;s agent.   At the same time, we checked their tsw-client-package.zip, it didn\u0026rsquo;t include the SkyWalking\u0026rsquo;s LICENSE and NOTICE. Also, they didn\u0026rsquo;t mention TSW agent is the re-ditribution SkyWalking on their website.\nWith all above information, we had enough reason to believe, from the tech perspective, they were violating the Apache 2.0 License.\nFrom the 18th Jan., 2021, we sent mail [Apache 2.0 License Violation] Tencent Cloud TSW service doesn't follow the Apache 2.0 License to brief the SkyWalking PMC, and took the following actions to connect with Tencent.\n Made direct call to Tencent Open Source Office. Connected with Tencent Cloud TVP program committee, as Sheng Wu(Our VP) is a Tencent Cloud TVP. Talked with the Tencent Cloud team lead.  In all above channels, we provided the evidences of copy-redistribution hebaviors, requested them to revaluate their statements on the website, and follow the License\u0026rsquo;s requirements.\nResolution At Jan. 19th night, UTC+8, 2021. We received response from the Tencent cloud team. They admited their violation behaviors, and did following changes\n  Tencent Cloud TSW service page states, the agent is the fork version(re-distribution) of Apache SkyWalking agent.   TSW agent distributions include the SkyWalking\u0026rsquo;s License and NOTICE. Below is the screenshot, you could download from their product page. We keep a copy of their Jan. 19th 2021 at here.   We have updated the status to the PMC mail list. This license violation issue has been resolved for now.\nThe SkyWalking community and program management committee will keep our eyes on Tencent TSW. ","excerpt":"Apache SkyWalking is an open source APM for distributed system, Apache Software Foundation top-level …","ref":"/blog/2021-01-23-tencent-cloud-violates-aplv2/","title":"[Resolved][License Issue] Tencent Cloud TSW service violates the Apache 2.0 License when using SkyWalking."},{"body":" 第一节：开篇介绍 第二节：数字游戏（Number Game） 第三节：社区原则（Community “Principles”） 第四节：基金会原则（For public good） 第五节：一些不太好的事情   ","excerpt":" 第一节：开篇介绍 第二节：数字游戏（Number Game） 第三节：社区原则（Community “Principles”） 第四节：基金会原则（For public good） 第五节：一些不太 …","ref":"/zh/2021-01-21-educate-community/","title":"[视频] 开放原子开源基金会2020年度峰会 - Educate community Over Support community"},{"body":"","excerpt":"","ref":"/zh_tags/conference/","title":"Conference"},{"body":"Elastic announced their license change, Upcoming licensing changes to Elasticsearch and Kibana.\n We are moving our Apache 2.0-licensed source code in Elasticsearch and Kibana to be dual licensed under Server Side Public License (SSPL) and the Elastic License, giving users the choice of which license to apply. This license change ensures our community and customers have free and open access to use, modify, redistribute, and collaborate on the code. It also protects our continued investment in developing products that we distribute for free and in the open by restricting cloud service providers from offering Elasticsearch and Kibana as a service without contributing back. This will apply to all maintained branches of these two products and will take place before our upcoming 7.11 release. Our releases will continue to be under the Elastic License as they have been for the last three years.\n Also, they provide the FAQ page for more information about the impact for the users, developers, and vendors.\nIn the perspective of Apache Software Foundation, SSPL has been confirmed as a Catalog X LICENSE(https://www.apache.org/legal/resolved.html#category-x), which means hard-dependency as a part of the core is not allowed. With that, we can\u0026rsquo;t only focus on it anymore. We need to consider other storage options. Right now, we still have InfluxDB, TiDB, H2 server still in Apache 2.0 licensed. Right now, we still have InfluxDB, TiDB, H2 server as storage options still in Apache 2.0 licensed.\nAs one optional plugin, we need to focus on the client driver license. Right now, we are only using ElasticSearch 7.5.0 and 6.3.2 drivers, which are both Apache 2.0 licensed. So, we are safe. For further upgrade, here is their announcement. They answer these typical cases in the FAQ page.\n  I build a SaaS application using Elasticsearch as the backend, how does this affect me?\n This source code license change should not affect you - you can use our default distribution or develop applications on top of it for free, under the Elastic License. This source-available license does not contain any copyleft provisions and the default functionality is free of charge. For a specific example, you can see our response to a question around this at Magento.\nOur users still could use, redistribute, sale the products/services, based on SkyWalking, even they are using self hosting Elastic Search unmodified server.\n  I\u0026rsquo;m using Elasticsearch via APIs, how does this change affect me?\n This change does not affect how you use client libraries to access Elasticsearch. Our client libraries remain licensed under Apache 2.0, with the exception of our Java High Level Rest Client (Java HLRC). The Java HLRC has dependencies on the core of Elasticsearch, and as a result this client library will be licensed under the Elastic License. Over time, we will eliminate this dependency and move the Java HLRC to be licensed under Apache 2.0. Until that time, for the avoidance of doubt, we do not consider using the Java HLRC as a client library in development of an application or library used to access Elasticsearch to constitute a derivative work under the Elastic License, and this will not have any impact on how you license the source code of your application using this client library or how you distribute it.\nThe client driver license incompatible issue will exist, we can\u0026rsquo;t upgrade the driver(s) until they release the Apache 2.0 licensed driver jars. But users are still safe to upgrade the drivers by themselves.\n Apache SkyWalking will discuss the further actions here. If you have any question, welcome to ask. In the later 2021, we will begin to invest the posibility of creating SkyWalking\u0026rsquo;s observability database implementation.\n","excerpt":"Elastic announced their license change, Upcoming licensing changes to Elasticsearch and Kibana.\n We …","ref":"/blog/2021-01-17-elastic-change-license/","title":"Response to Elastic 2021 License Change"},{"body":"SkyWalking Client JS 0.3.0 is released. Go to downloads page to find release tars.\n Support tracing starting at the browser. Add traceSDKInternal SDK for tracing SDK internal RPC. Add detailMode SDK for tracing http method and url as tags in spans. Fix conditions of http status.  ","excerpt":"SkyWalking Client JS 0.3.0 is released. Go to downloads page to find release tars.\n Support tracing …","ref":"/events/release-apache-skywalking-client-js-0-3-0/","title":"Release Apache SkyWalking Client JS 0.3.0"},{"body":"SkyWalking Eyes 0.1.0 is released. Go to downloads page to find release tars.\n License Header  Add check and fix command. check results can be reported to pull request as comments. fix suggestions can be filed on pull request as edit suggestions.    ","excerpt":"SkyWalking Eyes 0.1.0 is released. Go to downloads page to find release tars.\n License Header  Add …","ref":"/events/release-apache-skywalking-eyes-0-1-0/","title":"Release Apache SkyWalking Eyes 0.1.0"},{"body":"SkyWalking NodeJS 0.1.0 is released. Go to downloads page to find release tars.\n Initialize project core codes. Built-in http/https plugin. Express plugin. Axios plugin.  ","excerpt":"SkyWalking NodeJS 0.1.0 is released. Go to downloads page to find release tars.\n Initialize project …","ref":"/events/release-apache-skywalking-nodejs-0-1-0/","title":"Release Apache SkyWalking for NodeJS 0.1.0"},{"body":"SkyWalking Python 0.5.0 is released. Go to downloads page to find release tars.\n  New plugins\n Pyramid Plugin (#102) AioHttp Plugin (#101) Sanic Plugin (#91)    API and enhancements\n @trace decorator supports async functions Supports async task context Optimized path trace ignore Moved exception check to Span.__exit__ Moved Method \u0026amp; Url tags before requests    Fixes:\n BaseExceptions not recorded as errors Allow pending data to send before exit sw_flask general exceptions handled Make skywalking logging Non-global    Chores and tests\n Make tests really run on specified Python version Deprecate 3.5 as it\u0026rsquo;s EOL    ","excerpt":"SkyWalking Python 0.5.0 is released. Go to downloads page to find release tars.\n  New plugins …","ref":"/events/release-apache-skywalking-python-0-5-0/","title":"Release Apache SkyWalking Python 0.5.0"},{"body":"Apache SkyWalking is an open source APM for distributed system. Provide tracing, service mesh observability, metrics analysis, alarm and visualization.\nJust 11 months ago, on Jan. 20th, 2020, SkyWalking hit the 200 contributors mark. With the growth of the project and the community, SkyWalking now includes over 20 sub(ecosystem) projects covering multiple language agents and service mesh, integration with mature open source projects, like Prometheus, Spring(Sleuth), hundreds of libraries to support all tracing/metrics/logs fields. In the past year, the number of contributors grows super astoundingly , and all its metrics point to its community vibrancy. Many corporate titans are already using SkyWalking in a large-scale production environment, including, Alibaba, Huawei, Baidu, Tencent, etc.\nRecently, our SkyWalking main repository overs 300 contributors.\nOur website has thousands of views from most countries in the world every week.\nAlthough we know that, the metrics like GitHub stars and the numbers of open users and contributors, are not a determinant of vibrancy, they do show the trend, we are very proud to share the increased numbers here, too.\nWe double those numbers and are honored with the development of our community.\nThank you, all of our contributors. Not just these 300 contributors of the main repository, or nearly 400 contributors in all repositories, counted by GitHub. There are countless people contributing codes to SkyWalking\u0026rsquo;s subprojects, ecosystem projects, and private fork versions; writing blogs and guidances, translating documents, books, and presentations; setting up learning sessions for new users; convincing friends to join the community as end-users, contributors, even committers. Companies behinds those contributors support their employees to work with the community to provide feedback and contribute the improvements and features upstream. Conference organizers share the stages with speakers from the SkyWalking community.\nSkyWalking can’t make this happen without your help. You made this community extraordinary.\nAt this crazy distributed computing and cloud native age, we as a community could make DEV, OPS, and SRE teams' work easier by locating the issue(s) in the haystack quicker than before, like why we named the project as SkyWalking, we will have a clear site line when you stand on the glass bridge Skywalk at Grand Canyon West.\n 376 Contributors counted by GitHub account are following. Dec. 22st, 2020. Generated by a tool deveoped by Yousa\n 1095071913 50168383 Ahoo-Wang AirTrioa AlexanderWert AlseinX Ax1an BFergerson BZFYS CharlesMaster ChaunceyLin5152 CommissarXia Cvimer Doublemine ElderJames EvanLjp FatihErdem FeynmanZhou Fine0830 FingerLiu Gallardot GerryYuan HackerRookie Heguoya Hen1ng Humbertzhang IanCao IluckySi Indifer J-Cod3r JaredTan95 Jargon96 Jijun JohnNiang Jozdortraz Jtrust Just-maple KangZhiDong LazyLei LiWenGu Liu-XinYuan Miss-you O-ll-O Patrick0308 QHWG67 Qiliang RandyAbernethy RedzRedz Runrioter SataQiu ScienJus SevenPointOld ShaoHans Shikugawa SoberChina SummerOfServenteen TJ666 TerrellChen TheRealHaui TinyAllen TomMD ViberW Videl WALL-E WeihanLi WildWolfBang WillemJiang Wooo0 XhangUeiJong Xlinlin YczYanchengzhe YoungHu YunaiV ZhHong ZhuoSiChen ZS-Oliver a198720 a526672351 acurtain adamni135 adermxzs adriancole aeolusheath agile6v aix3 aiyanbo ajanthan alexkarezin alonelaval amogege amwyyyy arugal ascrutae augustowebd bai-yang beckhampu beckjin beiwangnull bigflybrother bostin brucewu-fly c1ay candyleer carlvine500 carrypann cheenursn cheetah012 chenpengfei chenvista chess-equality chestarss chidaodezhongsheng chopin-d clevertension clk1st cngdkxw codeglzhang codelipenghui coder-yqj coki230 coolbeevip crystaldust cui-liqiang cuiweiwei cyberdak cyejing dagmom dengliming devkanro devon-ye dimaaan dingdongnigetou dio dmsolr dominicqi donbing007 dsc6636926 duotai dvsv2 dzx2018 echooymxq efekaptan eoeac evanxuhe feelwing1314 fgksgf fuhuo geektcp geomonlin ggndnn gitter-badger glongzh gnr163 gonedays grissom-grissom grissomsh guodongq guyukou gxthrj gzshilu hailin0 hanahmily haotian2015 haoyann hardzhang harvies hepyu heyanlong hi-sb honganan hsoftxl huangyoje huliangdream huohuanhuan innerpeacez itsvse jasonz93 jialong121 jinlongwang jjlu521016 jjtyro jmjoy jsbxyyx justeene juzhiyuan jy00464346 kaanid karott kayleyang kevinyyyy kezhenxu94 kikupotter kilingzhang killGC klboke ksewen kuaikuai kun-song kylixs landonzeng langke93 langyan1022 langyizhao lazycathome leemove leizhiyuan libinglong lilien1010 limfriend linkinshi linliaoy liuhaoXD liuhaoyang liuyanggithup liuzhengyang liweiv lkxiaolou llissery louis-zhou lpf32 lsyf lucperkins lujiajing1126 lunamagic1978 lunchboxav lxliuxuankb lytscu lyzhang1999 magic-akari makingtime maolie masterxxo maxiaoguang64 membphis mestarshine mgsheng michaelsembwever mikkeschiren mm23504570 momo0313 moonming mrproliu muyun12 nacx neatlife neeuq nic-chen nikitap492 nileblack nisiyong novayoung oatiz oflebbe olzhy onecloud360 osiriswd peng-yongsheng pengweiqhca potiuk purgeyao qijianbo010 qinhang3 qiuyu-d qqeasonchen qxo raybi-asus refactor2 remicollet rlenferink rootsongjc rovast scolia sdanzo seifeHu shiluo34 sikelangya simonlei sk163 snakorse songzhendong songzhian sonxy spacewander stalary stenio2011 stevehu stone-wlg sungitly surechen swartz-k sxzaihua tanjunchen tankilo taskmgr tbdpmi terranhu terrymanu tevahp thanq thebouv tianyuak tincopper tinyu0 tom-pytel tristaZero tristan-tsl trustin tsuilouis tuohai666 tzsword-2020 tzy1316106836 vcjmhg vision-ken viswaramamoorthy wankai123 wbpcode web-xiaxia webb2019 weiqiang333 wendal wengangJi wenjianzhang whfjam wind2008hxy withlin wqr2016 wu-sheng wuguangkuo wujun8 wuxingye x22x22 xbkaishui xcaspar xiaoxiangmoe xiaoy00 xinfeingxia85 xinzhuxiansheng xudianyang yanbw yanfch yang-xiaodong yangxb2010000 yanickxia yanmaipian yanmingbi yantaowu yaowenqiang yazong ychandu ycoe yimeng yu199195 yuqichou yuyujulin yymoth zaunist zaygrzx zcai2 zeaposs zhang98722 zhanghao001 zhangjianweibj zhangkewei zhangsean zhaoyuguang zhentaoJin zhousiliang163 zhuCheer zifeihan zkscpqm zoidbergwill zoumingzm zouyx zshit zxbu zygfengyuwuzu  ","excerpt":"Apache SkyWalking is an open source APM for distributed system. Provide tracing, service mesh …","ref":"/blog/2021-01-01-300-contributors-mark/","title":"Celebrate SkyWalking single repository hits the 300 contributors mark"},{"body":"","excerpt":"","ref":"/zh_tags/open-source-contribution/","title":"Open Source Contribution"},{"body":"","excerpt":"","ref":"/zh_tags/open-source-promotion-plan/","title":"Open Source Promotion Plan"},{"body":"Ke Zhang (a.k.a. HumbertZhang) mainly focuses on the SkyWalking Python agent, he had participated in the \u0026ldquo;Open Source Promotion Plan - Summer 2020\u0026rdquo; and completed the project smoothly, and won the award \u0026ldquo;Most Potential Students\u0026rdquo; that shows his great willingness to continuously contribute to our community.\nUp to date, he has submitted 8 PRs in the Python agent repository, 7 PRs in the main repo, all in total include ~2000 LOC.\nAt Dec. 13th, 2020, the project management committee (PMC) passed the proposal of promoting him as a new committer. He has accepted the invitation at the same day.\nWelcome to join the committer team, Ke Zhang!\n","excerpt":"Ke Zhang (a.k.a. HumbertZhang) mainly focuses on the SkyWalking Python agent, he had participated in …","ref":"/events/welcome-ke-zhang-as-new-committer/","title":"Welcome Ke Zhang (张可) as new committer"},{"body":"今年暑假期间我参加了开源软件供应链点亮计划—暑期 2020 的活动，在这个活动中，我主要参加了 Apache SkyWalking 的 Python Agent 的开发，最终项目顺利结项并获得了”最具潜力奖“，今天我想分享一下我参与这个活动以及开源社区的感受与收获。\n缘起 其实我在参加暑期 2020 活动之前就听说过 SkyWalking 了。我研究生的主要研究方向是微服务和云原生，组里的学长们之前就在使用 SkyWalking 进行一些研究工作，也是通过他们，我了解到了 OpenTracing, SkyWalking 等与微服务相关的 Tracing 工具以及 APM 等，当时我就在想如果有机会可以深度参加这些开源项目就好了。 巧的是，也正是在差不多的时候，本科的一个学长发给了我暑期 2020 活动的链接，我在其中惊喜的发现了 SkyWalking 项目。\n虽然说想要参与 SkyWalking 的开发，但是真的有了机会我却有一些不自信——这可是 Star 上万的 Apache 顶级项目。万幸的是在暑期 2020 活动中，每一个社区都提供了很多题目以供选择，想参与的同学可以提前对要做的事情有所了解，并可以提前做一些准备。我当时也仔细地浏览了项目列表，最终决定申请为 Python Agent 支持 Flask 或 Django 埋点的功能。当时主要考虑的是，我对 Python 语言比较熟悉，同时也有使用 Flask 等 web 框架进行开发的经验，我认为应该可以完成项目要求。为了能让心里更有底一些，我阅读了 Python Agent 的源码，写下了对项目需要做的工作的理解，并向项目的导师柯振旭发送了自荐邮件，最终被选中去完成这个项目。\n过程 被选中后我很激动，也把这份激动化作了参与开源的动力。我在进一步阅读源码，搭建本地环境后，用了三周左右的时间完成了 Django 项目的埋点插件的开发，毕竟我选择的项目是一个低难度的项目，而我在 Python web 方面也有一些经验。在这之后，我的导师和我进行了沟通，在我表达了想要继续做贡献的意愿之后，他给我建议了一些可以进一步进行贡献的方向，我也就继续参与 Python Agent 的开发。接下来，我陆续完成了 PyMongo 埋点插件, 插件版本检查机制, 支持使用 kafka 协议进行数据上报等功能。在提交了暑期 2020 活动的结项申请书后，我又继续参与了在端到端测试中增加对百分位数的验证等功能。\n在整个过程中，我遇到过很多问题，包括对问题认识不够清晰，功能的设计不够完善等等，但是通过与导师的讨论以及 Code Review，这些问题最终都迎刃而解了。此外他还经常会和我交流项目进一步发展方向，并给我以鼓励和肯定，在这里我想特别感谢我的导师在整个项目过程中给我的各种帮助。\n收获 参加暑期 2020 的活动带给我了很多收获，主要有以下几点：\n第一是让我真正参与到了开源项目中。在之前我只向在项目代码或文档中发现的 typo 发起过一些 Pull Request，但是暑期 2020 活动通过列出项目 ＋ 导师指导的方式，明确了所要做的事情，并提供了相应的指导，降低了参与开源的门槛，使得我们学生可以参与到项目的开发中来。\n第二是对我的专业研究方向也有很多启发，我的研究方向就是微服务与云原生相关，通过参与到 SkyWalking 的开发中使得我可以更好地理解研究问题中的一些概念，也让我更得心应手得使用 SkyWalking 来解决一些实际的问题。\n第三是通过参与 SkyWalking Python Agent 以及其他部分的开发，我的贡献得到了社区的承认，并在最近被邀请作为 Committer 加入了社区，这对我而言是很高的认可，也提升了我的自信心。\n​\t第四点就是我通过这个活动认识了不少新朋友，同时也开拓了我的视野，使得我对于开源项目与开源社区有了很多新的认识。\n建议 最后同样是我对想要参与开源社区，想要参与此类活动的同学们的一些建议：\n 虽然奖金很吸引人，但是还是希望大家能抱着长期为项目进行贡献的心态来参与开源项目，以这样的心态参与开源可以让你更好地理解开源社区的运作方式，也可以让你更有机会参与完成激动人心的功能，你在一个东西上付出的时间精力越多，你能收获的往往也越多。 在申请项目的时候，可以提前阅读一下相关功能的源码，并结合自己的思考去写一份清晰明了的 proposal ，这样可以帮助你在申请人中脱颖而出。 在开始着手去完成一个功能之前，首先理清思路，并和自己的导师或了解这一部分的人进行沟通与确认，从而尽量避免在错误的方向上浪费太多时间。  ","excerpt":"今年暑假期间我参加了开源软件供应链点亮计划—暑期 2020 的活动，在这个活动中，我主要参加了 Apache SkyWalking 的 Python Agent 的开发，最终项目顺利结项并获得了”最具 …","ref":"/zh/2020-12-20-summer2020-activity-sharing2/","title":"暑期 2020 活动学生（张可）心得分享"},{"body":"背景 我是一个热爱编程、热爱技术的人，⼀直以来都向往着能参与到开源项⽬中锻炼⾃⼰，但当我面对庞大而复杂的项目代码时，却感到手足无措，不知该从何开始。⽽此次的“开源软件供应链点亮计划-暑期2020”活动则正好提供了这样⼀个机会：清晰的任务要求、开源社区成员作为导师提供指导以及一笔丰厚的奖金，让我顺利地踏上了开源这条道路。\n回顾 在“暑期2020”活动的这两个多月里，我为 SkyWalking 的命令行工具实现了一个 dashboard，此外在阅读项目源码的过程中，还发现并修复了几个 bug。到活动结束时，我共提交了11个 PR，贡献了两千多行改动，对 SkyWalking CLI 项目的贡献数量排名第二，还获得了“最具潜力奖”。\n我觉得之所以能够如此顺利地完成这个项⽬主要有两个原因。一方面，我选择的 SkyWalking CLI 项⽬当时最新的版本号为0.3.0，还处于起步阶段，代码量相对较少，⽽且项⽬结构非常清晰，文档也较为详细，这对于我理解整个项⽬⾮常有帮助，从⽽能够更快地上⼿。另一方面，我的项目导师非常认真负责，每次我遇到问题，导师都会及时地为我解答，然后我提交的 PR 也能够很快地被 review。⽽且导师不时会给予我肯定的评论与⿎励，这极⼤地提⾼了我的成就感，让我更加积极地投⼊到下⼀阶段的⼯作，形成⼀个正向的循环。\n收获 回顾整个参与过程，觉得自己收获颇多：\n首先，我学习到了很多可能在学校里接触不到的新技术，了解了开源项目是如何进行协作，开源社区是如何运转治理的，以及开源文化、Apache way 等知识，仿佛进入了一个崭新而精彩的世界。\n其次，我的编程能力得到了锻炼。因为开源项目对于代码的质量有较高的要求，因此我会在编程时有意识地遵守相关的规范，培养良好的编码习惯。然后在导师的 code review 中也学习到了一些编程技巧。\n此外，参与开源为我的科研带来了不少灵感。因为我的研究方向是智能软件工程，旨在将人工智能技术应用在软件工程的各个环节中，这需要我在实践中发现实际问题。而开源则提供了这样一个窗口，让我足不出户即可参与到软件项目的设计、开发、测试和发布等环节。\n最后也是本次活动最大的一个收获，我的贡献得到了社区的认可，被提名成为了 SkyWalking 社区的第一位学生 committer。\n建议 最后，对于将来想要参加此类活动的同学，附上我的一些建议：\n第一，选择活跃、知名的社区。社区对你的影响将是极其深远的，好的社区意味着成熟的协作流程、良好的氛围、严谨的代码规范，以及有更大几率遇到优秀的导师，这些对于你今后在开源方面的发展都是非常有帮助的。\n第二，以兴趣为导向来选择项目，同时要敢于走出舒适区。我最初在选择项目时，初步确定了两个，一个是低难度的 Python 项目，另一个是中等难度的 Go 项目。当时我很纠结：因为我对 Python 语言比较熟悉，选择一个低难度的项目是比较稳妥的，但是项目的代码我看的并不是很懂，具体要怎么做我完全没有头绪；而 Go 项目是一个命令行工具，我对这个比较感兴趣，且有一个大致的思路，但是我对 Go 语言并不是很熟悉，实践经验为零。最后凭借清晰具体的 proposal 我成功申请到了 Go 项目并顺利地完成了，还在实践中快速掌握了一门新的编程语言。\n这次的“暑期2020”活动虽已圆满结束，但我的开源之路才刚刚开始。\n","excerpt":"背景 我是一个热爱编程、热爱技术的人，⼀直以来都向往着能参与到开源项⽬中锻炼⾃⼰，但当我面对庞大而复杂的项目代码时，却感到手足无措，不知该从何开始。⽽此次的“开源软件供应链点亮计划-暑期2020”活动 …","ref":"/zh/2020-12-19-summer2020-activity-sharing/","title":"暑期2020活动心得分享"},{"body":"NGE2E is the next generation End-to-End Testing framework that aims to help developers to set up, debug, and verify E2E tests with ease. It\u0026rsquo;s built based on the lessons learnt from tens of hundreds of test cases in the SkyWalking main repo.\nGoal  Keep the feature parity with the existing E2E framework in SkyWalking main repo; Support both docker-compose and KinD to orchestrate the tested services under different environments; Get rid of the heavy Java/Maven stack, which exists in the current E2E; be language independent as much as possible, users only need to configure YAMLs and run commands, without writing codes;  Non-Goal  This framework is not involved with the build process, i.e. it won\u0026rsquo;t do something like mvn package or docker build, the artifacts (.tar, docker images) should be ready in an earlier process before this; This project doesn\u0026rsquo;t take the plugin tests into account, at least for now; This project doesn\u0026rsquo;t mean to add/remove any new/existing test case to/from the main repo; This documentation won\u0026rsquo;t cover too much technical details of how to implement the framework, that should go into an individual documentation;  Design Before diving into the design details, let\u0026rsquo;s take a quick look at how the end user might use NGE2E.\n All the following commands are mock, and are open to debate.\n To run a test case in a directory /path/to/the/case/directory\ne2e run /path/to/the/case/directory # or cd /path/to/the/case/directory \u0026amp;\u0026amp; e2e run This will run the test case in the specified directory, this command is a wrapper that glues all the following commands, which can be executed separately, for example, to debug the case:\nNOTE: because all the options can be loaded from a configuration file, so as long as a configuration file (say e2e.yaml) is given in the directory, every command should be able to run in bare mode (without any option explicitly specified in the command line);\nSet Up e2e setup --env=compose --file=docker-compose.yaml --wait-for=service/health e2e setup --env=kind --file=kind.yaml --manifests=bookinfo.yaml,gateway.yaml --wait-for=pod/ready e2e setup # If configuration file e2e.yaml is present  --env: the environment, may be compose or kind, represents docker-compose and KinD respectively; --file: the docker-compose.yaml or kind.yaml file that declares how to set up the environment; --manifests: for KinD, the resources files/directories to apply (using kubectl apply -f); --command: a command to run after the environment is started, this may be useful when users need to install some extra tools or apply resources from command line, like istioctl install --profile=demo; --wait-for: can be specified multiple times to give a list of conditions to be met; wait until the given conditions are met; the most frequently-used strategy should be --wait-for=service/health, --wait-for=deployments/available, etc. that make the e2e setup command to wait for all conditions to be met; other possible strategies may be something like --wait-for=\u0026quot;log:Started Successfully\u0026quot;, --wait-for=\u0026quot;http:localhost:8080/healthcheck\u0026quot;, etc. if really needed;  Trigger Inputs e2e trigger --interval=3s --times=0 --action=http --url=\u0026#34;localhost:8080/users\u0026#34; e2e trigger --interval=3s --times=0 --action=cmd --cmd=\u0026#34;curl localhost:8080/users\u0026#34; e2e trigger # If configuration file e2e.yaml is present  --interval=3s: trigger the action every 3 seconds; --times=0: how many times to trigger the action, 0=infinite; --action=http: the action of the trigger, i.e. \u0026ldquo;perform an http request as an input\u0026rdquo;; --action=cmd: the action of the trigger, i.e. \u0026ldquo;execute the cmd as an input\u0026rdquo;;  Query Output swctl service ls this is a project-specific step, different project may use different tools to query the actual output, for SkyWalking, it uses swctl to query the actual output.\nVerify e2e verify --actual=actual.data.yaml --expected=expected.data.yaml e2e verify --query=\u0026#34;swctl service ls\u0026#34; --expected=expected.data.yaml e2e verify # If configuration file e2e.yaml is present   --actual: the actual data file, only YAML file format is supported;\n  --expected: the expected data file, only YAML file format is supported;\n  --query: the query to get the actual data, the query result must have the same format as --actual and --expected;\n The --query option will get the output into a temporary file and use the --actual under the hood;\n   Cleanup e2e cleanup --env=compose --file=docker-compose.yaml e2e cleanup --env=kind --file=kind.yaml --resources=bookinfo.yaml,gateway.yaml e2e cleanup # If configuration file e2e.yaml is present This step requires the same options in the setup step so that it can clean up all things necessarily.\nSummarize To summarize, the directory structure of a test case might be\ncase-name ├── agent-service # optional, an arbitrary project that is used in the docker-compose.yaml if needed │ ├── Dockerfile │ ├── pom.xml │ └── src ├── docker-compose.yaml ├── e2e.yaml # see a sample below └── testdata ├── expected.endpoints.service1.yaml ├── expected.endpoints.service2.yaml └── expected.services.yaml or\ncase-name ├── kind.yaml ├── bookinfo │ ├── bookinfo.yaml │ └── bookinfo-gateway.yaml ├── e2e.yaml # see a sample below └── testdata ├── expected.endpoints.service1.yaml ├── expected.endpoints.service2.yaml └── expected.services.yaml a sample of e2e.yaml may be\nsetup: env: kind file: kind.yaml manifests: - path: bookinfo.yaml wait: # you can have multiple conditions to wait - namespace: bookinfo label-selector: app=product for: deployment/available - namespace: reviews label-selector: app=product for: deployment/available - namespace: ratings label-selector: app=product for: deployment/available run: - command: | # it can be a shell script or anything executable istioctl install --profile=demo -y kubectl label namespace default istio-injection=enabled wait: - namespace: istio-system label-selector: app=istiod for: deployment/available # OR # env: compose # file: docker-compose.yaml trigger: action: http interval: 3s times: 0 url: localhost:9090/users verify: - query: swctl service ls expected: expected.services.yaml - query: swctl endpoint ls --service=\u0026#34;YnVzaW5lc3Mtem9uZTo6cHJvamVjdEM=.1\u0026#34; expected: expected.projectC.endpoints.yaml then a single command should do the trick.\ne2e run Modules This project is divided into the following modules.\nController A controller command (e2e run) composes all the steps declared in the e2e.yaml, it should be progressive and clearly display which step is currently running. If it failed in a step, the error message should be as much comprehensive as possible. An example of the output might be\ne2e run ✔ Started Kind Cluster - Cluster Name ✔ Checked Pods Readiness - All pods are ready ? Generating Traffic - http localhost:9090/users (progress spinner) ✔ Verified Output - service ls (progress spinner) Verifying Output - endpoint ls ✘ Failed to Verify Output Data - endpoint ls \u0026lt;the diff content\u0026gt; ✔ Clean Up Compared with running the steps one by one, the controller is also responsible for cleaning up env (by executing cleanup command) no mater what status other commands are, even if they are failed, the controller has the following semantics in terms of setup and cleanup.\n// Java try { setup(); // trigger step // verify step // ... } finally { cleanup(); } // GoLang func run() { setup(); defer cleanup(); // trigger step // verify step // ... } Initializer The initializer is responsible for\n  When env==compose\n Start the docker-compose services; Check the services' healthiness; Wait until all services are ready according to the interval, etc.;    When env==kind\n Start the KinD cluster according to the config files; Apply the resources files (--manifests) or/and run the custom init command (--commands); Check the pods' readiness; Wait until all pods are ready according to the interval, etc.;    Verifier According to scenarios we have at the moment, the must-have features are:\n  Matchers\n Exact match Not null Not empty Greater than 0 Regexp match At least one of list element match    Functions\n Base64 encode/decode    in order to help to identify simple bugs from the GitHub Actions workflow, there are some \u0026ldquo;nice to have\u0026rdquo; features:\n Printing the diff content when verification failed is a super helpful bonus proved in the Python agent repo;  Logging When a test case failed, all the necessary logs should be collected into a dedicated directory, which could be uploaded to the GitHub Artifacts for downloading and analysis;\nLogs through the entire process of a test case are:\n KinD clusters logs; Containers/pods logs; The logs from the NGE2E itself;  More Planned Debugging Debugging the E2E locally has been a strong requirement and time killer that we haven\u0026rsquo;t solve up to date, though we have enhancements like https://github.com/apache/skywalking/pull/5198 , but in this framework, we will adopt a new method to \u0026ldquo;really\u0026rdquo; support debugging locally.\nThe most common case when debugging is to run the E2E tests, with one or more services forwarded into the host machine, where the services are run in the IDE or in debug mode.\nFor example, you may run the SkyWalking OAP server in an IDE and run e2e run, expecting the other services (e.g. agent services, SkyWalking WebUI, etc.) inside the containers to connect to your local OAP, instead of the one declared in docker-compose.yaml.\nFor Docker Desktop Mac/Windows, we can access the services running on the host machine inside containers via host.docker.internal, for Linux, it\u0026rsquo;s 172.17.0.1.\nOne possible solution is to add an option --debug-services=oap,other-service-name that rewrites all the router rules inside the containers from oap to host.docker.internal/172.17.0.1.\nCodeGen When adding new test case, a code generator would be of great value to eliminate the repeated labor and copy-pasting issues.\ne2e new \u0026lt;case-name\u0026gt; ","excerpt":"NGE2E is the next generation End-to-End Testing framework that aims to help developers to set up, …","ref":"/blog/e2e-design/","title":"[Design] NGE2E - Next Generation End-to-End Testing Framework"},{"body":"这篇文章暂时不讲告警策略, 直接看默认情况下激活的告警目标以及钉钉上的告警效果\nSkyWalking内置了很多默认的告警策略, 然后根据告警策略生成告警目标, 我们可以很容易的在界面上看到\n当我们想去让这些告警目标通知到我们时, 由于SkyWalking目前版本(8.3)已经自带了, 只需要简单配置一下即可\n我们先来钉钉群中创建机器人并勾选加签\n然后再修改告警部分的配置文件, 如果你是默认的配置文件(就像我一样), 你可以直接执行以下命令, 反之你也可以手动修改configs/alarm-settings.yml文件\ntee \u0026lt;your_skywalking_path\u0026gt;/configs/alarm-settings.yml \u0026lt;\u0026lt;-'EOF' dingtalkHooks: textTemplate: |- { \u0026quot;msgtype\u0026quot;: \u0026quot;text\u0026quot;, \u0026quot;text\u0026quot;: { \u0026quot;content\u0026quot;: \u0026quot;Apache SkyWalking Alarm: \\n %s.\u0026quot; } } webhooks: - url: https://oapi.dingtalk.com/robot/send?access_token=\u0026lt;access_token\u0026gt; secret: \u0026lt;加签值\u0026gt; EOF 最终效果如下\n参考文档:\nhttps://github.com/apache/skywalking/blob/master/docs/en/setup/backend/backend-alarm.md\nhttps://ding-doc.dingtalk.com/doc#/serverapi2/qf2nxq/uKPlK\n谢谢观看, 后续我会在SkyWalking告警这块写更多实战文章\n","excerpt":"这篇文章暂时不讲告警策略, 直接看默认情况下激活的告警目标以及钉钉上的告警效果\nSkyWalking内置了很多默认的告警策略, 然后根据告警策略生成告警目标, 我们可以很容易的在界面上看到\n当我们想去 …","ref":"/zh/2020-12-13-skywalking-alarm/","title":"SkyWalking报警发送到钉钉群"},{"body":"","excerpt":"","ref":"/zh_tags/user-manual/","title":"User Manual"},{"body":"Gui Cao began the code contributions since May 3, 2020. In the past 6 months, his 23 pull requests(GitHub, zifeihan[1]) have been accepted, which includes 5k+ lines of codes.\nMeanwhile, he took part in the tech discussion, and show the interests to contribute more to the project.\nAt Dec. 4th, 2020, the project management committee(PMC) passed the proposal of promoting him as a new committer. He has accepted the invitation at the same day.\nWelcome Gui Cao join the committer team.\n[1] https://github.com/apache/skywalking/commits?author=zifeihan\n","excerpt":"Gui Cao began the code contributions since May 3, 2020. In the past 6 months, his 23 pull …","ref":"/events/welcome-gui-cao-as-new-committer/","title":"Welcome Gui Cao as new committer"},{"body":" Author: Zhenxu Ke, Sheng Wu, and Tevah Platt. tetrate.io Original link, Tetrate.io blog Dec. 03th, 2020  Apache SkyWalking: an APM (application performance monitor) system, especially designed for microservices, cloud native, and container-based (Docker, Kubernetes, Mesos) architectures.\nEnvoy Access Log Service: Access Log Service (ALS) is an Envoy extension that emits detailed access logs of all requests going through Envoy.\nBackground Apache SkyWalking has long supported observability in service mesh with Istio Mixer adapter. But since v1.5, Istio began to deprecate Mixer due to its poor performance in large scale clusters. Mixer’s functionalities have been moved into the Envoy proxies, and is supported only through the 1.7 Istio release. On the other hand, Sheng Wu and Lizan Zhou presented a better solution based on the Apache SkyWalking and Envoy ALS on KubeCon China 2019, to reduce the performance impact brought by Mixer, while retaining the same observability in service mesh. This solution was initially implemented by Sheng Wu, Hongtao Gao, Lizan Zhou, and Dhi Aurrahman at Tetrate.io. If you are looking for a more efficient solution to observe your service mesh instead of using a Mixer-based solution, this is exactly what you need. In this tutorial, we will explain a little bit how the new solution works, and apply it to the bookinfo application in practice.\nHow it works From a perspective of observability, Envoy can be typically deployed in 2 modes, sidecar, and router. As a sidecar, Envoy mostly represents a single service to receive and send requests (2 and 3 in the picture below). While as a proxy, Envoy may represent many services (1 in the picture below).\nIn both modes, the logs emitted by ALS include a node identifier. The identifier starts with router~ (or ingress~) in router mode and sidecar~ in sidecar proxy mode.\nApart from the node identifier, there are several noteworthy properties in the access logs that will be used in this solution:\n  downstream_direct_remote_address: This field is the downstream direct remote address on which the request from the user was received. Note: This is always the physical peer, even if the remote address is inferred from for example the x-forwarded-for header, proxy protocol, etc.\n  downstream_remote_address: The remote/origin address on which the request from the user was received.\n  downstream_local_address: The local/destination address on which the request from the user was received.\n  upstream_remote_address: The upstream remote/destination address that handles this exchange.\n  upstream_local_address: The upstream local/origin address that handles this exchange.\n  upstream_cluster: The upstream cluster that upstream_remote_address belongs to.\n  We will discuss more about the properties in the following sections.\nSidecar When serving as a sidecar, Envoy is deployed alongside a service, and delegates all the incoming/outgoing requests to/from the service.\n  Delegating incoming requests: in this case, Envoy acts as a server side sidecar, and sets the upstream_cluster in form of inbound|portNumber|portName|Hostname[or]SidecarScopeID.\nThe SkyWalking analyzer checks whether either downstream_remote_address can be mapped to a Kubernetes service:\na. If there is a service (say Service B) whose implementation is running in this IP(and port), then we have a service-to-service relation, Service B -\u0026gt; Service A, which can be used to build the topology. Together with the start_time and duration fields in the access log, we have the latency metrics now.\nb. If there is no service that can be mapped to downstream_remote_address, then the request may come from a service out of the mesh. Since SkyWalking cannot identify the source service where the requests come from, it simply generates the metrics without source service, according to the topology analysis method. The topology can be built as accurately as possible, and the metrics detected from server side are still correct.\n  Delegating outgoing requests: in this case, Envoy acts as a client-side sidecar, and sets the upstream_cluster in form of outbound|\u0026lt;port\u0026gt;|\u0026lt;subset\u0026gt;|\u0026lt;serviceFQDN\u0026gt;.\nClient side detection is relatively simpler than (1. Delegating incoming requests). If upstream_remote_address is another sidecar or proxy, we simply get the mapped service name and generate the topology and metrics. Otherwise, we have no idea what it is and consider it an UNKNOWN service.\n  Proxy role When Envoy is deployed as a proxy, it is an independent service itself and doesn\u0026rsquo;t represent any other service like a sidecar does. Therefore, we can build client-side metrics as well as server-side metrics.\nExample In this section, we will use the typical bookinfo application to demonstrate how Apache SkyWalking 8.3.0+ (the latest version up to Nov. 30th, 2020) works together with Envoy ALS to observe a service mesh.\nInstalling Kubernetes SkyWalking 8.3.0 supports the Envoy ALS solution under both Kubernetes environment and virtual machines (VM) environment, in this tutorial, we’ll only focus on the Kubernetes scenario, for VM solution, please stay tuned for our next blog, so we need to install Kubernetes before taking further steps.\nIn this tutorial, we will use the Minikube tool to quickly set up a local Kubernetes(v1.17) cluster for testing. In order to run all the needed components, including the bookinfo application, the SkyWalking OAP and WebUI, the cluster may need up to 4GB RAM and 2 CPU cores.\nminikube start --memory=4096 --cpus=2 Next, run kubectl get pods --namespace=kube-system --watch to check whether all the Kubernetes components are ready. If not, wait for the readiness before going on.\nInstalling Istio Istio provides a very convenient way to configure the Envoy proxy and enable the access log service. The built-in configuration profiles free us from lots of manual operations. So, for demonstration purposes, we will use Istio through this tutorial.\nexport ISTIO_VERSION=1.7.1 curl -L https://istio.io/downloadIstio | sh - sudo mv $PWD/istio-$ISTIO_VERSION/bin/istioctl /usr/local/bin/ istioctl install --set profile=demo kubectl label namespace default istio-injection=enabled Run kubectl get pods --namespace=istio-system --watch to check whether all the Istio components are ready. If not, wait for the readiness before going on.\nEnabling ALS The demo profile doesn’t enable ALS by default. We need to reconfigure it to enable ALS via some configuration.\nistioctl manifest install \\  --set meshConfig.enableEnvoyAccessLogService=true \\  --set meshConfig.defaultConfig.envoyAccessLogService.address=skywalking-oap.istio-system:11800 The example command --set meshConfig.enableEnvoyAccessLogService=true enables the Envoy access log service in the mesh. And as we said earlier, ALS is essentially a gRPC service that emits requests logs. The config meshConfig.defaultConfig.envoyAccessLogService.address=skywalking-oap.istio-system:11800 tells this gRPC service where to emit the logs, say skywalking-oap.istio-system:11800, where we will deploy the SkyWalking ALS receiver later.\nNOTE: You can also enable the ALS when installing Istio so that you don’t need to restart Istio after installation:\nistioctl install --set profile=demo \\  --set meshConfig.enableEnvoyAccessLogService=true \\  --set meshConfig.defaultConfig.envoyAccessLogService.address=skywalking-oap.istio-system:11800 kubectl label namespace default istio-injection=enabled Deploying Apache SkyWalking The SkyWalking community provides a Helm Chart to make it easier to deploy SkyWalking and its dependent services in Kubernetes. The Helm Chart can be found at the GitHub repository.\n# Install Helm curl -sSLO https://get.helm.sh/helm-v3.0.0-linux-amd64.tar.gz sudo tar xz -C /usr/local/bin --strip-components=1 linux-amd64/helm -f helm-v3.0.0-linux-amd64.tar.gz # Clone SkyWalking Helm Chart git clone https://github.com/apache/skywalking-kubernetes cd skywalking-kubernetes/chart git reset --hard dd749f25913830c47a97430618cefc4167612e75 # Update dependencies helm dep up skywalking # Deploy SkyWalking helm -n istio-system install skywalking skywalking \\  --set oap.storageType=\u0026#39;h2\u0026#39;\\  --set ui.image.tag=8.3.0 \\  --set oap.image.tag=8.3.0-es7 \\  --set oap.replicas=1 \\  --set oap.env.SW_ENVOY_METRIC_ALS_HTTP_ANALYSIS=k8s-mesh \\  --set oap.env.JAVA_OPTS=\u0026#39;-Dmode=\u0026#39; \\  --set oap.envoy.als.enabled=true \\  --set elasticsearch.enabled=false We deploy SkyWalking to the namespace istio-system, so that SkyWalking OAP service can be accessed by skywalking-oap.istio-system:11800, to which we told ALS to emit their logs, in the previous step.\nWe also enable the ALS analyzer in the SkyWalking OAP: oap.env.SW_ENVOY_METRIC_ALS_HTTP_ANALYSIS=k8s-mesh. The analyzer parses the access logs and maps the IP addresses in the logs to the real service names in the Kubernetes, to build a topology.\nIn order to retrieve the metadata (such as Pod IP and service names) from a Kubernetes cluster for IP mappings, we also set oap.envoy.als.enabled=true, to apply for a ClusterRole that has access to the metadata.\nexport POD_NAME=$(kubectl get pods -A -l \u0026#34;app=skywalking,release=skywalking,component=ui\u0026#34; -o name) echo $POD_NAME kubectl -n istio-system port-forward $POD_NAME 8080:8080 Now navigate your browser to http://localhost:8080 . You should be able to see the SkyWalking dashboard. The dashboard is empty for now, but after we deploy the demo application and generate traffic, it should be filled up later.\nDeploying Bookinfo application Run:\nexport ISTIO_VERSION=1.7.1 kubectl apply -f https://raw.githubusercontent.com/istio/istio/$ISTIO_VERSION/samples/bookinfo/platform/kube/bookinfo.yaml kubectl apply -f https://raw.githubusercontent.com/istio/istio/$ISTIO_VERSION/samples/bookinfo/networking/bookinfo-gateway.yaml kubectl wait --for=condition=Ready pods --all --timeout=1200s minikube tunnel Then navigate your browser to http://localhost/productpage. You should be able to see the typical bookinfo application. Refresh the webpage several times to generate enough access logs.\nDone! And you’re all done! Check out the SkyWalking WebUI again. You should see the topology of the bookinfo application, as well the metrics of each individual service of the bookinfo application.\nTroubleshooting  Check all pods status: kubectl get pods -A. SkyWalking OAP logs: kubectl -n istio-system logs -f $(kubectl get pod -A -l \u0026quot;app=skywalking,release=skywalking,component=oap\u0026quot; -o name). SkyWalking WebUI logs: kubectl -n istio-system logs -f $(kubectl get pod -A -l \u0026quot;app=skywalking,release=skywalking,component=ui\u0026quot; -o name). Make sure the time zone at the bottom-right of the WebUI is set to UTC +0.  Customizing Service Names The SkyWalking community brought more improvements to the ALS solution in the 8.3.0 version. You can decide how to compose the service names when mapping from the IP addresses, with variables service and pod. For instance, configuring K8S_SERVICE_NAME_RULE to the expression ${service.metadata.name}-${pod.metadata.labels.version} gets service names with version label such as reviews-v1, reviews-v2, and reviews-v3, instead of a single service reviews, see the PR.\nWorking ALS with VM Kubernetes is popular, but what about VMs? From what we discussed above, in order to map the IPs to services, SkyWalking needs access to the Kubernetes cluster, fetching service metadata and Pod IPs. But in a VM environment, there is no source from which we can fetch those metadata. In the next post, we will introduce another ALS analyzer based on the Envoy metadata exchange mechanism. With this analyzer, you are able to observe a service mesh in the VM environment. Stay tuned! If you want to have commercial support for the ALS solution or hybrid mesh observability, Tetrate Service Bridge, TSB is another good option out there.\nAdditional Resources  KubeCon 2019 Recorded Video. Get more SkyWalking updates on the official website.  Apache SkyWalking founder Sheng Wu, SkyWalking core maintainer Zhenxu Ke are Tetrate engineers, and Tevah Platt is a content writer for Tetrate. Tetrate helps organizations adopt open source service mesh tools, including Istio, Envoy, and Apache SkyWalking, so they can manage microservices, run service mesh on any infrastructure, and modernize their applications.\n","excerpt":"Author: Zhenxu Ke, Sheng Wu, and Tevah Platt. tetrate.io Original link, Tetrate.io blog Dec. 03th, …","ref":"/blog/2020-12-03-obs-service-mesh-with-sw-and-als/","title":"Observe Service Mesh with SkyWalking and Envoy Access Log Service"},{"body":"","excerpt":"","ref":"/zh_tags/service-mesh/","title":"Service Mesh"},{"body":" 如果你正在寻找在 Mixer 方案以外观察服务网格的更优解，本文正符合你的需要。\n Apache Skywalking︰特别为微服务、云原生和容器化（Docker、Kubernetes、Mesos）架构而设计的 APM（应用性能监控）系统。\nEnvoy 访问日志服务︰访问日志服务（ALS）是 Envoy 的扩展组件，会将所有通过 Envoy 的请求的详细访问日志发送出来。\n背景 Apache SkyWalking 一直通过 Istio Mixer 的适配器，支持服务网格的可观察性。不过自从 v1.5 版本，由于 Mixer 在大型集群中差强人意的表现，Istio 开始弃用 Mixer。Mixer 的功能现已迁至 Envoy 代理，并获 Istio 1.7 版本支持。\n在去年的中国 KubeCon 中，吴晟和周礼赞基于 Apache SkyWalking 和 Envoy ALS，发布了新的方案：不再受制于 Mixer 带来的性能影响，也同时保持服务网格中同等的可观察性。这个方案最初是由吴晟、高洪涛、周礼赞和 Dhi Aurrahman 在 Tetrate.io 实现的。\n如果你正在寻找在 Mixer 方案之外，为你的服务网格进行观察的最优解，本文正是你当前所需的。在这个教程中，我们会解释此方案的运作逻辑，并将它实践到 bookinfo 应用上。\n运作逻辑 从可观察性的角度来说，Envoy 一般有两种部署模式︰Sidecar 和路由模式。 Envoy 代理可以代表多项服务（见下图之 1），或者当它作为 Sidecar 时，一般是代表接收和发送请求的单项服务（下图之 2 和 3）。\n在两种模式中，ALS 发放的日志都会带有一个节点标记符。该标记符在路由模式时，以 router~ （或 ingress~）开头，而在 Sidecar 代理模式时，则以 sidecar~ 开头。\n除了节点标记符之外，这个方案［1］所采用的访问日志也有几个值得一提的字段︰\ndownstream_direct_remote_address︰此字段是下游的直接远程地址，用作接收来自用户的请求。注意︰它永远是对端实体的地址，即使远程地址是从 x-forwarded-for header、代理协议等推断出来的。\ndownstream_remote_address︰远程或原始地址，用作接收来自用户的请求。\ndownstream_local_address︰本地或目标地址，用作接收来自用户的请求。\nupstream_remote_address︰上游的远程或目标地址，用作处理本次交换。\nupstream_local_address︰上游的本地或原始地址，用作处理本次交换。\nupstream_cluster︰upstream_remote_address 所属的上游集群。\n我们会在下面详细讲解各个字段。\nSidecar 当 Envoy 作为 Sidecar 的时候，会搭配服务一起部署，并代理来往服务的传入或传出请求。\n  代理传入请求︰在此情况下，Envoy 会作为服务器端的 Sidecar，以 inbound|portNumber|portName|Hostname[or]SidecarScopeID 格式设定 upstream_cluster。\nSkyWalking 分析器会检查 downstream_remote_address 是否能够找到对应的 Kubernetes 服务。\n如果在此 IP（和端口）中有一个服务（例如服务 B）正在运行，那我们就会建立起服务对服务的关系（即服务 B → 服务 A），帮助建立拓扑。再配合访问日志中的 start_time 和 duration 两个字段，我们就可以获得延迟的指标数据了。\n如果没有任何服务可以和 downstream_remote_address 相对应，那请求就有可能来自网格以外的服务。由于 SkyWalking 无法识别请求的服务来源，在没有源服务的情况下，它简单地根据拓扑分析方法生成数据。拓扑依然可以准确地建立，而从服务器端侦测出来的指标数据也依然是正确的。\n  代理传出请求︰在此情况下，Envoy 会作为客户端的 Sidecar，以 outbound|\u0026lt;port\u0026gt;|\u0026lt;subset\u0026gt;|\u0026lt;serviceFQDN\u0026gt; 格式设定 upstream_cluster。\n客户端的侦测相对来说比代理传入请求容易。如果 upstream_remote_address 是另一个 Sidecar 或代理的话，我们只需要获得它相应的服务名称，便可生成拓扑和指标数据。否则，我们没有办法理解它，只能把它当作 UNKNOWN 服务。\n  代理角色 当 Envoy 被部署为前端代理时，它是独立的服务，并不会像 Sidecar 一样，代表任何其他的服务。所以，我们可以建立客户端以及服务器端的指标数据。\n演示范例 在本章，我们会使用典型的 bookinfo 应用，来演示 Apache SkyWalking 8.3.0+ （截至 2020 年 11 月 30 日的最新版本）如何与 Envoy ALS 合作，联手观察服务网格。\n安装 Kubernetes 在 Kubernetes 和虚拟机器（VM）的环境下，SkyWalking 8.3.0 均支持 Envoy ALS 的方案。在本教程中，我们只会演示在 Kubernetes 的情境，至于 VM 方案，请耐心期待我们下一篇文章。所以在进行下一步之前，我们需要先安装 Kubernetes。\n在本教程中，我们会使用 Minikube 工具来快速设立本地的 Kubernetes（v1.17 版本）集群用作测试。要运行所有必要组件，包括 bookinfo 应用、SkyWalking OAP 和 WebUI，集群需要动用至少 4GB 内存和 2 个 CPU 的核心。\nminikube start --memory=4096 --cpus=2 然后，运行 kubectl get pods --namespace=kube-system --watch，检查所有 Kubernetes 的组件是否已准备好。如果还没，在进行下一步前，请耐心等待准备就绪。\n安装 Istio Istio 为配置 Envoy 代理和实现访问日志服务提供了一个非常方便的方案。内建的配置设定档为我们省去了不少手动的操作。所以，考虑到演示的目的，我们会在本教程全程使用 Istio。\nexport ISTIO_VERSION=1.7.1 curl -L https://istio.io/downloadIstio | sh - sudo mv $PWD/istio-$ISTIO_VERSION/bin/istioctl /usr/local/bin/ istioctl install --set profile=demo kubectl label namespace default istio-injection=enabled 然后，运行 kubectl get pods --namespace=istio-system --watch，检查 Istio 的所有组件是否已准备好。如果还没，在进行下一步前，请耐心等待准备就绪。\n启动访问日志服务 演示的设定档没有预设启动 ALS，我们需要重新配置才能够启动 ALS。\nistioctl manifest install \\  --set meshConfig.enableEnvoyAccessLogService=true \\  --set meshConfig.defaultConfig.envoyAccessLogService.address=skywalking-oap.istio-system:11800 范例指令 --set meshConfig.enableEnvoyAccessLogService=true 会在网格中启动访问日志服务。正如之前提到，ALS 本质上是一个会发放请求日志的 gRPC 服务。配置 meshConfig.defaultConfig.envoyAccessLogService.address=skywalking-oap.istio-system:11800 会告诉这个gRPC 服务往哪里发送日志，这里是往 skywalking-oap.istio-system:11800 发送，稍后我们会部署 SkyWalking ALS 接收器到这个地址。\n注意︰\n你也可以在安装 Istio 时启动 ALS，那就不需要在安装后重新启动 Istio︰\nistioctl install --set profile=demo \\  --set meshConfig.enableEnvoyAccessLogService=true \\  --set meshConfig.defaultConfig.envoyAccessLogService.address=skywalking-oap.istio-system:11800 kubectl label namespace default istio-injection=enabled 部署 Apache SkyWalking SkyWalking 社区提供了 Helm Chart ，让你更轻易地在 Kubernetes 中部署 SkyWalking 以及其依赖服务。 Helm Chart 可以在 GitHub 仓库找到。\n# Install Helm curl -sSLO https://get.helm.sh/helm-v3.0.0-linux-amd64.tar.gz sudo tar xz -C /usr/local/bin --strip-components=1 linux-amd64/helm -f helm-v3.0.0-linux-amd64.tar.gz # Clone SkyWalking Helm Chart git clone https://github.com/apache/skywalking-kubernetes cd skywalking-kubernetes/chart git reset --hard dd749f25913830c47a97430618cefc4167612e75 # Update dependencies helm dep up skywalking # Deploy SkyWalking helm -n istio-system install skywalking skywalking \\  --set oap.storageType=\u0026#39;h2\u0026#39;\\  --set ui.image.tag=8.3.0 \\  --set oap.image.tag=8.3.0-es7 \\  --set oap.replicas=1 \\  --set oap.env.SW_ENVOY_METRIC_ALS_HTTP_ANALYSIS=k8s-mesh \\  --set oap.env.JAVA_OPTS=\u0026#39;-Dmode=\u0026#39; \\  --set oap.envoy.als.enabled=true \\  --set elasticsearch.enabled=false 我们在 istio-system 的命名空间内部署 SkyWalking，使 SkyWalking OAP 服务可以使用地址 skywalking-oap.istio-system:11800 访问，在上一步中，我们曾告诉过 ALS 应往此处发放它们的日志。\n我们也在 SkyWalking OAP 中启动 ALS 分析器︰oap.env.SW_ENVOY_METRIC_ALS_HTTP_ANALYSIS=k8s-mesh。分析器会对访问日志进行分析，并解析日志中的 IP 地址和 Kubernetes 中的真实服务名称，以建立拓扑。\n为了从 Kubernetes 集群处获取元数据（例如 Pod IP 和服务名称），以识别相应的 IP 地址，我们还会设定 oap.envoy.als.enabled=true，用来申请一个对元数据有访问权的 ClusterRole。\nexport POD_NAME=$(kubectl get pods -A -l \u0026#34;app=skywalking,release=skywalking,component=ui\u0026#34; -o name) echo $POD_NAME kubectl -n istio-system port-forward $POD_NAME 8080:8080 现在到你的浏览器上访问 http://localhost:8080。你应该会看到 SkyWalking 的 Dashboard。 Dashboard 现在应该是空的，但稍后部署应用和生成流量后，它就会被填满。\n部署 Bookinfo 应用 运行︰\nexport ISTIO_VERSION=1.7.1 kubectl apply -f https://raw.githubusercontent.com/istio/istio/$ISTIO_VERSION/samples/bookinfo/platform/kube/bookinfo.yaml kubectl apply -f https://raw.githubusercontent.com/istio/istio/$ISTIO_VERSION/samples/bookinfo/networking/bookinfo-gateway.yaml kubectl wait --for=condition=Ready pods --all --timeout=1200s minikube tunnel 现在到你的浏览器上进入 http://localhost/productpage。你应该会看到典型的 bookinfo 应用画面。重新整理该页面几次，以生成足够的访问日志。\n完成了！ 这样做，你就成功完成设置了！再查看 SkyWalking 的 WebUI，你应该会看到 bookinfo 应用的拓扑，以及它每一个单独服务的指标数据。\n疑难解答  检查所有 pod 的状态︰kubectl get pods -A。 SkyWalking OAP 的日志︰kubectl -n istio-system logs -f $(kubectl get pod -A -l \u0026quot;app=skywalking,release=skywalking,component=oap\u0026quot; -o name)。 SkyWalking WebUI 的日志︰kubectl -n istio-system logs -f $(kubectl get pod -A -l \u0026quot;app=skywalking,release=skywalking,component=ui\u0026quot; -o name)。 确保 WebUI 右下方的时区设定在 UTC +0。  自定义服务器名称 SkyWalking 社区在 ALS 方案的 8.3.0 版本中，作出了许多改善。你现在可以在映射 IP 地址时，决定如何用 service 和 pod 变量去自定义服务器的名称。例如，将 K8S_SERVICE_NAME_RULE 设置为 ${service.metadata.name}-${pod.metadata.labels.version}，就可以使服务名称带上版本的标签，类似 reviews-v1、reviews-v2 和 reviews- v3，而不再是单个服务 review［2］。\n在 VM 上使用 ALS Kubernetes 很受欢迎，可是 VM 呢？正如我们之前所说，为了替 IP 找到对应的服务，SkyWalking 需要对 Kubernetes 集群有访问权，以获得服务的元数据和 Pod 的 IP。可是在 VM 环境中，我们并没有来源去收集这些元数据。\n在下一篇文章，我们会介绍另外一个 ALS 分析器，它是建立于 Envoy 的元数据交换机制。有了这个分析器，你就可以在 VM 环境中观察服务网格了。万勿错过！\n如果你希望在 ALS 方案或是混合式网格可观察性上获得商业支持，TSB 会是一个好选项。\n额外资源\n KubeCon 2019 的录影视频。 在官方网站上获得更多有关 SkyWalking 的最新消息吧。  如有任何问题或反馈，发送邮件至 learn@tetrate.io。\nApache SkyWalking 创始人吴晟和 SkyWalking 的核心贡献者柯振旭都是 Tetrate 的工程师。 Tetrate 的内容创造者编辑与贡献于本文章。 Tetrate 帮助企业采用开源服务网格工具，包括 Istio、Envoy 和 Apache SkyWalking，让它们轻松管理微服务，在任何架构上运行服务网格，以至现代化他们的应用。\n［1］https://github.com/envoyproxy/envoy/blob/549164c42cae84b59154ca4c36009e408aa10b52/generated_api_shadow/envoy/data/accesslog/v2/accesslog.proto\n［2］https://github.com/apache/skywalking/pull/5722\n","excerpt":"如果你正在寻找在 Mixer 方案以外观察服务网格的更优解，本文正符合你的需要。\n Apache Skywalking︰特别为微服务、云原生和容器化（Docker、Kubernetes、Mesos）架 …","ref":"/zh/observe-service-mesh-with-skywalking-and-envoy-access-log-service/","title":"使用 SkyWalking 和 Envoy 访问日志服务对服务网格进行观察"},{"body":"SkyWalking 8.3.0 is released. Go to downloads page to find release tars.\nProject  Test: ElasticSearch version 7.0.0 and 7.9.3 as storage are E2E tested. Test: Bump up testcontainers version to work around the Docker bug on MacOS.  Java Agent  Support propagate the sending timestamp in MQ plugins to calculate the transfer latency in the async MQ scenarios. Support auto-tag with the fixed values propagated in the correlation context. Make HttpClient 3.x, 4.x, and HttpAsyncClient 3.x plugins to support collecting HTTP parameters. Make the Feign plugin to support Java 14 Make the okhttp3 plugin to support Java 14 Polish tracing context related codes. Add the plugin for async-http-client 2.x Fix NPE in the nutz plugin. Provide Apache Commons DBCP 2.x plugin. Add the plugin for mssql-jtds 1.x. Add the plugin for mssql-jdbc 6.x -\u0026gt; 9.x. Fix the default ignore mechanism isn\u0026rsquo;t accurate enough bug. Add the plugin for spring-kafka 1.3.x. Add the plugin for Apache CXF 3.x. Fix okhttp-3.x and async-http-client-2.x did not overwrite the old trace header.  OAP-Backend  Add the @SuperDataset annotation for BrowserErrorLog. Add the thread pool to the Kafka fetcher to increase the performance. Add contain and not contain OPS in OAL. Add Envoy ALS analyzer based on metadata exchange. Add listMetrics GraphQL query. Add group name into services of so11y and istio relevant metrics Support keeping collecting the slowly segments in the sampling mechanism. Support choose files to active the meter analyzer. Support nested class definition in the Service, ServiceInstance, Endpoint, ServiceRelation, and ServiceInstanceRelation sources. Support sideCar.internalErrorCode in the Service, ServiceInstance, Endpoint, ServiceRelation, and ServiceInstanceRelation sources. Improve Kubernetes service registry for ALS analysis. Add health checker for cluster management Support the service auto grouping. Support query service list by the group name. Improve the queryable tags generation. Remove the duplicated tags to reduce the storage payload. Fix the threads of the Kafka fetcher exit if some unexpected exceptions happen. Fix the excessive timeout period set by the kubernetes-client. Fix deadlock problem when using elasticsearch-client-7.0.0. Fix storage-jdbc isExists not set dbname. Fix searchService bug in the InfluxDB storage implementation. Fix CVE in the alarm module, when activating the dynamic configuration feature. Fix CVE in the endpoint grouping, when activating the dynamic configuration feature. Fix CVE in the uninstrumented gateways configs, when activating the dynamic configuration feature. Fix CVE in the Apdex threshold configs, when activating the dynamic configuration feature. Make the codes and doc consistent in sharding server and core server. Fix that chunked string is incorrect while the tag contains colon. Fix the incorrect dynamic configuration key bug of endpoint-name-grouping. Remove unused min date timebucket in jdbc deletehistory logical Fix \u0026ldquo;transaction too large error\u0026rdquo; when use TiDB as storage. Fix \u0026ldquo;index not found\u0026rdquo; in trace query when use ES7 storage. Add otel rules to ui template to observe Istio control plane. Remove istio mixer Support close influxdb batch write model. Check SAN in the ALS (m)TLS process.  UI  Fix incorrect label in radial chart in topology. Replace node-sass with dart-sass. Replace serviceFilter with serviceGroup Removed \u0026ldquo;Les Miserables\u0026rdquo; from radial chart in topology. Add the Promise dropdown option  Documentation  Add VNode FAQ doc. Add logic endpoint section in the agent setup doc. Adjust configuration names and system environment names of the sharing server module Tweak Istio metrics collection doc. Add otel receiver.  All issues and pull requests are here\n","excerpt":"SkyWalking 8.3.0 is released. Go to downloads page to find release tars.\nProject  Test: …","ref":"/events/release-apache-skwaylking-apm-8-3-0/","title":"Release Apache SkyWalking APM 8.3.0"},{"body":"Python 作为一门功能强大的编程语言，被广泛的应用于计算机行业之中； 在微服务系统架构盛行的今天，Python 以其丰富的软件生态和灵活的语言特性在服务端编程领域也占有重要的一席之地。 本次分享将阐述 Apache SkyWalking 在微服务架构中要解决的问题，展示如何使用 Apache SkyWalking 来近乎自动化地监控 Python 后端应用服务，并对 Apache SkyWalking 的 Python 语言探针的实现技术进行解读。\n ","excerpt":"Python 作为一门功能强大的编程语言，被广泛的应用于计算机行业之中； 在微服务系统架构盛行的今天，Python 以其丰富的软件生态和灵活的语言特性在服务端编程领域也占有重要的一席之地。 本次分享将 …","ref":"/zh/2020-11-30-pycon/","title":"[视频] PyCon China 2020 - Python 微服务应用性能监控"},{"body":"SkyWalking CLI 0.5.0 is released. Go to downloads page to find release tars.\n  Features\n Use template files in yaml format instead Refactor metrics command to adopt metrics-v2 protocol Use goroutine to speed up dashboard global command Add metrics list command    Bug Fixes\n Add flags of instance, endpoint and normal for metrics command Fix the problem of unable to query database metrics    Chores\n Update release guide doc Add screenshots for use cases in README.md Introduce generated codes into codebase    ","excerpt":"SkyWalking CLI 0.5.0 is released. Go to downloads page to find release tars.\n  Features\n Use …","ref":"/events/release-apache-skywalking-cli-0-5-0/","title":"Release Apache SkyWalking CLI 0.5.0"},{"body":"","excerpt":"","ref":"/tags/satellite/","title":"Satellite"},{"body":" Author: Jiapeng Liu. Baidu. skywalking-satellite: The Sidecar Project of Apache SkyWalking Nov. 25th, 2020  A lightweight collector/sidecar which can be deployed close to the target monitored system, to collect metrics, traces, and logs. It also provides advanced features, such as local cache, format transformation, and sampling.\nDesign Thinking Satellite is a 2 level system to collect observability data from other core systems. So, the core element of the design is to guarantee data stability during Pod startup all the way to Pod shutdown avoiding alarm loss. All modules are designed as plugins, and if you have other ideas, you can add them yourself.\nSLO  Single gatherer supports \u0026gt; 1000 ops (Based 0.5 Core,50M) At least once delivery.(Optional) Data stability: 99.999%.(Optional)  Because they are influenced by the choice of plugins, some items in SLO are optional.\nRole Satellite would be running as a Sidecar. Although Daemonset mode would take up fewer resources, it will cause more troubles to the forwarding of agents. So we also want to use Sidecar mode by reducing the costs. But Daemonset mode would be also supported in the future plan.\nCore Modules The Satellite has 3 core modules which are Gatherer, Processor, and Sender.\n The Gatherer module is responsible for fetching or receiving data and pushing the data to Queue. The Processor module is responsible for reading data from the queue and processing data by a series of filter chains. The Sender module is responsible for async processing and forwarding the data to the external services in the batch mode. After sending success, Sender would also acknowledge the offset of Queue in Gatherer.  Detailed Structure The overall design is shown in detail in the figure below. We will explain the specific components one by one.\nGatherer Concepts The Gatherer has 4 components to support the data collection, which are Input, Collector, Worker, and Queue. There are 2 roles in the Worker, which are Fetcher and Receiver.\n The Input is an abstraction of the input source, which is usually mapped to a configuration file. The Collector is created by the Source, but many collectors could be created by the same Source. For example, when a log path has been configured as the /var/*.log in an Input, the number of collectors is the same as the file number in this path. The Fetcher and Receiver is the real worker to collect data. The receiver interface is an abstraction, which has multiple implementations, such as gRPC receiver and HTTP receiver.Here are some specific use cases:  Trace Receiver is a gRPC server for receiving trace data created by Skywalking agents. Log Receiver is also a gRPC server for receiving log data which is collected by Skywalking agents. (In the future we want Skywalking Agent to support log sending, and RPC-based log sending is more efficient and needs fewer resources than file reading. For example, the way of file reading will bring IO pressure and performance cost under multi-line splicing.) Log Fetcher is like Filebeat, which fits the common log collection scenario. This fetcher will have more responsibility than any other workers because it needs to record the offset and process the multi-line splicing. This feature will be implemented in the future. Prometheus Fetcher supports a new way to fetch Prometheus data and push the data to the upstream. \u0026hellip;\u0026hellip;   The Queue is a buffer module to decouple collection and transmission. In the 1st release version, we will use persistent storage to ensure data stability. But the implementation is a plug-in design that can support pure memory queues later.   The data flow We use the Trace Receiver as an example to introduce the data flow. Queue MmapQueue We have simplified the design of MmapQueue to reduce the resources cost on the memory and disk.\nConcepts There are 2 core concepts in MmapQueue.\n Segment: Segment is the real data store center, that provides large-space storage and does not reduce read and write performance as much as possible by using mmap. And we will avoid deleting files by reusing them. Meta: The purpose of meta is to find the data that the consumer needs.  Segment One MmapQueue has a directory to store the whole data. The Queue directory is made up with many segments and 1 meta file. The number of the segments would be computed by 2 params, which are the max cost of the Queue and the cost of each segment. For example, If the max cost is 512M and each segment cost is 256K, the directory can hold up to 2000 files. Once capacity is exceeded, an coverage policy is adopted that means the 2000th would override the first file.\nEach segment in Queue will be N times the size of the page cache and will be read and written in an appended sequence rather than randomly. These would improve the performance of Queue. For example, each Segment is a 128k file, as shown in the figure below.\nMeta The Meta is a mmap file that only contains 56Bit. There are 5 concepts in the Meta.\n Version: A version flag. Watermark Offset: Point to the current writing space.  ID: SegmentID Offset: The offset in Segment.   Writed Offset: Point to the latest refreshed data, that would be overridden by the write offset after period refresh.  ID: SegmentID Offset: The offset in Segment.   Reading Offset: Point to the current reading space.  ID: SegmentID Offset: The offset in Segment.   Committed Offset: Point to the latest committed offset , that is equal to the latest acked offset plus one.  ID: SegmentID Offset: The offset in Segment.    The following diagram illustrates the transformation process.\n The publisher receives data and wants to write to Queue.  The publisher would read Writing Offset to find a space and do plus one. After this, the publisher will write the data to the space.   The consumer wants to read the data from Queue.  The consumer would read Reading Offset to find the current read offset and do plus one. After this, the consumer will read the data from the space.   On period flush, the flusher would override Watermark Offset by using Writing Offset. When the ack operation is triggered, Committed Offset would plus the batch size in the ack batch. When facing crash, Writing Offset and Reading Offset would be overridden by Watermark Offset and Committed Offset. That is because the Reading Offset and Writing Offset cannot guarantee at least once delivery.  Mmap Performance Test The test is to verify the efficiency of mmap in low memory cost.\n The rate of data generation: 7.5K/item 1043 item/s (Based on Aifanfan online pod.) The test structure is based on Bigqueue because of similar structure. Test tool: Go Benchmark Test Command: go test -bench BenchmarkEnqueue -run=none -cpu=1 Result On Mac(15-inch, 2018,16 GB 2400 MHz DDR4, 2.2 GHz Intel Core i7 SSD):  BenchmarkEnqueue/ArenaSize-128KB/MessageSize-8KB/MaxMem-384KB 66501 21606 ns/op 68 B/op 1 allocs/op BenchmarkEnqueue/ArenaSize-128KB/MessageSize-8KB/MaxMem-1.25MB 72348 16649 ns/op 67 B/op 1 allocs/op BenchmarkEnqueue/ArenaSize-128KB/MessageSize-16KB/MaxMem-1.25MB 39996 33199 ns/op 103 B/op 1 allocs/op   Result On Linux(INTEL Xeon E5-2450 V2 8C 2.5GHZ2,INVENTEC PC3L-10600 16G8,INVENTEC SATA 4T 7.2K*8):  BenchmarkEnqueue/ArenaSize-128KB/MessageSize-8KB/MaxMem-384KB 126662\t12070 ns/op\t62 B/op\t1 allocs/op BenchmarkEnqueue/ArenaSize-128KB/MessageSize-8KB/MaxMem-1.25MB 127393\t12097 ns/op\t62 B/op\t1 allocs/op BenchmarkEnqueue/ArenaSize-128KB/MessageSize-16KB/MaxMem-1.25MB 63292\t23806 ns/op\t92 B/op\t1 allocs/op   Conclusion: Based on the above tests, mmap is both satisfied at the write speed and at little memory with very low consumption when running as a sidecar.  Processor The Processor has 3 core components, which are Consumer, Filter, and Context.\n The Consumer is created by the downstream Queue. The consumer has its own read offset and committed offset, which is similar to the offset concept of Spark Streaming. Due to the particularity of APM data preprocessing, Context is a unique concept in the Satellite filter chain, which supports storing the intermediate event because the intermediate state event also needs to be sent in sometimes. The Filter is the core data processing part, which is similar to the processor of beats. Due to the context, the upstream/downstream filters would be logically coupling.  Sender  BatchConverter decouples the Processor and Sender by staging the Buffer structure, providing parallelization. But if BatchBuffer is full, the downstream processors would be blocked. Follower is a real send worker that has a client, such as a gRPC client or Kafka client, and a fallback strategy. Fallback strategy is an interface, we can add more strategies to resolve the abnormal conditions, such as Instability in the network, upgrade the oap cluster. When sent success, Committed Offset in Queue would plus the number of this batch.  High Performance The scenario using Satellite is to collect a lot of APM data collection. We guarantee high performance by the following ways.\n Shorten transmission path, that means only join 2 components,which are Queue and Processor, between receiving and forwarding. High Performance Queue. MmapQueue provides a big, fast and persistent queue based on memory mapped file and ring structure. Processor maintains a linear design, that could be functional processed in one go-routine to avoid too much goroutines switching.  Stability Stability is a core point in Satellite. Stability can be considered in many ways, such as stable resources cost, stable running and crash recovery.\nStable resource cost In terms of resource cost, Memory and CPU should be a concern.\nIn the aspect of the CPU, we keep a sequence structure to avoid a large number of retries occurring when facing network congestion. And Satellite avoids keep pulling when the Queue is empty based on the offset design of Queue.\nIn the aspect of the Memory, we have guaranteed only one data caching in Satellite, that is Queue. For the queue structure, we also keep the size fixed based on the ring structure to maintain stable Memory cost. Also, MmapQueue is designed for minimizing memory consumption and providing persistence while keeping speed as fast as possible. Maybe supports some strategy to dynamically control the size of MmapQueue to process more extreme conditions in the future.\nStable running There are many cases of network congestion, such as the network problem on the host node, OAP cluster is under upgrating, and Kafka cluster is unstable. When facing the above cases, Follower would process fallback strategy and block the downstream processes. Once the failure strategy is finished, such that send success or give up this batch, the Follower would process the next batch.\nCrash Recovery The crash recovery only works when the user selects MmapQueue in Gatherer because of persistent file system design. When facing a crash, Reading Offset would be overridden by Committed Offset that ensure the at least once delivery. And Writed Offset would override Writing Offset that ensures the consumer always works properly and avoid encountering uncrossable defective data blocks.\nBuffer pool The Queue is to store fixed structure objects, object buffer pool would be efficient to reuse memory to avoid GC.\n ackChan batch convertor  Some metrics In Satellite, we should also collect its own monitoring metrics. The following metrics are necessary for Satellite.\n cpu memory go routine number gatherer_writing_offset gatherer_watermark_offset processor_reading_count sender_committed_offset sender_abandoned_count sender_retry_count  Input and Output We will reuse this diagram to explain the input and output.\n Input  Because the push-pull mode is both supported, Queue is a core component. Queue is designed to be a ring-shaped fixed capacity, that means the oldest data would be overridden by the latest data. If users find data loss, users should raise the ceiling of memory Queue. MmapQueue generally doesn\u0026rsquo;t face this problem unless the Sender transport is congested.   Ouput  If the BatchBuffer is full, the processor would be blocked. If the Channel is full, the downstream components would be blocked, such as BatchConvertor and Processor. When SenderWorker sends failure, the batch data would do a failure strategy that would block pulling data from the Channel. The strategy is a part of Sender,the operation mode is synchronous. Once the failure strategy is finished, such that send success or give up this batch, the Sendworker would keep pulling data from the Channel.    Questions How to avoid keep pulling when the Queue is empty? If Watermark Offset is less than or equal to Reading Offset, a signal would be sent to the consumer to avoid keep pulling.\nWhy reusing files in Queue? The unified model is a ring in Queue, that limits fixed resources cost in memory or disk.In Mmap Queue, reusing files turns the delete operations into an overwrite operations, effectively reducing the creation and deletion behavior in files.\nWhat are the strategies for file creation and deletion in MmapQueue? As Satellite running, the number of the files in MmapQueue would keep growing until up to the maximum capacity. After this, the old files will be overridden by the new data to avoid file deletion. When the Pod died, all resources were recycled.\n","excerpt":"Author: Jiapeng Liu. Baidu. skywalking-satellite: The Sidecar Project of Apache SkyWalking Nov. …","ref":"/blog/2020-11-25-skywalking-satellite-0.1.0-design/","title":"The first design of Satellite 0.1.0"},{"body":"SkyWalking Python 0.4.0 is released. Go to downloads page to find release tars.\n Feature: Support Kafka reporter protocol (#74) BugFix: Move generated packages into skywalking namespace to avoid conflicts (#72) BugFix: Agent cannot reconnect after server is down (#79) Test: Mitigate unsafe yaml loading (#76)  ","excerpt":"SkyWalking Python 0.4.0 is released. Go to downloads page to find release tars.\n Feature: Support …","ref":"/events/release-apache-skywalking-python-0-4-0/","title":"Release Apache SkyWalking Python 0.4.0"},{"body":"活动介绍 Apache SkyWalking 2020 开发者线下活动，社区创始人，PMC成员和Committer会亲临现场，和大家交流和分享项目中的使用经验。 以及邀请Apache Local Community 北京的成员一起分享Apache文化和Apache之道。\n日程安排 开场演讲 09：30-09：50 SkyWalking\u0026rsquo;s 2019-2020 and beyond\n吴晟，Tetrate.io创始工程师，Apache SkyWalking创始人\n  上午 09：55-10：30 贝壳全链路跟踪实践\n赵禹光，赵禹光，贝壳找房监控技术负责人，Apache SkyWalking PMC成员\n10：35-11：15 SkyWalking在百度爱番番部门实践\n刘嘉鹏，百度，SkyWalking contributor\n11：15-11：55 非计算机背景的同学如何贡献开源\n缘于一位本科在读的社会学系的同学的问题，这让我反思我们开源community的定位和Open的程度，于是，适兕从生产、分发、消费的软件供应的角度，根据涉及到的角色，然后再反观现代大学教育体系的专业，进一步对一个开源项目和community需要的专业背景多样性进行一个阐述和探究。并以ALC Beijing为例进行一个事例性的说明。\n适兕，开源布道师，ALC Beijing member，开源之道主创，开源社教育组成员。\n  下午 13：30-14：10 如何从 Apache SkyWalking 社区学习 Apache Way\n温铭，支流科技联合创始人＆CEO，Apache APISIX 项目 VP， Apache SkyWalking Committer\n14：10-14：50 Apache SkyWalking 在小米公司的应用\n宋振东，小米公司小米信息技术部 skywalking 研发负责人\n14：50-15：30 Istio全生命周期监控\n高洪涛，Tetrate.io创始工程师，Apache SkyWalking PMC成员\n15：30-15：45 茶歇\n15：45-16：25 针对HikariCP数据库连接池的监控\n张鑫 Apache SkyWalking PMC 成员\n16：25-17：00 SkyWalking 与 Nginx 的优化实践\n王院生 深圳支流科技创始人兼 CTO，Apache APISIX 创始人 \u0026amp; PMC成员\n ","excerpt":"活动介绍 Apache SkyWalking 2020 开发者线下活动，社区创始人，PMC成员和Committer会亲临现场，和大家交流和分享项目中的使用经验。 以及邀请Apache Local …","ref":"/zh/2020-11-23-devcon/","title":"[视频] SkyWalking DevCon 2020"},{"body":"The APM system provides the tracing or metrics for distributed systems or microservice architectures. Back to APM themselves, they always need backend storage to store the necessary massive data. What are the features required for backend storage? Simple, fewer dependencies, widely used query language, and the efficiency could be into your consideration. Based on that, traditional SQL databases (like MySQL) or NoSQL databases would be better choices. However, this topic will present another backend storage solution for the APM system viewing from NewSQL. Taking Apache Skywalking for instance, this talking will share how to make use of Apache ShardingSphere, a distributed database middleware ecosystem to extend the APM system\u0026rsquo;s storage capability.\nAs a senior DBA worked at JD.com, the responsibility is to develop the distributed database and middleware, and the automated management platform for database clusters. As a PMC of Apache ShardingSphere, I am willing to contribute to the OS community and explore the area of distributed databases and NewSQL.\n  ","excerpt":"The APM system provides the tracing or metrics for distributed systems or microservice …","ref":"/blog/2020-11-21-apachecon-obs-shardingsphere/","title":"[Video] Another backend storage solution for the APM system"},{"body":"Apache APISIX is a cloud-native microservices API gateway, delivering the ultimate performance, security, open-source and scalable platform for all your APIs and microservices. Apache SkyWalking: an APM(application performance monitor) system, especially designed for microservices, cloud-native and container-based (Docker, Kubernetes, Mesos) architectures. Through the powerful plug-in mechanism of Apache APISIX, Apache Skywalking is quickly supported, so that we can see the complete life cycle of requests from the edge to the internal service. Monitor and manage each request in a visual way, and improve the observability of the service.\n  ","excerpt":"Apache APISIX is a cloud-native microservices API gateway, delivering the ultimate performance, …","ref":"/blog/2020-11-21-apachecon-obs-apisix/","title":"[Video] Improve Apache APISIX observability with Apache SkyWalking"},{"body":"Today\u0026rsquo;s monitoring solutions are geared towards operational tasks, displaying behavior as time-series graphs inside dashboards and other abstractions. These abstractions are immensely useful but are largely designed for software operators, whose responsibilities require them to think in systems, rather than the underlying source code. This is problematic given that an ongoing trend of software development is the blurring boundaries between building and operating software. This trend makes it increasingly necessary for programming environments to not just support development-centric activities, but operation-centric activities as well. Such is the goal of the feedback-driven development approach. By combining IDE and APM technology, software developers can intuitively explore multiple dimensions of their software simultaneously with continuous feedback about their software from inception to production.\nBrandon Fergerson is an open-source software developer who does not regard himself as a specialist in the field of programming, but rather as someone who is a devoted admirer. He discovered the beauty of programming at a young age and views programming as an art and those who do it well to be artists. He has an affinity towards getting meta and combining that with admiration of programming, has found source code analysis to be exceptionally interesting. Lately, his primary focus involves researching and building AI-based pair programming technology.\n  ","excerpt":"Today\u0026rsquo;s monitoring solutions are geared towards operational tasks, displaying behavior as …","ref":"/blog/2020-11-21-apachecon-obs-sourcemarker/","title":"[Video] SourceMarker - Continuous Feedback for Developers"},{"body":"Over the past few years, and coupled with the growing adoption of microservices, distributed tracing has emerged as one of the most commonly used monitoring and troubleshooting methodologies. New tracing tools are increasingly being introduced, driving adoption even further. One of these tools is Apache SkyWalking, a popular open-source tracing, and APM platform. This talk explores the history of the SkyWalking storage module, shows the evolution of distributed tracing storage layers, from the traditional relational database to document-based search engine. I hope that this talk contributes to the understanding of history and also that it helps to clarify the different types of storage that are available to organizations today.\nHongtao Gao is the engineer of tetrate.io and the former Huawei Cloud expert. One of PMC members of Apache SkyWalking and participates in some popular open-source projects such as Apache ShardingSphere and Elastic-Job. He has an in-depth understanding of distributed databases, container scheduling, microservices, ServicMesh, and other technologies.\n  ","excerpt":"Over the past few years, and coupled with the growing adoption of microservices, distributed tracing …","ref":"/blog/2020-11-21-apachecon-obs-storage/","title":"[Video] The history of distributed tracing storage"},{"body":"","excerpt":"","ref":"/tags/conference/","title":"Conference"},{"body":"","excerpt":"","ref":"/tags/video/","title":"Video"},{"body":" 作者: 赵禹光 原文链接: 亲临百人盛况的Apache SkyWalking 2020 DevCon，看见了什么？ 2020 年 10 月 29 日  活动现场 2020年11月14日Apache SkyWalking 2020 DevCon由贝壳找房和tetrate赞助，Apache SkyWalking、云原生、Apache APISIX、Apache Pulsar 和 ALC Beijing 五大社区合作，在贝壳找房一年级会议室盛大举行，本次活动主要面对Apache SkyWalking的使用者、开发者和潜在用户。线上线下共有230多人报名。经统计，实际参加活动人数超过130人，近60%的人愿意抽出自己的休息时间，来交流学习Apache SkyWalking和开源文化。不难看见，在可预见的未来，中国的开源项目很快将进入下一个维度，那必定是更广的社区人员参与，更高技术知识体现，更强的线上稳定性和及时修复能力。\n活动历程： 09：30-09：50 SkyWalking\u0026rsquo;s 2019-2020 and beyond 吴晟老师本次分享：回顾2020年度SkyWalking发布的重要的新特性，出版的《Apache SkyWalking实战》图书，社区的进展，开源爱好者如何参与SkyWalking建设，和已知社区在主导的SkyWalking2021年孵化中的新特性。\n09：55-10：30 贝壳全链路跟踪实践 赵禹光老师（作者）本次分享：回顾了贝壳找房2018年至今，贝壳找房的全链路跟踪项目与SkyWalking的渊源，分享了SkyWalking在实践中遇到的问题，和解决方案。以及SkyWalking近10%的Committer都曾经或正在贝壳人店平台签中研发部，工作过的趣事。\n10：35-11：15 刘嘉鹏老师分享 SkyWalking在百度爱番番部门实践 刘嘉鹏老师本次分享：回顾了百度爱番番部门在使用SkyWalking的发展历程\u0026amp;现状，CRM SAAS产品在近1年使用SkyWalking实践经验，以及如何参与SkyWalking的贡献，并成为的Apache Committer。\n11：15-11：55 适兕老师分享 非计算机背景的同学如何贡献开源 适兕是国内很有名的开源布道师，本次分享从生产、分发、消费的软件供应的角度，根据涉及到的角色，然后再反观现代大学教育体系的专业，进一步对一个开源项目和community需要的专业背景多样性进行一个阐述和探究。并以ALC Beijing为例进行一个事例性的说明，非计算机背景的同学如何贡献开源。\n13：30-14：10 如何从 Apache SkyWalking 社区学习 Apache Way 14：10-14：50 Apache SkyWalking 在小米公司的应用 宋振东老师是小米信息技术部分布式链路追踪系统研发负责人，分别以小米公司，业务开发、架构师、SRE、Leader和QA等多个视角，回顾了SkyWalking在小米公司的应用实践。从APM的产品选型到实际落地，对其他公司准备使用SkyWalking落地，非常有借鉴意义。\n14：50-15：30 Istio全生命周期监控 高洪涛老师本次分享了SkyWalking和可观测云原生等非常前沿的知识布道，其中有，云原生在Logging、Metrics和Tracing的相关知识，Istio，K8S等方面的实践。对一些公司在前沿技术的落地，非常有借鉴意义。\n15：45-16：25 针对HikariCP数据库连接池的监控 张鑫老师本次分享了，以一个SkyWalking无法Tracing的实际线上故障的故事出发，讲述如何定位，和补充SkyWalking插件的不足，并将最后的实践贡献到社区。对大家参与开源很有帮助。\n16：25-17：00 SkyWalking 与 Nginx 的优化实践 王院生老师本次分享SkyWalking社区和APISIX社区合作，在Nginx插件的实践过程，对社区之间的如何开展合作，非常有借鉴意义，院生老师的工作\u0026amp;开源态度，很好的诠释Geek精神，也是我们互联网从业者需要学习恪守的。\nApache SkyWalking 2020 DevCon 讲师PPT Apache SkyWalking 2020 DevCon 讲师 PPT\nSkyWalking 后续发展计划 正如吴晟老师所说：No plan, open to the community，Apache SkyWalking是没有RoadMap。社区的后续发展，依赖于每个人在社区的贡献。与其期待，不如大胆设想，将自己的设计按照Apache Way贡献到SkyWalking，你就是下一个Apache SkyWalking Commiter，加入Member of SkyWalking大家庭，让社区因为你，而更加有活力。\n","excerpt":"作者: 赵禹光 原文链接: 亲临百人盛况的Apache SkyWalking 2020 DevCon，看见了什么？ 2020 年 10 月 29 日  活动现场 2020年11月14日Apache …","ref":"/zh/2020-11-21-what-do-we-see-at-the-apache-skywalking-2020-devcon-event/","title":"亲临百人盛况的Apache SkyWalking 2020 DevCon，看见了什么？"},{"body":"Sheng Wu is a founding engineer at tetrate.io, leads the observability for service mesh and hybrid cloud. A searcher, evangelist, and developer in the observability, distributed tracing, and APM. He is a member of the Apache Software Foundation. Love open source software and culture. Created the Apache SkyWalking project and being its VP and PMC member. Co-founder and PMC member of Apache ShardingSphere. Also as a PMC member of Apache Incubator and APISIX. He is awarded as Microsoft MVP, Alibaba Cloud MVP, Tencent Cloud TVP.\nIn the Apache FY2020 report, China is on the top of the download statistics. More China initiated projects joined the incubator, and some of them graduated as the Apache TLP. Sheng joined the Apache community since 2017, in the past 3 years, he witnessed the growth of the open-source culture and Apache way in China. Many developers have joined the ASF as new contributors, committers, foundation members. Chinese enterprises and companies paid more attention to open source contributions, rather than simply using the project like before. In the keynote, he would share the progress about China embracing the Apache culture, and willing of enhancing the whole Apache community.\n  ","excerpt":"Sheng Wu is a founding engineer at tetrate.io, leads the observability for service mesh and hybrid …","ref":"/blog/2020-11-21-apachecon-keynote/","title":"[Video] Apache grows in China"},{"body":"SkyWalking Client JS 0.2.0 is released. Go to downloads page to find release tars.\n Bug Fixes  Fixed a bug in sslTime calculate. Fixed a bug in server response status judgment.    ","excerpt":"SkyWalking Client JS 0.2.0 is released. Go to downloads page to find release tars.\n Bug Fixes  Fixed …","ref":"/events/release-apache-skywalking-client-js-0-2-0/","title":"Release Apache SkyWalking Client JS 0.2.0"},{"body":"SkyWalking Cloud on Kubernetes 0.1.0 is released. Go to downloads page to find release tars.\n Add OAPServer CRDs and controller.  ","excerpt":"SkyWalking Cloud on Kubernetes 0.1.0 is released. Go to downloads page to find release tars.\n Add …","ref":"/events/release-apache-skywalking-cloud-on-kubernetes-0.1.0/","title":"Release Apache SkyWalking Cloud on Kubernetes 0.1.0"},{"body":"Based on his continuous contributions, Jiapeng Liu (a.k.a evanljp) has been voted as a new committer.\n","excerpt":"Based on his continuous contributions, Jiapeng Liu (a.k.a evanljp) has been voted as a new …","ref":"/events/welcome-jiapeng-liu-as-new-committer/","title":"Welcome Jiapeng Liu as new committer"},{"body":"SkyWalking Kubernetes Helm Chart 4.0.0 is released. Go to downloads page to find release tars.\n Allow overriding configurations files under /skywalking/config. Unify the usages of different SkyWalking versions. Add Values for init container in case of using private regestry. Add services, endpoints resources in ClusterRole.  ","excerpt":"SkyWalking Kubernetes Helm Chart 4.0.0 is released. Go to downloads page to find release tars. …","ref":"/events/release-apache-skywalking-kubernetes-helm-chart-4.0.0/","title":"Release Apache SkyWalking Kubernetes Helm Chart 4.0.0"},{"body":"SkyWalking Client JS 0.1.0 is released. Go to downloads page to find release tars.\n Support Browser Side Monitoring. Require SkyWalking APM 8.2+.  ","excerpt":"SkyWalking Client JS 0.1.0 is released. Go to downloads page to find release tars.\n Support Browser …","ref":"/events/release-apache-skywalking-client-js-0-1-0/","title":"Release Apache SkyWalking Client JS 0.1.0"},{"body":"","excerpt":"","ref":"/tags/browser/","title":"Browser"},{"body":" Author: Zhenxu Ke, Sheng Wu, Hongtao Gao, and Tevah Platt. tetrate.io Original link, Tetrate.io blog Oct. 29th, 2020  Apache SkyWalking, the observability platform, and open-source application performance monitor (APM) project, today announced the general availability of its 8.2 release. The release extends Apache SkyWalking’s functionalities and monitoring boundary to the browser side.\nBackground SkyWalking is an observability platform and APM tool that works with or without a service mesh, providing automatic instrumentation for microservices, cloud-native and container-based applications. The top-level Apache project is supported by a global community and is used by Alibaba, Huawei, Tencent, Baidu, ByteDance, and scores of others.\nBrowser side monitoring APM helps SRE and Engineering teams to diagnose system failures, or optimize the systems before they become intolerably slow. But is it enough to always make the users happy?\nIn 8.2.0, SkyWalking extends its monitoring boundary to the browser side, e.g., Chrome, or the network between Chrome and the backend service, or the codes running in the browser. With this, not only can we monitor the backend services and requests sent by the browser as usual, but also the front end rendering speed, error logs, etc., which are the most efficient metrics for capturing the experiences of our end users. (This does not currently extend to IoT devices, but this feature moves SkyWalking a step in that direction).\nWhat\u0026rsquo;s more, SkyWalking browser monitoring also provides data about how the users use products, such as PV(page views), UV(unique visitors), top N PV(page views), etc., which can give a product team clues for optimizing their products.\nQuery traces by tags In SkyWalking\u0026rsquo;s Span data model, there are many important fields that are already indexed and can be queried by the users, but for the sake of performance, querying by Span tags was not supported until now. In SkyWalking 8.2.0, we allow users to query traces by specified tags, which is extremely useful. For example, SRE engineers running tests on the product environment can tag the synthetic traffic and query by this tag later.\nMeter Analysis Language In 8.2.0, the meter system provides a functional analysis language called MAL(Meter Analysis Language) that allows users to analyze and aggregate meter data in the OAP streaming system. The result of an expression can be ingested by either the agent analyzer or OpenTelemetry/Prometheus analyzer.\nComposite Alert Rules Alerting is a good way to discover system failures in time. A common problem is that we configure too many triggers just to avoid missing any possible issue. Nobody likes to be woken up by alert messages at midnight, only to find out that the trigger is too sensitive. These kinds of alerts become noisy and don\u0026rsquo;t help at all.\nIn 8.2.0, users can now configure composite alert rules, where composite rules take multiple metrics dimensions into account. With composite alert rules, we can leverage as many metrics as needed to more accurately determine whether there’s a real problem or just an occasional glitch.\nCommon scenarios like successful rate \u0026lt; 90% but there are only 1~2 requests can now be resolved by a composite rule, such as traffic(calls per minute) \u0026gt; n \u0026amp;\u0026amp; successful rate \u0026lt; m%.\nOther Notable Enhancements  The agent toolkit exposes some APIs for users to send customizable metrics. The agent exclude_plugins allows you to exclude some plugins; mount enables you to load a new set of plugins. More than 10 new plugins have been contributed to the agent. The alert system natively supports sending alert messages to Slack, WeChat, DingTalk.  Additional Resources  Read more about the SkyWalking 8.2 release highlights. Get more SkyWalking updates on Twitter.  ","excerpt":"Author: Zhenxu Ke, Sheng Wu, Hongtao Gao, and Tevah Platt. tetrate.io Original link, Tetrate.io blog …","ref":"/blog/2020-10-29-skywalking8-2-release/","title":"Features in SkyWalking 8.2: Browser Side Monitoring; Query Traces by Tags; Meter Analysis Language"},{"body":"","excerpt":"","ref":"/zh_tags/release-blog/","title":"Release Blog"},{"body":" 作者: 柯振旭, 吴晟, 高洪涛, Tevah Platt. tetrate.io 原文链接: What\u0026rsquo;s new with Apache SkyWalking 8.2? Browser monitoring and more 2020 年 10 月 29 日  Apache SkyWalking，一个可观测性平台，也是一个开源的应用性能监视器（APM）项目，今日宣布 8.2 发行版全面可用。该发行版拓展了核心功能，并将其监控边界拓展到浏览器端。\n背景 SkyWalking 是一个观测平台和 APM 工具。它可以选择性的与 Service Mesh 协同工作，为微服务、云原生和基于容器的应用提供自动的指标。该项目是全球社区支持的 Apache 顶级项目，阿里巴巴、华为、腾讯、百度、字节跳动等许多公司都在使用。\n浏览器端监控 APM 可以帮助 SRE 和工程团队诊断系统故障，也能在系统异常缓慢之前优化它。但它是否足以让用户总是满意呢？\n在 8.2.0 版本中， SkyWalking 将它的监控边界拓展到了浏览器端，比如 Chrome ，或者 Chrome 和后端服务之间的网络。这样，我们不仅可以像以前一样监控浏览器发送给后端服务的与请求，还能看到前端的渲染速度、错误日志等信息——这些信息是获取最终用户体验的最有效指标。（目前此功能尚未拓展到物联网设备中，但这项功能使得 SkyWalking 向着这个方向前进了一步）\n此外，SkyWalking浏览器监视也提供以下数据: PV（page views，页面浏览量）， UV（unique visitors，独立访客数），浏览量前 N 的页面（Top N Page Views）等。这些数据可以为产品队伍优化他们的产品提供线索。\n按标签 (tag) 查询链路数据 在 SkyWalking 的 Span 数据模型中，已经有了许多被索引并可供用户查询的重要字段。但出于性能考虑，使用 Span 标签查询链路数据的功能直到现在才正式提供。在 SkyWalking 8.2.0 中，我们允许用户查询被特定标签标记的链路，这非常有用。SRE 工程师可以在生产环境中运行测试，将其打上仿真流量的标签，并稍后通过该标签查找它。\n指标分析语言 在 8.2.0 中，仪表系统提供了一项名为MAL（Meter Analysis Language，指标分析语言）的强大分析语言。该语言允许用户在 OAP 流系统中分析并聚合（aggregate）指标数据。 表达式的结果可以被 Agent 分析器或 OpenTelemetry/Prometheus 分析器获取。\n复合警报规则 警报是及时发现系统失效的有效方式。一个常见的问题是，为了避免错过任何可能的问题，我们通常会配置过多的触发器（triggers）。没有人喜欢半夜被警报叫醒，结果只是因为触发系统太敏感。这种警报很嘈杂并毫无帮助。\n在 8.2.0 版本中，用户选择可以配置考虑了多个度量维度的复合警报规则。使用复合报警规则，我们可以根据需要添加尽可能多的指标来更精确地判断是否存在真正的问题，或者只是一个偶发的小问题。\n一些常见的情况，如 成功率 \u0026lt; 90% 但只有 1~2 个请求，现在可以通过复合规则解决，如流量(即每分钟调用数) \u0026gt; n \u0026amp;\u0026amp; 成功率 \u0026lt; m%。\n其它值得注意的功能增强  agent-toolkit SDK 公开了某些 API，供用户发送自定义指标。 Agent exclude_plgins 配置允许您排除某些插件（plugins）; mount 配置使您能够加载一套新的插件。 社区贡献了超过 10 个新 Agent 插件。 报警系统原生支持发送消息到 Slack，企业微信，钉钉。  附加资源   阅读更多关于SkyWalkng 8.2 发行版重点.\n  在推特上获取更多关于 SkyWalking 的更新。\n  Apache SkyWalking DevCon 报名信息 Apache SkyWalking DevCon 2020 开始报名了。 2020 年 11 月 14 日，欢迎大家来线下参加活动和交流, 或者报名观看线上直播。\n","excerpt":"作者: 柯振旭, 吴晟, 高洪涛, Tevah Platt. tetrate.io 原文链接: What\u0026rsquo;s new with Apache SkyWalking 8.2? Browser …","ref":"/zh/2020-10-29-skywalking8-2-release/","title":"SkyWalking 8.2.0 中的新特性: 浏览器端监控; 使用标签查询; 指标分析语言"},{"body":"SkyWalking 8.2.0 is released. Go to downloads page to find release tars.\nProject  Support Browser monitoring. Add e2e test for ALS solution of service mesh observability. Support compiling(include testing) in JDK11. Support build a single module.  Java Agent  Support metrics plugin. Support slf4j logs of gRPC and Kafka(when agent uses them) into the agent log files. Add PROPERTIES_REPORT_PERIOD_FACTOR config to avoid the properties of instance cleared. Limit the size of traced SQL to avoid OOM. Support mount command to load a new set of plugins. Add plugin selector mechanism. Enhance the witness classes for MongoDB plugin. Enhance the parameter truncate mechanism of SQL plugins. Enhance the SpringMVC plugin in the reactive APIs. Enhance the SpringMVC plugin to collect HTTP headers as the span tags. Enhance the Kafka plugin, about @KafkaPollAndInvoke Enhance the configuration initialization core. Plugin could have its own plugins. Enhance Feign plugin to collect parameters. Enhance Dubbo plugin to collect parameters. Provide Thrift plugin. Provide XXL-job plugin. Provide MongoDB 4.x plugin. Provide Kafka client 2.1+ plugin. Provide WebFlux-WebClient plugin. Provide ignore-exception plugin. Provide quartz scheduler plugin. Provide ElasticJob 2.x plugin. Provide Spring @Scheduled plugin. Provide Spring-Kafka plugin. Provide HBase client plugin. Provide JSON log format. Move Spring WebFlux plugin to the optional plugin. Fix inconsistent logic bug in PrefixMatch Fix duplicate exit spans in Feign LoadBalancer mechanism. Fix the target service blocked by the Kafka reporter. Fix configurations of Kafka report don\u0026rsquo;t work. Fix rest template concurrent conflict. Fix NPE in the ActiveMQ plugin. Fix conflict between Kafka reporter and sampling plugin. Fix NPE in the log formatter. Fix span layer missing in certain cases, in the Kafka plugin. Fix error format of time in serviceTraffic update. Upgrade bytebuddy to 1.10.14  OAP-Backend  Support Nacos authentication. Support labeled meter in the meter receiver. Separate UI template into multiple files. Provide support for Envoy tracing. Envoy tracer depends on the Envoy community. Support query trace by tags. Support composite alarm rules. Support alarm messages to DingTalk. Support alarm messages to WeChat. Support alarm messages to Slack. Support SSL for Prometheus fetcher and self telemetry. Support labeled histogram in the prometheus format. Support the status of segment based on entry span or first span only. Support the error segment in the sampling mechanism. Support SSL certs of gRPC server. Support labeled metrics in the alarm rule setting. Support to query all labeled data, if no explicit label in the query condition. Add TLS parameters in the mesh analysis. Add health check for InfluxDB storage. Add super dataset concept for the traces/logs. Add separate replicas configuration for super dataset. Add IN operator in the OAL. Add != operator in the OAL. Add like operator in the OAL. Add latest function in the prometheus analysis. Add more configurations in the gRPC server. Optimize the trace query performance. Optimize the CPU usage rate calculation, at least to be 1. Optimize the length of slow SQL column in the MySQL storage. Optimize the topology query, use client side component name when no server side mapping. Add component IDs for Python component. Add component ID range for C++. Fix Slack notification setting NPE. Fix some module missing check of the module manager core. Fix authentication doesn\u0026rsquo;t work in sharing server. Fix metrics batch persistent size bug. Fix trace sampling bug. Fix CLR receiver bug. Fix end time bug in the query process. Fix Exporter INCREMENT mode is not working. Fix an error when executing startup.bat when the log directory exists Add syncBulkActions configuration to set up the batch size of the metrics persistent. Meter Analysis Language.  UI  Add browser dashboard. Add browser log query page. Support query trace by tags. Fix JVM configuration. Fix CLR configuration.  Document  Add the document about SW_NO_UPSTREAM_REAL_ADDRESS. Update ALS setup document. Add Customization Config section for plugin development.  All issues and pull requests are here\n","excerpt":"SkyWalking 8.2.0 is released. Go to downloads page to find release tars.\nProject  Support Browser …","ref":"/events/release-apache-skywalking-apm-8-2-0/","title":"Release Apache SkyWalking APM 8.2.0"},{"body":"高洪涛 美国ServiceMesh服务商tetrate创始工程师。原华为软件开发云技术专家。目前为Apache SkyWalking核心贡献者，参与该开源项目在软件开发云的商业化进程。曾任职当当网系统架构师，开源达人，曾参与Apache ShardingSphere，Elastic-Job等知名开源项目。对分布式数据库，容器调度，微服务，ServicMesh等技术有深入的了解。\n议题简介 定制化Operator模式在面向Kubernetes的云化平台建构中变得越来越流行。Apache SkyWalking社区已经开始尝试使用Operator模式去构建基于Kubernetes平台的PaaS云组件。本次分享给将会给听众带来该项目的初衷，实现与未来演进等相关内容。分享的内容包含：\n 项目动机与设计理念 核心功能展示，包含SkyWalking核心组件的发布，更新与维护。 观测ServiceMesh，包含于Istio的自动集成。 目前的工作进展和对未来的规划。   ","excerpt":"高洪涛 美国ServiceMesh服务商tetrate创始工程师。原华为软件开发云技术专家。目前为Apache SkyWalking核心贡献者，参与该开源项目在软件开发云的商业化进程。曾任职当当网系统 …","ref":"/zh/2020-10-25-coscon20-swck/","title":"[视频] Apache SkyWalking Cloud on Kubernetes"},{"body":"SkyWalking LUA Nginx 0.3.0 is released. Go to downloads page to find release tars.\n Load the base64 module in utils, different ENV use different library. Add prefix skywalking, avoid conflicts with other lua libraries. Chore: only expose the method of setting random seed, it is optional. Coc: use correct code block type. CI: add upstream_status to tag http.status Add http.status  ","excerpt":"SkyWalking LUA Nginx 0.3.0 is released. Go to downloads page to find release tars.\n Load the base64 …","ref":"/events/release-apache-skywalking-lua-nginx-0.3.0/","title":"Release Apache SkyWalking LUA Nginx 0.3.0"},{"body":"SkyWalking CLI 0.4.0 is released. Go to downloads page to find release tars.\n Features  Add dashboard global command with auto-refresh Add dashboard global-metrics command Add traces search Refactor metrics thermodynamic command to adopt the new query protocol   Bug Fixes  Fix wrong golang standard time    ","excerpt":"SkyWalking CLI 0.4.0 is released. Go to downloads page to find release tars.\n Features  Add …","ref":"/events/release-apache-skywalking-cli-0-4-0/","title":"Release Apache SkyWalking CLI 0.4.0"},{"body":"Huaxi Jiang (江华禧) (a.k.a. fgksgf) mainly focuses on the SkyWalking CLI project, he had participated in the \u0026ldquo;Open Source Promotion Plan - Summer 2020\u0026rdquo; and completed the project smoothly, and won the award \u0026ldquo;Most Potential Students\u0026rdquo; that shows his great willingness to continuously contribute to our community.\nUp to date, he has submitted 26 PRs in the CLI repository, 3 PRs in the main repo, all in total include ~4000 LOC.\nAt Sep. 28th, 2020, the project management committee (PMC) passed the proposal of promoting him as a new committer. He has accepted the invitation at the same day.\nWelcome to join the committer team, Huaxi!\n","excerpt":"Huaxi Jiang (江华禧) (a.k.a. fgksgf) mainly focuses on the SkyWalking CLI project, he had participated …","ref":"/events/welcome-huaxi-jiang-as-new-committer/","title":"Welcome Huaxi Jiang (江华禧) as new committer"},{"body":"SkyWalking Python 0.3.0 is released. Go to downloads page to find release tars.\n  New plugins\n Urllib3 Plugin (#69) Elasticsearch Plugin (#64) PyMongo Plugin (#60) Rabbitmq Plugin (#53) Make plugin compatible with Django (#52)    API\n Add process propagation (#67) Add tags to decorators (#65) Add Check version of packages when install plugins (#63) Add thread propagation (#62) Add trace ignore (#59) Support snapshot context (#56) Support correlation context (#55)    Chores and tests\n Test: run multiple versions of supported libraries (#66) Chore: add pull request template for plugin (#61) Chore: add dev doc and reorganize the structure (#58) Test: update test health check (#57) Chore: add make goal to package release tar ball (#54)    ","excerpt":"SkyWalking Python 0.3.0 is released. Go to downloads page to find release tars.\n  New plugins …","ref":"/events/release-apache-skywalking-python-0-3-0/","title":"Release Apache SkyWalking Python 0.3.0"},{"body":"吴晟 吴晟，Apache 基金会会员，Apache SkyWalking 创始人、项目 VP 和 PMC 成员，Apache 孵化器 PMC 成员，Apache ShardingSphere PMC成员，Apache APISIX PMC 成员，Apache ECharts (incubating) 和Apache DolphinScheduler (incubating) 孵化器导师，Zipkin 成员和贡献者。\n分享大纲  分布式追踪兴起的背景 SkyWalking和其他分布式追踪的异同 定位问题的流程和方法 性能剖析的由来、用途和优势  听众收获 听众能够全面的了解分布式追踪的技术背景，和技术原理。以及为什么这些年，分布式追踪和基于分布式追踪的APM系统，Apache SkyWalking，得到了广泛的使用、集成，甚至云厂商的支持。同时，除了针对追踪数据，我们应该关注更多的是，如何利用其产生的监控数据，定位系统的性能问题。以及它有哪些短板，应该如何弥补。\n ","excerpt":"吴晟 吴晟，Apache 基金会会员，Apache SkyWalking 创始人、项目 VP 和 PMC 成员，Apache 孵化器 PMC 成员，Apache ShardingSphere PMC成 …","ref":"/zh/2020-08-13-cloud-native-academy/","title":"[视频] 云原生学院 - 后分布式追踪时代的性能问题定位——方法级性能剖析"},{"body":"SkyWalking Chart 3.1.0 is released. Go to downloads page to find release tars.\n Support SkyWalking 8.1.0 Support enable oap dynamic configuration through k8s configmap  ","excerpt":"SkyWalking Chart 3.1.0 is released. Go to downloads page to find release tars.\n Support SkyWalking …","ref":"/events/release-apache-skywalking-chart-3-1-0-for-skywalking-8-1-0/","title":"Release Apache SkyWalking Chart 3.1.0 for SkyWalking 8.1.0"},{"body":" Author: Sheng Wu Original link, Tetrate.io blog  SkyWalking, a top-level Apache project, is the open source APM and observability analysis platform that is solving the problems of 21st-century systems that are increasingly large, distributed, and heterogenous. It\u0026rsquo;s built for the struggles system admins face today: To identify and locate needles in a haystack of interdependent services, to get apples-to-apples metrics across polyglot apps, and to get a complete and meaningful view of performance.\nSkyWalking is a holistic platform that can observe microservices on or off a mesh, and can provide consistent monitoring with a lightweight payload.\nLet\u0026rsquo;s take a look at how SkyWalking evolved to address the problem of observability at scale, and grew from a pure tracing system to a feature-rich observability platform that is now used to analyze deployments that collect tens of billions of traces per day.\nDesigning for scale When SkyWalking was first initialized back in 2015, its primary use case was monitoring the first-generation distributed core system of China Top Telecom companies, China Unicom and China Mobile. In 2013-2014, the telecom companies planned to replace their old traditional monolithic applications with a distributed system. Supporting a super-large distributed system and scaleablity were the high-priority design goals from Day one. So, what matters at scale?\nPull vs. push Pull and push modes relate to the direction of data flow. If the agent collects data and pushes them to the backend for further analysis, we call it \u0026ldquo;push\u0026rdquo; mode. Debate over pull vs. push has gone on for a long time. The key for an observability system is to minimize the cost of the agent, and to be generally suitable for different kinds of observability data.\nThe agent would send the data out a short period after it is collected. Then, we would have less concern about overloading the local cache. One typical case would be endpoint (URI of HTTP, service of gRPC) metrics. Any service could easily have hundreds, even thousands of endpoints. An APM system must have these metrics analysis capabilities.\nFurthermore, metrics aren\u0026rsquo;t the only thing in the observability landscape; traces and logs are important too. SkyWalking is designed to provide a 100% sampling rate tracing capability in the production environment. Clearly, push mode is the only solution.\nAt the same time, using push mode natively doesn\u0026rsquo;t mean SkyWalking can\u0026rsquo;t do data pulling. In recent 8.x releases, SkyWalking supports fetching data from Prometheus-instrumented services for reducing the Non-Recurring Engineering of the end users. Also, pull mode is popular in the MQ based transport, typically as a Kafka consumer. The SkyWalking agent side uses the push mode, and the OAP server uses the pull mode.\nThe conclusion: push mode is the native way, but pull mode works in some special cases too.\nMetrics analysis isn\u0026rsquo;t just mathematical calculation Metrics rely on mathematical theories and calculations. Percentile is a good measure for identifying the long tail issue, and reasonable average response time and successful rate are good SLO(s). But those are not all. Distributed tracing provides not just traces with detailed information, but high values metrics that can be analyzed.\nThe service topology map is required from Ops and SRE teams for the NOC dashboard and confirmation of system data flow. SkyWalking uses the STAM (Streaming Topology Analysis Method) to analyze topology from the traces, or based on ALS (Envoy Access Log Service) in the service mesh environment. This topology and metrics of nodes (services) and lines (service relationships) can\u0026rsquo;t be pulled from simple metrics SDKs.\nAs with fixing the limitation of endpoint metrics collection, SkyWalking needs to do endpoint dependency analysis from trace data too. Endpoint dependency analysis provides more important and specific information, including upstream and downstream. Those dependency relationships and metrics help the developer team to locate the boundaries of a performance issue, to specific code blocks.\nPre-calculation vs. query stage calculation? Query stage calculation provides flexibility. Pre-calculation, in the analysis stage, provides better and much more stable performance. Recall our design principle: SkyWalking targets a large-scale distributed system. Query stage calculation was very limited in scope, and most metrics calculations need to be pre-defined and pre-calculated. The key of supporting large datasets is reducing the size of datasets in the design level. Pre-calculation allows the original data to be merged into aggregated results downstream, to be used in a query or even for an alert check.\nTTL of metrics is another important business enabler. With the near linear performance offered by queries because of pre-calculation, with a similar query infrastructure, organizations can offer higher TTL, thereby providing extended visibility of performance.\nSpeaking of alerts, query-stage calculation also means the alerting query is required to be based on the query engine. But in this case, when the dataset increasing, the query performance could be inconsistent. The same thing happens in a different metrics query.\nCases today Today, SkyWalking is monitoring super large-scale distributed systems in many large enterprises, including Alibaba, Huawei, Tencent, Baidu, China Telecom, and various banks and insurance companies. The online service companies have more traffic than the traditional companies, like banks and telecom suppliers.\nSkyWalking is the observability platform used for a variety of use cases for distributed systems that are super-large by many measures:\n Lagou.com, an online job recruitment platform  SkyWalking is observing \u0026gt;100 services, 500+ JVM instances SkyWalking collects and analyzes 4+ billion traces per day to analyze performance data, including metrics of 300k+ endpoints and dependencies Monitoring \u0026gt;50k traffic per second in the whole cluster   Yonghui SuperMarket, online service  SkyWalking analyzes at least 10+ billion (3B) traces with metrics per day SkyWalking\u0026rsquo;s second, smaller deployment, analyzes 200+ million traces per day   Baidu, internet and AI company, Kubernetes deployment  SkyWalking collects 1T+ traces a day from 1,400+ pods of 120+ services Continues to scale out as more services are added   Beike Zhaofang(ke.com), a Chinese online property brokerage backed by Tencent Holdings and SoftBank Group  Has used SkyWalking from its very beginning, and has two members in the PMC team. Deployments collect 16+ billion traces per day   Ali Yunxiao, DevOps service on the Alibaba Cloud,  SkyWalking collects and analyzes billions of spans per day SkyWalking keeps AliCloud\u0026rsquo;s 45 services and ~300 instances stable   A department of Alibaba TMall, one of the largest business-to-consumer online retailers, spun off from Taobao  A customized version of SkyWalking monitors billions of traces per day At the same time, they are building a load testing platform based on SkyWalking\u0026rsquo;s agent tech stack, leveraging its tracing and context propagation cabilities    Conclusion SkyWalking\u0026rsquo;s approach to observability follows these principles:\n Understand the logic model: don\u0026rsquo;t treat observability as a mathematical tool. Identify dependencies first, then their metrics. Scaling should be accomplished easily and natively. Maintain consistency across different architectures, and in the performance of APM itself.  Resources  Read about the SkyWalking 8.1 release highlights. Get more SkyWalking updates on Twitter. Sign up to hear more about SkyWalking and observability from Tetrate.  ","excerpt":"Author: Sheng Wu Original link, Tetrate.io blog  SkyWalking, a top-level Apache project, is the open …","ref":"/blog/2020-08-11-observability-at-scale/","title":"Observability at Scale: SkyWalking it is"},{"body":" 作者：吴晟 翻译：董旭 金蝶医疗 原文链接：Tetrate.io blog  SkyWalking做为Apache的顶级项目，是一个开源的APM和可观测性分析平台，它解决了21世纪日益庞大、分布式和异构的系统的问题。它是为应对当前系统管理所面临的困难而构建的：就像大海捞针，SkyWalking可以在服务依赖复杂且多语言环境下，获取服务对应的指标，以及完整而有意义的性能视图。\nSkyWalking是一个非常全面的平台，无论你的微服务是否在服务网格(Service Mesh)架构下，它都可以提供高性能且一致性的监控。\n让我们来看看，SkyWalking是如何解决大规模集群的可观测性问题，并从一个纯粹的链路跟踪系统，发展成为一个每天分析百亿级跟踪数据，功能丰富的可观测性平台。\n为超大规模而生 SkyWalking的诞生，时间要追溯到2015年，当时它主要应用于监控顶级电信公司（例如：中国联通和中国移动）的第一代分布式核心系统。2013-2014年，这些电信公司计划用分布式系统取代传统的单体架构应用。从诞生那天开始，SkyWalking首要的设计目标，就是能够支持超大型分布式系统，并具有很好可扩展性。那么支撑超大规模系统要考虑什么呢？\n拉取vs推送 与数据流向息息相关的：拉取模式和推送模式。Agent（客户端）收集数据并将其推送到后端，再对数据进一步分析，我们称之为“推送”模式。究竟应该使用拉取还是推送？这个话题已经争论已久。关键因素取决于可观测性系统的目标，即：在Agent端花最小的成本，使其适配不同类型的可观测性数据。\nAgent收集数据后，可以在短时间内发送出去。这样，我们就不必担心本地缓存压力过大。举一个典型的例子，任意服务都可以轻松地拥有数百个甚至数千个端点指标（如：HTTP的URI，gRPC的服务）。那么APM系统就必须具有分析这些数量庞大指标的能力。\n此外，度量指标并不是可观测性领域中的唯一关注点，链路跟踪和日志也很重要。在生产环境下，SkyWalking为了能提供100%采样率的跟踪能力，数据推送模式是唯一可行的解决方案。\nSkyWalking即便使用了推送模式，同时也可进行数据拉取。在最近的8.x的发版本中，SkyWalking支持从已经集成Prometheus的服务中获取终端用户的数据，避免重复工程建设，减少资源浪费。另外，比较常见的是基于MQ的传输构建拉取模式，Kafka消费者就是一个比较典型的例子。SkyWalking的Agent端使用推送模式，OAP服务器端使用拉取模式。\n结论：SkyWalking的推送模式是原生方式，但拉取式模式也适用于某些特殊场景。\n度量指标分析并不仅仅是数学统计 度量指标依赖于数学理论和计算。Percentile（百分位数）是用于反映响应时间的长尾效应。服务具备合理的平均响应时间和成功率，说明服务的服务等级目标(SLO）很好。除此之外，分布式跟踪还为跟踪提供了详细的信息，以及可分析的高价值指标。\n运维团队（OPS）和系统稳定性（SRE）团队通过服务拓扑图，用来观察网络情况（当做NOC dashboard使用）、确认系统数据流。SkyWalking依靠trace（跟踪数据），使用STAM（Streaming Topology Analysis Method）方法进行分析拓扑结构。在服务网格环境下，使用ALS（Envoy Access Log Service）进行拓扑分析。节点（services）和线路（service relationships）的拓扑结构和度量指标数据，无法通过sdk轻而易举的拿到。\n为了解决端点度量指标收集的局限性，SkyWalking还要从跟踪数据中分析端点依赖关系，从而拿到链路上游、下游这些关键具体的信息。这些依赖关系和度量指标信息，有助于开发团队定位引起性能问题的边界，甚至代码块。\n预计算还是查询时计算？ 相比查询时计算的灵活性，预计算可以提供更好、更稳定的性能，这在分析场景下尤为重要。回想一下我们的设计原则：SkyWalking是为了一个大规模的分布式系统而设计。查询时计算的使用范围非常有限，大多数度量计算都需要预先定义和预先计算。支持大数据集的关键是：在设计阶段，要减小数据集。预计算允许将原始数据合并到下游的聚合结果中，用于查询，甚至用于警报检查。\n使用SkyWalking的另一个重要因素是：指标的有效期，TTL（Time To Live）。由于采用了预先计算，查询提供了近似线性的高性能。这也帮助“查询系统”这类基础设施系统，提供更好的性能扩展。\n关于警报，使用查询时计算方案，也意味着警报查询需要基于查询引擎。但在这种情况下，随着数据集增加，查询性能会随之下降，其他指标查询也是一样的结果。\n目前使用案例 如今，SkyWalking在许多大型企业的超大规模分布式系统中使用，包括阿里巴巴、华为、腾讯、百度、中国通讯企业以及多家银行和保险公司。上线SkyWalking公司的流量，比银行和电信运营商这种传统公司还要大。\n在很多行业中，SkyWalking是被应用于超大型分布式系统各种场景下的一个可观测性平台：\n  拉勾网\n  SkyWalking正在观测超过100个服务，500多个JVM实例\n  SkyWalking每天收集和分析40多亿个跟踪数据，用来分析性能，其中包括30万个端点和依赖关系的指标\n  在整个群集中监控\u0026gt;50k流量/秒\n    永辉超市\n  SkyWalking每天分析至少100多亿（3B）的跟踪数据\n  其次，SkyWalking用较小的部署，每天分析2亿多个跟踪数据\n    百度\n  SkyWalking每天从1400多个pod中，从120多个服务收集1T以上的跟踪数据\n  随着更多服务的增加，规模会持续增大\n    贝壳找房(ke.com)\n  很早就使用了SkyWalking，有两名成员已经成为PMC\n  Deployments每天收集160多亿个跟踪数据\n    阿里云效\n  SkyWalking每天收集和分析数十亿个span\n  SkyWalking使阿里云的45项服务和~300个实例保持稳定\n    阿里巴巴天猫\n  SkyWalking个性化定制版，每天监控数十亿跟踪数据\n  与此同时，他们基于SkyWalking的Agent技术栈，利用其跟踪和上下文传播能力，正在构建一个全链路压测平台\n    结论 SkyWalking针对可观测性遵循以下原则：\n 理解逻辑模型：不要把可观测性当作数学统计工具。 首先确定依赖关系，然后确定它们的度量指标。 原生和方便的支撑大规模增长。 在不同的架构情况下，APM各方面表现依然保持稳定和一致。  资源  阅读SkyWalking 8.1发布亮点。 在Twitter上获取更多SkyWalking更新。 注册Tetrate以了解更多有关SkyWalking可观测性的信息。  ","excerpt":"作者：吴晟 翻译：董旭 金蝶医疗 原文链接：Tetrate.io blog  SkyWalking做为Apache的顶级项目，是一个开源的APM和可观测性分析平台，它解决了21世纪日益庞大、分布式和异 …","ref":"/zh/2020-08-11-observability-at-scale-skywalking-it-is/","title":"SkyWalking 为超大规模而生"},{"body":"","excerpt":"","ref":"/zh_tags/use-case/","title":"Use Case"},{"body":" Author: Sheng Wu, Hongtao Gao, and Tevah Platt(Tetrate) Original link, Tetrate.io blog  Apache SkyWalking, the observability platform, and open-source application performance monitor (APM) project, today announced the general availability of its 8.1 release that extends its functionalities and provides a transport layer to maintain the lightweight of the platform that observes data continuously.\nBackground SkyWalking is an observability platform and APM tool that works with or without a service mesh, providing automatic instrumentation for microservices, cloud-native and container-based applications. The top-level Apache project is supported by a global community and is used by Alibaba, Huawei, Tencent, Baidu, and scores of others.\nTransport traces For a long time, SkyWalking has used gRPC and HTTP to transport traces, metrics, and logs. They provide good performance and are quite lightweight, but people kept asking about the MQ as a transport layer because they want to keep the observability data continuously as much as possible. From SkyWalking’s perspective, the MQ based transport layer consumes more resources required in the deployment and the complexity of deployment and maintenance but brings more powerful throughput capacity between the agent and backend.\nIn 8.1.0, SkyWalking officially provides the typical MQ implementation, Kafka, to transport all observability data, including traces, metrics, logs, and profiling data. At the same time, the backend can support traditional gRPC and HTTP receivers, with the new Kafka consumer at the same time. Different users could choose the transport layer(s) according to their own requirements. Also, by referring to this implementation, the community could contribute various transport plugins for Apache Pulsar, RabbitMQ.\nAutomatic endpoint dependencies detection The 8.1 SkyWalking release offers automatic detection of endpoint dependencies. SkyWalking has long offered automatic endpoint detection, but endpoint dependencies, including upstream and downstream endpoints, are critical for Ops and SRE teams’ performance analysis. The APM system is expected to detect the relationships powered by the distributed tracing. While SkyWalking has been designed to include this important information at the beginning the latest 8.1 release offers a cool visualization about the dependency and metrics between dependent endpoints. It provides a new drill-down angle from the topology. Once you have the performance issue from the service level, you could check on instance and endpoint perspectives:\nSpringSleuth metrics detection In the Java field, the Spring ecosystem is one of the most widely used. Micrometer, the metrics API lib included in the Spring Boot 2.0, is now adopted by SkyWalking’s native meter system APIs and agent. For applications using Micrometer with the SkyWalking agent installed, all Micrometer collected metrics could then be shipped into SkyWalking OAP. With some configurations in the OAP and UI, all metrics are analyzed and visualized in the SkyWalking UI, with all other metrics detected by SkyWalking agents automatically.\nNotable enhancements The Java agent core is enhanced in this release. It could work better in the concurrency class loader case and is more compatible with another agent solution, such as Alibaba’s Arthas.\n With the logic endpoint supported, the local span can be analyzed to get metrics. One span could carry the raw data of more than one endpoint’s performance. GraphQL, InfluxDB Java Client, and Quasar fiber libs are supported to be observed automatically. Kubernetes Configmap can now for the first time be used as the dynamic configuration center– a more cloud-native solution for k8s deployment environments. OAP supports health checks, especially including the storage health status. If the storage (e.g., ElasticSearch) is not available, you could get the unhealth status with explicit reasons through the health status query. Opencensus receiver supports ingesting OpenTelemetry/OpenCensus agent metrics by meter-system.  Additional resources  Read more about the SkyWalking 8.1 release highlights. Read more about SkyWalking from Tetrate on our blog. Get more SkyWalking updates on Twitter. Sign up to hear more about SkyWalking and observability from Tetrate.  ","excerpt":"Author: Sheng Wu, Hongtao Gao, and Tevah Platt(Tetrate) Original link, Tetrate.io blog  Apache …","ref":"/blog/2020-08-03-skywalking8-1-release/","title":"Features in SkyWalking 8.1: SpringSleuth metrics, endpoint dependency detection, Kafka transport traces and metrics"},{"body":"","excerpt":"","ref":"/tags/kafka/","title":"Kafka"},{"body":"SkyWalking APM 8.1.0 is release. Go to downloads page to find release tars.\nProject  Support Kafka as an optional trace, JVM metrics, profiling snapshots and meter system data transport layer. Support Meter system, including the native metrics APIs and the Spring Sleuth adoption. Support JVM thread metrics.  Java Agent  [Core] Fix the concurrency access bug in the Concurrency ClassLoader Case. [Core] Separate the config of the plugins from the core level. [Core] Support instrumented class cached in memory or file, to be compatible with other agents, such as Arthas. Add logic endpoint concept. Could analysis any span or tags flagged by the logic endpoint. Add Spring annotation component name for UI visualization only. Add support to trace Call procedures in MySQL plugin. Support GraphQL plugin. Support Quasar fiber plugin. Support InfluxDB java client plugin. Support brpc java plugin Support ConsoleAppender in the logback v1 plugin. Enhance vert.x endpoint names. Optimize the code to prevent mongo statements from being too long. Fix WebFlux plugin concurrency access bug. Fix ShardingSphere plugins internal conflicts. Fix duplicated Spring MVC endpoint. Fix lettuce plugin sometimes trace doesn‘t show span layer. Fix @Tag returnedObject bug.  OAP-Backend  Support Jetty Server advanced configurations. Support label based filter in the prometheus fetcher and OpenCensus receiver. Support using k8s configmap as the configuration center. Support OAP health check, and storage module health check. Support sampling rate in the dynamic configuration. Add endpoint_relation_sla and endpoint_relation_percentile for endpoint relationship metrics. Add components for Python plugins, including Kafka, Tornado, Redis, Django, PyMysql. Add components for Golang SDK. Add Nacos 1.3.1 back as an optional cluster coordinator and dynamic configuration center. Enhance the metrics query for ElasticSearch implementation to increase the stability. Reduce the length of storage entity names in the self-observability for MySQL and TiDB storage. Fix labels are missing in Prometheus analysis context. Fix column length issue in MySQL/TiDB storage. Fix no data in 2nd level aggregation in self-observability. Fix searchService bug in ES implementation. Fix wrong validation of endpoint relation entity query. Fix the bug caused by the OAL debug flag. Fix endpoint dependency bug in MQ and uninstrumented proxy cases. Fix time bucket conversion issue in the InfluxDB storage implementation. Update k8s client to 8.0.0  UI  Support endpoint dependency graph. Support x-scroll of trace/profile page Fix database selector issue. Add the bar chart in the UI templates.  Document  Update the user logo wall. Add backend configuration vocabulary document. Add agent installation doc for Tomcat9 on Windows. Add istioctl ALS commands for the document. Fix TTL documentation. Add FAQ doc about thread instrumentation.  CVE  Fix fuzzy query sql injection in the MySQL/TiDB storage.  All issues and pull requests are here\n","excerpt":"SkyWalking APM 8.1.0 is release. Go to downloads page to find release tars.\nProject  Support Kafka …","ref":"/events/release-apache-skywalking-apm-8-1-0/","title":"Release Apache SkyWalking APM 8.1.0"},{"body":"","excerpt":"","ref":"/tags/spring/","title":"Spring"},{"body":"Based on his continuous contributions, Wei Hua (a.k.a alonelaval) has been voted as a new committer.\n","excerpt":"Based on his continuous contributions, Wei Hua (a.k.a alonelaval) has been voted as a new committer.","ref":"/events/welcome-wei-hua-as-new-committer/","title":"Welcome Wei Hua as new committer"},{"body":"SkyWalking Python 0.2.0 is released. Go to downloads page to find release tars.\n  Plugins:\n Kafka Plugin (#50) Tornado Plugin (#48) Redis Plugin (#44) Django Plugin (#37) PyMsql Plugin (#35) Flask plugin (#31)    API\n Add ignore_suffix Config (#40) Add missing log method and simplify test codes (#34) Add content equality of SegmentRef (#30) Validate carrier before using it (#29)    Chores and tests\n Test: print the diff list when validation failed (#46) Created venv builders for linux/windows and req flashers + use documentation (#38)    ","excerpt":"SkyWalking Python 0.2.0 is released. Go to downloads page to find release tars.\n  Plugins:\n Kafka …","ref":"/events/release-apache-skywalking-python-0-2-0/","title":"Release Apache SkyWalking Python 0.2.0"},{"body":"SkyWalking CLI 0.3.0 is released. Go to downloads page to find release tars.\n Command: health check command Command: Add trace command BugFix: Fix wrong metrics graphql path  ","excerpt":"SkyWalking CLI 0.3.0 is released. Go to downloads page to find release tars.\n Command: health check …","ref":"/events/release-apache-skywalking-cli-0-3-0/","title":"Release Apache SkyWalking CLI 0.3.0"},{"body":" Author: Srinivasan Ramaswamy, tetrate Original link, Tetrate.io blog  Asking How are you is more profound than What are your symptoms Background Recently I visited my preferred doctor. Whenever I visit, the doctor greets me with a series of light questions: How’s your day? How about the week before? Any recent trips? Did I break my cycling record? How’s your workout regimen? _Finally _he asks, “Do you have any problems?\u0026quot; On those visits when I didn\u0026rsquo;t feel ok, I would say something like, \u0026ldquo;I\u0026rsquo;m feeling dull this week, and I\u0026rsquo;m feeling more tired towards noon….\u0026quot; It\u0026rsquo;s at this point that he takes out his stethoscope, his pulse oximeter, and blood pressure apparatus. Then, if he feels he needs a more in-depth insight, he starts listing out specific tests to be made.\nWhen I asked him if the first part of the discussion was just an ice-breaker, he said, \u0026ldquo;That\u0026rsquo;s the essential part. It helps me find out how you feel, rather than what your symptoms are.\u0026quot; So, despite appearances, our opening chat about life helped him structure subsequent questions on symptoms, investigations and test results.\nOn the way back, I couldn\u0026rsquo;t stop asking myself, \u0026ldquo;Shouldn\u0026rsquo;t we be managing our mesh this way, too?\u0026quot;\nIf I strike parallels between my own health check and a health check, “tests” would be log analysis, “investigations” would be tracing, and “symptoms” would be the traditional RED (Rate, Errors and Duration) metrics. That leaves the “essential part,” which is what we are talking about here: the Wellness Factor, primarily the health of our mesh.\nHealth in the context of service mesh We can measure the performance of any observed service through RED metrics. RED metrics offer immense value in understanding the performance, reliability, and throughput of every service. Compelling visualizations of these metrics across the mesh make monitoring the entire mesh standardized and scalable. Also, setting alerts based on thresholds for each of these metrics helps to detect anomalies as and when they arise.\nTo establish the context of any service and observe them, it\u0026rsquo;s ideal to visualize the mesh as a topology.\nA topology visualization of the mesh not only allows for picking any service and watching its metrics, but also gives vital information about service dependencies and the potential impact of a given service on the mesh.\nWhile RED metrics of each service offer tremendous insights, the user is more concerned with the overall responsiveness of the mesh rather than each of these services in isolation.\nTo describe the performance of any service, right from submitting the request to receiving a completed http response, we’d be measuring the user\u0026rsquo;s perception of responsiveness. This measure of response time compared with a set threshold is called Apdex. This Apdex is an indicator of the health of a service in the mesh.\nApdex Apdex is a measure of response time considered against a set threshold**. **It is the ratio of satisfactory response times and unsatisfactory response times to total response times.\nApdex is an industry standard to measure the satisfaction of users based on the response time of applications and services. It measures how satisfied your users are with your services, as traditional metrics such as average response time could get skewed quickly.\nSatisfactory response time indicates the number of times when the roundtrip response time of a particular service was less than this threshold. Unsatisfactory response time while meaning the opposite, is further categorized as Tolerating and Frustrating. Tolerating accommodates any performance that is up to four times the threshold, and anything over that or any errors encountered is considered Frustrating. The threshold mentioned here is an ideal roundtrip performance that we expect from any service. We could even start with an organization-wide limit of say, 500ms.\nThe Apdex score is a ratio of satisfied and tolerating requests to the total requests made.\nEach satisfied request counts as one request, while each tolerating request counts as half a satisfied request.\nAn Apdex score takes values from 0 to 1, with 0 being the worst possible score indicating that users were always frustrated, and ‘1’ as the best possible score (100% of response times were Satisfactory).\nA percentage representation of this score also serves as the Health Indicator of the service.\nThe Math The actual computation of this Apdex score is achieved through the following formula.\n\tSatisfiedCount + ( ToleratingCount / 2 ) Apdex Score = ------------------------------------------------------ TotalSamples A percentage representation of this score is known as the Health Indicator of a service.\nExample Computation During a 2-minute period, a host handles 200 requests.\nThe Apdex threshold T = 0.5 seconds (500ms).\n 170 of the requests were handled within 500ms, so they are classified as Satisfied. 20 of the requests were handled between 500ms and 2 seconds (2000 ms), so they are classified as Tolerating. The remaining 10 were not handled properly or took longer than 2 seconds, so they are classified as Frustrated.  The resulting Apdex score is 0.9: (170 + (20/2))/200 = 0.9.\nThe next level At the next level, we can attempt to improve our topology visualization by coloring nodes based on their health. Also, we can include health as a part of the information we show when the user taps on a service.\nApdex specifications recommend the following Apdex Quality Ratings by classifying Apdex Score as Excellent (0.94 - 1.00), Good (0.85 - 0.93), Fair (0.70 - 0.84), Poor (0.50 - 0.69) and Unacceptable (0.00 - 0.49).\nTo visualize this, let’s look at our topology using traffic light colors, marking our nodes as Healthy, At-Risk and Unhealthy, where Unhealthy indicates health that falls below 80%. A rate between 80% and 95% indicates At-Risk, and health at 95% and above is termed Healthy.\nLet’s incorporate this coloring into our topology visualization and take its usability to the next level. If implemented, we will be looking at something like this.\nMoving further Apdex provides tremendous visibility into customer satisfaction on the responsiveness of our services. Even more, by extending the implementation to the edges calling this service we get further insight into the health of the mesh itself.\nTwo services with similar Apdex scores offer the same customer satisfaction to the customer. However, the size of traffic that flows into the service can be of immense help in prioritizing between services to address. A service with higher traffic flow is an indication that this experience is impacting a significant number of users on the mesh.\nWhile health relates to a service, we can also analyze the interactions between two services and calculate the health of the interaction. This health calculation of every interaction on the mesh helps us establish a critical path, based on the health of all interactions in the entire topology.\nIn a big mesh, showing traffic as yet another number will make it more challenging to visualize and monitor. We can, with a bit of creativity, improve the entire visualization by rendering the edges that connect services with different thickness depending on the throughput of the service.\nAn unhealthy service participating in a high throughput transaction could lead to excessive consumption of resources. On the other hand, this visualization also offers a great tip to maximize investment in tuning services.\nTuning service that is a part of a high throughput transaction offers exponential benefits when compared to tuning an occasionally used service.\nIf we look at implementing such a visualization, which includes the health of interactions and throughput of such interactions, we would be looking at something like below :\nThe day is not far These capabilities are already available to users today as one of the UI features of Tetrate’s service mesh platform, using the highly configurable and performant observability and performance management framework: Apache SkyWalking (https://skywalking.apache.org), which monitors traffic across the mesh, aggregates RED metrics for both services and their interactions, continuously computes and monitors health of the services, and enables users to configure alerts and notifications when services cross specific thresholds, thereby having a comprehensive health visibility of the mesh.\nWith such tremendous visibility into our mesh performance, the day is not far when we at our NOC (Network Operations Center) for the mesh have this topology as our HUD (Heads Up Display).\nThis HUD, with the insights and patterns gathered over time, would predict situations and proactively prompt us on potential focus areas to improve customer satisfaction.\nThe visualization with rich historical data can also empower the Network Engineers to go back in time and look at the performance of the mesh on a similar day in the past.\nAn earnest implementation of such a visualization would be something like below :\nTo conclude With all the discussion so far, the health of a mesh is more about how our users feel, and what we can proactively do as service providers to sustain, if not enhance, the experience of our users.\nAs the world advances toward personalized medicine, we\u0026rsquo;re not far from a day when my doctor will text me: \u0026ldquo;How about feasting yourself with ice cream today and take the Gray Butte Trail to Mount Shasta!\u0026rdquo; Likewise, we can do more for our customers by having better insight into their overall wellness.\nTetrate’s approach to “service mesh health” is not only to offer management, monitoring and support but to make infrastructure healthy from the start to reduce the probability of incidents. Powered by the Istio, Envoy, and SkyWalking, Tetrate\u0026rsquo;s solutions enable consistent end-to-end observability, runtime security, and traffic management for any workload in any environment.\nOur customers deserve healthy systems! Please do share your thoughts on making service mesh an exciting and robust experience for our customers.\nReferences  https://en.wikipedia.org/wiki/Apdex https://www.apdex.org/overview.html https://www.apdex.org/index.php/specifications/ https://skywalking.apache.org/  ","excerpt":"Author: Srinivasan Ramaswamy, tetrate Original link, Tetrate.io blog  Asking How are you is more …","ref":"/blog/2020-07-26-apdex-and-skywalking/","title":"The Apdex Score for Measuring Service Mesh Health"},{"body":" 作者: Srinivasan Ramaswamy, tetrate 翻译：唐昊杰，南京大学在读学生 校对：吴晟 Original link, Tetrate.io blog July. 26th, 2020  \u0026ldquo;你感觉怎么样\u0026rdquo; 比 \u0026ldquo;你的症状是什么\u0026rdquo; 更重要 背景 最近我拜访了我的医生。每次去看病，医生都会首先问我一连串轻快的问题，比如：你今天过得怎么样？上周过的怎么样？最近有什么出行吗？你打破了自己的骑车记录吗？你的锻炼计划实施如何？最后他会问：“你有什么麻烦吗？”如果这个时候我感觉自己不太好，我会说：“我这周感觉很沉闷，临近中午的时候感觉更累。”这时他就会拿出听诊器、脉搏血氧仪和血压仪。然后，如果他觉得自己需要更深入的了解情况，他就开始列出我需要做的具体检查。\n当我问他，最开始的讨论是否只是为了缓和氛围。他说：“这是必不可少的部分。它帮助我发现你感觉如何，而不是你的症状是什么。\u0026quot;。我们这样关于生活的开场聊天，帮助他组织了后续关于症状、调查和测试结果的问题。\n在回来的路上，我不停地问自己：“我们是不是也应该用这种方式管理我们的网格(service mesh)？”\n如果我把自己的健康检查和网格的健康检查进行类比，“医疗检查”就是日志分析，“调查”就是追踪，“症状”就是传统的RED指标（请求速率、请求错误和请求耗时）。那么根本的问题，就是我们在这里讨论的：健康因素（主要是网格的健康）。\n服务网格中的健康状况 我们可以通过RED指标来衡量任何被观察到的服务的性能。RED指标在了解每个服务的性能、可靠性和吞吐量方面提供了巨大的价值。这些指标在网格上的令人信服的可视化使得监控全部网格变得标准化和可扩展。此外，根据这些指标的阈值设置警报有助于在指标值异常的时候进行异常检测。\n为了建立任何服务的上下文环境并观察它们，理想的做法是将网格可视化为一个拓扑结构。\n网格的拓扑结构可视化不仅允许使用者挑选任意服务并观察其指标，还可以提供有关服务依赖和特定服务在网格上的潜在影响这些重要信息。\n虽然每个服务的RED指标为使用者提供了深刻的洞察能力，但使用者更关心网格的整体响应性，而非每个单独出来的服务的响应性。\n为了描述任意服务的性能（即从提交请求到收到完成了的http响应这段时间内的表现），我们会测量用户对响应性的感知。这种将响应时间与设定的阈值进行比较的衡量标准叫做Apdex。Apdex是衡量一个服务在网格中的健康程度的指标。\nApdex Apdex是根据设定的阈值和响应时间结合考虑的衡量标准。它是满意响应时间和不满意响应时间相对于总响应时间的比率。\nApdex是根据应用和服务的响应时间来衡量使用者满意程度的行业标准。它衡量的是用户对你的服务的满意程度，因为传统的指标（如平均响应时间）可能很快就会容易形成偏差。\n基于满意度的响应时间，表示特定服务的往返响应时间小于设定的阈值的次数。不满意响应时间虽然意思相反，但又进一步分为容忍型和失望型。容忍型包括了了任何响应时间不超过四倍阈值的表现，而任何超过四倍阈值或遇到了错误的表现都被认为是失望型。这里提到的阈值是我们对任意服务所期望的理想响应表现。我们可以设置一个全局范围的阈值，如，500ms。\nApdex得分是满意型请求和容忍型请求与做出的总请求的比率。\n每个_满意的请求_算作一个请求，而每个_容忍的请求_算作半个_满意_的请求。\n一个Apdex得分从0到1的范围内取值。0是最差的分数，表示用户总是感到失望；而'1\u0026rsquo;是最好的分数（100%的响应时间是令人满意的）。\n这个分数的百分比表示也可以用作服务的健康指标。\n数学表示 Apdex得分的实际计算是通过以下公式实现的：\n\t满意请求数 + ( 容忍请求数 / 2 ) Apdex 得分 = ------------------------------------------------------ 总请求数 此公示得到的百分率，即可视为服务的健康度。\n样例计算 在两分钟的采样时间内，主机处理200个请求。\nApdex阈值T设置为0.5秒（500ms）。\n*.\t170个请求在500ms内被处理完成，它们被分类为满意型。 *.\t20个请求在500ms和2秒间被处理，它们被分类为容忍型。 *.\t剩余的10个请求没有被正确处理或者处理时间超过了2秒，所以它们被分类为失望型。\n最终的Apdex得分是0.9，即（170 + （20 / 2））/ 200。\n深入使用 在接下来的层次，我们可以尝试通过根据节点的健康状况来着色节点以改进我们的拓扑可视化。此外，我们还可以在用户点击服务时将健康状况作为我们展示的信息的一部分。\nApdex规范推荐了以下Apdex质量评级，将Apdex得分分为优秀（0.94 - 1.00）、良好（0.85 - 0.93）、一般（0.70 - 0.84）、差（0.50 - 0.69）和不可接受（0.00 - 0.49）。\n为了可视化网格的健康状况，我们用交通灯的颜色将我们的节点标记为健康、有风险和不健康，其中不健康表示健康率低于80%。健康率在80%到95%之间的表示有风险，健康率在95%及以上的称为健康。\n让我们将这种着色融入到我们的拓扑可视化中，并将其可用性提升到一个新的水平。如果实施，我们将看到下图所示的情况。\n更进一步 Apdex为客户对我们服务响应性的满意度提供了可见性。更有甚者，通过将实施范围扩展到调用该服务的调用关系，我们可以进一步了解网格本身的健康状况。\n两个有着相似Apdex分数的服务，为客户提供了相同的客户满意度。然而，流入服务的流量大小对于优先处理哪一服务有着巨大的帮助。流量较高的服务表明这种服务体验影响了网格上更大量的使用者。\n虽然健康程度与单个服务有关，但我们也可以分析两个服务之间的交互并计算交互过程的健康程度。这种对网格上每一个交互的健康程度的计算，可以帮助我们根据整个拓扑结构中所有交互的健康程度，建立一个关键路径。\n在一个大的网格中，将流量展示为另一个数字将使可视化和监控更具挑战性。我们可以根据服务的吞吐量，通过用不同的粗细程度渲染连接服务的边来改善整个可视化的效果。\n一个位于高吞吐量事务的不健康的服务可能会导致资源的过度消耗。另一方面，这种可视化也为调整服务时获取最大化投资效果提供了一个很好的提示。\n与调整一个偶尔使用的服务相比，调整作为高吞吐量事务的一部分的那些服务会带来指数级的收益。\n实施这种包括了交互的健康状况和吞吐量的可视化，我们会看到下图所示的情况:\n这一天即将到来 目前，这些功能已经作为Tetrate服务网格平台的UI功能之一来提供给用户。该平台使用了高速可配置化、高性能的可观测性和监控性能管理平台：Apache SkyWalking (https://skywalking.apache.org)，SkyWalking可以监控整个网格的流量，为服务及它们的交互合计RED指标，持续计算和监控服务的健康状况，并使用户能够在服务超过特定阈值时配置报警和通知。这些功能使得SkyWalking对网格拥有全面的健康状况可见性。\n有了这样强大的网格性能可视性，我们将可以在为网格准备的网络运营中心使用这种拓扑结构作为我们的HUD（Heads Up Display）。\nHUD随着时间的推移收集了解到的信息和模式，并将预测各种情况和主动提示我们潜在的重点领域以提高客户满意度。\n丰富的历史数据的可视化也可以使网络工程师能够看看过去中类似的一天的网格表现。\n可视化效果如下图所示。\n总结 综合到目前为止的所有讨论，网格的健康状况更多地是关于用户的感受，以及我们作为服务提供商可以采取积极行动来维持（如果不能增强）用户的体验。\n着个人化医学的发展，现在距离我的医生给我发这样短信的日子并不遥远：“要不今天享用冰淇淋并且沿着灰色小山步道到达沙斯塔山！”相似的，我们可以通过更好地了解客户的整体健康状况为他们做更多的事情。\nTetrate的“服务网格健康程度”方法不仅提供了管理，监视和支持，而且从一开始就使基础架构保持健康以减少事故发生的可能性。在Istio，Envoy和SkyWalking的支持下，Tetrate的解决方案可为任何环境中的任何工作负载提供持续的端到端可观察性，运行时安全性和流量管理。\n我们的客户应该拥有健康的系统！请分享您对使用服务网格为我们的客户带来令人兴奋和强健的体验的想法。\n引用  https://en.wikipedia.org/wiki/Apdex https://www.apdex.org/overview.html https://www.apdex.org/index.php/specifications/ https://skywalking.apache.org/  ","excerpt":"作者: Srinivasan Ramaswamy, tetrate 翻译：唐昊杰，南京大学在读学生 校对：吴晟 Original link, Tetrate.io blog July. 26th, …","ref":"/zh/2020-07-26-apdex-and-skywalking/","title":"度量服务网格健康度——Apdex得分"},{"body":"SkyWalking Python 0.1.0 is released. Go to downloads page to find release tars.\n API: agent core APIs, check the APIs and the examples Plugin: built-in libraries http, urllib.request and third-party library requests are supported. Test: agent test framework is setup, and the corresponding tests of aforementioned plugins are also added.  ","excerpt":"SkyWalking Python 0.1.0 is released. Go to downloads page to find release tars.\n API: agent core …","ref":"/events/release-apache-skywalking-python-0-1-0/","title":"Release Apache SkyWalking Python 0.1.0"},{"body":"SkyWalking Chart 3.0.0 is released. Go to downloads page to find release tars.\n Support SkyWalking 8.0.1  ","excerpt":"SkyWalking Chart 3.0.0 is released. Go to downloads page to find release tars.\n Support SkyWalking …","ref":"/events/release-apache-skywalking-chart-3-0-0-for-skywalking-8-0-1/","title":"Release Apache SkyWalking Chart 3.0.0 for SkyWalking 8.0.1"},{"body":"Apache SkyWalking 8.0.1 已发布。SkyWalking 是观察性分析平台和应用性能管理系统，提供分布式追踪、服务网格遥测分析、度量聚合和可视化一体化解决方案，支持 Java, .Net Core, PHP, NodeJS, Golang, LUA 语言探针，支持 Envoy + Istio 构建的 Service Mesh。\n与 8.0.0 相比，此版本包含一个热修复程序。\nOAP-Backend\n 修复 no-init 模式在 Elasticsearch 存储中无法运行的错误  8.0.0 值得关注的变化：\n 添加并实现了 v3 协议，旧版本与 8.x 不兼容 移除服务、实例、端点注册机制和 inventory 存储实体 (inventory storage entities) 提供新的 GraphQL 查询协议，同时支持旧协议（计划在今年年底移除） 支持 Prometheus 网络协议，可将 Prometheus 格式的指标传输到 SkyWalking 中 提供 Python agent 移除所有 inventory 缓存 提供 Apache ShardingSphere (4.0.0, 4.1.1) agent 插件 UI dashboard 100% 可配置，可采用后台定义的新指标 修复 H2/MySQL 实现中的 SQL 注入漏洞 Upgrade Nacos to avoid the FastJson CVE in high frequency. 升级 Nacos 以避免 FastJson CVE 升级 jasckson-databind 至 2.9.10  下载地址：http://skywalking.apache.org/downloads/\n","excerpt":"Apache SkyWalking 8.0.1 已发布。SkyWalking 是观察性分析平台和应用性能管理系统，提供分布式追踪、服务网格遥测分析、度量聚合和可视化一体化解决方案，支持 Java, …","ref":"/zh/2020-06-21-skywalking8-0-1-release/","title":"Apache SkyWalking 8.0.1 发布"},{"body":"SkyWalking Nginx LUA 0.2.0 is release. Go to downloads page to find release tars.\n Adapt the new v3 protocol. Implement correlation protocol. Support batch segment report.  ","excerpt":"SkyWalking Nginx LUA 0.2.0 is release. Go to downloads page to find release tars.\n Adapt the new v3 …","ref":"/events/release-apache-skywalking-nginx-lua-0-2-0/","title":"Relase Apache SkyWalking Nginx LUA 0.2.0"},{"body":"SkyWalking APM 8.0.0 is release. Go to downloads page to find release tars.\nProject  v3 protocol is added and implemented. All previous releases are incompatible with 8.x releases. Service, Instance, Endpoint register mechanism and inventory storage entities are removed. New GraphQL query protocol is provided, the legacy procotol is still supported(plan to remove at the end of this year). Support Prometheus network protocol. Metrics in Prometheus format could be transferred into SkyWalking. Python agent provided. All inventory caches have been removed. Apache ShardingSphere(4.1.0, 4.1.1) agent plugin provided.  Java Agent  Add MariaDB plugin. Vert.x plugin enhancement. More cases are covered. Support v3 extension header. Fix ElasticSearch 5.x plugin TransportClient error. Support Correlation protocol v1. Fix Finagle plugin bug, in processing Noop Span. Make CommandService daemon to avoid blocking target application shutting down gracefully. Refactor spring cloud gateway plugin and support tracing spring cloud gateway 2.2.x  OAP-Backend  Support meter system for Prometheus adoption. In future releases, we will add native meter APIs and MicroMeter(Sleuth) system. Support endpoint grouping. Add SuperDataSet annotation for storage entity. Add superDatasetIndexShardsFactor in the ElasticSearch storage, to provide more shards for @SuperDataSet annotated entites. Typically TraceSegment. Support alarm settings for relationship of service, instance, and endpoint level metrics. Support alarm settings for database(conjecture node in tracing scenario). Data Model could be added in the runtime, don\u0026rsquo;t depend on the bootstrap sequence anymore. Reduce the memory cost, due to no inventory caches. No buffer files in tracing and service mesh cases. New ReadWriteSafe cache implementation. Simplify codes. Provide default way for metrics query, even the metrics doesn\u0026rsquo;t exist. New GraphQL query protocol is provided. Support the metrics type query. Set up length rule of service, instance, and endpoint. Adjust the default jks for ElasticSearch to empty. Fix Apdex function integer overflow issue. Fix profile storage issue. Fix TTL issue. Fix H2 column type bug. Add JRE 8-14 test for the backend.  UI  UI dashboard is 100% configurable to adopt new metrics definited in the backend.  Document  Add v8 upgrade document. Make the coverage accurate including UT and e2e tests. Add miss doc about collecting parameters in the profiled traces.  CVE  Fix SQL Injection vulnerability in H2/MySQL implementation. Upgrade Nacos to avoid the FastJson CVE in high frequency. Upgrade jasckson-databind to 2.9.10.  All issues and pull requests are here\n","excerpt":"SkyWalking APM 8.0.0 is release. Go to downloads page to find release tars.\nProject  v3 protocol is …","ref":"/events/release-apache-skywalking-apm-8-0-0/","title":"Release Apache SkyWalking APM 8.0.0"},{"body":"可观察性平台和开源应用程序性能监控（APM）项目 Apache SkyWalking，今天刚宣布 8.0 的发布版本。素以强劲指标、追踪与服务网格能力见称的 SkyWalking ，在最新版本中的功能性延展到用户渴求已久的功能 —— 将指标功能和包括 Prometheus 的其他指标收集系统进行了融合。\n什么是 Apache SkyWalking？ SkyWalking 是可观察性平台和 APM 工具，可以选择是否搭载服务网格的使用，为微服务、云原生和容器化应用提供自动度量功能。顶尖的 Apache 项目由来自世界各地的社区人员支持，应用在阿里巴巴、华为、腾讯、百度和大量其他企业。SkyWalking 提供记录、监控和追踪功能，同时也得力于其架构而拥有数据收集终端、分析平台，还有用户界面。\n值得关注的优化包括：  用户界面 Dashboard 上提供百分百的自由度，用户可以任意进行配置，采用后台新定义的指标。 支持 Prometheus 导出格式。Prometheus 格式的指标可以转换至 SkyWalking。 SkyWalking 现已可以自主监控服务网格，为 Istio 和 Envoy 提供指标。 服务、实例、终端地址的注册机制，和库存存储实体已经被移除了。  无须修改原始码的前提下，为用户界面加入新的指标 对于 SkyWalking 的用户，8.0 版本的亮点将会是数据模型的更新，而且传播格式也针对更多语言进行优化。再加上引进了新的 MeterSystem ，除了可以同步运行传统追踪模式，用户还可自定义需要收集的指标。追踪和服务网格专注在拓扑和服务流量的指标上，而 MeterSystem 则汇报用户感兴趣的业务指标，例如是数据库存取性能、圣诞节期间的下单率，或者用户注册或下单的百分比。这些指标数据会在 SkyWalking 的用户界面 Dashboard 上以图像显示。指标的面板数据和拓扑图可以通过 Envoy 的指标绘制，而追踪分析也可以支持 Istio 的遥测。Dashboard 还支持以 JSON 格式导入、导出，而 Dashboard 上的自定义指标也支持设定指标名称、实体种类（服务、实例、终端地址或全部）、标记值等。用户界面模板上已详细描述了用户界面的逻辑和原型配置，以及它的 Dashboard、tab 和组件。\n观察任何配备了 Prometheus 的应用 在这次最新的社区发布中，SkyWalking 可以观察任何配备了 Prometheus 或者提供了 Prometheus 终端地址的应用。这项更新为很多想采用 SkyWalking 指标和追踪的用户节省了不少时间，现在你不再需要重新设置指标工具，就可以获得 Prometheus 数据。因为 Prometheus 更简单、更为人熟悉，是不少用户的不二选择。有了 8.0 版本，Prometheus 网络协议就能够读取所有已设定在 API 上的数据，另外 Prometheus 格式的指标也可转换至 SkyWalking 上。如此一来，通过图像方式展示，所有的指标和拓扑都能一目了然。同时，也支持 Prometheus 的 fetcher。\n监控你的网格 SkyWalking 现在不再只是监控服务或平台，而是监控整个网格。有了 8.0 版本，你除了能获取关于你的网格的指标（包括 Istio 和 Envoy 在内），同时也能通过 SkyWalking 监控自身的性能。因为当监控服务在观察业务集群的同时，它也能实现自我观察，确保运维团队拥有稳定可靠的平台。\n性能优化 最后，8.0 发布移除了注册机制，也不再需要使用独一无二的整数来代表实体。这项改变将大幅优化性能。想了解完整的更新功能列表，可以阅读在 SkyWalking 社区发布的公告页面。\n额外资源  追踪 Twitter 获取更多 SkyWalking 最新资讯 SkyWalking 未来的发布会加入原生指标 API 和融合 Micrometer (Sleuth) 指标集合。  ","excerpt":"可观察性平台和开源应用程序性能监控（APM）项目 Apache SkyWalking，今天刚宣布 8.0 的发布版本。素以强劲指标、追踪与服务网格能力见称的 SkyWalking ，在最新版本中的功能 …","ref":"/zh/whats-new-in-skywalking-metersystem-and-mesh-monitoring-in-8-0/","title":"SkyWalking 的最新动向？8.0 版本的 MeterSystem 和网格监控"},{"body":"作者：宋净超、张伟\n日前，云原生网络代理 MOSN v0.12.0 发布，观察性分析平台和应用性能管理系统 SkyWalking 完成了与 MOSN 的集成，作为 MOSN 中的支持的分布式追踪系统之一，旨在实现在微服务和 Service Mesh 中的更强大的可观察性。\n背景 相比传统的巨石（Monolith）应用，微服务的一个主要变化是将应用中的不同模块拆分为了独立的进程。在微服务架构下，原来进程内的方法调用成为了跨进程的远程方法调用。相对于单一进程内的方法调用而言，跨进程调用的调试和故障分析是非常困难的，难以使用传统的代码调试程序或者日志打印来对分布式的调用过程进行查看和分析。\n如上图右边所示，微服务架构中系统中各个微服务之间存在复杂的调用关系。\n一个来自客户端的请求在其业务处理过程中经过了多个微服务进程。我们如果想要对该请求的端到端调用过程进行完整的分析，则必须将该请求经过的所有进程的相关信息都收集起来并关联在一起，这就是“分布式追踪”。\n以上关于分布式追踪的介绍引用自 Istio Handbook。\nMOSN 中 tracing 的架构 MOSN 的 tracing 框架由 Driver、Tracer 和 Span 三个部分组成。\nDriver 是 Tracer 的容器，管理注册的 Tracer 实例，Tracer 是 tracing 的入口，根据请求信息创建一个 Span，Span 存储当前跨度的链路信息。\n目前 MOSN tracing 有 SOFATracer 和 SkyWalking 两种实现。SOFATracer 支持 http1 和 xprotocol 协议的链路追踪，将 trace 数据写入本地日志文件中。SkyWalking 支持 http1 协议的链路追踪，使用原生的 Go 语言探针 go2sky 将 trace 数据通过 gRPC 上报到 SkyWalking 后端服务。\n快速开始 下面将使用 Docker 和 docker-compose 来快速开始运行一个集成了 SkyWalking 的分布式追踪示例，该示例代码请见 MOSN GitHub。\n准备 安装 docker 和 docker-compose。\n  安装 docker\n  安装 docker-compose\n  需要一个编译好的 MOSN 程序，您可以下载 MOSN 源码自行编译，或者直接下载 MOSN v0.12.0 发行版以获取 MOSN 的运行时二进制文件。\n下面将以源码编译的方式演示 MOSN 如何与 SkyWalking 集成。\ncd ${projectpath}/cmd/mosn/main go build 获取示例代码目录。\n${targetpath} = ${projectpath}/examples/codes/trace/skywalking/http/ 将编译好的程序移动到示例代码目录。\nmv main ${targetpath}/ cd ${targetpath} 目录结构 下面是 SkyWalking 的目录结构。\n* skywalking └─── http │ main # 编译完成的 MOSN 程序 | server.go # 模拟的 Http Server | clint.go # 模拟的 Http Client | config.json # MOSN 配置 | skywalking-docker-compose.yaml # skywalking docker-compose 运行说明 启动 SkyWalking oap \u0026amp; ui。\ndocker-compose -f skywalking-docker-compose.yaml up -d 启动一个 HTTP Server。\ngo run server.go 启动 MOSN。\n./main start -c config.json 启动一个 HTTP Client。\ngo run client.go 打开 http://127.0.0.1:8080 查看 SkyWalking-UI，SkyWalking Dashboard 界面如下图所示。\n在打开 Dashboard 后请点击右上角的 Auto 按钮以使页面自动刷新。\nDemo 视频 下面来看一下该 Demo 的操作视频。\n\n清理 要想销毁 SkyWalking 后台运行的 docker 容器只需要下面的命令。\ncd ${projectpath}/examples/codes/trace/skywalking/http/ docker-compose -f skywalking-docker-compose.yaml down 未来计划 在今年五月份，SkyWalking 8.0 版本会进行一次全面升级，采用新的探针协议和分析逻辑，探针将更具互感知能力，更好的在 Service Mesh 下使用探针进行监控。同时，SkyWalking 将开放之前仅存在于内核中的 metrics 指标分析体系。Prmoetheus、Spring Cloud Sleuth、Zabbix 等常用的 metrics 监控方式，都会被统一的接入进来，进行分析。此外， SkyWalking 与 MOSN 社区将继续合作：支持追踪 Dubbo 和 SOFARPC，同时适配 sidecar 模式下的链路追踪。\n关于 MOSN MOSN 是一款使用 Go 语言开发的网络代理软件，由蚂蚁金服开源并经过几十万容器的生产级验证。 MOSN 作为云原生的网络数据平面，旨在为服务提供多协议、模块化、智能化、安全的代理能力。 MOSN 是 Modular Open Smart Network 的简称。 MOSN 可以与任何支持 xDS API 的 Service Mesh 集成，亦可以作为独立的四、七层负载均衡，API Gateway、云原生 Ingress 等使用。\n GitHub：https://github.com/mosn/mosn 官网：https://mosn.io  关于 Skywalking SkyWalking 是观察性分析平台和应用性能管理系统。提供分布式追踪、服务网格遥测分析、度量聚合和可视化一体化解决方案。支持 Java、.Net Core、PHP、NodeJS、Golang、LUA 语言探针，支持 Envoy/MOSN + Istio 构建的 Service Mesh。\n GitHub：https://github.com/apache/skywalking 官网：https://skywalking.apache.org  关于本文中的示例请参考 MOSN GitHub 和 MOSN 官方文档。\n","excerpt":"作者：宋净超、张伟\n日前，云原生网络代理 MOSN v0.12.0 发布，观察性分析平台和应用性能管理系统 SkyWalking 完成了与 MOSN 的集成，作为 MOSN 中的支持的分布式追踪系统之 …","ref":"/zh/2020-04-28-skywalking-and-mosn/","title":"SkyWalking 支持云原生网络代理 MOSN 做分布式追踪"},{"body":"Based on his continuous contributions, Wei Zhang (a.k.a arugal) has been invited to join the PMC. Welcome aboard.\n","excerpt":"Based on his continuous contributions, Wei Zhang (a.k.a arugal) has been invited to join the PMC. …","ref":"/events/welcome-wei-zhang-to-join-the-pmc/","title":"Welcome Wei Zhang to join the PMC"},{"body":"目录：\n 1. 概述 2. 搭建 SkyWalking 单机环境 3. 搭建 SkyWalking 集群环境 4. 告警 5. 注意事项 6. Spring Boot 使用示例 6. Spring Cloud 使用示例    作者：芋道源码 原文地址   1. 概述 1.1 概念 SkyWalking 是什么？\n FROM http://skywalking.apache.org/\n分布式系统的应用程序性能监视工具，专为微服务、云原生架构和基于容器（Docker、K8s、Mesos）架构而设计。\n提供分布式追踪、服务网格遥测分析、度量聚合和可视化一体化解决方案。\n 1.2 功能列表 SkyWalking 有哪些功能？\n FROM http://skywalking.apache.org/\n 多种监控手段。可以通过语言探针和 service mesh 获得监控是数据。 多个语言自动探针。包括 Java，.NET Core 和 Node.JS。 轻量高效。无需大数据平台，和大量的服务器资源。 模块化。UI、存储、集群管理都有多种机制可选。 支持告警。 优秀的可视化解决方案。   1.3 整体架构 SkyWalking 整体架构如何？\n FROM http://skywalking.apache.org/\n 整个架构，分成上、下、左、右四部分：\n 考虑到让描述更简单，我们舍弃掉 Metric 指标相关，而着重在 Tracing 链路相关功能。\n  上部分 Agent ：负责从应用中，收集链路信息，发送给 SkyWalking OAP 服务器。目前支持 SkyWalking、Zikpin、Jaeger 等提供的 Tracing 数据信息。而我们目前采用的是，SkyWalking Agent 收集 SkyWalking Tracing 数据，传递给服务器。 下部分 SkyWalking OAP ：负责接收 Agent 发送的 Tracing 数据信息，然后进行分析(Analysis Core) ，存储到外部存储器( Storage )，最终提供查询( Query )功能。 右部分 Storage ：Tracing 数据存储。目前支持 ES、MySQL、Sharding Sphere、TiDB、H2 多种存储器。而我们目前采用的是 ES ，主要考虑是 SkyWalking 开发团队自己的生产环境采用 ES 为主。 左部分 SkyWalking UI ：负责提供控台，查看链路等等。  1.4 官方文档 在 https://github.com/apache/skywalking/tree/master/docs 地址下，提供了 SkyWalking 的英文文档。\n考虑到大多数胖友的英语水平和艿艿不相伯仲，再加上胖友一开始对 SkyWalking 比较陌生，所以比较推荐先阅读 https://github.com/SkyAPM/document-cn-translation-of-skywalking 地址，提供了 SkyWalking 的中文文档。\n考虑到胖友使用 SkyWalking 的目的，是实现分布式链路追踪的功能，所以最好去了解下相关的知识。这里推荐阅读两篇文章：\n 《OpenTracing 官方标准 —— 中文版》 Google 论文 《Dapper，大规模分布式系统的跟踪系统》  2. 搭建 SkyWalking 单机环境 考虑到让胖友更快的入门，我们来搭建一个 SkyWalking 单机环境，步骤如下：\n 第一步，搭建一个 Elasticsearch 服务。 第二步，下载 SkyWalking 软件包。 第三步，搭建一个 SkyWalking OAP 服务。 第四步，启动一个 Spring Boot 应用，并配置 SkyWalking Agent。 第五步，搭建一个 SkyWalking UI 服务。  仅仅五步，按照艿艿标题党的性格，应该给本文取个《10 分钟快速搭建 SkyWalking 服务》标题才对，哈哈哈。\n2.1 Elasticsearch 搭建  FROM https://www.elastic.co/cn/products/elasticsearch\nElasticsearch 是一个分布式、RESTful 风格的搜索和数据分析引擎，能够解决不断涌现出的各种用例。 作为 Elastic Stack 的核心，它集中存储您的数据，帮助您发现意料之中以及意料之外的情况。\n 参考《Elasticsearch 极简入门》的「1. 单机部署」小节，搭建一个 Elasticsearch 单机服务。\n不过要注意，本文使用的是 Elasticsearch 7.5.1 版本。因为 SkyWalking 6.6.0 版本，增加了对 Elasticsearch 7.X 版本的支持。当然，如果胖友使用 Elasticsearch 6.X 版本也是可以的。\n2.2 下载 SkyWalking 软件包 对于 SkyWalking 的软件包，有两种方式获取：\n 手动编译 官方包  一般情况下，我们建议使用官方包。手动编译，更多是尝鲜或者等着急修复的 BUG 的版本。\n2.2.1 官方包 在 http://skywalking.apache.org/downloads/ 下，我们下载操作系统对应的发布版。\n这里，我们选择 Binary Distribution for ElasticSearch 7 (Linux) 版本，因为艿艿是 Mac 环境，再加上想使用 Elasticsearch 7.X 版本作为存储。如果胖友想用 Elasticsearch 6.X 版本作为存储，记得下载 Binary Distribution (Linux) 版本。\n① 下载：\n# 创建目录 $ mkdir -p /Users/yunai/skywalking $ cd /Users/yunai/skywalking # 下载 $ wget http://mirror.bit.edu.cn/apache/skywalking/6.6.0/apache-skywalking-apm-es7-6.6.0.tar.gz ② 解压：\n# 解压 $ tar -zxvf apache-skywalking-apm-es7-6.6.0.tar.gz $ cd apache-skywalking-apm-bin-es7 $ ls -ls 4 drwxr-xr-x 8 root root 4096 Sep 9 15:09 agent # SkyWalking Agent 4 drwxr-xr-x 2 root root 4096 Sep 9 15:44 bin # 执行脚本 4 drwxr-xr-x 2 root root 4096 Sep 9 15:44 config # SkyWalking OAP Server 配置文件 32 -rwxr-xr-x 1 root root 28903 Sep 9 14:32 LICENSE 4 drwxr-xr-x 3 root root 4096 Sep 9 15:44 licenses 32 -rwxr-xr-x 1 root root 31850 Sep 9 14:32 NOTICE 16 drwxr-xr-x 2 root root 16384 Sep 9 15:22 oap-libs # SkyWalking OAP Server 4 -rw-r--r-- 1 root root 1978 Sep 9 14:32 README.txt 4 drwxr-xr-x 2 root root 4096 Sep 9 15:44 webapp # SkyWalking UI 2.2.2 手动编译  友情提示：如果胖友没有编译 SkyWalking 源码的诉求，可以跳过本小节。\n 参考 How to build project 文章。\n需要前置安装如下：\n GIT JDK 8+ Maven  ① 克隆代码：\n$ git clone https://github.com/apache/skywalking.git  因为网络问题，可能克隆会有点久。  ② 初始化子模块：\n$ cd skywalking $ git submodule init $ git submodule update ③ 编译\n$ ./mvnw clean package -DskipTests  编译过程，如果机子比较差，花费时间会比较久。  ④ 查看编译结果\n$ cd apm-dist # 编译结果目录 $ cd target $ tar -zxvf apache-skywalking-apm-bin.tar.gz # 解压 Linux 包 $ cd apache-skywalking-apm-bin $ ls -ls 4 drwxr-xr-x 8 root root 4096 Sep 9 15:09 agent # SkyWalking Agent 4 drwxr-xr-x 2 root root 4096 Sep 9 15:44 bin # 执行脚本 4 drwxr-xr-x 2 root root 4096 Sep 9 15:44 config # SkyWalking OAP Server 配置文件 32 -rwxr-xr-x 1 root root 28903 Sep 9 14:32 LICENSE 4 drwxr-xr-x 3 root root 4096 Sep 9 15:44 licenses 32 -rwxr-xr-x 1 root root 31850 Sep 9 14:32 NOTICE 16 drwxr-xr-x 2 root root 16384 Sep 9 15:22 oap-libs # SkyWalking OAP Server 4 -rw-r--r-- 1 root root 1978 Sep 9 14:32 README.txt 4 drwxr-xr-x 2 root root 4096 Sep 9 15:44 webapp # SkyWalking UI 2.3 SkyWalking OAP 搭建 ① 修改 OAP 配置文件\n 友情提示：如果配置文件，适合 SkyWalking 6.X 版本。\n $ vi config/application.yml storage: elasticsearch7: nameSpace: ${SW_NAMESPACE:\u0026#34;elasticsearch\u0026#34;} clusterNodes: ${SW_STORAGE_ES_CLUSTER_NODES:localhost:9200} protocol: ${SW_STORAGE_ES_HTTP_PROTOCOL:\u0026#34;http\u0026#34;} # trustStorePath: ${SW_SW_STORAGE_ES_SSL_JKS_PATH:\u0026#34;../es_keystore.jks\u0026#34;} # trustStorePass: ${SW_SW_STORAGE_ES_SSL_JKS_PASS:\u0026#34;\u0026#34;} user: ${SW_ES_USER:\u0026#34;\u0026#34;} password: ${SW_ES_PASSWORD:\u0026#34;\u0026#34;} indexShardsNumber: ${SW_STORAGE_ES_INDEX_SHARDS_NUMBER:2} indexReplicasNumber: ${SW_STORAGE_ES_INDEX_REPLICAS_NUMBER:0} # Those data TTL settings will override the same settings in core module. recordDataTTL: ${SW_STORAGE_ES_RECORD_DATA_TTL:7} # Unit is day otherMetricsDataTTL: ${SW_STORAGE_ES_OTHER_METRIC_DATA_TTL:45} # Unit is day monthMetricsDataTTL: ${SW_STORAGE_ES_MONTH_METRIC_DATA_TTL:18} # Unit is month # Batch process setting, refer to https://www.elastic.co/guide/en/elasticsearch/client/java-api/5.5/java-docs-bulk-processor.html bulkActions: ${SW_STORAGE_ES_BULK_ACTIONS:1000} # Execute the bulk every 1000 requests flushInterval: ${SW_STORAGE_ES_FLUSH_INTERVAL:10} # flush the bulk every 10 seconds whatever the number of requests concurrentRequests: ${SW_STORAGE_ES_CONCURRENT_REQUESTS:2} # the number of concurrent requests resultWindowMaxSize: ${SW_STORAGE_ES_QUERY_MAX_WINDOW_SIZE:10000} metadataQueryMaxSize: ${SW_STORAGE_ES_QUERY_MAX_SIZE:5000} segmentQueryMaxSize: ${SW_STORAGE_ES_QUERY_SEGMENT_SIZE:200} # h2: # driver: ${SW_STORAGE_H2_DRIVER:org.h2.jdbcx.JdbcDataSource} # url: ${SW_STORAGE_H2_URL:jdbc:h2:mem:skywalking-oap-db} # user: ${SW_STORAGE_H2_USER:sa} # metadataQueryMaxSize: ${SW_STORAGE_H2_QUERY_MAX_SIZE:5000}  storage.elasticsearch7 配置项，设置使用 Elasticsearch 7.X 版本作为存储器。  这里，我们打开注释，并记得通过 nameSpace 设置 Elasticsearch 集群名。   storage.elasticsearch 配置项，设置使用 Elasticsearch 6.X 版本作为存储器。  这里，我们无需做任何改动。 如果胖友使用 Elasticsearch 6.X 版本作为存储器，记得设置这个配置项，而不是 storage.elasticsearch7 配置项。   storage.h2 配置项，设置使用 H2 作为存储器。  这里，我们需要手动注释掉，因为 H2 是默认配置的存储器。     友情提示：如果配置文件，适合 SkyWalking 7.X 版本。\n  重点修改 storage 配置项，通过 storage.selector 配置项来设置具体使用的存储器。 storage.elasticsearch 配置项，设置使用 Elasticsearch 6.X 版本作为存储器。胖友可以主要修改 nameSpace、clusterNodes 两个配置项即可，设置使用的 Elasticsearch 的集群和命名空间。 storage.elasticsearch7 配置项，设置使用 Elasticsearch 7.X 版本作为存储器。 还有 MySQL、H2、InfluxDB 等等存储器的配置可以选择，胖友自己根据需要去选择哈~  ② 启动 SkyWalking OAP 服务\n$ bin/oapService.sh SkyWalking OAP started successfully! 是否真正启动成功，胖友打开 logs/skywalking-oap-server.log 日志文件，查看是否有错误日志。首次启动时，因为 SkyWalking OAP 会创建 Elasticsearch 的索引，所以会“疯狂”的打印日志。最终，我们看到如下日志，基本可以代表 SkyWalking OAP 服务启动成功：\n 友情提示：因为首次启动会创建 Elasticsearch 索引，所以可能会比较慢。\n 2020-01-02 18:22:53,635 - org.eclipse.jetty.server.Server - 444 [main] INFO [] - Started @35249ms 2.4 SkyWalking UI 搭建 ① 启动 SkyWalking UI 服务\nbin/webappService.sh SkyWalking Web Application started successfully! 是否真正启动成功，胖友打开 logs/logs/webapp.log 日志文件，查看是否有错误日志。最终，我们看到如下日志，基本可以代表 SkyWalking UI 服务启动成功：\n2020-01-02 18:27:02.824 INFO 48250 --- [main] o.a.s.apm.webapp.ApplicationStartUp : Started ApplicationStartUp in 7.774 seconds (JVM running for 8.316) 如果想要修改 SkyWalking UI 服务的参数，可以编辑 webapp/webapp.yml 配置文件。例如说：\n server.port ：SkyWalking UI 服务端口。 collector.ribbon.listOfServers ：SkyWalking OAP 服务地址数组。因为 SkyWalking UI 界面的数据，是通过请求 SkyWalking OAP 服务来获得的。  ② 访问 UI 界面：\n浏览器打开 http://127.0.0.1:8080 。界面如下图：2.5 SkyWalking Agent 大多数情况下，我们在启动项目的 Shell 脚本上，通过 -javaagent 参数进行配置 SkyWalking Agent 。我们在 「2.3.1 Shell」 小节来看。\n考虑到偶尔我们需要在 IDE 中，也希望使用 SkyWalking Agent ，所以我们在 「2.3.2 IDEA」 小节来看。\n2.3.1 Shell ① Agent 软件包\n我们需要将 apache-skywalking-apm-bin/agent 目录，拷贝到 Java 应用所在的服务器上。这样，Java 应用才可以配置使用该 SkyWalking Agent。我们来看看 Agent 目录下有哪些：\n$ ls -ls total 35176 0 drwxr-xr-x@ 7 yunai staff 224 Dec 24 14:20 activations 0 drwxr-xr-x@ 4 yunai staff 128 Dec 24 14:21 bootstrap-plugins 0 drwxr-xr-x@ 3 yunai staff 96 Dec 24 14:12 config # SkyWalking Agent 配置 0 drwxr-xr-x@ 3 yunai staff 96 Jan 2 19:29 logs # SkyWalking Agent 日志 0 drwxr-xr-x@ 13 yunai staff 416 Dec 24 14:22 optional-plugins # 可选插件 0 drwxr-xr-x@ 68 yunai staff 2176 Dec 24 14:20 plugins # 插件 35176 -rw-r--r--@ 1 yunai staff 18006420 Dec 24 14:12 skywalking-agent.jar # SkyWalking Agent  关于 SkyWalking Agent 提供的插件列表，可以看看《SkyWalking 文档 —— 插件支持列表》。  因为艿艿是在本机测试，所以无需拷贝，SkyWalking Agent 目录是 /Users/yunai/skywalking/apache-skywalking-apm-bin-es7/agent/。\n考虑到方便胖友，艿艿这里提供了一个最简的 Spring Boot 应用 lab-39-demo-2.2.2.RELEASE.jar。对应 Github 仓库是 lab-39-demo。\n② 配置 Java 启动脚本\n# SkyWalking Agent 配置 export SW_AGENT_NAME=demo-application # 配置 Agent 名字。一般来说，我们直接使用 Spring Boot 项目的 `spring.application.name` 。 export SW_AGENT_COLLECTOR_BACKEND_SERVICES=127.0.0.1:11800 # 配置 Collector 地址。 export SW_AGENT_SPAN_LIMIT=2000 # 配置链路的最大 Span 数量。一般情况下，不需要配置，默认为 300 。主要考虑，有些新上 SkyWalking Agent 的项目，代码可能比较糟糕。 export JAVA_AGENT=-javaagent:/Users/yunai/skywalking/apache-skywalking-apm-bin-es7/agent/skywalking-agent.jar # SkyWalking Agent jar 地址。 # Jar 启动 java -jar $JAVA_AGENT -jar lab-39-demo-2.2.2.RELEASE.jar  通过环境变量，进行配置。 更多的变量，可以在 /work/programs/skywalking/apache-skywalking-apm-bin/agent/config/agent.config 查看。要注意，可能有些变量是被注释掉的，例如说 SW_AGENT_SPAN_LIMIT 对应的 agent.span_limit_per_segment 。  ③ 执行脚本：\n直接执行上述的 Shell 脚本，启动 Java 项目。在启动日志中，我们可以看到 SkyWalking Agent 被加载的日志。日志示例如下：\nDEBUG 2020-01-02 19:29:29:400 main AgentPackagePath : The beacon class location is jar:file:/Users/yunai/skywalking/apache-skywalking-apm-bin-es7/agent/skywalking-agent.jar!/org/apache/skywalking/apm/agent/core/boot/AgentPackagePath.class. INFO 2020-01-02 19:29:29:402 main SnifferConfigInitializer : Config file found in /Users/yunai/skywalking/apache-skywalking-apm-bin-es7/agent/config/agent.config. 同时，也可以在 /Users/yunai/skywalking/apache-skywalking-apm-bin-es7/agent/agent/logs/skywalking-api.log 查看对应的 SkyWalking Agent 日志。日志示例如下：\nDEBUG 2020-01-02 19:37:22:539 SkywalkingAgent-5-ServiceAndEndpointRegisterClient-0 ServiceAndEndpointRegisterClient : ServiceAndEndpointRegisterClient running, status:CONNECTED.  这里，我们看到 status:CONNECTED ，表示 SkyWalking Agent 连接 SkyWalking OAP 服务成功。  ④ 简单测试\n完事，可以去 SkyWalking UI 查看是否链路收集成功。\n1、首先，使用浏览器，访问下 http://127.0.0.1:8079/demo/echo 地址，请求下 Spring Boot 应用提供的 API。因为，我们要追踪下该链路。\n2、然后，继续使用浏览器，打开 http://127.0.0.1:8080/ 地址，进入 SkyWalking UI 界面。如下图所示：这里，我们会看到 SkyWalking 中非常重要的三个概念：\n  服务(Service) ：表示对请求提供相同行为的一系列或一组工作负载。在使用 Agent 或 SDK 的时候，你可以定义服务的名字。如果不定义的话，SkyWalking 将会使用你在平台（例如说 Istio）上定义的名字。\n 这里，我们可以看到 Spring Boot 应用的服务为 \u0026quot;demo-application\u0026quot;，就是我们在环境变量 SW_AGENT_NAME 中所定义的。\n   服务实例(Service Instance) ：上述的一组工作负载中的每一个工作负载称为一个实例。就像 Kubernetes 中的 pods 一样, 服务实例未必就是操作系统上的一个进程。但当你在使用 Agent 的时候, 一个服务实例实际就是操作系统上的一个真实进程。\n 这里，我们可以看到 Spring Boot 应用的服务为 {agent_name}-pid:{pid}@{hostname}，由 Agent 自动生成。关于它，我们在「5.1 hostname」小节中，有进一步的讲解，胖友可以瞅瞅。\n   端点(Endpoint) ：对于特定服务所接收的请求路径, 如 HTTP 的 URI 路径和 gRPC 服务的类名 + 方法签名。\n 这里，我们可以看到 Spring Boot 应用的一个端点，为 API 接口 /demo/echo。\n   3、之后，点击「拓扑图」菜单，进入查看拓扑图的界面。如下图所示：4、再之后，点击「追踪」菜单，进入查看链路数据的界面。如下图所示：2.3.2 IDEA 我们统一使用 IDEA 作为开发 IDE ，所以忽略 Eclipse 的配置方式。\n具体参考下图，比较简单：3. 搭建 SkyWalking 集群环境 在生产环境下，我们一般推荐搭建 SkyWalking 集群环境。😈 当然，如果公司比较抠门，也可以在生产环境下使用 SkyWalking 单机环境，毕竟 SkyWalking 挂了之后，不影响业务的正常运行。\n搭建一个 SkyWalking 集群环境，步骤如下：\n 第一步，搭建一个 Elasticsearch 服务的集群。 第二步，搭建一个注册中心的集群。目前 SkyWalking 支持 Zookeeper、Kubernetes、Consul、Nacos 作为注册中心。 第三步，搭建一个 SkyWalking OAP 服务的集群，同时参考《SkyWalking 文档 —— 集群管理》，将 SkyWalking OAP 服务注册到注册中心上。 第四步，启动一个 Spring Boot 应用，并配置 SkyWalking Agent。另外，在设置 SkyWaling Agent 的 SW_AGENT_COLLECTOR_BACKEND_SERVICES 地址时，需要设置多个 SkyWalking OAP 服务的地址数组。 第五步，搭建一个 SkyWalking UI 服务的集群，同时使用 Nginx 进行负载均衡。另外，在设置 SkyWalking UI 的 collector.ribbon.listOfServers 地址时，也需要设置多个 SkyWalking OAP 服务的地址数组。  😈 具体的搭建过程，并不复杂，胖友自己去尝试下。\n4. 告警 在 SkyWaling 中，已经提供了告警功能，具体可见《SkyWalking 文档 —— 告警》。\n默认情况下，SkyWalking 已经内置告警规则。同时，我们可以参考告警规则，进行自定义。\n在满足 SkyWalking 告警规则的触发规则时，我们在 SkyWaling UI 的告警界面，可以看到告警内容。如下图所示：同时，我们自定义 Webhook ，对接 SkyWalking 的告警请求。而具体的邮箱、钉钉等告警方式，需要自己进行开发。至于自定义 WebHook 如何实现，可以参考：\n Java 语言：  《基于 SkyWalking 的分布式跟踪系统 - 异常告警》   Go 语言：  dingding-notify-for-skywalking infra-skywalking-webhook    5. 注意事项 5.1 hostname 配置 在 SkyWalking 中，每个被监控的实例的名字，会包含 hostname 。格式为：{agent_name}-pid:{pid}@{hostname} ，例如说：\u0026quot;scrm-scheduler-pid:27629@iZbp1e2xlyvr7fh67qi59oZ\u0026quot; 。\n因为有些服务器未正确设置 hostname ，所以我们一定要去修改，不然都不知道是哪个服务器上的实例（😈 鬼知道 \u0026quot;iZbp1e2xlyvr7fh67qi59oZ\u0026quot; 一串是哪个服务器啊）。\n修改方式如下：\n1、修改 /etc/hosts 的 hostname ：\n127.0.0.1 localhost ::1 localhost localhost.localdomain localhost6 localhost6.localdomain6 10.80.62.151 pre-app-01 # 就是这个，其中 10.80.62.151 是本机内网 IP ，pre-app-01 是 hostname 。 2、修改本机 hostname ：\n参考 《CentOS7 修改主机名（hostname）》\n$ hostname pre-app-01 # 其中 pre-app-01 就是你希望的 hostname 。 $ hostnamectl set-hostname pre-app-01 # 其中 pre-app-01 就是你希望的 hostname 。 6. Spring Boot 使用示例 在 《芋道 Spring Boot 链路追踪 SkyWalking 入门》 中，我们来详细学习如何在 Spring Boot 中，整合并使用 SkyWalking 收集链路数据。😈 相比「2.5 SkyWaling Agent」来说，我们会提供更加丰富的示例哟。\n7. Spring Cloud 使用示例 在 《芋道 Spring Cloud 链路追踪 SkyWalking 入门》 中，我们来详细学习如何在 Spring Cloud 中，整合并使用 SkyWalking 收集链路数据。😈 相比「2.5 SkyWaling Agent」来说，我们会提供更加丰富的示例哟。\n666. 彩蛋 本文仅仅是简单的 SkyWalking 入门文章，如果胖友想要更好的使用 SkyWalking，推荐通读下《SkyWalking 文档》。\n想要进一步深入的胖友，也可以阅读如下资料：\n 《SkyWalking 源码解析》 《APM 巅峰对决：Apache Skywalking P.K. Pinpoint》 《SkyWalking 官方 —— 博客合集》  😈 最后弱弱的问一句，上完 SkyWaling 之后，有没发现自己系统各种地方慢慢慢！嘻嘻。\n","excerpt":"目录：\n 1. 概述 2. 搭建 SkyWalking 单机环境 3. 搭建 SkyWalking 集群环境 4. 告警 5. 注意事项 6. Spring Boot 使用示例 6. Spring …","ref":"/zh/2020-04-19-skywalking-quick-start/","title":"SkyWalking 极简入门"},{"body":"This post originally appears on The New Stack\nThis post introduces a way to automatically profile code in production with Apache SkyWalking. We believe the profile method helps reduce maintenance and overhead while increasing the precision in root cause analysis.\nLimitations of the Distributed Tracing In the early days, metrics and logging systems were the key solutions in monitoring platforms. With the adoption of microservice and distributed system-based architecture, distributed tracing has become more important. Distributed tracing provides relevant service context, such as system topology map and RPC parent-child relationships.\nSome claim that distributed tracing is the best way to discover the cause of performance issues in a distributed system. It’s good at finding issues at the RPC abstraction, or in the scope of components instrumented with spans. However, it isn’t that perfect.\nHave you been surprised to find a span duration longer than expected, but no insight into why? What should you do next? Some may think that the next step is to add more instrumentation, more spans into the trace, thinking that you would eventually find the root cause, with more data points. We’ll argue this is not a good option within a production environment. Here’s why:\n There is a risk of application overhead and system overload. Ad-hoc spans measure the performance of specific scopes or methods, but picking the right place can be difficult. To identify the precise cause, you can “instrument” (add spans to) many suspicious places. The additional instrumentation costs more CPU and memory in the production environment. Next, ad-hoc instrumentation that didn’t help is often forgotten, not deleted. This creates a valueless overhead load. In the worst case, excess instrumentation can cause performance problems in the production app or overload the tracing system. The process of ad-hoc (manual) instrumentation usually implies at least a restart. Trace instrumentation libraries, like Zipkin Brave, are integrated into many framework libraries. To instrument a method’s performance typically implies changing code, even if only an annotation. This implies a re-deploy. Even if you have the way to do auto instrumentation, like Apache SkyWalking, you still need to change the configuration and reboot the app. Otherwise, you take the risk of GC caused by hot dynamic instrumentation. Injecting instrumentation into an uninstrumented third party library is hard and complex. It takes more time and many won’t know how to do this. Usually, we don’t have code line numbers in the distributed tracing. Particularly when lambdas are in use, it can be difficult to identify the line of code associated with a span. Regardless of the above choices, to dive deeper requires collaboration with your Ops or SRE team, and a shared deep level of knowledge in distributed tracing.  Regardless of the above choices, to dive deeper requires collaboration with your Ops or SRE team, and a shared deep level of knowledge in distributed tracing.\nProfiling in Production Introduction To reuse distributed tracing to achieve method scope precision requires an understanding of the above limitations and a different approach. We called it PROFILE.\nMost high-level languages build and run on a thread concept. The profile approach takes continuous thread dumps. We merge the thread dumps to estimate the execution time of every method shown in the thread dumps. The key for distributed tracing is the tracing context, identifiers active (or current) for the profiled method. Using this trace context, we can weave data harvested from profiling into existing traces. This allows the system to automate otherwise ad-hoc instrumentation. Let’s dig deeper into how profiling works:\nWe consider a method invocation with the same stack depth and signature (method, line number etc), the same operation. We derive span timestamps from the thread dumps the same operation is in. Let’s put this visually:\nAbove, represents 10 successive thread dumps. If this method is in dumps 4-8, we assume it started before dump 4 and finished after dump 8. We can’t tell exactly when the method started and stopped. but the timestamps of thread dumps are close enough.\nTo reduce overhead caused by thread dumps, we only profile methods enclosed by a specific entry point, such as a URI or MVC Controller method. We identify these entry points through the trace context and the APM system.\nThe profile does thread dump analysis and gives us:\n The root cause, precise to the line number in the code. Reduced maintenance as ad-hoc instrumentation is obviated. Reduced overload risk caused by ad-hoc instrumentation. Dynamic activation: only when necessary and with a very clear profile target.  Implementing Precise Profiling with Apache SkyWalking 7 Distributed profiling is built-into Apache SkyWalking application performance monitoring (APM). Let’s demonstrate how the profiling approach locates the root cause of the performance issue.\nfinal CountDownLatchcountDownLatch= new CountDownLatch(2); threadPool.submit(new Task1(countDownLatch)); threadPool.submit(new Task2(countDownLatch)); try { countDownLatch.await(500, TimeUnit.MILLISECONDS); } catch (InterruptedExceptione) { } Task1 and Task2 have a race condition and unstable execution time: they will impact the performance of each other and anything calling them. While this code looks suspicious, it is representative of real life. People in the OPS/SRE team are not usually aware of all code changes and who did them. They only know something in the new code is causing a problem.\nTo make matters interesting, the above code is not always slow: it only happens when the condition is locked. In SkyWalking APM, we have metrics of endpoint p99/p95 latency, so, we are easy to find out the p99 of this endpoint is far from the avg response time. However, this is not the same as understanding the cause of the latency. To locate the root cause, add a profile condition to this endpoint: duration greater than 500ms. This means faster executions will not add profiling load.\nThis is a typical profiled trace segment (part of the whole distributed trace) shown on the SkyWalking UI. We now notice the “service/processWithThreadPool” span is slow as we expected, but why? This method is the one we added the faulty code to. As the UI shows that method, we know the profiler is working. Now, let’s see what the profile analysis result say.\nThis is the profile analysis stack view. We see the stack element names, duration (include/exclude the children) and slowest methods have been highlighted. It shows clearly, “sun.misc.Unsafe.park” costs the most time. If we look for the caller, it is the code we added: CountDownLatch.await.\nThe Limitations of the Profile Method No diagnostic tool can fit all cases, not even the profile method.\nThe first consideration is mistaking a repeatedly called method for a slow method. Thread dumps are periodic. If there is a loop of calling one method, the profile analysis result would say the target method is slow because it is captured every time in the dump process. There could be another reason. A method called many times can also end up captured in each thread dump. Even so, the profile did what it is designed for. It still helps the OPS/SRE team to locate the code having the issue.\nThe second consideration is overhead, the impact of repeated thread dumps is real and can’t be ignored. In SkyWalking, we set the profile dump period to at least 10ms. This means we can’t locate method performance issues if they complete in less than 10ms. SkyWalking has a threshold to control the maximum parallel degree as well.\nUnderstanding the above keeps distributed tracing and APM systems useful for your OPS/SRE team.\nHow to Try This Everything we discussed, including the Apache SkyWalking Java Agent, profile analysis code, and UI, could be found in our GitHub repository. We hope you enjoyed this new profile method, and love Apache SkyWalking. If so, give us a star on GitHub to encourage us.\nSkyWalking 7 has just been released. You can contact the project team through the following channels:\n Follow SkyWalking twitter. Subscribe mailing list: dev@skywalking.apache.org. Send to dev-subscribe@kywalking.apache.org to subscribe to the mail list.  Co-author Sheng Wu is a Tetrate founding engineer and the founder and VP of Apache SkyWalking. He is solving the problem of observability for large-scale service meshes in hybrid and multi-cloud environments.\nAdrian Cole works in the Spring Cloud team at VMware, mostly on Zipkin\nHan Liu is a tech expert at Lagou. He is an Apache SkyWalking committer\n","excerpt":"This post originally appears on The New Stack\nThis post introduces a way to automatically profile …","ref":"/blog/2020-04-13-apache-skywalking-profiling/","title":"Apache SkyWalking: Use Profiling to Fix the Blind Spot of Distributed Tracing"},{"body":"","excerpt":"","ref":"/tags/profiling/","title":"Profiling"},{"body":"SkyWalking Chart 2.0.0 is released. Go to downloads page to find release tars.\n Support SkyWalking 7.0.0 Support set ES user/password Add CI for release  ","excerpt":"SkyWalking Chart 2.0.0 is released. Go to downloads page to find release tars.\n Support SkyWalking …","ref":"/events/release-apache-skywalking-chart-2-0-0-for-skywalking-7-0-0/","title":"Release Apache SkyWalking Chart 2.0.0 for SkyWalking 7.0.0"},{"body":"SkyWalking APM 7.0.0 is release. Go to downloads page to find release tars.\n Upgrade JDK minimal JDK requirement to JDK8 Support profiling code level performance Don\u0026rsquo;t support SkyWalking v5 agent in-wire and out-wire protocol. V6 is required.  ","excerpt":"SkyWalking APM 7.0.0 is release. Go to downloads page to find release tars.\n Upgrade JDK minimal JDK …","ref":"/events/release-apache-skywalking-apm-7-0-0/","title":"Release Apache SkyWalking APM 7.0.0"},{"body":"","excerpt":"","ref":"/zh_tags/agent/","title":"Agent"},{"body":"","excerpt":"","ref":"/zh_tags/java/","title":"Java"},{"body":"","excerpt":"","ref":"/zh_tags/profiling/","title":"Profiling"},{"body":"","excerpt":"","ref":"/zh_tags/tracing/","title":"Tracing"},{"body":" 作者：吴晟，刘晗 原文地址  在本文中，我们详细介绍了代码级的性能剖析方法，以及我们在 Apache SkyWalking 中的实践。希望能够帮助大家在线定位系统性能短板，缓解系统压力。\n分布式链路追踪的局限性 在传统的监控系统中，我们如果想要得知系统中的业务是否正常，会采用进程监控、日志收集分析等方式来对系统进行监控。当机器或者服务出现问题时，则会触发告警及时通知负责人。通过这种方式，我们可以得知具体哪些服务出现了问题。但是这时我们并不能得知具体的错误原因出在了哪里，开发人员或者运维人员需要到日志系统里面查看错误日志，甚至需要到真实的业务服务器上查看执行情况来解决问题。\n如此一来，仅仅是发现问题的阶段，可能就会耗费相当长的时间；另外，发现问题但是并不能追溯到问题产生具体原因的情况，也常有发生。这样反反复复极其耗费时间和精力，为此我们便有了基于分布式追踪的 APM 系统。\n通过将业务系统接入分布式追踪中，我们就像是给程序增加了一个放大镜功能，可以清晰看到真实业务请求的整体链路，包括请求时间、请求路径，甚至是操作数据库的语句都可以看得一清二楚。通过这种方式，我们结合告警便可以快速追踪到真实用户请求的完整链路信息，并且这些数据信息完全是持久化的，可以随时进行查询，复盘错误的原因。\n然而随着我们对服务监控理解的加深，我们发现事情并没有那么简单。在分布式链路追踪中我们有这样的两个流派：代码埋点和字节码增强。无论使用哪种方式，底层逻辑一定都逃不过面向切面这个基础逻辑。因为只有这样才可以做到大面积的使用。这也就决定了它只能做到框架级别和 RPC 粒度的监控。这时我们可能依旧会遇到程序执行缓慢或者响应时间不稳定等情况，但无法具体查询到原因。这时候，大家很自然的会考虑到增加埋点粒度，比如对所有的 Spring Bean 方法、甚至主要的业务层方法都加上埋点。但是这种思路会遇到不小的挑战：\n第一，增加埋点时系统开销大，埋点覆盖不够全面。通过这种方式我们确实可以做到具体业务场景具体分析。但随着业务不断迭代上线，弊端也很明显：大量的埋点无疑会加大系统资源的开销，造成 CPU、内存使用率增加，更有可能拖慢整个链路的执行效率。虽然每个埋点消耗的性能很小，在微秒级别，但是因为数量的增加，甚至因为业务代码重用造成重复埋点或者循环使用，此时的性能开销已经无法忽略。\n第二，动态埋点作为一项埋点技术，和手动埋点的性能消耗上十分类似，只是减少的代码修改量，但是因为通用技术的特别，上一个挑战中提到的循环埋点和重复使用的场景甚至更为严重。比如选择所有方法或者特定包下的所有方法埋点，很可能造成系统性能彻底崩溃。\n第三，即使我们通过合理设计和埋点，解决了上述问题，但是 JDK 函数是广泛使用的，我们很难限制对 JDK API 的使用场景。对 JDK 过多方法、特别是非 RPC 方法的监控会造成系统的巨大延迟风险。而且有一些基础类型和底层工具类，是很难通过字节码进行增强的。当我们的 SDK 使用不当或者出现 bug 时，我们无法具体得知真实的错误原因。\n代码级性能剖析方法 方法介绍 基于以上问题，在系统性能监控方法上，我们提出了代码级性能剖析这种在线诊断方法。这种方法基于一个高级语言编程模型共性，即使再复杂的系统，再复杂的业务逻辑，都是基于线程去进行执行的，而且多数逻辑是在单个线程状态下执行的。\n代码级性能剖析就是利用方法栈快照，并对方法执行情况进行分析和汇总。并结合有限的分布式追踪 span 上下文，对代码执行速度进行估算。\n性能剖析激活时，会对指定线程周期性的进行线程栈快照，并将所有的快照进行汇总分析，如果两个连续的快照含有同样的方法栈，则说明此栈中的方法大概率在这个时间间隔内都处于执行状态。从而，通过这种连续快照的时间间隔累加成为估算的方法执行时间。时间估算方法如下图所示：\n在上图中，d0-d10 代表 10 次连续的内存栈快照，实际方法执行时间在 d3-d4 区间，结束时间在 d8-d9 之间。性能剖析无法告诉你方法的准确执行时间，但是他会估算出方法执行时间为 d4-d8 的 4 个快照采集间隔时间之和，这已经是非常的精确的时间估算了。\n而这个过程因为不涉及代码埋点，所以自然性能消耗是稳定和可控的，也无需担心是否被埋点，是否是 JDK 方法等问题。同时，由于上层已经在分布式追踪之下，性能剖析方法可以明确地确定分析开始和结束时间，减少不必要的性能开销。\n性能剖析可以很好的对线程的堆栈信息进行监控，主要有以下几点优势：\n 精确的问题定位，直接到代码方法和代码行； 无需反复的增删埋点，大大减少了人力开发成本； 不用承担过多埋点对目标系统和监控系统的压力和性能风险； 按需使用，平时对系统无消耗，使用时的消耗稳定可能。  SkyWalking 实践实例 我们首先在 Apache SkyWalking APM 中实现此技术方法，下面我们就以一个真实的例子来说明此方法的执行效果。\nfinal CountDownLatchcountDownLatch= new CountDownLatch(2); threadPool.submit(new Task1(countDownLatch)); threadPool.submit(new Task2(countDownLatch)); try { countDownLatch.await(500, TimeUnit.MILLISECONDS); } catch (InterruptedExceptione) { } 这是我们故意加入的问题代码，我们使用 CountDownLanth 设置了两个任务完成后方法执行结束，Task1 和 Task2 是两个执行时间不稳定的任务，所以主任务也会执行速度不稳定。但对于运维和监控团队来说，很难定位到这个方法片段。\n针对于这种情况，我们看看性能剖析会怎样直接定位此问题。\n上图所示的就是我们在进行链路追踪时所看到的真实执行情况，其中我们可以看到在 service/processWithThreadPool 执行速度缓慢，这正是我们植入问题代码的方法。此时在这个调用中没有后续链路了，所以并没有更细致的原因，我们也不打算去 review 代码，从而增加新埋点。这时，我们可以对 HelloService 进行性能剖析，并执行只剖析响应速度大于 500 毫秒的请求。\n注意，指定特定响应时间的剖析是保证剖析有效性的重要特性，如果方法在平均响应时间上已经出现问题，往往通过分布式链路可以快速定位，因为此时链路总时间长，新埋点带来的性能影响相对可控。但是方法性能抖动是不容易用新增埋点来解决的，而且往往只发生在生产环境。\n上图就是我们进行性能剖析后的真实结果图。从左到右分别表示：栈帧名称、该栈帧总计耗时（包含其下面所有自栈帧）、当前栈帧自身耗时和监控次数。我们可以在最后一行看到，线程卡在了 sun.misc.Unsafe.park 中了。如果你熟悉 Java 就可以知道此时进行了锁等待，我们继续按照树的结构向上推，便可以看到线程真正是卡在了 CountDownLatch.await 方法中。\n方法局限性 当然任何的方法都不是万能的，性能剖析也有一些局限性。\n第一， 对于高频反复执行的方法，如循环调用，可能会误报为缓慢方法。但这并不是大问题，因为如果反复执行的耗时较长，必然是系统需要关注的性能瓶颈。\n第二， 由于性能栈快照有一定的性能消耗，所以采集周期不宜过密，如 SkyWalking 实践中，不支持小于 10ms 的采集间隔。所以如果问题方法执行时间过小（比如在 10 毫秒内波动），此方法并不适用。我们也再此强调，方法论和工具的强大，始终不能代替程序员。\n","excerpt":"作者：吴晟，刘晗 原文地址  在本文中，我们详细介绍了代码级的性能剖析方法，以及我们在 Apache SkyWalking 中的实践。希望能够帮助大家在线定位系统性能短板，缓解系统压力。\n分布式链路追 …","ref":"/zh/2020-03-23-using-profiling-to-fix-the-blind-spot-of-distributed-tracing/","title":"在线代码级性能剖析，补全分布式追踪的最后一块“短板”"},{"body":"SkyWalking CLI 0.2.0 is released. Go to downloads page to find release tars.\n Support visualization of heat map Support top N entities, swctl metrics top 5 --name service_sla Support thermodynamic metrics, swctl metrics thermodynamic --name all_heatmap Support multiple linear metrics, swctl --display=graph --debug metrics multiple-linear --name all_percentile  ","excerpt":"SkyWalking CLI 0.2.0 is released. Go to downloads page to find release tars.\n Support visualization …","ref":"/events/release-apache-skywalking-cli-0-2-0/","title":"Release Apache SkyWalking CLI 0.2.0"},{"body":"SkyWalking Chart 1.1.0 is released. Go to downloads page to find release tars.\n Support SkyWalking 6.6.0 Support deploy Elasticsearch 7 The official helm repo was changed to the official Elasticsearch repo (https://helm.elastic.co/)  ","excerpt":"SkyWalking Chart 1.1.0 is released. Go to downloads page to find release tars.\n Support SkyWalking …","ref":"/events/release-apache-skywalking-chart-1-1-0-for-skywalking-6-6-0/","title":"Release Apache SkyWalking Chart 1.1.0 for SkyWalking 6.6.0"},{"body":"Support tracing and collect metrics from Nginx server. Require SkyWalking APM 7.0+.\n","excerpt":"Support tracing and collect metrics from Nginx server. Require SkyWalking APM 7.0+.","ref":"/events/skywalking-nginx-lua-0-1-0-release/","title":"SkyWalking Nginx LUA 0.1.0 release"},{"body":"Based on his continuous contributions, Ming Wen (a.k.a moonming) has been voted as a new committer.\n","excerpt":"Based on his continuous contributions, Ming Wen (a.k.a moonming) has been voted as a new committer.","ref":"/events/welcome-ming-wen-as-new-committer/","title":"Welcome Ming Wen as new committer"},{"body":"Based on his continuous contributions, Haochao Zhuang (a.k.a dmsolr) has been invited to join the PMC. Welcome aboard.\n","excerpt":"Based on his continuous contributions, Haochao Zhuang (a.k.a dmsolr) has been invited to join the …","ref":"/events/welcome-haochao-zhuang-to-join-the-pmc/","title":"Welcome Haochao Zhuang to join the PMC"},{"body":"Based on his continuous contributions, Zhusheng Xu (a.k.a aderm) has been voted as a new committer.\n","excerpt":"Based on his continuous contributions, Zhusheng Xu (a.k.a aderm) has been voted as a new committer.","ref":"/events/welcome-zhusheng-xu-as-new-committer/","title":"Welcome Zhusheng Xu as new committer"},{"body":"Based on his continuous contributions, Han Liu (a.k.a mrproliu) has been voted as a new committer.\n","excerpt":"Based on his continuous contributions, Han Liu (a.k.a mrproliu) has been voted as a new committer.","ref":"/events/welcome-han-liu-as-new-committer/","title":"Welcome Han Liu as new committer"},{"body":" Author: Wu Sheng, tetrate.io, SkyWalking original creator, SkyWalking V.P. GitHub, Twitter, Linkedin  The SkyWalking project provides distributed tracing, topology map analysis, service mesh telemetry analysis, metrics analysis and a super cool visualization targeting distributed systems in k8s or traditional VM deployments.\nThe project is widely used in Alibaba, Huawei, Tencent, DiDi, xiaomi, Pingan, China’s top 3 telecom companies (China Mobile, China telecom, China Unicom), airlines, banks and more. It has over 140 company users listed on our powered by page.\nToday, we welcome and celebrate reaching 200 code contributors on our main repo. We hereby mark this milestone as official today, : Jan. 20th 2020.\nAt this great moment, I would like to share SkyWalking’s 4-year open source journey.\nI wrote the first line on Nov. 1st, 2015, guiding people to understand a distributed system just as micro-services and distributed architecture were becoming popular. In the first 2 years, I never thought it would become such a big and active community. I didn’t even expect it would be an open source project. Initially, the goal was primarily to teach others about distributed tracing and analysis.\nIt was a typical open source project in obscurity in its first two years. But people still showed up, asked questions, and tried to improve the project. I got several invitations to share the project at local meetups.All these made me realize people really needed a good open source APM project.\nIn 2017, I decided to dedicate myself as much as possible to make the project successful, and it became my day job. To be honest, I had no clue about how to do that; at that time in China, it was rare to have this kind of job. So, I began to ask friends around me, “Do you want to collaborate on the open source APM with me?” Most people were busy and gave a clear NO, but two of them agreed to help: Xin Zhang and Yongsheng Peng. We built SkyWalking 3.x and shared the 3.2 release at GOPS Shanghai, China.\nIt became the first adoption version used in production\nCompared to today\u0026rsquo;s SkyWalking, it was a toy prototype, but it had the same tracing design, protocol and analysis method.\nThat year the contributor team was 15-20, and the project had obvious potential to expand. I began to consider bringing the project into a worldwide, top-level open source foundation. Thanks to our initial incubator mentors, Michael Semb Wever, William Jiang, and Luke Han, this really worked. At the end of 2017, SkyWalking joined the Apache Incubator, and kept following the Apache Way to build community. More contributors joined the community.\nWith more people spending time on the project collaborations, including codes, tests, blogs, conference talks, books and uses of the project, a chemical reaction happens. New developers begin to provide bug fixes, new feature requirements and new proposals. At the moment of graduation in spring 2019, the project had 100 contributors. Now, only 9 months later, it’s surged to 200 super quickly. They enhance the project and extend it to frontiers we never imaged: 5 popular language agents, service mesh adoption, CLI tool, super cool visualization. We are even moving on thread profiling, browser performance and Nginx tracing NOW.\nOver the whole 4+ years open source journey, we have had supports from leaders in the tracing open source community around the world, including Adrian Cole, William Jiang, Luke Han, Michael Semb Wever, Ben Sigelman, and Jonah Kowall. And we’ve had critical foundations' help, especially Apache Software Foundation and the Cloud Native Computing Foundation.\nOur contributors also have their support from their employers, including, to the best of my knowledge, Alibaba, Huawei, China Mobile, ke.com, DaoCloud, Lizhi.fm, Yonghui Supermarket, and dangdang.com. I also have support from my employers, tetrate.io, Huawei, and OneAPM.\nThanks to our 200+ contributors and the companies behind them. You make this magic happen.\n","excerpt":"Author: Wu Sheng, tetrate.io, SkyWalking original creator, SkyWalking V.P. GitHub, Twitter, Linkedin …","ref":"/blog/2020-01-20-celebrate-200th-contributor/","title":"SkyWalking hits 200 contributors mark"},{"body":"Based on his continuous contributions, Hongwei Zhai (a.k.a innerpeacez) has been invited to join the PMC. Welcome aboard.\n","excerpt":"Based on his continuous contributions, Hongwei Zhai (a.k.a innerpeacez) has been invited to join the …","ref":"/events/welcome-hongwei-zhai-to-join-the-pmc/","title":"Welcome Hongwei Zhai to join the PMC"},{"body":"Apache APM 6.6.0 release. Go to downloads page to find release tars.\n Service Instance dependency detection are available. Support ElasticSearch 7 as a storage option. Reduce the register load.  ","excerpt":"Apache APM 6.6.0 release. Go to downloads page to find release tars.\n Service Instance dependency …","ref":"/events/release-apache-skywalking-apm-6-6-0/","title":"Release Apache SkyWalking APM 6.6.0"},{"body":"SkyWalking Chart 1.0.0 is released. Go to downloads page to find release tars.\n Deploy SkyWalking 6.5.0 by Chart. Elasticsearch deploy optional.  ","excerpt":"SkyWalking Chart 1.0.0 is released. Go to downloads page to find release tars.\n Deploy SkyWalking …","ref":"/events/release-apache-skywalking-chart-1-0-0-for-skywalking-6-5-0/","title":"Release Apache SkyWalking Chart 1.0.0 for SkyWalking 6.5.0"},{"body":"SkyWalking CLI 0.1.0 is released. Go to downloads page to find release tars.\n Add command swctl service to list services Add command swctl instance and swctl search to list and search instances of service. Add command swctl endpoint to list endpoints of service. Add command swctl linear-metrics to query linear metrics and plot the metrics in Ascii Graph mode. Add command swctl single-metrics to query single-value metrics.  ","excerpt":"SkyWalking CLI 0.1.0 is released. Go to downloads page to find release tars.\n Add command swctl …","ref":"/events/release-apache-skywalking-cli-0-1-0/","title":"Release Apache SkyWalking CLI 0.1.0"},{"body":"Based on his continuous contributions, Weiyi Liu (a.k.a wayilau) has been voted as a new committer.\n","excerpt":"Based on his continuous contributions, Weiyi Liu (a.k.a wayilau) has been voted as a new committer.","ref":"/events/welcome-weiyi-liu-as-new-committer/","title":"Welcome Weiyi Liu as new committer"},{"body":"Based on his contributions to the project, he has been accepted as SkyWalking committer. Welcome aboard.\n","excerpt":"Based on his contributions to the project, he has been accepted as SkyWalking committer. Welcome …","ref":"/events/welcome-lang-li-as-a-new-committer/","title":"Welcome Lang Li as a new committer"},{"body":"Based on her continuous contributions, Qiuxia Fan (a.k.a Fine0830) has been voted as a new committer.\n","excerpt":"Based on her continuous contributions, Qiuxia Fan (a.k.a Fine0830) has been voted as a new …","ref":"/events/welcome-qiuxia-fan-as-new-committer/","title":"Welcome Qiuxia Fan as new committer"},{"body":"6.5.0 release. Go to downloads page to find release tars.\n New metrics comparison view in UI. Dynamic Alert setting supported. JDK9-12 supported in backend.  ","excerpt":"6.5.0 release. Go to downloads page to find release tars.\n New metrics comparison view in UI. …","ref":"/events/release-apache-skywalking-apm-6-5-0/","title":"Release Apache SkyWalking APM 6.5.0"},{"body":"Based on his continuous contributions, Wei Zhang (a.k.a arugal) has been voted as a new committer.\n","excerpt":"Based on his continuous contributions, Wei Zhang (a.k.a arugal) has been voted as a new committer.","ref":"/events/welcome-wei-zhang-as-new-committer/","title":"Welcome Wei Zhang as new committer"},{"body":"PS：本文仅仅是在我的测试环境实验过，如果有问题，请自行优化调整\n前记：记得skywlking还是6.0版本的时候我就在试用，当时是skywalking基本在两三天左右就会监控数据完全查不出来，elasticsearch日志报错，由于当时也算是初用es，主要用来日志收集，并且时间有限，没有继续深入研究，最近空闲，更新到最新的6.5.0(开发版本)还是会出现同样的问题，下定决心解决下，于是有了本文的浅知拙见\n本次调优环境 skywalking: 6.5.0 elasticsearch:6.3.2(下文用es代替)\n调优过程   当然是百度了，百度后其实翻来翻去就找到一个相关的文章https://my.oschina.net/keking/blog/3025303 ，参考之。\n  调整skywalking的这两个参数试试 bulkActions: 4000 # Execute the bulk every 2000 requests  bulkSize: 60 # flush the bulk every 20mb 然后es还是继续挂，继续频繁的重启\n  继续看这个文章，发现了另外一篇https://www.easyice.cn/archives/207 ，继续参考之\n  这篇文章发现每一个字我都认识，看起来也能懂，但是对于es小白的我来说，着实不知道怎么调整这些参数，姑且先加到es的配置文件里边试试看吧，于是就加了，然后重启es的时候说发现index参数配置，自从5.0之后就不支持这样配置了，还给调了个es的接口去设置，但是设置失败（真够不错的），朝着这个思路去百度，百度到快放弃，后来就寻思，再试试看吧，（百度的结果是知道了index有静态参数和动态参数，动态的参数是可以随时设置，静态的只能创建或者关闭状态的索引才可以设置） 然鹅并不知道怎么关闭索引，继续百度，（怎么全特么百度，好吧不百度了，直接来干货）\n 关闭索引（我的skywalking索引命名空间是dry_trace） curl -XPOST \u0026quot;http://localhost:9200/dry_trace*/_close\u0026quot; 设置参数 curl -XPUT 'http://localhost:9200/dry_trace*/_settings?preserve_existing=true' -H 'Content-type:application/json' -d '{ \u0026quot;index.refresh_interval\u0026quot; : \u0026quot;10s\u0026quot;, \u0026quot;index.translog.durability\u0026quot; : \u0026quot;async\u0026quot;, \u0026quot;index.translog.flush_threshold_size\u0026quot; : \u0026quot;1024mb\u0026quot;, \u0026quot;index.translog.sync_interval\u0026quot; : \u0026quot;120s\u0026quot; }'  打开索引 curl -XPOST \u0026quot;http://localhost:9200/dry_trace*/_open\u0026quot;    还有一点，第四步的方式只适用于现有的索引设置，那么新的索引设置呢，总不能每天重复下第四步吧。当然不需要，来干货 首先登陆kinaba控制台找到开发工具 贴入以下代码\n   PUT /_template/dry_trace_tmp { \u0026quot;index_patterns\u0026quot;: \u0026quot;dry_trace*\u0026quot;, \u0026quot;order\u0026quot;: 1, \u0026quot;settings\u0026quot;: { \u0026quot;index\u0026quot;: { \u0026quot;refresh_interval\u0026quot;: \u0026quot;30s\u0026quot;, \u0026quot;translog\u0026quot;: { \u0026quot;flush_threshold_size\u0026quot;: \u0026quot;1GB\u0026quot;, \u0026quot;sync_interval\u0026quot;: \u0026quot;60s\u0026quot;, \u0026quot;durability\u0026quot;: \u0026quot;async\u0026quot; } } } } 截止目前为止运行一周，还未发现挂掉，一切看起来正常   完结\u0026mdash; 于 2019年11月\n","excerpt":"PS：本文仅仅是在我的测试环境实验过，如果有问题，请自行优化调整\n前记：记得skywlking还是6.0版本的时候我就在试用，当时是skywalking基本在两三天左右就会监控数据完全查不出 …","ref":"/zh/2019-11-07-skywalking-elasticsearch-storage-optimization/","title":"SkyWalking 使用 ElasticSearch 存储的优化"},{"body":"Based on his continuous contributions, Haochao Zhuang (a.k.a dmsolr) has been voted as a new committer.\n","excerpt":"Based on his continuous contributions, Haochao Zhuang (a.k.a dmsolr) has been voted as a new …","ref":"/events/welcome-haochao-zhuang-as-new-committer/","title":"Welcome Haochao Zhuang as new committer"},{"body":" 作者：innerpeacez 原文地址  本文主要讲述的是如何使用 Helm Charts 将 SkyWalking 部署到 Kubernetes 集群中，相关文档可以参考skywalking-kubernetes 和 backend-k8s 文档 。\n目前推荐的四种方式：\n 使用 helm 2 提供的 helm serve 启动本地 helm repo 使用本地 chart 文件部署 使用 harbor 提供的 repo 功能 直接从官方 repo 进行部署  注意：目前 skywalking 的 chart 还没有提交到官方仓库，请先参照前三种方式进行部署\nHelm 2 提供的 helm serve 打包对应版本的 skywalking chart 1.配置 helm 环境，参考 Helm 环境配置 ，如果你要部署 helm2 相关 chart 可以直接配置 helm2 的相关环境\n2.克隆/下载ZIP skywalking-kubernetes 这个仓库，仓库关于chart的目录结构如下\n helm-chart\n helm2  6.0.0-GA 6.1.0   helm3  6.3.0 6.4.0     克隆/下载ZIP 完成后进入指定目录打包对应版本的chart\ncd skywalking-kubernetes/helm-chart/\u0026lt;helm-version\u0026gt;/\u0026lt;skywalking-version\u0026gt; 注意：helm-version 为对应的 helm 版本目录，skywalking-version 为对应的 skywalking 版本目录，下面以helm3 和 skywalking 6.3.0 为例\ncd skywalking-kubernetes/helm-chart/helm3/6.3.0 3.由于skywalking 依赖 elasticsearch 作为存储库，执行以下命令更新依赖，默认会从官方repo进行拉取\nhelm dep up skywalking  Hang tight while we grab the latest from your chart repositories\u0026hellip; \u0026hellip;Successfully got an update from the \u0026ldquo;stable\u0026rdquo; chart repository Update Complete. ⎈Happy Helming!⎈ Saving 1 charts Downloading elasticsearch from repo https://kubernetes-charts.storage.googleapis.com/ Deleting outdated charts\n 如果官方 repo 不存在，请先添加官方仓库\nhelm repo add stable https://kubernetes-charts.storage.googleapis.com  \u0026ldquo;stable\u0026rdquo; has been added to your repositories\n 4.打包 skywalking , 执行以下命令\nhelm package skywalking/  Successfully packaged chart and saved it to: C:\\code\\innerpeacez_github\\skywalking-kubernetes\\helm-chart\\helm3\\6.3.0\\skywalking-0.1.0.tgz\n 打包完成后会在当前目录的同级目录生成 .tgz 文件\n ls  skywalking/ skywalking-0.1.0.tgz\n 启动 helm serve 由于上文配置的 helm 为 helm3 ,但是 helm 3中移除了 helm serve 的相关命令，所以需要另外一个环境配置helm2 的相关环境，下载 helm 2.14.3 的二进制文件，配置基本上没有大的差别，不在赘述\n初始化 helm\nhelm init 将上文生成的 skywalking-0.1.0.tgz 文件复制到 helm 相关目录 /root/.helm/repository/local,启动 serve\nhelm serve --address \u0026lt;ip\u0026gt;:8879 --repo-path /root/.helm/repository/local 注意： ip 为要能够被上文配置 helm 3 环境的机器访问到\n可以访问一下看看服务 serve 是否启动成功\ncurl ip:8879 部署 skywalking 1.在helm3 环境中添加启动的本地 repo\nhelm repo add local http://\u0026lt;ip\u0026gt;:8879 2.查看 skywalking chart 是否存在于本地仓库中\nhelm search skywalking  NAME CHART VERSION\tAPP VERSION\tDESCRIPTION local/skywalking 0.1.0 6.3.0 Apache SkyWalking APM System\n 3.部署\nhelm -n test install skywalking local/skywalking 这样 skywalking 就部署到了 k8s 集群中的 test 命名空间了，至此本地安装skywalking 就完成了。\n本地文件部署 如果你不想存储到 chart 到仓库中也可以直接使用本地文件部署 skywalking,按照上面的步骤将skywalking chart 打包完成之后，直接使用以下命令进行部署\nhelm -n test install skywalking skywalking-0.1.0.tgz harbor 作为 repo 存储 charts harbor 目前已经提供了，charts repo 的能力，这样就可以将 docker 镜像和 chart 存储在一个仓库中了，方便维护，具体harbor 的部署方法参考 Harbor 作为存储仓库存储 chart\n官方 repo 部署 目前没有发布到官方 repo 中，后续发布完成后，只需要执行下面命令即可\nhelm install -n test stable/skywalking 总结 四种方式都可以进行部署，如果你想要自定义 chart ,需要使用上述两种本地方法及 harbor 存储的方式，以便你修改好 chart 之后进行部署.\n","excerpt":"作者：innerpeacez 原文地址  本文主要讲述的是如何使用 Helm Charts 将 SkyWalking 部署到 Kubernetes 集群中，相关文档可以参 …","ref":"/zh/2019-10-08-how-to-use-sw-chart/","title":"使用 chart 部署 SkyWalking"},{"body":" Author: Wei Qiang GitHub  Background SkyWalking backend provides the alarm function, we can define some Alarm rules, call webhook after the rule is triggered. I share my implementation\nDemonstration SkyWalking alarm UI\ndingtalk message body\nIntroduction  install  go get -u github.com/weiqiang333/infra-skywalking-webhook cd $GOPATH/src/github.com/weiqiang333/infra-skywalking-webhook/ bash build/build.sh ./bin/infra-skywalking-webhook help  Configuration  main configs file: configs/production.yml dingtalk: p3: token...  Example  ./bin/infra-skywalking-webhook --config configs/production.yml --address 0.0.0.0:8000  SkyWalking backend alarm settings  webhooks: - http://127.0.0.1:8000/dingtalk Collaboration Hope that we can improve together webhook\nSkyWalking alarm rules may add more metric names (eg priority name), we can send different channels by locating different levels of alerts (dingtalk / SMS / phone)\nThanks.\n","excerpt":"Author: Wei Qiang GitHub  Background SkyWalking backend provides the alarm function, we can define …","ref":"/blog/2019-09-25-alarm-webhook-share/","title":"SkyWalking alarm webhook sharing"},{"body":"","excerpt":"","ref":"/tags/user-manual/","title":"User Manual"},{"body":"作者： SkyWalking committer，Kdump\n本文介绍申请Apache SkyWalking Committer流程, 流程包括以下步骤\n 与PMC成员表达想成为committer的意愿(主动/被动) PMC内部投票 PMC正式邮件邀请 填写Apache iCLA申请表 设置ApacheID和邮箱 设置GitHub加入Apache组织 GitHub其它一些不重要设置  前期过程  与PMC成员表达想成为committer的意愿(主动/被动) PMC内部投票  当你对项目的贡献活跃度足够高或足够多时, Skywalking项目的PMC(项目管理委员会)会找到你并询问你是否有意愿成为项目的Committer, 或者也可以主动联系项目的PMC表达自己的意向, 在此之后PMC们会进行内部讨论和投票并告知你是否可以进入下一个环节.这个过程可能需要一周. 如果PMC主动邀请你进行非正式的意愿咨询, 你可以选择接受或拒绝.\nPS:PMC会向你索要你的个人邮箱, 建议提供Gmail, 因为后期绑定Apache邮箱需要用到, 其它邮箱我不确定是否能绑定.\nPS:从Apache官方的流程来讲, 现有的PMC会在没有通知候选人的情况下先进行候选人投票, 但是Skywalking项目的PMC有可能更倾向于先得到候选人的意愿再进行投票.\n正式阶段   PMC正式邮件邀请\n 当你收到PMC正式的邀请邮件时, 恭喜你, 你已经通过了PMC的内部投票, 你需要用英文回答接受邀请或者拒绝邀请, 记住回复的时候一定要选择全部回复.    填写Apache iCLA申请表\n  在你收到的PMC邮件中, 有几个ASF官方链接需要你去浏览, 重点的内容是查看CLAs, 并填写Individual Contributor License Agreement, 你可以将icla.pdf文件下载到本地, 使用PDF工具填写里面所需的信息, 并打印出来签名(一定要手写签名, 否则会被要求重新签名), 再扫描(或手机拍照)成电子文档(需要回复PDF格式, 文件名建议重命名为你的名字-icla.pdf), 使用gpg对电子文档进行签名(参考[HOW-TO: SUBMITTING LICENSE AGREEMENTS AND GRANTS\n](http://www.apache.org/licenses/contributor-agreements.html#submitting)), Window可以使用GnuPG或者Gpg4win.\n  完成gpg签名后, 请将你签名用的公钥上送到pool.sks-keyservers.net服务器, 并在这个页面中验证你的公钥是否可以被搜索到, 搜索关键词可以是你秘钥中填写的名字或者邮箱地址.\n  gpg签名后, 会生成.pdf.asc的文件, 需要将你的你的名字-icla.pdf和你的名字-icla.pdf.asc以附件的方式一起发送到secretary@apache.org, 并抄送给private@skywalking.apache.org.\n    设置ApacheID和邮箱\n 大概5个工作日内, 你会收到一封来至于root@apache.org的邮件, 主题为Welcome to the Apache Software Foundation (ASF)!, 恭喜你, 你已经获得了ApacheID, 这时候你需要根据邮件内容的提示去设置你的ApacheID密码, 密码设置完成后, 需要在Apache Account Utility页面中重点设置Forwarding email address和Your GitHub Username两个信息.保存信息的时候需要你填写当前的ApacheID的密码. 现在进入Gmail, 选择右上角的齿轮-\u0026gt;设置-\u0026gt;账号和导入-\u0026gt;添加其他电子邮件地址-\u0026gt;参考Sending email from your apache.org email address给出的信息根据向导填写Apache邮箱.    设置GitHub加入Apache组织\n 进入Welcome to the GitBox Account Linking Utility!, 按照顺序将Apache Account和GitHub Account点绿, 想点绿MFA Status, 需要去GitHub开启2FA, 请参考配置双重身份验证完成2FA的功能. 等待1~2小时后登陆自己的GitHub的dashboard界面, 你应该会看到一条Apache组织邀请你加入的通知, 这个时候接受即可享有Skywalking相关GitHub项目权限了.    其它提示  GitHub其它一些不重要设置  在GitHub首页展示Apache组织的logo: 进入Apache GitHub组织-\u0026gt;People-\u0026gt;搜索自己的GitHubID-\u0026gt;将Private改成Public    ","excerpt":"作者： SkyWalking committer，Kdump\n本文介绍申请Apache SkyWalking Committer流程, 流程包括以下步骤\n 与PMC成员表达想成为committer的意 …","ref":"/zh/2019-09-12-apache-skywalking-committer-apply-process/","title":"Apache SkyWalking Committer申请流程"},{"body":"Based on his contributions to the skywalking ui project, Weijie Zou (a.k.a Kdump) has been accepted as a new committer.\n","excerpt":"Based on his contributions to the skywalking ui project, Weijie Zou (a.k.a Kdump) has been accepted …","ref":"/events/welcome-weijie-zou-as-a-new-committer/","title":"Welcome Weijie Zou as a new committer"},{"body":"6.4.0 release. Go to downloads page to find release tars.\n Highly recommend to upgrade due to Pxx metrics calculation bug. Make agent working in JDK9+ Module system.  Read changelog for the details.\n","excerpt":"6.4.0 release. Go to downloads page to find release tars.\n Highly recommend to upgrade due to Pxx …","ref":"/events/release-apache-skywalking-apm-6-4-0/","title":"Release Apache SkyWalking APM 6.4.0"},{"body":"  作者：innerpeacez 原文地址   如果你还不知道 Skywalking agent 是什么，请点击这里查看 Probe 或者这里查看快速了解agent,由于我这边大部分都是 JAVA 服务，所以下文以 Java 中使用 agent 为例，提供了以下三种方式供你选择\n三种方式：  使用官方提供的基础镜像 将 agent 包构建到已经存在的基础镜像中 sidecar 模式挂载 agent  1.使用官方提供的基础镜像 查看官方 docker hub 提供的基础镜像，只需要在你构建服务镜像是 From 这个镜像即可，直接集成到 Jenkins 中可以更加方便\n2.将 agent 包构建到已经存在的基础镜像中 提供这种方式的原因是：官方的镜像属于精简镜像，并且是 openjdk ，可能很多命令没有，需要自己二次安装，以下是我构建的过程\n  下载 oracle jdk\n这个现在 oracle 有点恶心了，wget 各种不行，然后我放弃了，直接从官网下载了\n  下载 skywalking 官方发行包，并解压（以6.3.0为例）\nwget https://www.apache.org/dyn/closer.cgi/skywalking/6.3.0/apache-skywalking-apm-6.3.0.tar.gz \u0026amp;\u0026amp; tar -zxvf apache-skywalking-apm-6.3.0.tar.gz   通过以下 dockerfile 构建基础镜像\nFROMalpine:3.8  ENV LANG=C.UTF-8 RUN set -eux \u0026amp;\u0026amp; \\  apk update \u0026amp;\u0026amp; apk upgrade \u0026amp;\u0026amp; \\  wget -q -O /etc/apk/keys/sgerrand.rsa.pub https://alpine-pkgs.sgerrand.com/sgerrand.rsa.pub \u0026amp;\u0026amp;\\  wget https://github.com/sgerrand/alpine-pkg-glibc/releases/download/2.30-r0/glibc-2.30-r0.apk \u0026amp;\u0026amp;\\  apk --no-cache add unzip vim curl git bash ca-certificates glibc-2.30-r0.apk file \u0026amp;\u0026amp; \\  rm -rf /var/lib/apk/* \u0026amp;\u0026amp;\\  mkdir -p /usr/skywalking/agent/ # A streamlined jreADD jdk1.8.0_221/ /usr/java/jdk1.8.0_221/ADD apache-skywalking-apm-bin/agent/ /usr/skywalking/agent/ # set envENV JAVA_HOME /usr/java/jdk1.8.0_221ENV PATH ${PATH}:${JAVA_HOME}/bin # run container with base path:/WORKDIR/ CMD bash  这里由于 alpine 是基于mini lib 的，但是 java 需要 glibc ,所以加入了 glibc 相关的东西，最后构建出的镜像大小在 490M 左右，因为加了挺多命令还是有点大，仅供参考，同样构建出的镜像也可以直接配置到 jenkins 中。\n3.sidecar 模式挂载 agent 如果你们的服务是部署在 Kubernetes 中，你还可以使用这种方式来使用 Skywalking Agent ,这种方式的好处在与不需要修改原来的基础镜像，也不用重新构建新的服务镜像，而是以sidecar 模式，通过共享volume的方式将agent 所需的相关文件挂载到已经存在的服务镜像中\n构建 skywalking agent sidecar 镜像的方法\n  下载skywalking 官方发行包，并解压\nwget https://www.apache.org/dyn/closer.cgi/skywalking/6.3.0/apache-skywalking-apm-6.3.0.tar.gz \u0026amp;\u0026amp; tar -zxvf apache-skywalking-apm-6.3.0.tar.gz   通过以下 dockerfile 进行构建\nFROMbusybox:latest  ENV LANG=C.UTF-8 RUN set -eux \u0026amp;\u0026amp; mkdir -p /usr/skywalking/agent/ ADD apache-skywalking-apm-bin/agent/ /usr/skywalking/agent/ WORKDIR/  注意：这里我没有在dockerfile中下载skywalking 发行包是因为保证构建出的 sidecar 镜像保持最小，bosybox 只有700 k左右，加上 agent 最后大小小于20M\n如何使用 sidecar 呢？\napiVersion: apps/v1 kind: Deployment metadata: labels: name: demo-sw name: demo-sw spec: replicas: 1 selector: matchLabels: name: demo-sw template: metadata: labels: name: demo-sw spec: initContainers: - image: innerpeacez/sw-agent-sidecar:latest name: sw-agent-sidecar imagePullPolicy: IfNotPresent command: [\u0026#39;sh\u0026#39;] args: [\u0026#39;-c\u0026#39;,\u0026#39;mkdir -p /skywalking/agent \u0026amp;\u0026amp; cp -r /usr/skywalking/agent/* /skywalking/agent\u0026#39;] volumeMounts: - mountPath: /skywalking/agent name: sw-agent containers: - image: nginx:1.7.9 name: nginx volumeMounts: - mountPath: /usr/skywalking/agent name: sw-agent ports: - containerPort: 80 volumes: - name: sw-agent emptyDir: {} 以上是挂载 sidecar 的 deployment.yaml 文件，以nginx 作为服务为例，主要是通过共享 volume 的方式挂载 agent，首先 initContainers 通过 sw-agent 卷挂载了 sw-agent-sidecar 中的 /skywalking/agent ，并且将上面构建好的镜像中的 agent 目录 cp 到了 /skywalking/agent 目录，完成之后 nginx 启动时也挂载了 sw-agent 卷，并将其挂载到了容器的 /usr/skywalking/agent 目录，这样就完成了共享过程。\n总结 这样除去 ServiceMesh 以外，我能想到的方式就介绍完了，希望可以帮助到你。最后给 Skywalking 一个 Star 吧，国人的骄傲。\n","excerpt":"作者：innerpeacez 原文地址   如果你还不知道 Skywalking agent 是什么，请点击这里查看 Probe 或者这里查看快速了解agent,由于我这边大部分都是 JAVA 服务， …","ref":"/zh/2019-08-30-how-to-use-skywalking-agent/","title":"如何使用 SkyWalking Agent ？"},{"body":"Based on his continuous contributions, Yuguang Zhao (a.k.a zhaoyuguang) has been invited to join the PMC. Welcome aboard.\n","excerpt":"Based on his continuous contributions, Yuguang Zhao (a.k.a zhaoyuguang) has been invited to join the …","ref":"/events/welcome-yuguang-zhao-to-join-the-pmc/","title":"Welcome Yuguang Zhao to join the PMC"},{"body":"Based on his continuous contributions, Zhenxu Ke (a.k.a kezhenxu94) has been invited to join the PMC. Welcome aboard.\n","excerpt":"Based on his continuous contributions, Zhenxu Ke (a.k.a kezhenxu94) has been invited to join the …","ref":"/events/welcome-zhenxu-ke-to-join-the-pmc/","title":"Welcome Zhenxu Ke to join the PMC"},{"body":"Based on his contributions to the skywalking PHP project, Yanlong He (a.k.a heyanlong has been accepted as a new committer.\n","excerpt":"Based on his contributions to the skywalking PHP project, Yanlong He (a.k.a heyanlong has been …","ref":"/events/welcome-yanlong-he-as-a-new-committer/","title":"Welcome Yanlong He as a new committer"},{"body":"6.3.0 release. Go to downloads page to find release tars.\n Improve ElasticSearch storage implementation performance again. OAP backend re-install w/o agent reboot required.  Read changelog for the details.\n","excerpt":"6.3.0 release. Go to downloads page to find release tars.\n Improve ElasticSearch storage …","ref":"/events/release-apache-skywalking-apm-6-3-0/","title":"Release Apache SkyWalking APM 6.3.0"},{"body":"6.2.0 release. Go to downloads page to find release tars. ElasticSearch storage implementation changed, high reduce payload to ElasticSearch cluster.\nRead changelog for the details.\n","excerpt":"6.2.0 release. Go to downloads page to find release tars. ElasticSearch storage implementation …","ref":"/events/release-apache-skywalking-apm-6-2-0/","title":"Release Apache SkyWalking APM 6.2.0"},{"body":"Based on his continuous contributions, Zhenxu Ke (a.k.a kezhenxu94) has been voted as a new committer.\n","excerpt":"Based on his continuous contributions, Zhenxu Ke (a.k.a kezhenxu94) has been voted as a new …","ref":"/events/welcome-zhenxu-ke-as-a-new-committer/","title":"Welcome Zhenxu Ke as a new committer"},{"body":"6.1.0 release. Go to downloads page to find release tars. This is the first top level project version.\nKey updates\n RocketBot UI OAP performance improvement  ","excerpt":"6.1.0 release. Go to downloads page to find release tars. This is the first top level project …","ref":"/events/release-apache-skywalking-apm-6-1-0/","title":"Release Apache SkyWalking APM 6.1.0"},{"body":"Apache SkyWalking PMC accept the RocketBot UI contributions. After IP clearance, it will be released in SkyWalking 6.1 soon.\n","excerpt":"Apache SkyWalking PMC accept the RocketBot UI contributions. After IP clearance, it will be released …","ref":"/events/rocketbot-ui-has-been-accepted-as-skywalking-primary-ui/","title":"RocketBot UI has been accepted as SkyWalking primary UI"},{"body":"Apache board approved SkyWalking graduated as TLP at April 17th 2019.\n","excerpt":"Apache board approved SkyWalking graduated as TLP at April 17th 2019.","ref":"/events/skywalking-graduated-as-apache-top-level-project/","title":"SkyWalking graduated as Apache Top Level Project"},{"body":"Based on his continuous contributions, he has been accepted as a new committer.\n","excerpt":"Based on his continuous contributions, he has been accepted as a new committer.","ref":"/events/welcome-yuguang-zhao-as-a-new-committer/","title":"Welcome Yuguang Zhao as a new committer"},{"body":"APM和调用链跟踪 随着企业经营规模的扩大，以及对内快速诊断效率和对外SLA（服务品质协议，service-level agreement)的追求，对于业务系统的掌控度的要求越来越高，主要体现在：\n 对于第三方依赖的监控，实时/准实时了解第三方的健康状况/服务品质，降低第三方依赖对于自身系统的扰动（服务降级、故障转移） 对于容器的监控，实时/准实时的了解应用部署环境（CPU、内存、进程、线程、网络、带宽）情况，以便快速扩容/缩容、流量控制、业务迁移 业务方对于自己的调用情况，方便作容量规划，同时对于突发的请求也能进行异常告警和应急准备 自己业务的健康、性能监控，实时/准实时的了解自身的业务运行情况，排查业务瓶颈，快速诊断和定位异常，增加对自己业务的掌控力  同时，对于企业来说，能够更精确的了解资源的使用情况，对于成本核算和控制也有非常大的裨益。\n在这种情况下，一般都会引入APM（Application Performance Management \u0026amp; Monitoring）系统，通过各种探针采集数据，收集关键指标，同时搭配数据呈现和监控告警，能够解决上述的大部分问题。\n然而随着RPC框架、微服务、云计算、大数据的发展，同时业务的规模和深度相比过往也都增加了很多，一次业务可能横跨多个模块/服务/容器，依赖的中间件也越来越多，其中任何一个节点出现异常，都可能导致业务出现波动或者异常，这就导致服务质量监控和异常诊断/定位变得异常复杂，于是催生了新的业务监控模式：调用链跟踪\n 能够分布式的抓取多个节点的业务记录，并且通过统一的业务id（traceId，messageId，requestId等）将一次业务在各个节点的记录串联起来，方便排查业务的瓶颈或者异常点  产品对比 APM和调用链跟踪均不是新诞生事务，很多公司已经有了大量的实践，不过开源的并且能够开箱即用的产品并不多，这里主要选取了Pinpoint，Skywalking，CAT来进行对比（当然也有其他的例如Zipkin，Jaeger等产品，不过总体来说不如前面选取的3个完成度高），了解一下APM和调用链跟踪在开源方面的发展状态。\nPinpoint Pinpoint是一个比较早并且成熟度也非常高的APM+调用链监控的项目，在全世界范围内均有用户使用，支持Java和PHP的探针，数据容器为HBase，其界面参考：\nSkywalking Skywalking是一个新晋的项目，最近一两年发展非常迅猛，本身支持OpenTracing规范，优秀的设计提供了良好的扩展性，支持Java、PHP、.Net、NodeJs探针，数据容器为ElasticSearch，其界面参考：\nCAT CAT是由美团开源的一个APM项目，也历经了多年的迭代升级，拥有大量的企业级用户，对于监控和报警整合比较紧密，支持Java、C/C++、.Net、Python、Go、NodeJs，不过CAT目前主要通过侵入性的方式接入，数据容器包括HDFS（存储原始数据）和mysql（二次统计），其界面参考：\n横向对比 上面只是做了一个简介，那这三个项目各自有什么特色或者优势/劣势呢（三者的主要产品均针对Java，这里也主要针对Java的特性）？\n Pinpoint  优势  大企业/长时间验证，稳定性和完成度高 探针收集的数据粒度比较细 HBase的数据密度较大，支持PB级别下的数据查询 代码设计考虑的扩展性较弱，二次开发难度较大（探针为插件式，开发比较简单） 拥有完整的APM和调用链跟踪功能   劣势  代码针对性强，扩展较难 容器为HBase，查询功能较弱（主要为时间维度） 探针的额外消耗较多（探针采集粒度细，大概10%~20%） 项目趋于成熟，而扩展难度较大，目前社区活跃度偏低，基本只进行探针的增加或者升级 缺少自定义指标的设计     Skywalking  优势  数据容器为ES，查询支持的维度较多并且扩展潜力大 项目设计采用微内核+插件，易读性和扩展性都比较强 主要的研发人员为华人并且均比较活跃，能够进行更加直接的沟通 拥有完整的APM和调用链跟踪功能   劣势  项目发展非常快，稳定性有待验证 ES数据密度较小，在PB级别可能会有性能压力 缺少自定义指标的设计     CAT  优势  大企业/长时间验证，稳定性和完成度高 采用手动数据埋点而不是探针，数据采集的灵活性更强 支持自定义指标 代码设计考虑的扩展性较弱，并且数据结构复杂，二次开发难度较大 拥有完善的监控告警机制   劣势  代码针对性强，扩展较难 需要手动接入埋点，代码侵入性强 APM功能完善，但是不支持调用链跟踪      基本组件 如果分别去看Pinpoint/Skywalking/CAT的整体设计，我们会发现三者更像是一个规范的三种实现，虽然各自有不同的机制和特性，但是从模块划分和功能基本是一致的：\n当然也有一些微小的区别：\n Pinpoint基本没有aggregator，同时query和alarm集成在了web中，只有agent，collector和web Skywalking则是把collector、aggregator、alarm集成为OAP（Observability Analysis Platform），并且可以通过集群部署，不同的实例可以分别承担collector或者aggregator+alarm的角色 CAT则和Skywalking类似，把collector、aggregator、alarm集成为cat-consumer，而由于CAT有比较复杂的配置管理，所以query和配置一起集成为cat-home 当然最大的区别是Pinpoint和Skywalking均是通过javaagent做字节码的扩展，通过切面编程采集数据，类似于探针，而CAT的agent则更像是一个工具集，用于手动埋点  Skywalking 前戏这么多，终于开始进入主题，介绍今天的主角：Skywalking，不过通过之前的铺垫，我们基本都知道了Skywalking期望解决的问题以及总体的结构，下面我们则从细节来看Skywalking是怎么一步一步实现的。\n模块构成 首先，Skywalking进行了精准的领域模型划分：\n整个系统分为三部分：\n agent：采集tracing（调用链数据）和metric（指标）信息并上报 OAP：收集tracing和metric信息通过analysis core模块将数据放入持久化容器中（ES，H2（内存数据库），mysql等等），并进行二次统计和监控告警 webapp：前后端分离，前端负责呈现，并将查询请求封装为graphQL提交给后端，后端通过ribbon做负载均衡转发给OAP集群，再将查询结果渲染展示  而整个Skywalking（包括agent和OAP，而webapp后端业务非常简单主要就是认证和请求转发）均通过微内核+插件式的模式进行编码，代码结构和扩展性均非常强，具体设计可以参考： 从Skywalking看如何设计一个微核+插件式扩展的高扩展框架 ，Spring Cloud Gateway的GatewayFilterFactory的扩展也是通过这种plugin define的方式来实现的。\nSkywalking也提供了其他的一些特性：\n 配置重载：支持通过jvm参数覆写默认配置，支持动态配置管理 集群管理：这个主要体现在OAP，通过集群部署分担数据上报的流量压力和二次计算的计算压力，同时集群也可以通过配置切换角色，分别面向数据采集（collector）和计算（aggregator，alarm），需要注意的是agent目前不支持多collector负载均衡，而是随机从集群中选择一个实例进行数据上报 支持k8s和mesh 支持数据容器的扩展，例如官方主推是ES，通过扩展接口，也可以实现插件去支持其他的数据容器 支持数据上报receiver的扩展，例如目前主要是支持gRPC接受agent的上报，但是也可以实现插件支持其他类型的数据上报（官方默认实现了对Zipkin，telemetry和envoy的支持） 支持客户端采样和服务端采样，不过服务端采样最有意义 官方制定了一个数据查询脚本规范：OAL（Observability Analysis Language），语法类似Linq，以简化数据查询扩展的工作量 支持监控预警，通过OAL获取数据指标和阈值进行对比来触发告警，支持webhook扩展告警方式，支持统计周期的自定义，以及告警静默防止重复告警  数据容器 由于Skywalking并没有自己定制的数据容器或者使用多种数据容器增加复杂度，而是主要使用ElasticSearch（当然开源的基本上都是这样来保持简洁，例如Pinpoint也只使用了HBase），所以数据容器的特性以及自己数据结构基本上就限制了业务的上限，以ES为例：\n ES查询功能异常强大，在数据筛选方面碾压其他所有容器，在数据筛选潜力巨大（Skywalking默认的查询维度就比使用HBase的Pinpoint强很多） 支持sharding分片和replicas数据备份，在高可用/高性能/大数据支持都非常好 支持批量插入，高并发下的插入性能大大增强 数据密度低，源于ES会提前构建大量的索引来优化搜索查询，这是查询功能强大和性能好的代价，但是链路跟踪往往有非常多的上下文需要记录，所以Skywalking把这些上下文二进制化然后通过Base64编码放入data_binary字段并且将字段标记为not_analyzed来避免进行预处理建立查询索引  总体来说，Skywalking尽量使用ES在大数据和查询方面的优势，同时尽量减少ES数据密度低的劣势带来的影响，从目前来看，ES在调用链跟踪方面是不二的数据容器，而在数据指标方面，ES也能中规中矩的完成业务，虽然和时序数据库相比要弱一些，但在PB级以下的数据支持也不会有太大问题。\n数据结构 如果说数据容器决定了上限，那么数据结构则决定了实际到达的高度。Skywalking的数据结构主要为：\n 数据维度（ES索引为skywalking_*_inventory)  service：服务 instance：实例 endpoint：接口 network_adress：外部依赖   数据内容  原始数据  调用链跟踪数据（调用链的trace信息，ES索引为skywalking_segment，Skywalking主要的数据消耗都在这里） 指标（主要是jvm或者envoy的运行时指标，例如ES索引skywalking_instance_jvm_cpu）   二次统计指标  指标（按维度/时间二次统计出来的例如pxx、sla等指标，例如ES索引skywalking_database_access_p75_month） 数据库慢查询记录（数据库索引：skywalking_top_n_database_statement）   关联关系（维度/指标之间的关联关系，ES索引为skywalking_*_relation_*) 特别记录  告警信息（ES索引为skywalking_alarm_record） 并发控制（ES索引为skywalking_register_lock）      其中数量占比最大的就是调用链跟踪数据和各种指标，而这些数据均可以通过OAP设置过期时间，以降低历史数据的对磁盘占用和查询效率的影响。\n调用链跟踪数据 作为Skywalking的核心数据，调用链跟踪数据（skywalking_segment）基本上奠定了整个系统的基础，而如果要详细的了解调用链跟踪的话，就不得不提到openTracing。\nopenTracing基本上是目前开源调用链跟踪系统的一个事实标准，它制定了调用链跟踪的基本流程和基本的数据结构，同时也提供了各个语言的实现。如果用一张图来表现openTracing，则是如下：\n其中：\n SpanContext：一个类似于MDC（Slfj)或者ThreadLocal的组件，负责整个调用链数据采集过程中的上下文保持和传递 Trace：一次调用的完整记录  Span：一次调用中的某个节点/步骤，类似于一层堆栈信息，Trace是由多个Span组成，Span和Span之间也有父子或者并列的关系来标志这个节点/步骤在整个调用中的位置  Tag：节点/步骤中的关键信息 Log：节点/步骤中的详细记录，例如异常时的异常堆栈   Baggage：和SpanContext一样并不属于数据结构而是一种机制，主要用于跨Span或者跨实例的上下文传递，Baggage的数据更多是用于运行时，而不会进行持久化    以一个Trace为例：\n首先是外部请求调用A，然后A依次同步调用了B和C，而B被调用时会去同步调用D，C被调用的时候会依次同步调用E和F，F被调用的时候会通过异步调用G，G则会异步调用H，最终完成一次调用。\n上图是通过Span之间的依赖关系来表现一个Trace，而在时间线上，则可以有如下的表达：\n当然，如果是同步调用的话，父Span的时间占用是包括子Span的时间消耗的。\n而落地到Skywalking中，我们以一条skywalking_segment的记录为例：\n{ \u0026quot;trace_id\u0026quot;: \u0026quot;52.70.15530767312125341\u0026quot;, \u0026quot;endpoint_name\u0026quot;: \u0026quot;Mysql/JDBI/Connection/commit\u0026quot;, \u0026quot;latency\u0026quot;: 0, \u0026quot;end_time\u0026quot;: 1553076731212, \u0026quot;endpoint_id\u0026quot;: 96142, \u0026quot;service_instance_id\u0026quot;: 52, \u0026quot;version\u0026quot;: 2, \u0026quot;start_time\u0026quot;: 1553076731212, \u0026quot;data_binary\u0026quot;: \u0026quot;CgwKCjRGnPvp5eikyxsSXhD///////////8BGMz62NSZLSDM+tjUmS0wju8FQChQAVgBYCF6DgoHZGIudHlwZRIDc3FsehcKC2RiLmluc3RhbmNlEghyaXNrZGF0YXoOCgxkYi5zdGF0ZW1lbnQYAiA0\u0026quot;, \u0026quot;service_id\u0026quot;: 2, \u0026quot;time_bucket\u0026quot;: 20190320181211, \u0026quot;is_error\u0026quot;: 0, \u0026quot;segment_id\u0026quot;: \u0026quot;52.70.15530767312125340\u0026quot; } 其中：\n trace_id：本次调用的唯一id，通过snowflake模式生成 endpoint_name：被调用的接口 latency：耗时 end_time：结束时间戳 endpoint_id：被调用的接口的唯一id service_instance_id：被调用的实例的唯一id version：本数据结构的版本号 start_time：开始时间戳 data_binary：里面保存了本次调用的所有Span的数据，序列化并用Base64编码，不会进行分析和用于查询 service_id：服务的唯一id time_bucket：调用所处的时段 is_error：是否失败 segment_id：数据本身的唯一id，类似于主键，通过snowflake模式生成  这里可以看到，目前Skywalking虽然相较于Pinpoint来说查询的维度要多一些，但是也很有限，而且除了endPoint，并没有和业务有关联的字段，只能通过时间/服务/实例/接口/成功标志/耗时来进行非业务相关的查询，如果后续要增强业务相关的搜索查询的话，应该还需要增加一些用于保存动态内容（如messageId，orderId等业务关键字）的字段用于快速定位。\n指标 指标数据相对于Tracing则要简单得多了，一般来说就是指标标志、时间戳、指标值，而Skywalking中的指标有两种：一种是采集的原始指标值，例如jvm的各种运行时指标（例如cpu消耗、内存结构、GC信息等）；一种是各种二次统计指标（例如tp性能指标、SLA等，当然也有为了便于查询的更高时间维度的指标，例如基于分钟、小时、天、周、月）\n例如以下是索引skywalking_endpoint_cpm_hour中的一条记录，用于标志一个小时内某个接口的cpm指标：\n{ \u0026quot;total\u0026quot;: 8900, \u0026quot;service_id\u0026quot;: 5, \u0026quot;time_bucket\u0026quot;: 2019031816, \u0026quot;service_instance_id\u0026quot;: 5, \u0026quot;entity_id\u0026quot;: \u0026quot;7\u0026quot;, \u0026quot;value\u0026quot;: 148 } 各个字段的释义如下：\n total：一分钟内的调用总量 service_id：所属服务的唯一id time_bucket：统计的时段 service_instance_id：所属实例的唯一id entity_id：接口（endpoint）的唯一id value：cpm的指标值（cpm=call per minute，即total/60）  工程实现 Skywalking的工程实现堪比Dubbo，框架设计和代码质量都达到非常高的水准，以dubbo为例，即使2012年发布的老版本放到当今，其设计和编码看起来也依然赏心悦目，设计简洁但是覆盖了所有的核心需求，同时又具备非常强的扩展性，二次开发非常简单，然而却又不会像Spring那样过度封装（当然Spring作为一个更加高度通用的框架，更高的封装也是有必要的）导致代码阅读异常困难。\nagent agent（apm-sniffer）是Skywalking的Java探针实现，主要负责：\n 采集应用实例的jvm指标 通过切向编程进行数据埋点，采集调用链数据 通过RPC将采集的数据上报  当然，agent还实现了客户端采样，不过在APM监控系统里进行客户端数据采样都是没有灵魂的，所以这里就不再赘述了。\n首先，agent通过 org.apache.skywalking.apm.agent.core.boot.BootService 实现了整体的插件化，agent启动会加载所有的BootService实现，并通过 ServiceManager 来管理这些插件的生命周期，采集jvm指标、gRPC连接管理、调用链数据维护、数据上报OAP这些服务均是通过这种方式扩展。\n然后，agent还通过bytebuddy以javaagent的模式，通过字节码增强的机制来构造AOP环境，再提供PluginDefine的规范方便探针的开发，最终实现非侵入性的数据埋点，采集调用链数据。\n最终落地到代码上则异常清晰：\n//通过bytebuddy的AgentBuilder构造javaagent增强classLoader new AgentBuilder.Default(byteBuddy) .ignore( //忽略这些包的内容，不进行增强 nameStartsWith(\u0026quot;net.bytebuddy.\u0026quot;) .or(nameStartsWith(\u0026quot;org.slf4j.\u0026quot;)) .or(nameStartsWith(\u0026quot;org.apache.logging.\u0026quot;)) .or(nameStartsWith(\u0026quot;org.groovy.\u0026quot;)) .or(nameContains(\u0026quot;javassist\u0026quot;)) .or(nameContains(\u0026quot;.asm.\u0026quot;)) .or(nameStartsWith(\u0026quot;sun.reflect\u0026quot;)) .or(allSkyWalkingAgentExcludeToolkit()) .or(ElementMatchers.\u0026lt;TypeDescription\u0026gt;isSynthetic())) //通过pluginFinder加载所有的探针扩展，并获取所有可以增强的class .type(pluginFinder.buildMatch()) //按照pluginFinder的实现，去改变字节码增强类 .transform(new Transformer(pluginFinder)) //通过listener订阅增强的操作记录，方便调试 .with(new Listener()) .installOn(instrumentation); try { //加载所有的service实现并启动 ServiceManager.INSTANCE.boot(); } catch (Exception e) { logger.error(e, \u0026quot;Skywalking agent boot failure.\u0026quot;); } agent也提供了非常简单的扩展实现机制，以增强一个普通类的方法为例，首先你需要定义一个切向点：\npublic interface InstanceMethodsInterceptPoint { //定义切向方法的适配器，符合适配器的class将被增强 ElementMatcher\u0026lt;MethodDescription\u0026gt; getMethodsMatcher(); //增强的具体实现类，classReference String getMethodsInterceptor(); //是否重写参数 boolean isOverrideArgs(); } 然后你还需要一个增强的实现类：\npublic interface InstanceMethodsAroundInterceptor { //方法真正执行前执行 void beforeMethod(EnhancedInstance objInst, Method method, Object[] allArguments, Class\u0026lt;?\u0026gt;[] argumentsTypes, MethodInterceptResult result) throws Throwable; //方法真正执行后执行 Object afterMethod(EnhancedInstance objInst, Method method, Object[] allArguments, Class\u0026lt;?\u0026gt;[] argumentsTypes, Object ret) throws Throwable; //当异常发生时执行 void handleMethodException(EnhancedInstance objInst, Method method, Object[] allArguments, Class\u0026lt;?\u0026gt;[] argumentsTypes, Throwable t); } 一般在执行前和执行后进行数据埋点，就可以采集到想要的数据，当然实际编程要稍微复杂一点，不过官方也实现了对应的abstract类和数据埋点工具类，所以探针的二次开发在Skywalking这个级别确实是非常简单，只需要处理好资源占用和并发问题即可。真正的难点是要对需要增强的对象非常了解，熟悉其运作机制，才能找准切向点，既要所有的流程都需要经过这个点，又可以抓取到期望抓取的上下文信息。同时，多版本的适配和测试也是非常大的工作量，官方虽然提供witness的机制（通过验证某个class是否存在来验证版本），但是作为影响全局的探针，开发和测试都是需要慎之又慎的。\nOAP 同agent类似，OAP作为Skywalking最核心的模块，也实现了自己的扩展机制，不过在这里叫做Module，具体可以参考library-module，在module的机制下，Skywalking实现了自己必须核心组件：\n core：整个OAP核心业务（remoting、cluster、storage、analysis、query、alarm）的规范和接口 cluster：集群管理的具体实现 storage：数据容器的具体实现 query：为前端提供的查询接口的具体实现 receiver：接收探针上报数据的接收器的具体实现 alarm：监控告警的具体实现  以及一个可选组件：\n telemetry：用于监控OAP自身的健康状况  而前面提到的OAP的高扩展性则体现在核心业务的规范均定义在了core中，如果有需要自己扩展的，只需要自己单独做自己的实现，而不需要做侵入式的改动，最典型的示例则是官方支持的storage，不仅支持单机demo的内存数据库H2和经典的ES，连目前开源的Tidb都可以接入。\n初步实践 对于Skywalking的实践我们经历了三个阶段\n 线下测试 第一次生产环境小规模测试 第二次生产环境小规模测试+全量接入  线下测试 环境 由于是线下测试，所以我们直接使用物理机（E5-2680v2 x2, 128G)虚拟了一个集群（实际性能相比云服务器应该偏好一些）：\n ES：单机实例，v6.5，4C8G，jvm内存分配为4G OAP：单机实例，v6.1.0-SNAPSHOT，4C8G，jvm内存分配为4G 应用：基于SpringCloud的4个测试实例,调用关系为A-\u0026gt;B-\u0026gt;C-\u0026gt;D，QPS为200  测试结果 拓扑图：\nOAP机器监控：\nES机器监控：\n服务监控面板：\n其中一个调用链记录：\n可以看出，Skywalking非常依赖CPU（不论是OAP还是ES），同时对于网络IO也有一定的要求，至于ES的文件IO在可接受范围内，毕竟确实有大量内容需要持久化。测试结果也基本达到预期要求，调用链和各个指标的监控都工作良好。\n第一次生产环境测试 在线下测试之后，我们再进行了一次基于实际业务针对探针的测试，测试没有发现探针的异常问题，也没有影响业务的正常运作，同时对于jvm实例影响也不是很大，CPU大概提高了5%左右，并不很明显。在这个基础上我们选择了线上的一台服务器，进行了我们第一次生产环境的测试。\n环境  ES：基于现有的一个ES集群，node x 3，v6.0 OAP：2C4G x 2，v6.1.0-SNAPSHOT，jvm内存分配为2G 应用：两个jvm实例  测试时间：03.11-03.16\n测试结果 业务机器负载情况：\n从最敏感的CPU指标上来看，增加agent并没有导致可见的CPU使用率的变化，而其他的内存、网络IO、连接数也基本没有变化。\nOAP负载情况：\n可以看到机器的CPU和网络均有较大的波动，但是也都没有真正打爆服务器，但是我们的实例却经常出现两种日志：\n One trace segment has been abandoned, cause by buffer is full.\n  Collector traceSegment service doesn\u0026rsquo;t response in xxx seconds.\n 通过阅读源码发现：\n agent和OAP只会使用一个长连接阻塞式的交换数据，如果某次数据交换没有得到响应，则会阻塞后续的上报流程（一般长连接的RPC请求会在数据传输期间互相阻塞，但是不会在等待期间互相阻塞，当然这也是源于agent并没有并发上报的机制），所以一旦OAP在接收数据的过程中发生阻塞，就会导致agent本地的缓冲区满，最终只能将监控数据直接丢弃防止内存泄漏  而导致OAP没有及时响应的一方面是OAP本身性能不够（OAP需要承担大量的二次统计工作，通过Jstack统计，长期有超过几十个线程处于RUNNABLE状态，据吴晟描述目前OAP都是高性能模式，后续将会提供配置来支持低性能模式），另一方面可能是ES批量插入效率不够，因此我们修改了OAP的批量插入参数来增加插入频率，降低单次插入数量：\n bulkActions: ${SW_STORAGE_ES_BULK_ACTIONS:2000 -\u0026gt; 20} # Execute the bulk every 2000 requests bulkSize: ${SW_STORAGE_ES_BULK_SIZE:20 -\u0026gt; 2} # flush the bulk every 20mb flushInterval: ${SW_STORAGE_ES_FLUSH_INTERVAL:10 -\u0026gt; 2} # flush the bulk every 10 seconds whatever the number of requests  虽然 service doesn\u0026rsquo;t response 出现的频率明显降低，但是依然还是会偶尔出现，而每一次出现都会伴随大量的 trace segment has been abandoned ，推测OAP和ES可能都存在性能瓶颈（应该进行更进一步的诊断确定问题，不过当时直接和吴晟沟通，确认确实OAP非常消耗CPU资源，考虑到当时部署只是2C，并且还部署有其他业务，就没有进一步的测试）。\n同时，在频繁的数据丢弃过程中，也偶发了一个bug：当agent上报数据超时并且大量丢弃数据之后，即使后续恢复正常也能通过日志看到数据正常上报，在查询界面查询的时候，会查不到这个实例上报的数据，不过在重启OAP和agent之后，之前上报的数据又能查询到，这个也和吴晟沟通过，没有其他的案例，后续想重现却也一直没有成功。\n而同时还发现两个更加严重的问题：\n 我们使用的是线上已经部署好的ES集群，其版本只有6.0，而新的Skywalking使用了6.3的查询特性，导致很多查询执行报错，只能使用最简单的查询 我们的kafka集群版本也非常古老，不支持v1或者更高版本的header，而kafka的探针强依赖header来传输上下文信息，导致kafka客户端直接报错影响业务，所以也立即移除了kafka的探针  在这一次测试中，我们基本确认了agent对于应用的影响，同时也发现了一些我们和Skywalking的一些问题，留待后续测试确认。\n第二次生产环境测试 为了排除性能和ES版本的影响，测试Skywalking本身的可用性，参考吴晟的建议（这也是在最初技术选型的时候没有选择Pinpoint和CAT的部分原因：一方面Skywalking的功能符合我们的要求，更重要的是有更加直接和效率的和项目维护者直接沟通的渠道），所以这一次我们新申请了ES集群和OAP机器。\n环境  ES：腾讯云托管ES集群，4C16G x 3 SSD，v6.4 OAP：16C32G，standalone，jvm分配24G 应用：2~8个jvm实例  测试时间：03.18-至今\n测试结果 OAP负载情况：\nES集群负载：\n测试过程中，我们先接入了一台机器上的两个实例，完全没有遇到一测中的延迟或者数据丢弃的问题，三天后我们又接入了另外两台机器的4个实例，这之后两天我们又接入了另外两台机器的2个实例。依然没有遇到一测中的延迟或者数据丢弃的问题。\n而ES负载的监控也基本验证了一测延迟的问题，Skywalking由于较高的并发插入，对于ES的性能压力很大（批量插入时需要针对每条数据分析并且构建查询索引），大概率是ES批量插入性能不够导致延迟，考虑到我们仅仅接入了8个实例，日均segment插入量大概5000万条（即日均5000万次独立调用），如果想支持更大规模的监控，对于ES容量规划势必要留够足够的冗余。同时OAP和ES集群的网络开销也不容忽视，在支撑大规模的监控时，需要集群并且receiver和aggregattor分离部署来分担网络IO的压力。\n而在磁盘容量占用上，我们设置的原始数据7天过期，目前刚刚开始滚动过期，目前segment索引已经累计了314757240条记录总计158G数据，当然我们目前异常记录较少，如果异常记录较多的话，其磁盘开销将会急剧增加（span中会记录异常堆栈信息）。而由于选择的SSD，磁盘的写入和查询性能都很高，即使只有3个节点，也完全没有任何压力。\n而在新版本的ES集群下，Skywalking的所有查询功能都变得可用，和我们之前自己的单独编写的异常指标监控都能完美对照。当然我们也遇到一个问题：Skywalking仅采集了调用记录，但是对于调用过程中的过程数据，除了异常堆栈其他均没有采集，导致真的出现异常也缺少充足的上下文信息还原现场，于是我们扩展了Skywalking的两个探针（我们项目目前重度依赖的组件）：OkHttp（增加对requestBody和responseBody的采集）和SpringMVC（增加了对requestBody的采集），目前工作正常，如果进一步的增加其他的探针，采集到足够的数据，那么我们基本可以脱离ELK了。\n而OAP方面，CPU和内存的消耗远远低于预期的估计，CPU占用率一直较低，而分配的24G内存也仅使用了10+G，完全可以支持更大规模的接入量，不过在网络IO方面可能存在一定的风险，推测应该8C16G的容器就足以支持十万CPM级别的数据接入。\n当然我们在查询也遇到了一些瓶颈，最大的问题就是无法精确的命中某一条调用记录，就如前面的分析，因为segment的数据结构问题，无法进行面向业务的查询（例如messageId、requestId、orderId等），所以如果想精确匹配某一次调用请求，需要通过各个维度的条件约束慢慢缩小范围最后定位。\nSkywalking展望 通过上述对Skywalking的剖析和实践，Skywalking确实是一个优秀的APM+调用链跟踪监控系统，能够覆盖大部分使用场景，让研发和运维能够更加实时/准实时的了解线上服务的运行情况。当然Skywailking也不是尽善尽美，例如下面就是个人觉得目前可见的不满足我们期望的：\n 数据准实时通过gRPC上报，本地缓存的瓶颈（当然官方主要是为了简化模型，减少依赖，否则Skywalking还依赖ELK就玩得有点大了）  缓存队列的长度，过长占据内存，过短容易buffer满丢弃数据 优雅停机同时又不丢失缓存   数据上报需要在起点上报，链路回传的时候需要携带SPAN及子SPAN的信息，当链路较长或者SPAN保存的信息较多时，会额外消耗一定的带宽 skywalking更多是一个APM系统而不是分布式调用链跟踪系统  在整个链路的探针上均缺少输入输出的抓取 在调用链的筛查上并没用进行增强，并且体现在数据结构的设计，例如TAG信息均保存在SPAN信息中，而SPAN信息均被BASE64编码作为数据保存，无法检索，最终trace的筛查只能通过时间/traceId/service/endPoint/state进行非业务相关的搜索   skywalking缺少对三方接口依赖的指标，这个对于系统稳定往往非常重要  而作为一个初级的使用者，个人觉得我们可以使用有限的人力在以下方向进行扩展：\n 增加receiver：整合ELK，通过日志采集采集数据，降低异构系统的采集开发成本 优化数据结构，提供基于业务关键数据的查询接口 优化探针，采集更多的业务数据，争取代替传统的ELK日志简单查询，绝大部分异常诊断和定位均可以通过Skywalking即可完成 增加业务指标监控的模式，能够自定义业务指标（目前官方已经在实现 Metric Exporter ）  ","excerpt":"APM和调用链跟踪 随着企业经营规模的扩大，以及对内快速诊断效率和对外SLA（服务品质协议，service-level agreement)的追求，对于业务系统的掌控度的要求越来越高，主要体现在：\n  …","ref":"/zh/2019-03-29-introduction-of-skywalking-and-simple-practice/","title":"SkyWalking调研与初步实践"},{"body":"前言 首先描述下问题的背景，博主有个习惯，每天上下班的时候看下skywalking的trace页面的error情况。但是某天突然发现生产环境skywalking页面没有任何数据了，页面也没有显示任何的异常，有点慌，我们线上虽然没有全面铺开对接skywalking，但是也有十多个应用。看了应用agent端日志后，其实也不用太担心，对应用毫无影响。大概情况就是这样，但是问题还是要解决，下面就开始排查skywalking不可用的问题。\n使用到的工具arthas Arthas是阿里巴巴开源的一款在线诊断java应用程序的工具，是greys工具的升级版本，深受开发者喜爱。当你遇到以下类似问题而束手无策时，Arthas可以帮助你解决：\n 这个类从哪个 jar 包加载的？为什么会报各种类相关的 Exception？ 我改的代码为什么没有执行到？难道是我没 commit？分支搞错了？ 遇到问题无法在线上 debug，难道只能通过加日志再重新发布吗？ 线上遇到某个用户的数据处理有问题，但线上同样无法 debug，线下无法重现！ 是否有一个全局视角来查看系统的运行状况？ 有什么办法可以监控到JVM的实时运行状态？ Arthas采用命令行交互模式，同时提供丰富的 Tab 自动补全功能，进一步方便进行问题的定位和诊断。  项目地址：https://github.com/alibaba/arthas\n先定位问题一 查看skywalking-oap-server.log的日志，发现会有一条异常疯狂的在输出，异常详情如下：\n2019-03-01 09:12:11,578 - org.apache.skywalking.oap.server.core.register.worker.RegisterPersistentWorker -3264081149 [DataCarrier.IndicatorPersistentWorker.endpoint_inventory.Consumser.0.Thread] ERROR [] - Validation Failed: 1: id is too long, must be no longer than 512 bytes but was: 684; org.elasticsearch.action.ActionRequestValidationException: Validation Failed: 1: id is too long, must be no longer than 512 bytes but was: 684; at org.elasticsearch.action.ValidateActions.addValidationError(ValidateActions.java:26) ~[elasticsearch-6.3.2.jar:6.3.2] at org.elasticsearch.action.index.IndexRequest.validate(IndexRequest.java:183) ~[elasticsearch-6.3.2.jar:6.3.2] at org.elasticsearch.client.RestHighLevelClient.performRequest(RestHighLevelClient.java:515) ~[elasticsearch-rest-high-level-client-6.3.2.jar:6.3.2] at org.elasticsearch.client.RestHighLevelClient.performRequestAndParseEntity(RestHighLevelClient.java:508) ~[elasticsearch-rest-high-level-client-6.3.2.jar:6.3.2] at org.elasticsearch.client.RestHighLevelClient.index(RestHighLevelClient.java:348) ~[elasticsearch-rest-high-level-client-6.3.2.jar:6.3.2] at org.apache.skywalking.oap.server.library.client.elasticsearch.ElasticSearchClient.forceInsert(ElasticSearchClient.java:141) ~[library-client-6.0.0-alpha.jar:6.0.0-alpha] at org.apache.skywalking.oap.server.storage.plugin.elasticsearch.base.RegisterEsDAO.forceInsert(RegisterEsDAO.java:66) ~[storage-elasticsearch-plugin-6.0.0-alpha.jar:6.0.0-alpha] at org.apache.skywalking.oap.server.core.register.worker.RegisterPersistentWorker.lambda$onWork$0(RegisterPersistentWorker.java:83) ~[server-core-6.0.0-alpha.jar:6.0.0-alpha] at java.util.HashMap$Values.forEach(HashMap.java:981) [?:1.8.0_201] at org.apache.skywalking.oap.server.core.register.worker.RegisterPersistentWorker.onWork(RegisterPersistentWorker.java:74) [server-core-6.0.0-alpha.jar:6.0.0-alpha] at org.apache.skywalking.oap.server.core.register.worker.RegisterPersistentWorker.access$100(RegisterPersistentWorker.java:35) [server-core-6.0.0-alpha.jar:6.0.0-alpha] at org.apache.skywalking.oap.server.core.register.worker.RegisterPersistentWorker$PersistentConsumer.consume(RegisterPersistentWorker.java:120) [server-core-6.0.0-alpha.jar:6.0.0-alpha] at org.apache.skywalking.apm.commons.datacarrier.consumer.ConsumerThread.consume(ConsumerThread.java:101) [apm-datacarrier-6.0.0-alpha.jar:6.0.0-alpha] at org.apache.skywalking.apm.commons.datacarrier.consumer.ConsumerThread.run(ConsumerThread.java:68) [apm-datacarrier-6.0.0-alpha.jar:6.0.0-alpha] 2019-03-01 09:12:11,627 - org.apache.skywalking.oap.server.core.register.worker.RegisterPersistentWorker -3264081198 [DataCarrier.IndicatorPersistentWorker.endpoint_inventory.Consumser.0.Thread] ERROR [] - Validation Failed: 1: id is too long, must be no longer than 512 bytes but was: 684; org.elasticsearch.action.ActionRequestValidationException: Validation Failed: 1: id is too long, must be no longer than 512 bytes but was: 684; at org.elasticsearch.action.ValidateActions.addValidationError(ValidateActions.java:26) ~[elasticsearch-6.3.2.jar:6.3.2] at org.elasticsearch.action.index.IndexRequest.validate(IndexRequest.java:183) ~[elasticsearch-6.3.2.jar:6.3.2] at org.elasticsearch.client.RestHighLevelClient.performRequest(RestHighLevelClient.java:515) ~[elasticsearch-rest-high-level-client-6.3.2.jar:6.3.2] at org.elasticsearch.client.RestHighLevelClient.performRequestAndParseEntity(RestHighLevelClient.java:508) ~[elasticsearch-rest-high-level-client-6.3.2.jar:6.3.2] at org.elasticsearch.client.RestHighLevelClient.index(RestHighLevelClient.java:348) ~[elasticsearch-rest-high-level-client-6.3.2.jar:6.3.2] at org.apache.skywalking.oap.server.library.client.elasticsearch.ElasticSearchClient.forceInsert(ElasticSearchClient.java:141) ~[library-client-6.0.0-alpha.jar:6.0.0-alpha] at org.apache.skywalking.oap.server.storage.plugin.elasticsearch.base.RegisterEsDAO.forceInsert(RegisterEsDAO.java:66) ~[storage-elasticsearch-plugin-6.0.0-alpha.jar:6.0.0-alpha] at org.apache.skywalking.oap.server.core.register.worker.RegisterPersistentWorker.lambda$onWork$0(RegisterPersistentWorker.java:83) ~[server-core-6.0.0-alpha.jar:6.0.0-alpha] at java.util.HashMap$Values.forEach(HashMap.java:981) [?:1.8.0_201] at org.apache.skywalking.oap.server.core.register.worker.RegisterPersistentWorker.onWork(RegisterPersistentWorker.java:74) [server-core-6.0.0-alpha.jar:6.0.0-alpha] at org.apache.skywalking.oap.server.core.register.worker.RegisterPersistentWorker.access$100(RegisterPersistentWorker.java:35) [server-core-6.0.0-alpha.jar:6.0.0-alpha] at org.apache.skywalking.oap.server.core.register.worker.RegisterPersistentWorker$PersistentConsumer.consume(RegisterPersistentWorker.java:120) [server-core-6.0.0-alpha.jar:6.0.0-alpha] at org.apache.skywalking.apm.commons.datacarrier.consumer.ConsumerThread.consume(ConsumerThread.java:101) [apm-datacarrier-6.0.0-alpha.jar:6.0.0-alpha] at org.apache.skywalking.apm.commons.datacarrier.consumer.ConsumerThread.run(ConsumerThread.java:68) [apm-datacarrier-6.0.0-alpha.jar:6.0.0-alpha] 可以看到，上面的异常输出的时间节点，以这种频率在疯狂的刷新。通过异常message，得知到是因为skywalking在写elasticsearch时，索引的id太长了。下面是elasticsearch的源码：\nif (id != null \u0026amp;\u0026amp; id.getBytes(StandardCharsets.UTF_8).length \u0026gt; 512) { validationException = addValidationError(\u0026#34;id is too long, must be no longer than 512 bytes but was: \u0026#34; + id.getBytes(StandardCharsets.UTF_8).length, validationException); } 具体可见：elasticsearch/action/index/IndexRequest.java#L240\n问题一： 通过日志，初步定位是哪个系统的url太长，skywalking在注册url数据时触发elasticsearch针对索引id校验的异常，而skywalking注册失败后会不断的重试，所以才有了上面日志不断刷的现象。\n问题解决： elasticsearch client在写es前通过硬编码的方式写死了索引id的长度不能超过512字节大小。也就是我们不能通过从ES侧找解决方案了。回到异常的message，只能看到提示id太长，并没有写明id具体是什么，这个异常提示其实是不合格的，博主觉得应该把id的具体内容抛出来，问题就简单了。因为异常没有明确提示，系统又比较多，不能十多个系统依次关闭重启来验证到底是哪个系统的哪个url有问题。这个时候Arthas就派上用场了，在不重启应用不开启debug模式下，查看实例中的属性对象。下面通过Arthas找到具体的url。\n从异常中得知，org.elasticsearch.action.index.IndexRequest这个类的validate方法触发的，这个方法是没有入参的，校验的id属性其实是对象本身的属性，那么我们使用Arthas的watch指令来看下这个实例id属性。先介绍下watch的用法：\n功能说明 让你能方便的观察到指定方法的调用情况。能观察到的范围为：返回值、抛出异常、入参，通过编写 \u0008OGNL 表达式进行对应变量的查看。\n参数说明 watch 的参数比较多，主要是因为它能在 4 个不同的场景观察对象\n   参数名称 参数说明     class-pattern 类名表达式匹配   method-pattern 方法名表达式匹配   express 观察表达式   condition-express 条件表达式   [b] 在方法调用之前观察   [e] 在方法异常之后观察   [s] 在方法返回之后观察   [f] 在方法结束之后(正常返回和异常返回)观察   [E] 开启正则表达式匹配，默认为通配符匹配   [x:] 指定输出结果的属性遍历深度，默认为 1    从上面的用法说明结合异常信息，我们得到了如下的指令脚本：\nwatch org.elasticsearch.action.index.IndexRequest validate \u0026ldquo;target\u0026rdquo;\n执行后，就看到了我们希望了解到的内容，如：\n索引id的具体内容看到后，就好办了。我们暂时把定位到的这个应用启动脚本中的的skywalking agent移除后（计划后面重新设计下接口）重启了下系统验证下。果然疯狂输出的日志停住了，但是问题并没完全解决，skywalking页面上的数据还是没有恢复。\n定位问题二 skywalking数据存储使用了elasticsearch，页面没有数据，很有可能是elasticsearch出问题了。查看elasticsearch日志后，发现elasticsearch正在疯狂的GC，日志如：\n: 139939K-\u0026gt;3479K(153344K), 0.0285655 secs] 473293K-\u0026gt;336991K(5225856K), 0.0286918 secs] [Times: user=0.05 sys=0.00, real=0.03 secs] 2019-02-28T20:05:38.276+0800: 3216940.387: Total time for which application threads were stopped: 0.0301495 seconds, Stopping threads took: 0.0001549 seconds 2019-02-28T20:05:38.535+0800: 3216940.646: [GC (Allocation Failure) 2019-02-28T20:05:38.535+0800: 3216940.646: [ParNew Desired survivor size 8716288 bytes, new threshold 6 (max 6) - age 1: 1220136 bytes, 1220136 total - age 2: 158496 bytes, 1378632 total - age 3: 88200 bytes, 1466832 total - age 4: 46240 bytes, 1513072 total - age 5: 126584 bytes, 1639656 total - age 6: 159224 bytes, 1798880 total : 139799K-\u0026gt;3295K(153344K), 0.0261667 secs] 473311K-\u0026gt;336837K(5225856K), 0.0263158 secs] [Times: user=0.06 sys=0.00, real=0.03 secs] 2019-02-28T20:05:38.562+0800: 3216940.673: Total time for which application threads were stopped: 0.0276971 seconds, Stopping threads took: 0.0001030 seconds 2019-02-28T20:05:38.901+0800: 3216941.012: [GC (Allocation Failure) 2019-02-28T20:05:38.901+0800: 3216941.012: [ParNew Desired survivor size 8716288 bytes, new threshold 6 (max 6) 问题二： 查询后得知，elasticsearch的内存配置偏大了，GC时间太长，导致elasticsearch脱离服务了。elasticsearch所在主机的内存是8G的实际内存7.6G,刚开始配置了5G的堆内存大小，可能Full GC的时候耗时太久了。查询elasticsearch官方文档后，得到如下的jvm优化建议：\n 将最小堆大小（Xms）和最大堆大小（Xmx）设置为彼此相等。 Elasticsearch可用的堆越多，它可用于缓存的内存就越多。但请注意，过多的堆可能会使您陷入长时间的垃圾收集暂停。 设置Xmx为不超过物理RAM的50％，以确保有足够的物理RAM用于内核文件系统缓存。 不要设置Xmx为JVM用于压缩对象指针（压缩oops）的截止值之上; 确切的截止值变化但接近32 GB。  详情见：https://www.elastic.co/guide/en/elasticsearch/reference/6.5/heap-size.html\n问题解决： 根据Xmx不超过物理RAM的50％上面的jvm优化建议。后面将Xms和Xmx都设置成了3G。然后先停掉skywalking（由于skywalking中会缓存部分数据，如果直接先停ES，会报索引找不到的类似异常，这个大部分skywalking用户应该有遇到过），清空skywalking缓存目录下的内容，如：\n在重启elasticsearch，接着启动skywalking后页面终于恢复了\n结语 整个问题排查到解决大概花了半天时间，幸好一点也不影响线上应用的使用，这个要得益于skywalking的设计，不然就是大灾难了。然后要感谢下Arthas的技术团队，写了这么好用的一款产品并且开源了，如果没有Arthas，这个问题真的不好定位，甚至一度想到了换掉elasticsearch，采用mysql来解决索引id过长的问题。Arthas真的是线上找问题的利器，博主在Arthas刚面世的时候就关注了，并且一直在公司推广使用，在这里在硬推一波。\n作者简介： 陈凯玲，2016年5月加入凯京科技。曾任职高级研发和项目经理，现任凯京科技研发中心架构\u0026amp;运维部负责人。pmp项目管理认证，阿里云MVP。热爱开源，先后开源过多个热门项目。热爱分享技术点滴，独立博客KL博客（http://www.kailing.pub）博主。\n","excerpt":"前言 首先描述下问题的背景，博主有个习惯，每天上下班的时候看下skywalking的trace页面的error情况。但是某天突然发现生产环境skywalking页面没有任何数据了，页面也没有显示任何的 …","ref":"/zh/2019-03-01-skywalking-troubleshoot/","title":"SkyWalking线上问题排查定位"},{"body":" 作者：王振飞, 写于：2019-02-24 说明：此文是个人所写，版本归属作者，代表个人观点，仅供参考，不代表skywalking官方观点。 说明：本次对比基于skywalking-6.0.0-GA和Pinpoint-1.8.2（截止2019-02-19最新版本）。另外，我们这次技术选型直接否定了Zipkin，其最大原因是它对代码有侵入性，CAT也是一样。这是我们所完全无法接受的。\n 这应该是目前最优秀的两款开源APM产品了，而且两款产品都通过字节码注入的方式，实现了对代码完全无任何侵入，他们的对比信息如下：\nOAP说明: skywalking6.x才有OAP这个概念，skywalking5.x叫collector。\n接下来，对每个PK项进行深入分析和对比。更多精彩和首发内容请关注公众号：【阿飞的博客】。\n社区比较\n这一点上面skywalking肯定完胜。一方面，skywalking已经进入apache孵化，社区相当活跃。而且项目发起人是中国人，我们能够进入官方群（Apache SkyWalking交流群：392443393）和项目发起人吴晟零距离沟通，很多问题能第一时间得到大家的帮助（玩过开源的都知道，这个价值有多大）。 而Pinpoint是韩国人开发的，免不了有沟通障碍。至于github上最近一年的commit频率，skywalking和Pinpoint旗鼓相当，都是接近20的水平: 所以，社区方面，skywalking更胜一筹。\n支持语言比较 Pinpoint只支持Java和PHP，而skywalking支持5种语言：Java, C#, PHP, Node.js, Go。如果公司的服务涉及到多个开发语言，那么skywalking会是你更好的选择。并且，如果你要实现自己的探针（比如python语言），skywalking的二次开发成本也比Pinpoint更低。\n 说明：Github上有开发者为Pinpoint贡献了对Node.js的支持，请戳链接：https://github.com/peaksnail/pinpoint-node-agent。但是已经停止维护，几年没更新了！\n 所以，支持语言方面，skywalking更胜一筹。\n协议比较 SkyWalking支持gRPC和http，不过建议使用gRPC，skywalking6.x版本已经不提供http方式（但是还会保留接收5.x的数据），以后会考虑删除。 而Pinpoint使用的是thrift协议。 协议本身没有谁好谁坏。\n存储比较(重要) 笔者认为，存储是skywalking和Pinpoint最大的差异所在，因为底层存储决定了上层功能。\nPinpoint只支持HBase，且扩展代价较大。这就意味着，如果选择Pinpoint，还要有能力hold住一套HBase集群（daocloud从Pinpoint切换到skywalking就是因为HBase的维护代价有点大）。在这方面，skywalking支持的存储就多很多，这样的话，技术选型时可以根据团队技术特点选择合适的存储，而且还可以自行扩展（不过生产环境上应该大部分是以es存储为主）。\nPinpoint只支持HBase的另一个缺陷就是，HBase本身查询能力有限（HBase只能支持三种方式查询：RowKey精确查找，SCAN范围查找，全表扫描）限制了Pinpoint的查询能力，所以其支持的查询一定是在时间的基础上（Pinpoint通过鼠标圈定一个时间范围后查看这个范围内的Trace信息）。而skywalking可以多个维度任意组合查询，例如：时间范围，服务名，Trace状态，请求路径，TraceId等。\n另外，Pinpoint和skywalking都支持TTL，即历史数据保留策略。skywalking是在OAP模块的application.yml中配置从而指定保留时间。而Pinpoint是通过HBase的ttl功能实现，通过Pinpoint提供的hbase脚本https://github.com/naver/pinpoint/blob/master/hbase/scripts/hbase-create.hbase可以看到：ApplicationTraceIndex配置了TTL =\u0026gt; 5184000，SqlMetaData_Ver2配合了TTL =\u0026gt; 15552000，单位是秒。\n 说明：es并不是完全碾压HBase，es和HBase没有绝对的好和坏。es强在检索能力，存储能力偏弱(千亿以下，es还是完全有能力hold的住的)。HBase强在存储能力，检索能力偏弱。如果搜集的日志量非常庞大，那么es存储就比较吃力。当然，没有蹩脚的中间件，只有蹩脚的程序员，无论是es还是HBase，调优才是最关键的。同样的，如果对检索能力有一定的要求，那么HBase肯定满足不了你。所以，又到了根据你的业务和需求决定的时刻了，trade-off真是无所不在。\n UI比较 Pinpoint的UI确实比skywalking稍微好些，尤其是服务的拓扑图展示。不过daocloud根据Pinpoint的风格为skywalking定制了一款UI。请戳链接：https://github.com/TinyAllen/rocketbot，项目介绍是：rocketbot: A UI for Skywalking。截图如下所示； 所以，只比较原生UI的话，Pinpoint更胜一筹。\n扩展性比较 Pinpoint好像设计之初就没有过多考虑扩展性，无论是底层的存储，还是自定义探针实现等。而skywalking核心设计目标之一就是Pluggable，即可插拔。\n以存储为例，pinpoint完全没有考虑扩展性，而skywalking如果要自定义实现一套存储，只需要定义一个类实现接口org.apache.skywalking.oap.server.library.module.ModuleProvider，然后实现一些DAO即可。至于Pinpoint则完全没有考虑过扩展底层存储。\n再以实现一个自己的探针为例（比如我要实现python语言的探针），Pinpoint选择thrift作为数据传输协议标准，而且为了节省数据传输大小，在传递常量的时候也尽量使用数据参考字典，传递一个数字而不是直接传递字符串等等。这些优化也增加了系统的复杂度：包括使用 Thrift 接口的难度、UDP 数据传输的问题、以及数据常量字典的注册问题等等。Pinpoint发展这么年才支持Java和PHP，可见一斑。而skywalking的数据接口就标准很多，并且支持OpenTracing协议，除了官方支持Java以外，C#、PHP和Node.js的支持都是由社区开发并维护。\n还有后面会提到的告警，skywalking的可扩展性也要远好于Pinpoint。\n最后，Pinpoint和skywalking都支持插件开发，Pinpoint插件开发参考：http://naver.github.io/pinpoint/1.8.2/plugindevguide.html。skywalking插件开发参考：https://github.com/apache/incubator-skywalking/blob/master/docs/en/guides/Java-Plugin-Development-Guide.md。\n所以，扩展性方面skywalking更胜一筹。\n告警比较 Pinpoint和skywalking都支持自定义告警规则。\n但是恼人的是，Pinpoint如果要配置告警规则，还需要安装MySQL(配置告警时的用户，用户组信息以及告警规则都持久化保存在MySQL中)，这就导致Pinpoint的维护成本又高了一些，既要维护HBase又要维护MySQL。\nPinpoint支持的告警规则有：SLOW COUNT|RATE, ERROR COUNT|RATE, TOTAL COUNT, SLOW COUNT|RATE TO CALLEE, ERROR COUNT|RATE TO CALLEE, ERROR RATE TO CALLEE, HEAP USAGE RATE, JVM CPU USAGE RATE, DATASOURCE CONNECTION USAGE RATE。\nPinpoint每3分钟周期性检查过去5分钟的数据，如果有符合规则的告警，就会发送sms/email给用户组下的所有用户。需要说明的是，实现发送sms/email的逻辑需要自己实现，Pinpoint只提供了接口com.navercorp.pinpoint.web.alarm.AlarmMessageSender。并且Pinpoint发现告警持续时，会递增发送sms/email的时间间隔 3min -\u0026gt; 6min -\u0026gt; 12min -\u0026gt; 24min，防止sms/email狂刷。\n Pinpoint告警参考：http://naver.github.io/pinpoint/1.8.2/alarm.html\n skywalking配置告警不需要引入任何其他存储。skywalking在config/alarm-settings.xml中可以配置告警规则，告警规则支持自定义。\nskywalking支持的告警规则（配置项中的名称是indicator-name）有：service_resp_time, service_sla, service_cpm, service_p99, service_p95, service_p90, service_p75, service_p50, service_instance_sla, service_instance_resp_time, service_instance_cpm, endpoint_cpm, endpoint_avg, endpoint_sla, endpoint_p99, endpoint_p95, endpoint_p90, endpoint_p75, endpoint_p50。\nSkywalking通过HttpClient的方式远程调用在配置项webhooks中定义的告警通知服务地址。skywalking也支持silence-period配置，假设在TN这个时间点触发了告警，那么TN -\u0026gt; TN+period 这段时间内不会再重复发送该告警。\n skywalking告警参考：https://github.com/apache/incubator-skywalking/blob/master/docs/en/setup/backend/backend-alarm.md。目前只支持official_analysis.oal脚本中Service, Service Instance, Endpoint scope的metric，其他scope的metric需要等待后续扩展。\n Pinpoint和skywalking都支持常用的告警规则配置，但是skywalking采用webhooks的方式就灵活很多：短信通知，邮件通知，微信通知都是可以支持的。而Pinpoint只能sms/email通知，并且还需要引入MySQL存储，增加了整个系统复杂度。所以，告警方面，skywalking更胜一筹。\nJVM监控 skywalking支持监控：Heap, Non-Heap, GC(YGC和FGC)。 Pinpoint能够监控的指标主要有：Heap, Non-Heap, FGC, DirectBufferMemory, MappedBufferMemory，但是没有YGC。另外，Pinpoint还支持多个指标同一时间点查看的功能。如下图所示：\n所以，对JVM的监控方面，Pinpoint更胜一筹。\n服务监控 包括操作系统，和部署的服务实例的监控。 Pinpoint支持的维度有：CPU使用率，Open File Descriptor，数据源，活动线程数，RT，TPS。 skywalking支持的维度有：CPU使用率，SLA，RT，CPM（Call Per Minutes）。 所以，这方面两者旗鼓相当，没有明显的差距。\n跟踪粒度比较 Pinpoint在这方面做的非常好，跟踪粒度非常细。如下图所示，是Pinpoint对某个接口的trace信息： 而同一个接口skywalking的trace信息如下图所示：  备注: 此截图是skywalking加载了插件apm-spring-annotation-plugin-6.0.0-GA.jar（这个插件允许跟踪加了@Bean, @Service, @Component and @Repository注解的spring context中的bean的方法）。\n 通过对比发现，在跟踪粒度方面，Pinpoint更胜一筹。\n过滤追踪 Pinpoint和skywalking都可以实现，而且配置的表达式都是基于ant风格。 Pinpoint在Web UI上配置 filter wizard 即可自定义过滤追踪。 skywalking通过加载apm-trace-ignore-plugin插件就能自定义过滤跟踪，skywalking这种方式更灵活，比如一台高配服务器上有若干个服务，在共用的agent配置文件apm-trace-ignore-plugin.config中可以配置通用的过滤规则，然后通过-D的方式为每个服务配置个性化过滤。\n所以，在过滤追踪方面，skywalking更胜一筹。\n性能损耗 由于Pinpoint采集信息太过详细，所以，它对性能的损耗最大。而skywalking默认策略比较保守，对性能损耗很小。 有网友做过压力测试，对比如下：\n 图片来源于：https://juejin.im/post/5a7a9e0af265da4e914b46f1\n 所以，在性能损耗方面，skywalking更胜一筹。\n发布包比较 skywalking与时俱进，全系标配jar包，部署只需要执行start.sh脚本即可。而Pinpoint的collector和web还是war包，部署时依赖web容器（比如Tomcat）。拜托，都9012年了。\n所以，在发布包方面，skywalking更胜一筹。\n支持组件比较 skywalking和Pinpoint支持的中间件对比说明：\n WEB容器说明：Pinpoint支持几乎所有的WEB容器，包括开源和商业的。而wkywalking只支持开源的WEB容器，对2款大名鼎鼎的商业WEB容器Weblogic和Wevsphere都不支持。 RPC框架说明：对RPC框架的支持，skywalking简直秒杀Pinpoint。连小众的motan和sofarpc都支持。 MQ说明：skywalking比Pinpoint多支持一个国产的MQ中间件RocketMQ，毕竟RocketMQ在国内名气大，而在国外就一般了。加之skywalking也是国产的。 RDBMS/NoSQL说明：Pinpoint对RDBMS和NoSQL的支持都要略好于skywalking，RDBMS方面，skywalking不支持MSSQL和MariaDB。而NoSQL方面，skywalking不支持Cassandra和HBase。至于Pinpoint不支持的H2，完全不是问题，毕竟生产环境是肯定不会使用H2作为底层存储的。 Redis客户端说明：虽然skywalking和Pinpoint都支持Redis，但是skywalking支持三种流行的Redis客户端：Jedis，Redisson，Lettuce。而Pinpoint只支持Jedis和Lettuce，再一次，韩国人开发的Pinpoint无视了目前中国人开发的GitHub上star最多的Redis Client \u0026ndash; Redisson。 日志框架说明：Pinpoint居然不支持log4j2？但是已经有人开发了相关功能，详情请戳链接：log4j plugin support log4j2 or not? https://github.com/naver/pinpoint/issues/3055  通过对skywalking和Pinpoint支持中间件的对比我们发现，skywalking对国产软件的支持真的是全方位秒杀Pinpoint，比如小众化的RPC框架：motan（微博出品），sofarpc，阿里的RocketMQ，Redis客户端Redisson，以及分布式任务调度框架elastic-job等。当然也从另一方面反应国产开源软件在世界上的影响力还很小。\n这方面没有谁好谁坏，毕竟每个公司使用的技术栈不一样。如果你对RocketMQ有强需求，那么skywalking是你的最佳选择。如果你对es有强需求，那么skywalking也是你的最佳选择。如果HBase是你的强需求，那么Pinpoint就是你的最佳选择。如果MSSQL是你的强需求，那么Pinpoint也是你的最佳选择。总之，这里完全取决你的项目了。\n总结 经过前面对skywalking和Pinpoint全方位对比后我们发现，对于两款非常优秀的APM软件，有一种既生瑜何生亮的感觉。Pinpoint的优势在于：追踪数据粒度非常细、功能强大的用户界面，以及使用HBase作为存储带来的海量存储能力。而skywalking的优势在于：非常活跃的中文社区，支持多种语言的探针，对国产开源软件非常全面的支持，以及使用es作为底层存储带来的强大的检索能力，并且skywalking的扩展性以及定制化要更优于Pinpoint：\n 如果你有海量的日志存储需求，推荐Pinpoint。 如果你更看重二次开发的便捷性，推荐skywalking。  最后，参考上面的对比，结合你的需求，哪些不能妥协，哪些可以舍弃，从而更好的选择一款最适合你的APM软件。\n参考链接  参考[1]. https://github.com/apache/incubator-skywalking/blob/master/docs/en/setup/service-agent/java-agent/Supported-list.md 参考[2]. http://naver.github.io/pinpoint/1.8.2/main.html#supported-modules 参考[3]. https://juejin.im/post/5a7a9e0af265da4e914b46f1    如果觉得本文不错，请关注作者公众号：【阿飞的博客】，多谢！\n ","excerpt":"作者：王振飞, 写于：2019-02-24 说明：此文是个人所写，版本归属作者，代表个人观点，仅供参考，不代表skywalking官方观点。 说明：本次对比基于skywalking-6.0.0-GA …","ref":"/zh/2019-02-24-skywalking-pk-pinpoint/","title":"APM巅峰对决：SkyWalking P.K. Pinpoint"},{"body":"According to Apache Software Foundation branding policy all docker images of Apache Skywalking should be transferred from skywalking to apache with a prefix skywalking-. The transfer details are as follows\n skywalking/base -\u0026gt; apache/skywalking-base skywalking/oap -\u0026gt; apache/skywalking-oap-server skywalking/ui -\u0026gt; apache/skywalking-ui  All of repositories in skywalking will be removed after one week.\n","excerpt":"According to Apache Software Foundation branding policy all docker images of Apache Skywalking …","ref":"/events/transfer-docker-images-to-apache-official-repository/","title":"Transfer Docker Images to Apache Official Repository"},{"body":"6.0.0-GA release. Go to downloads page to find release tars. This is an important milestone version, we recommend all users upgrade to this version.\nKey updates\n Bug fixed Register bug fix, refactor and performance improvement New trace UI  ","excerpt":"6.0.0-GA release. Go to downloads page to find release tars. This is an important milestone version, …","ref":"/events/release-apache-skywalking-apm-6-0-0-ga/","title":"Release Apache SkyWalking APM 6.0.0-GA"},{"body":"Based on his contributions to the project, he has been accepted as SkyWalking PPMC. Welcome aboard.\n","excerpt":"Based on his contributions to the project, he has been accepted as SkyWalking PPMC. Welcome aboard.","ref":"/events/welcome-jian-tan-as-a-new-ppmc/","title":"Welcome Jian Tan as a new PPMC"},{"body":"","excerpt":"","ref":"/tags/performance/","title":"Performance"},{"body":" Author: Hongtao Gao, Apache SkyWalking \u0026amp; ShardingShpere PMC GitHub, Twitter, Linkedin  Service mesh receiver was first introduced in Apache SkyWalking 6.0.0-beta. It is designed to provide a common entrance for receiving telemetry data from service mesh framework, for instance, Istio, Linkerd, Envoy etc. What’s the service mesh? According to Istio’s explain:\nThe term service mesh is used to describe the network of microservices that make up such applications and the interactions between them.\nAs a PMC member of Apache SkyWalking, I tested trace receiver and well understood the performance of collectors in trace scenario. I also would like to figure out the performance of service mesh receiver.\nDifferent between trace and service mesh Following chart presents a typical trace map:\nYou could find a variety of elements in it just like web service, local method, database, cache, MQ and so on. But service mesh only collect service network telemetry data that contains the entrance and exit data of a service for now(more elements will be imported soon, just like Database). A smaller quantity of data is sent to the service mesh receiver than the trace.\nBut using sidecar is a little different.The client requesting “A” that will send a segment to service mesh receiver from “A”’s sidecar. If “A” depends on “B”, another segment will be sent from “A”’s sidecar. But for a trace system, only one segment is received by the collector. The sidecar model splits one segment into small segments, that will increase service mesh receiver network overhead.\nDeployment Architecture In this test, I will pick two different backend deployment. One is called mini unit, consist of one collector and one elasticsearch instance. Another is a standard production cluster, contains three collectors and three elasticsearch instances.\nMini unit is a suitable architecture for dev or test environment. It saves your time and VM resources, speeds up depolyment process.\nThe standard cluster provides good performance and HA for a production scenario. Though you will pay more money and take care of the cluster carefully, the reliability of the cluster will be a good reward to you.\nI pick 8 CPU and 16GB VM to set up the test environment. This test targets the performance of normal usage scenarios, so that choice is reasonable. The cluster is built on Google Kubernetes Engine(GKE), and every node links each other with a VPC network. For running collector is a CPU intensive task, the resource request of collector deployment should be 8 CPU, which means every collector instance occupy a VM node.\nTesting Process Receiving mesh fragments per second(MPS) depends on the following variables.\n Ingress query per second(QPS) The topology of a microservice cluster Service mesh mode(proxy or sidecar)  In this test, I use Bookinfo app as a demo cluster.\nSo every request will touch max 4 nodes. Plus picking the sidecar mode(every request will send two telemetry data), the MPS will be QPS * 4 *2.\nThere are also some important metrics that should be explained\n Client Query Latency: GraphQL API query response time heatmap. Client Mesh Sender: Send mesh segments per second. The total line represents total send amount and the error line is the total number of failed send. Mesh telemetry latency: service mesh receiver handling data heatmap. Mesh telemetry received: received mesh telemetry data per second.  Mini Unit You could find collector can process up to 25k data per second. The CPU usage is about 4 cores. Most of the query latency is less than 50ms. After login the VM on which collector instance running, I know that system load is reaching the limit(max is 8).\nAccording to the previous formula, a single collector instance could process 3k QPS of Bookinfo traffic.\nStandard Cluster Compare to the mini-unit, cluster’s throughput increases linearly. Three instances provide total 80k per second processing power. Query latency increases slightly, but it’s also very small(less than 500ms). I also checked every collector instance system load that all reached the limit. 10k QPS of BookInfo telemetry data could be processed by the cluster.\nConclusion Let’s wrap them up. There are some important things you could get from this test.\n QPS varies by the there variables. The test results in this blog are not important. The user should pick property value according to his system. Collector cluster’s processing power could scale out. The collector is CPU intensive application. So you should provide sufficient CPU resource to it.  This blog gives people a common method to evaluate the throughput of Service Mesh Receiver. Users could use this to design their Apache Skywalking backend deployment architecture.\n","excerpt":"Author: Hongtao Gao, Apache SkyWalking \u0026amp; ShardingShpere PMC GitHub, Twitter, Linkedin  Service …","ref":"/blog/2019-01-25-mesh-loadtest/","title":"SkyWalking performance in Service Mesh scenario"},{"body":"","excerpt":"","ref":"/zh_tags/development/","title":"Development"},{"body":"ps:本文仅写给菜鸟，以及不知道如何远程调试的程序员，并且仅仅适用skywalking的远程调试\n概述 远程调试的目的是为了解决代码或者说程序包部署在服务器上运行，只能通过log来查看问题，以及不能跟在本地IDE运行debug那样查找问题，观看程序运行流程\u0026hellip; 想想当你的程序运行在服务器上，你在本地的IDE随时debug，是不是很爽的感觉。\n好了不废话，切入正题。\n环境篇 IDE：推荐 IntelliJ IDEA\n开发语言: 本文仅限于java，其他语言请自行询问google爸爸或者baidu娘娘\n源代码：自行从github下载，并且确保你运行的skywalking包也源代码的一致，（也就是说你自己从源代码编译打包运行，虽然不一样也可以调试，但是你想想你在本地开发，更改完代码，没有重新运行，debug出现的诡异情况）\n场景篇 假定有如下三台机器\n   IP 用途 备注     10.193.78.1 oap-server skywalking 的oap服务（或者说collector所在的服务器）   10.193.78.2 agent skywalking agent运行所在的服务器   10.193.78.0 IDE 你自己装IDE也就是IntelliJ IDEA的机器    以上环境，场景请自行安装好，并确认正常运行。本文不在赘述\n废话终于说完了\n操作篇 首要条件，下载源码后，先用maven 打包编译。然后使用Idea打开源码的父目录，整体结构大致如下图 1 :agent调试 1)Idea 配置部分 点击Edit Configurations 在弹出窗口中依次找到（红色线框的部分）并点击 打开的界面如下 修改Name值，自己随意，好记即可 然后Host输入10.193.78.2 Port默认或者其他的，重要的是这个端口在10.193.78.2上没有被占用\n然后找到Use module classpath 选择 apm-agent 最终的结果如下： 注意选择目标agent运行的jdk版本，很重要\n然后点击Apply，并找到如下内容，并且复制待用 2）agent配置部分 找到agent配置的脚本，并打开，找到配置agent的地方， 就这个地方，在这个后边加上刚才复制的内容 最终的结果如下 提供一个我配置的weblogic的配置（仅供参考） 然后重启应用（agent）\n3）调试 回到Idea中找到这个地方，并点击debug按钮，你没看错，就是红色圈住的地方 然后控制台如果出现以下字样： 那么恭喜你，可以愉快的加断点调试了。 ps:需要注意的是agent的、 service instance的注册可能不能那么愉快的调试。因为这个注册比较快，而且是在agent启动的时候就发生的， 而远程调试也需要agent打开后才可以调试，所以，如果你手快当我没说这句话。\n2 :oap-server的调试（也就是collector的调试） 具体过程不在赘述，和上一步的agent调试大同小异，不同的是 Use module classpath需要选择oap-server\n","excerpt":"ps:本文仅写给菜鸟，以及不知道如何远程调试的程序员，并且仅仅适用skywalking的远程调试\n概述 远程调试的目的是为了解决代码或者说程序包部署在服务器上运行，只能通过log来查看问题，以及不能跟 …","ref":"/zh/2019-01-24-skywalking-remote-debug/","title":"SkyWalking的远程调试"},{"body":"引言 《SkyWalking Java 插件贡献实践》：本文将基于SkyWalking 6.0.0-GA-SNAPSHOT版本，以编写Redis客户端Lettuce的SkyWalking Java Agent 插件为例，与大家分享我贡献PR的过程，希望对大家了解SkyWalking Java Agent插件有所帮助。\n基础概念 OpenTracing和SkyWalking链路模块几个很重要的语义概念。\n  Span:可理解为一次方法调用，一个程序块的调用，或一次RPC/数据库访问。只要是一个具有完整时间周期的程序访问，都可以被认为是一个span。SkyWalking Span对象中的重要属性\n   属性 名称 备注     component 组件 插件的组件名称，如：Lettuce，详见:ComponentsDefine.Class。   tag 标签 k-v结构，关键标签，key详见：Tags.Class。   peer 对端资源 用于拓扑图，若DB组件，需记录集群信息。   operationName 操作名称 若span=0，operationName将会搜索的下拉列表。   layer 显示 在链路页显示，详见SpanLayer.Class。      Trace:调用链，通过归属于其的Span来隐性的定义。一条Trace可被认为是一个由多个Span组成的有向无环图（DAG图），在SkyWalking链路模块你可以看到，Trace又由多个归属于其的trace segment组成。\n  Trace segment:Segment是SkyWalking中的一个概念，它应该包括单个OS进程中每个请求的所有范围，通常是基于语言的单线程。由多个归属于本线程操作的Span组成。\n  核心API 跨进程ContextCarrier核心API  为了实现分布式跟踪，需要绑定跨进程的跟踪，并且应该传播上下文 整个过程。 这就是ContextCarrier的职责。 以下是实现有关跨进程传播的步骤：  在客户端，创建一个新的空的ContextCarrier，将ContextCarrier所有信息放到HTTP heads、Dubbo attachments 或者Kafka messages。 通过服务调用，将ContextCarrier传递到服务端。 在服务端，在对应组件的heads、attachments或messages获取ContextCarrier所有消息。将服务端和客户端的链路信息绑定。    跨线程ContextSnapshot核心API  除了跨进程，跨线程也是需要支持的，例如异步线程（内存中的消息队列）和批处理在Java中很常见，跨进程和跨线程十分相似，因为都是需要传播 上下文。 唯一的区别是，不需要跨线程序列化。 以下是实现有关跨线程传播的步骤：  使用ContextManager＃capture获取ContextSnapshot对象。 让子线程以任何方式，通过方法参数或由现有参数携带来访问ContextSnapshot。 在子线程中使用ContextManager#continued。    详尽的核心API相关知识，可点击阅读 《插件开发指南-中文版本》\n插件实践 Lettuce操作redis代码 @PostMapping(\u0026#34;/ping\u0026#34;) public String ping(HttpServletRequest request) throws ExecutionException, InterruptedException { RedisClient redisClient = RedisClient.create(\u0026#34;redis://\u0026#34; + \u0026#34;127.0.0.1\u0026#34; + \u0026#34;:6379\u0026#34;); StatefulRedisConnection\u0026lt;String, String\u0026gt; connection0 = redisClient.connect(); RedisAsyncCommands\u0026lt;String, String\u0026gt; asyncCommands0 = connection0.async(); AsyncCommand\u0026lt;String, String, String\u0026gt; future = (AsyncCommand\u0026lt;String, String, String\u0026gt;)asyncCommands0.set(\u0026#34;key_a\u0026#34;, \u0026#34;value_a\u0026#34;); future.onComplete(s -\u0026gt; OkHttpClient.call(\u0026#34;http://skywalking.apache.org\u0026#34;)); future.get(); connection0.close(); redisClient.shutdown(); return \u0026#34;pong\u0026#34;; } 插件源码架构 Lettuce对Redis封装与Redisson Redisson 类似，目的均是实现简单易用，且无学习曲线的Java的Redis客户端。所以要是先对Redis操作的拦截，需要学习对应客户端的源码。\n设计插件 理解插件实现过程，找到最佳InterceptPoint位置是实现插件融入SkyWalking的核心所在。\n代码实现 PR的url：Support lettuce plugin\n实践中遇到的问题  多线程编程使用debug断点会将链路变成同步，建议使用run模式增加log，或者远程debug来解决。 多线程编程，需要使用跨线程ContextSnapshot核心API，否则链路会断裂。 CompleteableCommand.onComplete方法有时会同步执行，这个和内部机制有关，有时候不分离线程。 插件编译版本若为1.7+，需要将插件放到可选插件中。因为sniffer支持的版本是1.6。  插件兼容 为了插件得到插件最终的兼容兼容版本，我们需要使用docker对所有插件版本的测试，具体步骤如下：\n 编写测试用例：关于如何编写测试用例，请按照如何编写文档来实现。 提供自动测试用例。 如：Redisson插件testcase 确保本地几个流行的插件版本，在本地运行起来是和自己的预期是一致的。 在提供自动测试用例并在CI中递交测试后，插件提交者会批准您的插件。 最终得到完整的插件测试报告。  Pull Request 提交PR 提交PR的时候，需要简述自己对插件的设计，这样有助于与社区的贡献者讨论完成codereview。\n申请自动化测试 测试用例编写完成后，可以申请自动化测试，在自己的PR中会生成插件兼容版本的报告。\n插件文档 插件文档需要更新：Supported-list.md相关插件信息的支持。\n插件如果为可选插件需要在agent-optional-plugins可选插件文档中增加对应的描述。\n注释 Lettuce是一个完全无阻塞的Redis客户端，使用netty构建，提供反应，异步和同步数据访问。了解细节可点击阅读 lettuce.io;\nOpenTracing是一个跨编程语言的标准，了解细节可点击阅读 《OpenTracing语义标准》;\nspan:org.apache.skywalking.apm.agent.core.context.trace.AbstractSpan接口定义了所有Span实现需要完成的方法;\nRedisson是一个非常易用Java的Redis客户端， 它没有学习曲线，无需知道任何Redis命令即可开始使用它。了解细节可点击阅读 redisson.org;\n","excerpt":"引言 《SkyWalking Java 插件贡献实践》：本文将基于SkyWalking 6.0.0-GA-SNAPSHOT版本，以编写Redis客户端Lettuce的SkyWalking Java …","ref":"/zh/2019-01-21-agent-plugin-practice/","title":"SkyWalking Java 插件贡献实践"},{"body":"Jinlin Fu has contributed 4 new plugins, including gson, activemq, rabbitmq and canal, which made SkyWalking supporting all mainstream OSS MQ. Also provide several documents and bug fixes. The SkyWalking PPMC based on these, promote him as new committer. Welcome on board.\n","excerpt":"Jinlin Fu has contributed 4 new plugins, including gson, activemq, rabbitmq and canal, which made …","ref":"/events/welcome-jinlin-fu-as-new-committer/","title":"Welcome Jinlin Fu as new committer"},{"body":" 作者：赵瑞栋 原文地址  引言 微服务框架落地后，分布式部署架构带来的问题就会迅速凸显出来。服务之间的相互调用过程中，如果业务出现错误或者异常，如何快速定位问题？如何跟踪业务调用链路？如何分析解决业务瓶颈？\u0026hellip;本文我们来看看如何解决以上问题。\n一、SkyWalking初探 Skywalking 简介 Skywalking是一款国内开源的应用性能监控工具，支持对分布式系统的监控、跟踪和诊断。\n它提供了如下的主要功能特性： Skywalking 技术架构 SW总体可以分为四部分：\n1.Skywalking Agent：使用Javaagent做字节码植入，无侵入式的收集，并通过HTTP或者gRPC方式发送数据到Skywalking Collector。\nSkywalking Collector ：链路数据收集器，对agent传过来的数据进行整合分析处理并落入相关的数据存储中。 Storage：Skywalking的存储，时间更迭，sw已经开发迭代到了6.x版本，在6.x版本中支持以ElasticSearch、Mysql、TiDB、H2、作为存储介质进行数据存储。 UI ：Web可视化平台，用来展示落地的数据。  Skywalking Agent配置 通过了解配置，可以对一个组件功能有一个大致的了解。让我们一起看一下skywalking的相关配置。\n解压开skywalking的压缩包，在agent/config文件夹中可以看到agent的配置文件。\n从skywalking支持环境变量配置加载，在启动的时候优先读取环境变量中的相关配置。\n agent.namespace: 跨进程链路中的header，不同的namespace会导致跨进程的链路中断 agent.service_name:一个服务（项目）的唯一标识，这个字段决定了在sw的UI上的关于service的展示名称 agent.sample_n_per_3_secs: 客户端采样率，默认是-1代表全采样 agent.authentication: 与collector进行通信的安全认证，需要同collector中配置相同 agent.ignore_suffix: 忽略特定请求后缀的trace collecttor.backend_service: agent需要同collector进行数据传输的IP和端口 logging.level: agent记录日志级别  skywalking agent使用javaagent无侵入式的配合collector实现对分布式系统的追踪和相关数据的上下文传递。\nSkywalking Collector关键配置 Collector支持集群部署，zookeeper、kubernetes（如果你的应用是部署在容器中的）、consul（GO语言开发的服务发现工具）是sw可选的集群管理工具，结合大家具体的部署方式进行选择。详细配置大家可以去Skywalking官网下载介质包进行了解。\nCollector端口设置\n downsampling: 采样汇总统计维度，会分别按照分钟、【小时、天、月】（可选）来统计各项指标数据。 通过设置TTL相关配置项可以对数据进行自动清理。  Skywalking 在6.X中简化了配置。collector提供了gRPC和HTTP两种通信方式。\nUI使用rest http通信，agent在大多数场景下使用grpc方式通信，在语言不支持的情况下会使用http通信。\n关于绑定IP和端口需要注意的一点是，通过绑定IP，agent和collector必须配置对应ip才可以正常通信。\nCollector存储配置\n在application.yml中配置的storage模块配置中选择要使用的数据库类型，并填写相关的配置信息。\nCollector Receiver\nReceiver是Skywalking在6.x提出的新的概念，负责从被监控的系统中接受指标数据。用户完全可以参照OpenTracing规范来上传自定义的监控数据。Skywalking官方提供了service-mesh、istio、zipkin的相关能力。\n现在Skywalking支持服务端采样，配置项为sampleRate，比例采样，如果配置为5000则采样率就是50%。\n关于采样设置的一点注意事项\n关于服务采样配置的一点建议，如果Collector以集群方式部署，比如：Acollector和Bcollector，建议Acollector.sampleRate = Bcollector.sampleRate。如果采样率设置不相同可能会出现数据丢失问题。\n假设Agent端将所有数据发送到后端Collector处，A采样率设置为30%，B采样率为50%。\n假设有30%的数据，发送到A上，这些数据被全部正确接受并存储，极端情况（与期望的采样数据量相同）下，如果剩下20%待采样的数据发送到了B，这个时候一切都是正常的，如果这20%中有一部分数据被送到了A那么，这些数据将是被忽略的，由此就会造成数据丢失。\n二、业务调用链路监控 Service Topology监控 调用链路监控可以从两个角度去看待。我们先从整体上来认识一下我们所监控的系统。\n通过给服务添加探针并产生实际的调用之后，我们可以通过Skywalking的前端UI查看服务之间的调用关系。\n我们简单模拟一次服务之间的调用。新建两个服务，service-provider以及service-consumer，服务之间简单的通过Feign Client 来模拟远程调用。\n从图中可以看到:\n 有两个服务节点：provider \u0026amp; consumer 有一个数据库节点：localhost【mysql】 一个注册中心节点  consumer消费了provider提供出来的接口。\n一个系统的拓扑图让我们清晰的认识到系统之间的应用的依赖关系以及当前状态下的业务流转流程。细心的可能发现图示节点consumer上有一部分是红色的，红色是什么意思呢？\n红色代表当前流经consumer节点的请求有一断时间内是响应异常的。当节点全部变红的时候证明服务现阶段内就彻底不可用了。运维人员可以通过Topology迅速发现某一个服务潜在的问题，并进行下一步的排查并做到预防。\nSkywalking Trace监控 Skywalking通过业务调用监控进行依赖分析，提供给我们了服务之间的服务调用拓扑关系、以及针对每个endpoint的trace记录。\n我们在之前看到consumer节点服务中发生了错误，让我们一起来定位下错误是发生在了什么地方又是什么原因呢？\n在每一条trace的信息中都可以看到当前请求的时间、GloableId、以及请求被调用的时间。我们分别看一看正确的调用和异常的调用。\nTrace调用链路监控 图示展示的是一次正常的响应，这条响应总耗时19ms，它有4个span：\n span1 /getStore = 19ms 响应的总流转时间 span2 /demo2/stores = 14ms feign client 开始调用远程服务后的响应的总时间 span3 /stores = 14ms 接口服务响应总时间 span4 Mysql = 1ms 服务提供端查询数据库的时间  这里span2和span3的时间表现相同，其实是不同的，因为这里时间取了整。\n在每个Span中可以查看当前Span的相关属性。\n 组件类型: SpringMVC、Feign Span状态: false HttpMethod: GET Url: http://192.168.16.125:10002/demo2/stores  这是一次正常的请求调用Trace日志，可能我们并不关心正常的时候，毕竟一切正常不就是我们期待的么！\n我们再来看下，异常状态下我们的Trace以及Span又是什么样的呢。\n发生错误的调用链中Span中的is error标识变为true，并且在名为Logs的TAB中可以看到错误发生的具体原因。根据异常情况我们就可以轻松定位到影响业务的具体原因，从而快速定位问题，解决问题。\n通过Log我们看到连接被拒，那么可能是我们的网络出现了问题（可能性小，因为实际情况如果网络出现问题我们连这个trace都看不到了），也有可能是服务端配置问题无法正确建立连接。通过异常日志，我们迅速就找到了问题的关键。\n实际情况是，我把服务方停掉了，做了一次简单的模拟。可见，通过拓扑图示我们可以清晰的看到众多服务中哪个服务是出现了问题的，通过trace日志我们可以很快就定位到问题所在，在最短的时间内解决问题。\n三、服务性能指标监控 Skywalking还可以查看具体Service的性能指标，根据相关的性能指标可以分析系统的瓶颈所在并提出优化方案。\nSkywalking 性能监控 在服务调用拓扑图上点击相应的节点我们可以看到该服务的\n SLA: 服务可用性（主要是通过请求成功与失败次数来计算） CPM: 每分钟调用次数 Avg Response Time: 平均响应时间  从应用整体外部来看我们可以监测到应用在一定时间段内的\n 服务可用性指标SLA 每分钟平均响应数 平均响应时间 服务进程PID 服务所在物理机的IP、HostName、Operation System  Service JVM信息监控 还可以监控到Service运行时的CPU、堆内存、非堆内存使用率、以及GC情况。这些信息来源于JVM。注意这里的数据可不是机器本身的数据。\n四、服务告警 前文我们提到了通过查看拓扑图以及调用链路可以定位问题，可是运维人员又不可能一直盯着这些数据，那么我们就需要告警能力，在异常达到一定阈值的时候主动的提示我们去查看系统状态。\n在Sywalking 6.x版本中新增了对服务状态的告警能力。它通过webhook的方式让我们可以自定义我们告警信息的通知方式。诸如:邮件通知、微信通知、短信通知等。\nSkywalking 服务告警 先来看一下告警的规则配置。在alarm-settings.xml中可以配置告警规则，告警规则支持自定义。\n一份告警配置由以下几部分组成：\n service_resp_time_rule：告警规则名称 ***_rule （规则名称可以自定义但是必须以’_rule’结尾 indicator-name：指标数据名称： 定义参见http://t.cn/EGhfbmd op: 操作符： \u0026gt; , \u0026lt; , = 【当然你可以自己扩展开发其他的操作符】 threshold：目标值：指标数据的目标数据 如sample中的1000就是服务响应时间，配合上操作符就是大于1000ms的服务响应 period: 告警检查周期：多久检查一次当前的指标数据是否符合告警规则 counts: 达到告警阈值的次数 silence-period：忽略相同告警信息的周期 message：告警信息 webhooks：服务告警通知服务地址  Skywalking通过HttpClient的方式远程调用在配置项webhooks中定义的告警通知服务地址。\n了解了SW所传送的数据格式我们就可以对告警信息进行接收处理，实现我们需要的告警通知服务啦！\n我们将一个服务停掉，并将另外一个服务的某个对外暴露的接口让他休眠一定的时间。然后调用一定的次数观察服务的状态信息以及告警情况。\n总结 本文简单的通过skwaylking的配置来对skywlaking的功能进行一次初步的了解，对skwaylking新提出的概念以及新功能进行简单的诠释，方便大家了解和使用。通过使用APM工具，可以让我们方便的查看微服务架构中系统瓶颈以及性能问题等。\n精选提问 问1：想问问选型的时候用pinpoint还是SK好？\n答：选型问题\n 要结合具体的业务场景， 比如你的代码运行环境 是java、php、net还是什么。 pinpoint在安装部署上要比skywalking略微复杂 pinpoint和sw支持的组件列表是不同的。 https://github.com/apache/incubator-skywalking/blob/master/docs/en/setup/service-agent/java-agent/Supported-list.md你可以参照这里的支持列表对比下pinpoint的支持对象做一个简单对比。 sw经过测试在并发量较高的情况下比pinpoint的吞吐量更好一些。  问2：有没有指标统计，比如某个url 的top10 请求、响应最慢的10个请求？某个服务在整个链条中的耗时占比？\n答：1.sw自带有响应最慢的请求top10统计针对所有的endpoint的统计。 2.针对每个url的top10统计，sw本身没有做统计，数据都是现成的通过简单的检索就可以搜到你想要的结果。 3.没有具体的耗时占比，但是有具体总链路时间统计以及某个服务的耗时统计，至于占比自己算吧，可以看ppt中的调用链路监控的span时间解释。\n问3：能不能具体说一下在你们系统中的应用？\n答：EOS8LA版本中，我们整合sw对应用提供拓扑、调用链路、性能指标的监控、并在sw数据的基础上增加系统的维度。 当服务数很庞大的时候，整体的拓扑其实就是一张密密麻麻的蜘蛛网。我们可以通过系统来选择具体某个系统下的应用。 8LA中SW是5.0.0alpha版本，受限于sw功能，我们并没有提供告警能力，这在之后会是我们的考虑目标。\n问4：业务访问日志大概每天100G，kubernetes 环境中部署，使用稳定吗？\n答：监控数据没有长时间的存储必要，除非你有特定的需求。它有一定的时效性，你可以设置ttl自动清除过时信息。100g，es集群还是能轻松支撑的。\n问5：和pinpoint相比有什么优势吗？\n答：\n 部署方式、使用方式简单 功能特性支持的更多 高并发性能会更好一些  问6：skywalking的侵入式追踪功能方便进行单服务链的服务追踪。但是跨多台服务器多项目的整体服务链追踪是否有整体设计考虑？\n答：sw本身特性就是对分布式系统的追踪，他是无侵入式的。无关你的应用部署在多少台服务器上。\n问7：应用在加上代理之后性能会下降。请问您有什么解决方法吗？\n答：性能下降是在所难免的，但是据我了解，以及官方的测试，他的性能影响是很低的。这是sw的测试数据供你参考。 https://skywalkingtest.github.io/Agent-Benchmarks/README_zh.html。\n问8：有异构系统需求的话可以用sw吗？\n答：只要skywalking的探针支持的应该都是可以的。\n问9：sw对于商用的web中间件，如bes、tongweb、websphere、weblogic的支持如何？\n答：商业组件支持的比较少，因为涉及到相关license的问题，sw项目组需要获得他们的支持来进行数据上报，据我了解，支持不是很好。\n","excerpt":"作者：赵瑞栋 原文地址  引言 微服务框架落地后，分布式部署架构带来的问题就会迅速凸显出来。服务之间的相互调用过程中，如果业务出现错误或者异常，如何快速定位问题？如何跟踪业务调用链路？如何分析解决业务 …","ref":"/zh/2019-01-03-monitor-microservice/","title":"SkyWalking 微服务监控分析"},{"body":"","excerpt":"","ref":"/zh_tags/elasticsearch/","title":"ElasticSearch"},{"body":"SkyWalking 依赖 elasticsearch 集群，如果 elasticsearch 安装有 x-pack 插件的话，那么就会存在一个 Basic 认证，导致 skywalking 无法调用 elasticsearch, 解决方法是使用 nginx 做代理，让 nginx 来做这个 Basic 认证，那么这个问题就自然解决了。\n方法如下:\n 安装 nginx   yum install -y nginx\n 配置 nginx  server { listen 9200 default_server; server_name _; location / { proxy_set_header Host $host; proxy_set_header X-Real-IP $remote_addr; proxy_set_header X-Forwarded-For $proxy_add_x_forwarded_for; proxy_pass http://localhost:9200; #Basic字符串就是使用你的用户名(admin),密码(12345)编码后的值 #注意:在进行Basic加密的时候要使用如下格式如:admin:123456 注意中间有个冒号 proxy_set_header Authorization \u0026#34;Basic YWRtaW4gMTIzNDU2\u0026#34;; } } 验证   curl localhost:9200\n { \u0026#34;name\u0026#34; : \u0026#34;Yd0rCp9\u0026#34;, \u0026#34;cluster_name\u0026#34; : \u0026#34;es-cn-4590xv9md0009doky\u0026#34;, \u0026#34;cluster_uuid\u0026#34; : \u0026#34;jAPLrqY5R6KWWgHnGCWOAA\u0026#34;, \u0026#34;version\u0026#34; : { \u0026#34;number\u0026#34; : \u0026#34;6.3.2\u0026#34;, \u0026#34;build_flavor\u0026#34; : \u0026#34;default\u0026#34;, \u0026#34;build_type\u0026#34; : \u0026#34;tar\u0026#34;, \u0026#34;build_hash\u0026#34; : \u0026#34;053779d\u0026#34;, \u0026#34;build_date\u0026#34; : \u0026#34;2018-07-20T05:20:23.451332Z\u0026#34;, \u0026#34;build_snapshot\u0026#34; : false, \u0026#34;lucene_version\u0026#34; : \u0026#34;7.3.1\u0026#34;, \u0026#34;minimum_wire_compatibility_version\u0026#34; : \u0026#34;5.6.0\u0026#34;, \u0026#34;minimum_index_compatibility_version\u0026#34; : \u0026#34;5.0.0\u0026#34; }, \u0026#34;tagline\u0026#34; : \u0026#34;You Know, for Search\u0026#34; } 看到如上结果那么恭喜你成功了。\n","excerpt":"SkyWalking 依赖 elasticsearch 集群，如果 elasticsearch 安装有 x-pack 插件的话，那么就会存在一个 Basic 认证，导致 skywalking …","ref":"/zh/2019-01-02-skywalking-elasticsearch-basic/","title":"关于 ElastiSsearch 因 basic 认证导致 SkyWalking 无法正常调用接口问题"},{"body":" 作者: Wu Sheng, tetrate, SkyWalking original creator GitHub, Twitter, Linkedin 翻译: jjlu521016  背景 在当前的微服务架构中分布式链路追踪是很有必要的一部分，但是对于一些用户来说如何去理解和使用分布式链路追踪的相关数据是不清楚的。 这个博客概述了典型的分布式跟踪用例，以及Skywalking的V6版本中新的可视化功能。我们希望新的用户通过这些示例来更好的理解。\n指标和拓扑图 跟踪数据支持两个众所周知的分析特性：指标和拓扑图\n指标: 每个service, service instance, endpoint的指标都是从跟踪中的入口span派生的。指标代表响应时间的性能。所以可以有一个平均响应时间，99%的响应时间，成功率等。它们按service, service instance, endpoint进行分解。\n拓扑图: 拓扑表示服务之间的链接，是分布式跟踪最有吸引力的特性。拓扑结构允许所有用户理解分布式服务关系和依赖关系，即使它们是不同的或复杂的。这一点很重要，因为它为所有相关方提供了一个单一的视图，无论他们是开发人员、设计者还是操作者。\n这里有一个拓扑图的例子包含了4个项目，包括kafka和两个外部依赖。\n-在skywalking的可选择UI0RocketBot的拓扑图-\nTrace 在分布式链路追踪系统中，我们花费大量资源（CPU、内存、磁盘和网络）来生成、传输和持久跟踪数据。让我们试着回答为什么要这样做？我们可以用跟踪数据回答哪些典型的诊断和系统性能问题？\nSkywalking v6包含两种追踪视图:\n   TreeMode: 第一次提供,帮助您更容易识别问题。    ListMode: 常规的时间线视图，通常也出现在其他跟踪系统中，如Zipkin。    发生错误 在trace视图，最简单的部分是定位错误，可能是由代码异常或网络故障引起的。通过span详情提供的细节，ListMode和TreeMode都能够找到错误 -ListMode 错误span-\n-TreeMode 错误span-\n慢span 一个高优先级的特性是识别跟踪中最慢的span。这将使用应用程序代理捕获的执行持续时间。在旧的ListMode跟踪视图中，由于嵌套，父span几乎总是包括子span的持续时间。换句话说，一个缓慢的span通常会导致它的父节点也变慢，在Skywalking 6中，我们提供了 最慢的前5个span 过滤器来帮助你您直接定位span。\n-最慢的前5个span-\n太多子span 在某些情况下，个别持续时间很快，但跟踪速度仍然很慢，如： -没有慢span的追踪-\n如果要了解根问题是否与太多操作相关，请使用子范围号的Top 5 of children span number,筛选器显示每个span的子级数量，突出显示前5个。 -13个数据库访问相关的span-\n在这个截图中，有一个包含13个子项的span，这些子项都是数据库访问。另外，当您看到跟踪的概述时，这个2000ms跟踪的数据库花费了1380ms。 -1380ms花费在数据库访问-\n在本例中，根本原因是数据库访问太多。这在其他场景中也很常见，比如太多的RPC或缓存访问。\n链路深度 跟踪深度也与延迟有关。像太多子span的场景一样，每个span延迟看起来不错，但整个链路追踪的过程很慢。 -链路深度-\n上图所示,最慢的span小鱼500ms,对于2000毫秒的跟踪来说，速度并不太慢。当您看到第一行时，有四种不同的颜色表示这个分布式跟踪中涉及的四个services。每一个都需要100~400ms，这四个都需要近2000ms，从这里我们知道这个缓慢的跟踪是由一个序列中的3个RPC造成的。\n结束语 分布式链路追踪和APM 工具帮助我们确定造成问题的根源，允许开发和操作团队进行相应的优化。我们希望您喜欢这一点，并且喜欢Apache Skywalking和我们的新链路追踪可视化界面。如果你喜欢的话，在github上面给我们加start来鼓励我们\nSkywakling 6计划在2019年的1月底完成release。您可以通过以下渠道联系项目团队成员\n 关注 skywalking推特 订阅邮件:dev@skywalking.apache.org。发送邮件到 dev-subscribe@kywalking.apache.org 来订阅. 加入Gitter聊天室  ","excerpt":"作者: Wu Sheng, tetrate, SkyWalking original creator GitHub, Twitter, Linkedin 翻译: jjlu521016  背景 在当前的 …","ref":"/zh/2019-01-02-understand-trace-trans2cn/","title":"更容易理解将要到来的分布式链路追踪 6.0GA (翻译)"},{"body":"Background Distributed tracing is a necessary part of modern microservices architecture, but how to understand or use distributed tracing data is unclear to some end users. This blog overviews typical distributed tracing use cases with new visualization features in SkyWalking v6. We hope new users will understand more through these examples.\nMetric and topology Trace data underpins in two well known analysis features: metric and topology\nMetric of each service, service instance, endpoint are derived from entry spans in trace. Metrics represent response time performance. So, you could have average response time, 99% response time, success rate, etc. These are broken down by service, service instance, endpoint.\nTopology represents links between services and is distributed tracing\u0026rsquo;s most attractive feature. Topologies allows all users to understand distributed service relationships and dependencies even when they are varied or complex. This is important as it brings a single view to all interested parties, regardless of if they are a developer, designer or operator.\nHere\u0026rsquo;s an example topology of 4 projects, including Kafka and two outside dependencies.\nTopology in SkyWalking optional UI, RocketBot\nTrace In a distributed tracing system, we spend a lot of resources(CPU, Memory, Disk and Network) to generate, transport and persistent trace data. Let\u0026rsquo;s try to answer why we do this? What are the typical diagnosis and system performance questions we can answer with trace data?\nSkyWalking v6 includes two trace views:\n TreeMode: The first time provided. Help you easier to identify issues. ListMode: Traditional view in time line, also usually seen in other tracing system, such as Zipkin.  Error occurred In the trace view, the easiest part is locating the error, possibly caused by a code exception or network fault. Both ListMode and TreeMode can identify errors, while the span detail screen provides details.\nListMode error span\nTreeMode error span\nSlow span A high priority feature is identifying the slowest spans in a trace. This uses execution duration captured by application agents. In the old ListMode trace view, parent span almost always includes the child span\u0026rsquo;s duration, due to nesting. In other words, a slow span usually causes its parent to also become slow. In SkyWalking 6, we provide Top 5 of slow span filter to help you locate the spans directly.\nTop 5 slow span\nThe above screenshot highlights the top 5 slow spans, excluding child span duration. Also, this shows all spans' execution time, which helps identify the slowest ones.\nToo many child spans In some cases, individual durations are quick, but the trace is still slow, like this one:\nTrace with no slow span\nTo understand if the root problem is related to too many operations, use Top 5 of children span number. This filter shows the amount of children each span has, highlighting the top 5.\n13 database accesses of a span\nIn this screenshot, there is a span with 13 children, which are all Database accesses. Also, when you see overview of trace, database cost 1380ms of this 2000ms trace.\n1380ms database accesses\nIn this example, the root cause is too many database accesses. This is also typical in other scenarios like too many RPCs or cache accesses.\nTrace depth Trace depth is also related latency. Like the too many child spans scenario, each span latency looks good, but the whole trace is slow.\nTrace depth\nHere, the slowest spans are less than 500ms, which are not too slow for a 2000ms trace. When you see the first line, there are four different colors representing four services involved in this distributed trace. Every one of them costs 100~400ms. For all four, there nearly 2000ms. From here, we know this slow trace is caused by 3 RPCs in a serial sequence.\nAt the end Distributed tracing and APM tools help users identify root causes, allowing development and operation teams to optimize accordingly. We hope you enjoyed this, and love Apache SkyWalking and our new trace visualization. If so, give us a star on GitHub to encourage us.\nSkyWalking 6 is scheduled to release at the end of January 2019. You can contact the project team through the following channels:\n Follow SkyWalking twitter Subscribe mailing list: dev@skywalking.apache.org . Send to dev-subscribe@kywalking.apache.org to subscribe the mail list. Join Gitter room.  ","excerpt":"Background Distributed tracing is a necessary part of modern microservices architecture, but how to …","ref":"/blog/2019-01-01-understand-trace/","title":"Understand distributed trace easier in the incoming 6-GA"},{"body":"6.0.0-beta release. Go to downloads page to find release tars.\nKey updates\n Bugs fixed, closed to GA New protocols provided, old still compatible. Spring 5 supported MySQL and TiDB as optional storage  ","excerpt":"6.0.0-beta release. Go to downloads page to find release tars.\nKey updates\n Bugs fixed, closed to GA …","ref":"/events/release-apache-skywalking-apm-6-0-0-beta/","title":"Release Apache SkyWalking APM 6.0.0-beta"},{"body":"Based on his contributions. Including created RocketBot as our secondary UI, new website and very cool trace view page in next release. he has been accepted as SkyWalking PPMC. Welcome aboard.\n","excerpt":"Based on his contributions. Including created RocketBot as our secondary UI, new website and very …","ref":"/events/welcome-yao-wang-as-a-new-ppmc/","title":"Welcome Yao Wang as a new PPMC"},{"body":"导读  SkyWalking 中 Java 探针是使用 JavaAgent 的两大字节码操作工具之一的 Byte Buddy（另外是 Javassist）实现的。项目还包含.Net core 和 Nodejs 自动探针，以及 Service Mesh Istio 的监控。总体上，SkyWalking 是一个多语言，多场景的适配，特别为微服务、云原生和基于容器架构设计的可观测性分析平台（Observability Analysis Platform）。 本文基于 SkyWalking 5.0.0-RC2 和 Byte Buddy 1.7.9 版本，会从以下几个章节，让大家掌握 SkyWalking Java 探针的使用，进而让 SkyWalking 在自己公司中的二次开发变得触手可及。  Byte Buddy 实现 JavaAgent 项目 迭代 JavaAgent 项目的方法论 SkyWalking agent 项目如何 Debug SkyWalking 插件开发实践   文章底部有 SkyWalking 和 Byte Buddy 相应的学习资源。  Byte Buddy 实现  首先如果你对 JavaAgent 还不是很了解可以先百度一下，或在公众号内看下《JavaAgent 原理与实践》简单入门下。 SpringMVC 分发请求的关键方法相信已经不用我在赘述了，那我们来编写 Byte Buddy JavaAgent 代码吧。  public class AgentMain { public static void premain(String agentOps, Instrumentation instrumentation) { new AgentBuilder.Default() .type(ElementMatchers.named(\u0026#34;org.springframework.web.servlet.DispatcherServlet\u0026#34;)) .transform((builder, type, classLoader, module) -\u0026gt; builder.method(ElementMatchers.named(\u0026#34;doDispatch\u0026#34;)) .intercept(MethodDelegation.to(DoDispatchInterceptor.class))) .installOn(instrumentation); } }  编写 DispatcherServlet doDispatch 拦截器代码（是不是跟 AOP 如出一辙）  public class DoDispatchInterceptor { @RuntimeType public static Object intercept(@Argument(0) HttpServletRequest request, @SuperCall Callable\u0026lt;?\u0026gt; callable) { final StringBuilder in = new StringBuilder(); if (request.getParameterMap() != null \u0026amp;\u0026amp; request.getParameterMap().size() \u0026gt; 0) { request.getParameterMap().keySet().forEach(key -\u0026gt; in.append(\u0026#34;key=\u0026#34; + key + \u0026#34;_value=\u0026#34; + request.getParameter(key) + \u0026#34;,\u0026#34;)); } long agentStart = System.currentTimeMillis(); try { return callable.call(); } catch (Exception e) { System.out.println(\u0026#34;Exception :\u0026#34; + e.getMessage()); return null; } finally { System.out.println(\u0026#34;path:\u0026#34; + request.getRequestURI() + \u0026#34; 入参:\u0026#34; + in + \u0026#34; 耗时:\u0026#34; + (System.currentTimeMillis() - agentStart)); } } }  resources/META-INF/MANIFEST.MF  Manifest-Version: 1.0 Premain-Class: com.z.test.agent.AgentMain Can-Redefine-Classes: true  pom.xml 文件  dependencies +net.bytebuddy.byte-buddy +javax.servlet.javax.servlet-api *scope=provided plugins +maven-jar-plugin *manifestFile=src/main/resources/META-INF/MANIFEST.MF +maven-shade-plugin *include:net.bytebuddy:byte-buddy:jar: +maven-compiler-plugin  小结：没几十行代码就完成了，通过 Byte Buddy 实现应用组件 SpringMVC 记录请求路径、入参、执行时间 JavaAgent 项目，是不是觉得自己很优秀。  持续迭代 JavaAgent  本章节主要介绍 JavaAgent 如何 Debug，以及持续集成的方法论。 首先我的 JavaAgent 项目目录结构如图所示: 应用项目是用几行代码实现的 SpringBootWeb 项目:  @SpringBootApplication(scanBasePackages = {\u0026#34;com\u0026#34;}) public class TestBootWeb { public static void main(String[] args) { SpringApplication.run(TestBootWeb.class, args); } @RestController public class ApiController { @PostMapping(\u0026#34;/ping\u0026#34;) public String ping(HttpServletRequest request) { return \u0026#34;pong\u0026#34;; } } }  下面是关键 JavaAgent 项目如何持续迭代与集成:  VM options增加:-JavaAgent:{$HOME}/Code/github/z_my_test/test-agent/target/test-agent-1.0-SNAPSHOT.jar=args Before launch 在Build之前增加： Working directory:{$HOME}/Code/github/incubator-skywalking Command line:-T 1C -pl test-agent -am clean package -Denforcer.skip=true -Dmaven.test.skip=true -Dmaven.compile.fork=true  小结：看到这里的将 JavaAgent 持续迭代集成方法，是不是瞬间觉得自己手心已经发痒起来，很想编写一个自己的 agent 项目了呢，等等还有一个好消息:test-demo 这 10 几行的代码实现的 Web 服务，居然有 5k 左右的类可以使用 agent 增强。 注意 mvn 编译加速的命令是 maven3 + 版本以上才支持的哈。  SkyWalking Debug  峰回路转，到了文章的主题《SkyWalking 之高级用法》的正文啦。首先，JavaAgent 项目想 Debug，还需要将 agent 代码与接入 agent 项目至少在同一个工作空间内，网上方法有很多，这里我推荐大家一个最简单的方法。File-\u0026gt;New-\u0026gt;Module from Exisiting Sources… 引入 skywalking-agent 源码即可 详细的 idea 编辑器配置： 优化 SkyWalking agent 编译时间，我的集成时间优化到 30 秒左右：  VM options增加:-JavaAgent:-JavaAgent:{$HOME}/Code/github/incubator-skywalking/skywalking-agent/skywalking-agent.jar：不要用dist里面的skywalking-agent.jar，具体原因大家可以看看源码：apm-sniffer/apm-agent/pom.xml中的maven插件的使用。 Before launch 在Build之前增加： Working directory:{$HOME}/Code/github/incubator-skywalking Command line:-T 1C -pl apm-sniffer/apm-sdk-plugin -amd clean package -Denforcer.skip=true -Dmaven.test.skip=true -Dmaven.compile.fork=true： 这里我针对插件包，因为紧接着下文要开发插件 另外根pom注释maven-checkstyle-plugin也可加速编译 kob 之 SkyWalking 插件编写  kob（贝壳分布式作业调度框架）是贝壳找房项目微服务集群中的基础组件，通过编写贝壳分布式作业调度框架的 SkyWalking 插件，可以实时收集作业调度任务的执行链路信息，从而及时得到基础组件的稳定性，了解细节可点击阅读《贝壳分布式调度框架简介》。想详细了解 SkyWalking 插件编写可在文章底部参考链接中，跳转至对应的官方资源，好话不多说，代码一把唆起来。 apm-sdk-plugin pom.xml 增加自己的插件 model  \u0026lt;artifactId\u0026gt;apm-sdk-plugin\u0026lt;/artifactId\u0026gt; \u0026lt;modules\u0026gt; \u0026lt;module\u0026gt;kob-plugin\u0026lt;/module\u0026gt; ... \u0026lt;modules\u0026gt;  resources.skywalking-plugin.def 增加自己的描述  kob=org.apache.skywalking.apm.plugin.kob.KobInstrumentation  在 SkyWalking 的项目中，通过继承 ClassInstanceMethodsEnhancePluginDefine 可以定义需要拦截的类和增强的方法，编写作业调度方法的 instrumentation  public class KobInstrumentation extends ClassInstanceMethodsEnhancePluginDefine { private static final String ENHANCE_CLASS = \u0026#34;com.ke.kob.client.spring.core.TaskDispatcher\u0026#34;; private static final String INTERCEPT_CLASS = \u0026#34;org.apache.skywalking.apm.plugin.kob.KobInterceptor\u0026#34;; @Override protected ClassMatch enhanceClass() { return NameMatch.byName(ENHANCE_CLASS); } @Override protected ConstructorInterceptPoint[] getConstructorsInterceptPoints() { return null; } @Override protected InstanceMethodsInterceptPoint[] getInstanceMethodsInterceptPoints() { return new InstanceMethodsInterceptPoint[] { new InstanceMethodsInterceptPoint() { @Override public ElementMatcher\u0026lt;MethodDescription\u0026gt; getMethodsMatcher() { return named(\u0026#34;dispatcher1\u0026#34;); } @Override public String getMethodsInterceptor() { return INTERCEPT_CLASS; } @Override public boolean isOverrideArgs() { return false; } } }; } }  通过实现 InstanceMethodsAroundInterceptor 后，定义 beforeMethod、afterMethod 和 handleMethodException 的实现方法，可以环绕增强指定目标方法，下面自定义 interceptor 实现 span 的跟踪（这里需要注意 SkyWalking 中 span 的生命周期，在 afterMethod 方法中结束 span）  public class KobInterceptor implements InstanceMethodsAroundInterceptor { @Override public void beforeMethod(EnhancedInstance objInst, Method method, Object[] allArguments, Class\u0026lt;?\u0026gt;[] argumentsTypes, MethodInterceptResult result) throws Throwable { final ContextCarrier contextCarrier = new ContextCarrier(); com.ke.kob.client.spring.model.TaskContext context = (TaskContext) allArguments[0]; CarrierItem next = contextCarrier.items(); while (next.hasNext()) { next = next.next(); next.setHeadValue(JSON.toJSONString(context.getUserParam())); } AbstractSpan span = ContextManager.createEntrySpan(\u0026#34;client:\u0026#34;+allArguments[1]+\u0026#34;,task:\u0026#34;+context.getTaskKey(), contextCarrier); span.setComponent(ComponentsDefine.TRANSPORT_CLIENT); SpanLayer.asRPCFramework(span); } @Override public Object afterMethod(EnhancedInstance objInst, Method method, Object[] allArguments, Class\u0026lt;?\u0026gt;[] argumentsTypes, Object ret) throws Throwable { ContextManager.stopSpan(); return ret; } @Override public void handleMethodException(EnhancedInstance objInst, Method method, Object[] allArguments, Class\u0026lt;?\u0026gt;[] argumentsTypes, Throwable t) { } }  实现效果，将操作名改成任务执行节点 + 任务执行方法，实现 kob 的 SkyWalking 的插件编写，加上报警体系，可以进一步增加公司基础组件的稳定性。  参考链接  Apache SkyWalking Byte Buddy（runtime code generation for the Java virtual machine）  ","excerpt":"导读  SkyWalking 中 Java 探针是使用 JavaAgent 的两大字节码操作工具之一的 Byte Buddy（另外是 Javassist）实现的。项目还包含.Net core …","ref":"/zh/2018-12-21-skywalking-apm-sniffer-beginning/","title":"SkyWalking apm-sniffer 原理学习与插件编写"},{"body":"搭建调试环境 阅读 SkyWalking 源码，从配置调试环境开始。\n一定一定一定不要干读代码，而是通过调试的方式。\n 01 通过 Skywalking-5.x 版本的源码构建并运行 👉：哔哩哔哩 | 腾讯视频 02 通过 Skywalking-6.x 版本的源码构建并运行 👉：哔哩哔哩 | 腾讯视频 03 Java 应用（探针）接入 Skywalking[6.x] 👉：哔哩哔哩 | 腾讯视频  SkyWalking 3.X 源码解析合集 虽然是基于 3.X 版本的源码解析，但是对于阅读 SkyWalking Java Agent 和插件部分，同样适用。\n对于 SkyWalking Collector 部分，可以作为一定的参考。\n 《SkyWalking 源码分析 —— 调试环境搭建》 《SkyWalking 源码分析 —— Agent 初始化》 《SkyWalking 源码分析 —— Agent 插件体系》 《SkyWalking 源码分析 —— Collector 初始化》 《SkyWalking 源码分析 —— Collector Cluster 集群管理》 《SkyWalking 源码分析 —— Collector Client Component 客户端组件》 《SkyWalking 源码分析 —— Collector Server Component 服务器组件》 《SkyWalking 源码分析 —— Collector Jetty Server Manager》 《SkyWalking 源码分析 —— Collector gRPC Server Manager》 《SkyWalking 源码分析 —— Collector Naming Server 命名服务》 《SkyWalking 源码分析 —— Collector Queue 队列组件》 《SkyWalking 源码分析 —— Collector Storage 存储组件》 《SkyWalking 源码分析 —— Collector Streaming Computing 流式处理（一）》 《SkyWalking 源码分析 —— Collector Streaming Computing 流式处理（二）》 《SkyWalking 源码分析 —— Collector Cache 缓存组件》 《SkyWalking 源码分析 —— Collector Remote 远程通信服务》 《SkyWalking 源码分析 —— DataCarrier 异步处理库》 《SkyWalking 源码分析 —— Agent Remote 远程通信服务》 《SkyWalking 源码分析 —— 应用于应用实例的注册》 《SkyWalking 源码分析 —— Agent DictionaryManager 字典管理》 《SkyWalking 源码分析 —— Agent 收集 Trace 数据》 《SkyWalking 源码分析 —— Agent 发送 Trace 数据》 《SkyWalking 源码分析 —— Collector 接收 Trace 数据》 《SkyWalking 源码分析 —— Collector 存储 Trace 数据》 《SkyWalking 源码分析 —— JVM 指标的收集与存储》 《SkyWalking 源码分析 —— 运维界面（一）之应用视角》 《SkyWalking 源码分析 —— 运维界面（二）之应用实例视角》 《SkyWalking 源码分析 —— 运维界面（三）之链路追踪视角》 《SkyWalking 源码分析 —— 运维界面（四）之操作视角》 《SkyWalking 源码分析 —— @Trace 注解想要追踪的任何方法》 《SkyWalking 源码分析 —— traceId 集成到日志组件》 《SkyWalking 源码分析 —— Agent 插件（一）之 Tomcat》 《SkyWalking 源码分析 —— Agent 插件（二）之 Dubbo》 《SkyWalking 源码分析 —— Agent 插件（三）之 SpringMVC》 《SkyWalking 源码分析 —— Agent 插件（四）之 MongoDB》  SkyWalking 6.X 源码解析合集  《SkyWalking 6.x 源码分析 —— 调试环境搭建》  ","excerpt":"搭建调试环境 阅读 SkyWalking 源码，从配置调试环境开始。\n一定一定一定不要干读代码，而是通过调试的方式。\n 01 通过 Skywalking-5.x 版本的源码构建并运行 👉：哔哩哔哩 | …","ref":"/zh/2018-12-21-skywalking-source-code-read/","title":"SkyWalking 源码解析合集"},{"body":"","excerpt":"","ref":"/zh_tags/source-code/","title":"Source Code"},{"body":"版本选择 我们采用的是 5.0.0-RC2 的版本，SkyWalking 的版本信息可以参考 https://github.com/apache/incubator-skywalking/blob/5.x/CHANGES.md\n那么为什么我们没有采用 5.1.0 版本呢，这是因为我们公司内部需要支持 es x-pack，但是在官方发布里面，没有支持 xpack 的版本。\n在 Apache SkyWalking 官方文档 https://github.com/CharlesMaster/incubator-skywalking/tree/master/docs/others/cn 中有提到，SkyWalking 5.x 仍受社区支持。\n对于用户计划从 5.x 升级到 6.x，您应该知道关于有一些概念的定义的变更。最重要的两个改变了的概念是：\n Application（在 5.x 中）更改为 Service（在 6.x 中），Application Instance 也更改为 Service Instance。 Service（在 5.x 中）更改为 Endpoint（在 6.x 中）。  图文详解 Apache SkyWalking 的监控界面由 Monitor 和 Trace 两者构成，Monitor 菜单又包括 Dashbord、Topology、Application、Service、Alarm 五个子菜单构成。本文就是围绕这些菜单分别逐一进行介绍。\nMonitor 当用户通过 SkyWalking 登陆界面使用用户名、密码登陆以后，就会默认进入到 SkyWalking 的 Monitor 下的 Dashboard 界面\nDashboard 下图就是用户登陆之后都会看到的关键 Dashboard 页面，在这个页面的下方的关键指标，图中都做了详细的解释。\n上图中 app 需要强调的是，52 个 app 并不代表 52 个应用，比如 paycenter 有两台 paycenter1 和 paycenter2 就算了 2 个 app，当然还有一些应用是 3 个以上的。在我们公司，paycenter1、paycenter2 这些运维都和我们跳板机管理平台上的名称设置的一样，约定大于配置，开发人员可以更加便捷的排查问题。\n 再次修正一下，关于 dashboard 页面的 app 数，语言类探针，是探针的 app_code 来决定的。比如我们公司的线上配置就是 agent.application_code=auth-center-1\n 上图中需要解释两个概念：\n cpm 代表每分钟请求次数 SLA=(TRANSACTION_CALLS- TRANSACTION_ERROR_CALLS ) * 10000 ) / TRANSACTION_CALLS  该页面主要支持四个跳转：\n一、在上图中，App 板块上的帮助选项是可以直接跳转到 Application 监控页面的。 二、 Service 板块上的帮助选项是可以直接跳转到 Service 监控页面的。\n三、 Slow Service 列表中的每一个慢服务点击以后都会进入到其专项的 Service 监控页面。\n四、 Application Throughput 列表中的每一个 Application 点击以后也都是可以进入到其专项的 Application 监控页面。\n 关于 Application 和 Service 的详细介绍我们后续会展开\n 在 Dashboard 的页面上部分，还有一个选择功能模块： 左侧部分可以定期 refresh Dashboard 的数据，右侧则可以调整整体的查询区间。\nTopology 点击 Monitor 菜单下的 Topology 你会看到下面这张拓扑图\n当然这张图太过于夸张了，如果接入 SkyWalking 的应用并不是很多，会如下图所示： 左侧的三个小按钮可以调整你的视图，支持拖拽。右侧可以输入你所关心的应用名。比如我们输入一个支付和订单两个应用，左侧的拓扑图会变得更加清晰：\n另外，上图中的绿色圆圈都是可以点击的，如果你点击以后，还会出现节点信息： Application 点击 Monitor 菜单下的 Application 你会看到下面这张图，这张图里你可以看到的东西都做了注解。\n这张图里有一个惊喜，就是如果你点开 More Server Details，你可以看到更多的信息\n是的，除了 Host、IPv4、Pid、OS 以外，你还可以看到 CPU、Heap、Non-Heap、GC（Young GC、Old GC）等详细监控信息。\nService 点击 Monitor 菜单下的 Service 你会看到下面这张图，这张图里你可以看到的同样都做了注解。 关于 Dependency Map 这张图我们再补充一下，鼠标悬停可以看到每个阶段的执行时间，这是 Service 下的功能 我们点开图中该图中 Top 20 Slow Traces 下面的被我马赛克掉的 trace 的按钮框，可以看到如下更加详细的信息：\n这些信息可以帮助我们知道每一个方法在哪个阶段那个具体实现耗时了多久。\n如上图所示，每一行基本都是可以打开的，每一行都包含了 Tags、Logs 等监控内容\nAlarm 点击 Monitor 菜单下的 Alarm 你会看到告警菜单。目前 5.X 版本的还没有接入邮件、短信等告警方式，后续 6 支持 webhook，用户可以自己去接短信和邮件。\n告警内容中你可以看到 Applicaion、Server 和 Service 三个层面的告警内容\nTrace Trace 是一个非常实用的功能，用户可以根据精确的 TraceId 去查找\n也可以设定时间段去查找\n我在写使用手册时候，非常巧的是，看到了上图三起异常，于是我们往下拉列表看到了具体的数据\n点击进去，我们可以看到具体的失败原因 当然用户也可以直接将 Trace State 调整为 Error 级别进行查询\n再回顾一遍 一、首先我们进入首页：\n二、点击一下首页的 Slow Service 的 projectC，可以看到如下信息：\n三、如果点击首页的 Appliation Throughput 中的 projectD，可以看到如下信息：\n四、继续点进去右下角的这个 slow service 里的 Consumer，我们可以看到下图：\n参考资料  https://twitter.com/AsfSkyWalking/status/1013616673218179072 https://twitter.com/AsfSkyWalking/status/1013617100143800320  ","excerpt":"版本选择 我们采用的是 5.0.0-RC2 的版本，SkyWalking …","ref":"/zh/2018-12-18-apache-skywalking-5-0-userguide/","title":"Apache SkyWalking 5.0 中文版图文详解使用手册"},{"body":"","excerpt":"","ref":"/zh_tags/web-ui/","title":"Web UI"},{"body":"Based on his contributions to the project, he has been accepted as SkyWalking committer. Welcome aboard.\n","excerpt":"Based on his contributions to the project, he has been accepted as SkyWalking committer. Welcome …","ref":"/events/welcome-yixiong-cao-as-a-new-committer/","title":"Welcome Yixiong Cao as a new committer"},{"body":"Original link, Tetrate.io blog\nContext The integration of SkyWalking and Istio Service Mesh yields an essential open-source tool for resolving the chaos created by the proliferation of siloed, cloud-based services.\nApache SkyWalking is an open, modern performance management tool for distributed services, designed especially for microservices, cloud native and container-based (Docker, K8s, Mesos) architectures. We at Tetrate believe it is going to be an important project for understanding the performance of microservices. The recently released v6 integrates with Istio Service Mesh and focuses on metrics and tracing. It natively understands the most common language runtimes (Java, .Net, and NodeJS). With its new core code, SkyWalking v6 also supports Istrio telemetry data formats, providing consistent analysis, persistence, and visualization.\nSkyWalking has evolved into an Observability Analysis Platform that enables observation and monitoring of hundreds of services all at once. It promises solutions for some of the trickiest problems faced by system administrators using complex arrays of abundant services: Identifying why and where a request is slow, distinguishing normal from deviant system performance, comparing apples-to-apples metrics across apps regardless of programming language, and attaining a complete and meaningful view of performance.\nSkyWalking History Launched in China by Wu Sheng in 2015, SkyWalking started as just a distributed tracing system, like Zipkin, but with auto instrumentation from a Java agent. This enabled JVM users to see distributed traces without any change to their source code. In the last two years, it has been used for research and production by more than 50 companies. With its expanded capabilities, we expect to see it adopted more globally.\nWhat\u0026rsquo;s new Service Mesh Integration Istio has picked up a lot of steam as the framework of choice for distributed services. Based on all the interest in the Istio project, and community feedback, some SkyWalking (P)PMC members decided to integrate with Istio Service Mesh to move SkyWalking to a higher level.\nSo now you can use Skywalking to get metrics and understand the topology of your applications. This works not just for Java, .NET and Node using our language agents, but also for microservices running under the Istio service mesh. You can get a full topology of both kinds of applications.\nObservability analysis platform With its roots in tracing, SkyWalking is now transitioning into an open-standards based Observability Analysis Platform, which means the following:\n It can accept different kinds and formats of telemetry data from mesh like Istio telemetry. Its agents support various popular software technologies and frameworks like Tomcat, Spring, Kafka. The whole supported framework list is here. It can accept data from other compliant sources like Zipkin-formatted traces reported from Zipkin, Jaeger, or OpenCensus clients.  SkyWalking is logically split into four parts: Probes, Platform Backend, Storage and UI:\nThere are two kinds of probes:\n Language agents or SDKs following SkyWalking across-thread propagation formats and trace formats, run in the user’s application process. The Istio mixer adaptor, which collects telemetry from the Service Mesh.  The platform backend provides gRPC and RESTful HTTP endpoints for all SkyWalking-supported trace and metric telemetry data. For example, you can stream these metrics into an analysis system.\nStorage supports multiple implementations such as ElasticSearch, H2 (alpha), MySQL, and Apache ShardingSphere for MySQL Cluster. TiDB will be supported in next release.\nSkyWalking’s built-in UI with a GraphQL endpoint for data allows intuitive, customizable integration.\nSome examples of SkyWalking’s UI:\n Observe a Spring app using the SkyWalking JVM-agent   Observe on Istio without any agent, no matter what langugage the service is written in   See fine-grained metrics like request/Call per Minute, P99/95/90/75/50 latency, avg response time, heatmap   Service dependencies and metrics  Service Focused At Tetrate, we are focused on discovery, reliability, and security of your running services. This is why we are embracing Skywalking, which makes service performance observable.\nBehind this admittedly cool UI, the aggregation logic is very easy to understand, making it easy to customize SkyWalking in its Observability Analysis Language (OAL) script.\nWe’ll post more about OAL for developers looking to customize SkyWalking, and you can read the official OAL introduction document.\nScripts are based on three core concepts:\n  Service represents a group of workloads that provide the same behaviours for incoming requests. You can define the service name whether you are using instrument agents or SDKs. Otherwise, SkyWalking uses the name you defined in the underlying platform, such as Istio.\n  Service Instance Each workload in the Service group is called an instance. Like Pods in Kubernetes, it doesn\u0026rsquo;t need to be a single OS process. If you are using an instrument agent, an instance does map to one OS process.\n  Endpoint is a path in a certain service that handles incoming requests, such as HTTP paths or a gRPC service + method. Mesh telemetry and trace data are formatted as source objects (aka scope). These are the input for the aggregation, with the script describing how to aggregate, including input, conditions, and the resulting metric name.\n  Core Features The other core features in SkyWalking v6 are:\n Service, service instance, endpoint metrics analysis. Consistent visualization in Service Mesh and no mesh. Topology discovery, Service dependency analysis. Distributed tracing. Slow services and endpoints detected. Alarms.  Of course, SkyWalking has some more upgrades from v5, such as:\n ElasticSearch 6 as storage is supported. H2 storage implementor is back. Kubernetes cluster management is provided. You don’t need Zookeeper to keep the backend running in cluster mode. Totally new alarm core. Easier configuration. More cloud native style. MySQL will be supported in the next release.  Please: Test and Provide Feedback! We would love everyone to try to test our new version. You can find everything you need in our Apache repository,read the document for further details. You can contact the project team through the following channels:\n Submit an issue on GitHub repository Mailing list: dev@skywalking.apache.org . Send to dev-subscribe@kywalking.apache.org to subscribe the mail list. Gitter Project twitter  Oh, and one last thing! If you like our project, don\u0026rsquo;t forget to give us a star on GitHub.\n","excerpt":"Original link, Tetrate.io blog\nContext The integration of SkyWalking and Istio Service Mesh yields …","ref":"/blog/2018-12-12-skywalking-service-mesh-ready/","title":"SkyWalking v6 is Service Mesh ready"},{"body":"Based on his contributions to the project, he has been accepted as SkyWalking committer. Welcome aboard.\n","excerpt":"Based on his contributions to the project, he has been accepted as SkyWalking committer. Welcome …","ref":"/events/welcome-jian-tan-as-a-new-committer/","title":"Welcome Jian Tan as a new committer"},{"body":"APM consistently compatible in language agent(Java, .Net, NodeJS), 3rd party format(Zipkin) and service mesh telemetry(Istio). Go to downloads page to find release tars.\n","excerpt":"APM consistently compatible in language agent(Java, .Net, NodeJS), 3rd party format(Zipkin) and …","ref":"/events/release-apache-skywalking-6-0-0-alpha/","title":"Release Apache SkyWalking 6.0.0-alpha"},{"body":"A stable version of 5.x release. Go to downloads page to find release tars.\n","excerpt":"A stable version of 5.x release. Go to downloads page to find release tars.","ref":"/events/release-apache-skywalking-5-0-0-ga/","title":"Release Apache SkyWalking 5.0.0-GA"},{"body":"5.0.0-RC2 release. Go to downloads page to find release tars.\n","excerpt":"5.0.0-RC2 release. Go to downloads page to find release tars.","ref":"/events/release-apache-skywalking-5-0-0-rc2/","title":"Release Apache SkyWalking 5.0.0-RC2"},{"body":"5.0.0-beta2 release. Go to downloads page to find release tars.\n","excerpt":"5.0.0-beta2 release. Go to downloads page to find release tars.","ref":"/events/release-apache-skywalking-5-0-0-beta2/","title":"Release Apache SkyWalking 5.0.0-beta2"},{"body":"Translated by Sheng Wu.\nIn many big systems, distributed and especially microservice architectures become more and more popular. With the increase of modules and services, one incoming request could cross dozens of service. How to pinpoint the issues of the online system, and the bottleneck of the whole distributed system? This became a very important problem, which must be resolved.\nTo resolve the problems in distributed system, Google published the paper “Dapper, a Large-Scale Distributed Systems Tracing Infrastructure”, which mentioned the designs and ideas of building a distributed system. Many projects are inspired by it, created in the last 10 years. At 2015, Apache SkyWalking was created by Wu Sheng as a simple distributed system at first and open source. Through almost 3 years developments, at 2018, according to its 5.0.0-alpha/beta releases, it had already became a cool open source APM system for cloud native, container based system.\nAt the early of this year, I was trying to build the Butterfly open source APM in .NET Core, and that is when I met the Apache SkyWalking team and its creator. I decided to join them, and cooperate with them, to provide .NET Core agent native compatible with SkyWalking. At April, I released the first version .NET core agent 0.1.0. After several weeks interation, we released 0.2.0, for increasing the stability and adding HttpClient, Database driver supports.\nBefore we used .NET Core agent, we need to deploy SkyWalking collector, UI and ElasticSearch 5.x. You can download the release versions at here: http://skywalking.apache.org/downloads/ and follow the docs (Deploy-backend-in-standalone-mode, Deploy-backend-in-cluster-mode) to setup the backend.\nAt here, I are giving a quick start to represent, how to monitor a demo distributed .NET Core applications. I can say, that is easy.\n git clone https://github.com/OpenSkywalking/skywalking-netcore.git\n  cd skywalking-netcore\n  dotnet restore\n  dotnet run -p sample/SkyWalking.Sample.Backend dotnet run -p sample/SkyWalking.Sample.Frontend\n Now you can open http://localhost:5001/api/values to access the demo application. Then you can open SkyWalking WebUI http://localhost:8080\n  Overview of the whole distributed system   Topology of distributed system   Application view   Trace query   Span’s tags, logs and related traces   GitHub  Website: http://skywalking.apache.org/ SkyWalking Github Repo: https://github.com/apache/incubator-skywalking SkyWalking-NetCore Github Repo: https://github.com/OpenSkywalking/skywalking-netcore  ","excerpt":"Translated by Sheng Wu.\nIn many big systems, distributed and especially microservice architectures …","ref":"/blog/2018-05-24-skywalking-net/","title":"Apache SkyWalking provides open source APM and distributed tracing in .NET Core field"},{"body":"在大型网站系统设计中，随着分布式架构，特别是微服务架构的流行，我们将系统解耦成更小的单元，通过不断的添加新的、小的模块或者重用已经有的模块来构建复杂的系统。随着模块的不断增多，一次请求可能会涉及到十几个甚至几十个服务的协同处理，那么如何准确快速的定位到线上故障和性能瓶颈，便成为我们不得不面对的棘手问题。\n为解决分布式架构中复杂的服务定位和性能问题，Google 在论文《Dapper, a Large-Scale Distributed Systems Tracing Infrastructure》中提出了分布式跟踪系统的设计和构建思路。在这样的背景下，Apache SkyWalking 创建于 2015 年，参考 Dapper 论文实现分布式追踪功能，并逐渐进化为一个完整功能的 Application Performance Management 系统，用于追踪、监控和诊断大型分布式系统，尤其是容器和云原生下的微服务系统。\n今年初我在尝试使用.NET Core 构建分布式追踪系统 Butterfly 时接触到 SkyWalking 团队，开始和 SkyWalking 团队合作探索 SkyWalking 对.NET Core 的支持，并于 4 月发布 SkyWalking .NET Core 探针的 第一个版本，同时我也有幸加入 SkyWalking 团队共同进行 SkyWalking 在多语言生态的推动。在.NET Core 探针 v0.1 版本发布之后，得到了一些同学的尝鲜使用，也得到诸多改进的建议。经过几周的迭代，SkyWalking .NET Core 探针于今天发布 v0.2 release，在 v0.1 的基础上增加了\u0008稳定性和 HttpClient 及数据库驱动的追踪支持。\n在使用 SkyWalking 对.NET Core 应用追踪之前，我们需要先部署 SkyWalking Collector 收集分析 Trace 和 Elasticsearch 作为 Trace 数据存储。SkyWalking 支持 5.x 的 ES，所以我们需要下载安装对应版本的 ES，并配置 ES 的 cluster.name 为 CollectorDBCluster。然后部署 SkyWalking 5.0 beta 或更高版本 (下载地址:http://skywalking.apache.org/downloads/)。更详细的 Collector 部署文档，请参考 Deploy-backend-in-standalone-mode 和 Deploy-backend-in-cluster-mode。\n最后我们使用示例项目来演示在.NET Core 应用中使用 SkyWalking 进行追踪和监控，克隆 SkyWalking-NetCore 项目到本地：\ngit clone https://github.com/OpenSkywalking/skywalking-netcore.git 进入 skywalking-netcore 目录：\ncd skywalking-netcore 还原 nuget package：\ndotnet restore 启动示例项目：\ndotnet run -p sample/SkyWalking.Sample.Backend dotnet run -p sample/SkyWalking.Sample.Frontend 访问示例应用：\n打开 SkyWalking WebUI 即可看到我们的应用监控面板 http://localhost:8080\nDashboard 视图\nTopologyMap 视图\nApplication 视图\nTrace 视图\nTraceDetails 视图\nGitHub  SkyWalking Github Repo：https://github.com/apache/incubator-skywalking SkyWalking-NetCore Github Repo：https://github.com/OpenSkywalking/skywalking-netcore  ","excerpt":"在大型网站系统设计中，随着分布式架构，特别是微服务架构的流行，我们将系统解耦成更小的单元，通过不断的添加新的、小的模块或者重用已经有的模块来构建复杂的系统。随着模块的不断增多，一次请求可能会涉及到十几 …","ref":"/zh/2018-05-24-skywalking-net/","title":"Apache SkyWalking 为.NET Core带来开箱即用的分布式追踪和应用性能监控"},{"body":"","excerpt":"","ref":"/zh_tags/dotnetcore/","title":"DotNetCore"},{"body":"","excerpt":"","ref":"/tags/dotnetcore/","title":"DotNetCore"},{"body":"5.0.0-beta release. Go to downloads page to find release tars.\n","excerpt":"5.0.0-beta release. Go to downloads page to find release tars.","ref":"/events/release-apache-skywalking-5-0-0-beta/","title":"Release Apache SkyWalking 5.0.0-beta"},{"body":"5.0.0-alpha release. Go to downloads page to find release tars.\n","excerpt":"5.0.0-alpha release. Go to downloads page to find release tars.","ref":"/events/release-apache-skywalking-apm-5-0-0-alpha/","title":"Release Apache SkyWalking APM 5.0.0-alpha"},{"body":"","excerpt":"","ref":"/index.json","title":""},{"body":"  #td-cover-block-0 { background-image: url(/home_background_hu0af614851632d061c3c80a153395694b_2812588_960x540_fill_q75_catmullrom_top.jpg); } @media only screen and (min-width: 1200px) { #td-cover-block-0 { background-image: url(/home_background_hu0af614851632d061c3c80a153395694b_2812588_1920x1080_fill_q75_catmullrom_top.jpg); } }  Apache SkyWalking Quick Start  GitHub  Application performance monitor tool for distributed systems, especially designed for microservices, cloud native and container-based (Docker, Kubernetes, Mesos) architectures.        What is SkyWalking?  SkyWalking is an Observability Analysis Platform and Application Performance Management system. Tracing, Metrics and Logging all-in-one solution.\nJava, .Net Core, PHP, NodeJS, Golang, LUA, C++ agents supported\nIstio + Envoy Service Mesh supported\n                Live Demo User: skywalking \u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp; Password: skywalking  Go      Feature List Consistent Observability  Tracing Metrics\nLogging\nBrowser monitoring\n  Multiple Language Agents  Java Golang\n.Net Core\nPython\nNodeJS\nC++\nPHP\nLua\n  Lightweight  No big data stack Adopt different scale\n  Modular  Storage pluggable Cluster coordinator pluggable\nSupport pull/push transportation\n  Alarm Supported  Alarm HTTP/gRPC forwarder Slack notification\nDingding notification\nWeChat notification\nRaw metrics data exporter\n  Fancy Visualization  Customizable dashboard\nTopology map\nTrace and profile explorer\nCLI dashboard\nIntelliJ IDE plugin, metrics side by side with codes\n       Events \u0026amp; News Release Apache SkyWalking APM 8.6.0 Thu, Jun 10, 2021 SkyWalking 8.6.0 is released. Go to downloads page to find release tars. Changes by Version Project …\n SkyWalkingDay Conference 2021, relocating at Beijing Mon, Jun 7, 2021 Abstract Apache SkyWalking hosts SkyWalkingDay Conference 2021 in June 26th, jointly with Tencent …\n Release Apache SkyWalking for NodeJS 0.3.0 Thu, May 27, 2021 SkyWalking NodeJS 0.3.0 is released. Go to downloads page to find release tars. Add ioredis plugin. …\n Release Apache SkyWalking Client JS 0.5.1 Fri, May 21, 2021 SkyWalking Client JS 0.5.1 is released. Go to downloads page to find release tars. Add …\n     Our Users  Various companies and organizations use SkyWalking for research, production and commercial products.                                                                                                                                                                                                                                                                                                                                                                                                                                     Users are encouraged to add themselves to this page. Send a pull request to add your company or organization information [here].\n     Any questions? Features request, ask questions or report bugs? Feel free to file a issue or join our slack workspace.\n   Contributions welcome! We do a Pull Request contributions workflow on GitHub. New users are always welcome!\n   Follow us on Twitter! For announcement of latest features etc on @ASFSkyWalking.\n    ","excerpt":"#td-cover-block-0 { background-image: …","ref":"/","title":"Apache SkyWalking"},{"body":"This is the blog section. It has two categories: News and Releases.\nFiles in these directories will be listed in reverse chronological order.\n","excerpt":"This is the blog section. It has two categories: News and Releases.\nFiles in these directories will …","ref":"/blog/","title":"Blog"},{"body":"","excerpt":"","ref":"/docs/","title":"Documentation"},{"body":"Download the SkyWalking recommended releases Use the links below to download the Apache SkyWalking from one of our mirrors.\nOnly source code releases are official Apache releases, binary distributions are just for end user convenience.\nGeneral S SkyWalking APM  SkyWalking is an Observability Analysis Platform and Application Performance Management system.\nSource   v8.6.0 | June. 10th, 2021 [src] [asc] [sha512]  v8.5.0 | Apr. 12th, 2021 [src] [asc] [sha512]  v8.4.0 | Feb. 4th, 2021 [src] [asc] [sha512]  v8.3.0 | Dec. 3rd, 2020 [src] [asc] [sha512]  v8.2.0 | Oct. 27th, 2020 [src] [asc] [sha512]      Distribution   v8.6.0 for ElasticSearch 6 | June. 10th, 2021 [tar] [asc] [sha512]  v8.6.0 for H2/MySQL/TiDB/InfluxDB/ElasticSearch 7 | June. 10th, 2021 [tar] [asc] [sha512]  v8.5.0 for ElasticSearch 6 | Apr. 12th, 2021 [tar] [asc] [sha512]  v8.5.0 for H2/MySQL/TiDB/InfluxDB/ElasticSearch 7 | Apr. 12th, 2021 [tar] [asc] [sha512]  v8.4.0 for ElasticSearch 6 | Feb. 4th, 2021 [tar] [asc] [sha512]  v8.4.0 for H2/MySQL/TiDB/InfluxDB/ElasticSearch 7 | Feb. 4th, 2021 [tar] [asc] [sha512]  v8.3.0 for ElasticSearch 6 | Dec. 3rd, 2020 [tar] [asc] [sha512]  v8.3.0 for H2/MySQL/TiDB/InfluxDB/ElasticSearch 7 | Dec. 3rd, 2020 [tar] [asc] [sha512]  v8.2.0 for ElasticSearch 6 | Oct. 27th, 2020 [tar] [asc] [sha512]  v8.2.0 for H2/MySQL/TiDB/InfluxDB/ElasticSearch 7 | Oct. 27th, 2020 [tar] [asc] [sha512]          Agent L SkyWalking Nginx LUA  SkyWalking Nginx Agent provides the native tracing capability for Nginx powered by Nginx LUA module.\nSource   v0.5.0 | Apr. 25th, 2021 [src] [asc] [sha512]      Distribution   v0.5.0 | Apr. 25th, 2021 [Install via luarocks]        S SkyWalking Kong  SkyWalking Kong Agent provides the native tracing capability.\nSource   v0.1.1 | May. 13th, 2021 [src] [asc] [sha512]      Distribution   v0.1.1 | May. 13th, 2021 [Install via luarocks]        P SkyWalking Python  The Python Agent for Apache SkyWalking, which provides the native tracing abilities for Python.\nSource   v0.6.0 | Mar. 31th, 2021 [src] [asc] [sha512]      Distribution   v0.6.0 | Mar. 31th, 2021 [Install via pip]        J SkyWalking NodeJS  The NodeJS Agent for Apache SkyWalking, which provides the native tracing abilities for NodeJS backend.\nSource   v0.3.0 | May. 27th, 2020 [src] [asc] [sha512]      Distribution   v0.3.0 | May. 27th, 2021 [Install via npm]        S SkyWalking Client JavaScript  Apache SkyWalking Client-side JavaScript exception and tracing library.\nSource   v0.5.1 | May. 21th, 2021 [src] [asc] [sha512]      Distribution   v0.5.1 | May. 21th, 2021 [Install via npm]        S SkyWalking Satellite  A lightweight collector/sidecar could be deployed closing to the target monitored system, to collect metrics, traces, and logs.\nSource   v0.1.0 | Feb. 26th, 2021 [src] [asc] [sha512]      Distribution   v0.1.0 | Feb. 26th, 2021 [tar] [asc] [sha512]          Operation C SkyWalking CLI  SkyWalking CLI is a command interaction tool for the SkyWalking user or OPS team.\nSource   v0.6.0 | Feb. 9th, 2021 [src] [asc] [sha512]      Distribution   v0.6.0 | Feb. 9th, 2021 [tar] [asc] [sha512]        H SkyWalking Helm  SkyWalking Kubernetes repository provides ways to install and configure SkyWalking in a Kubernetes cluster. The scripts are written in Helm 3.\nSource   v4.0.0 | Nov. 3rd, 2020 [src] [asc] [sha512]         K SkyWalking Cloud on Kubernetes  A bridge project between Apache SkyWalking and Kubernetes.\nSource   v0.3.0 | April. 6th, 2021 [src] [asc] [sha512]      Distribution   v0.3.0 | April. 6th, 2021 [tar] [asc] [sha512]          Tools I SkyWalking Eyes  A full-featured license tool to check and fix license headers and resolve dependencies\u0026#39; licenses.\nSource   v0.1.0 | Jan. 12th, 2021 [src] [asc] [sha512]      Distribution   v0.1.0 | Jan. 12th, 2021 [tar] [asc] [sha512]           All Releases  Find all SkyWalking releases in the Archive repository. Archive incubating repository hosts older releases when SkyWalking was an incubator project.  Docker Images for convenience Docker images are not official ASF releases but provided for convenience. Recommended usage is always to build the source\nO SkyWalking OAP Server  This image would start up SkyWalking OAP server only. Note, choose *-es6 tags when use ElasticSearch 6, *-es7 tags when use ElasticSearch 7.\nDocker Image     U SkyWalking UI Image  This image would start up SkyWalking UI only.\nDocker Image     I SkyWalking Eyes Image  A full-featured license tool to check and fix license headers and resolve dependencies\u0026#39; licenses.\nDocker Image     K SkyWalking Cloud on Kubernetes  A platform for the SkyWalking user, provisions, upgrades, maintains SkyWalking relevant components, and makes them work natively on Kubernetes.\nDocker Image     J SkyWalking Java Agent  The Docker image for Java users to conveniently use SkyWalking agent in containerized scenario.\nDocker Image       Verify the releases PGP signatures KEYS\nIt is essential that you verify the integrity of the downloaded files using the PGP or SHA signatures. The PGP signatures can be verified using GPG or PGP. Please download the KEYS as well as the asc signature files for relevant distribution. It is recommended to get these files from the main distribution directory and not from the mirrors.\ngpg -i KEYS or pgpk -a KEYS or pgp -ka KEYS To verify the binaries/sources you can download the relevant asc files for it from main distribution directory and follow the below guide.\ngpg --verify apache-skywalking-apm-********.asc apache-skywalking-apm-********* or pgpv apache-skywalking-apm-********.asc or pgp apache-skywalking-apm-********.asc ","excerpt":"Download the SkyWalking recommended releases Use the links below to download the Apache SkyWalking …","ref":"/downloads/","title":"Downloads"},{"body":"SkyWalking events.\n","excerpt":"SkyWalking events.","ref":"/events/","title":"Events"},{"body":"","excerpt":"","ref":"/false/","title":"False"},{"body":"","excerpt":"","ref":"/search/","title":"Search Results"},{"body":"","excerpt":"","ref":"/tango/","title":"Tango"},{"body":"SkyWalking Team The SkyWalking team is comprised of Members and Contributors. Members have direct access to the source of SkyWalking project and actively evolve the code-base. Contributors improve the project through submission of patches and suggestions to the Members. The number of Contributors to the project is unbounded. All contributions to SkyWalking are greatly appreciated, whether for trivial cleanups, big new features or other material rewards. More details see here.\nMembers Members include Project Management Committee members and committers. The list is in alphabet order.\nProject Management Committee    Name Apache ID Twitter     Can Li lican Candy198088   DongXue Si ilucky    Han Liu liuhan dalek_zero   Haochao Zhuang daming    Haoyang Liu liuhaoyangzz    Hongtao Gao hanahmily    Hongwei Zhai innerpeacez    Ignasi Barrera nacx    Jian Tan tanjian    Jiaqi Lin linjiaqi    Jinlin Fu withlin    Juntao Zhang zhangjuntao    Kai Wang wangkai    Lang Li lilang    Michael Semb Wever mck    Qiuxia Fan qiuxiafan    Sheng Wu (V.P. and Chair of PMC) wusheng wusheng1108   Shinn Zhang zhangxin ascrutae   Wei Zhang zhangwei24    Wenbing Wang wangwenbin    Willem Ning Jiang ningjiang    Yang Bai baiyang    Yao Wang ywang    Yixiong Cao caoyixiong    Yongsheng Peng pengys    Yuguang Zhao zhaoyuguang    Zhang Kewei zhangkewei    Zhenxu Ke kezhenxu94 kezhenxu94    Committer    Name Apache ID Twitter     Brandon Fergerson bfergerson    Gui Cao zifeihan zifeihan007   Hailin Wang wanghailin    Huaxi Jiang hoshea Zerone___01   Jiapeng Liu liujiapeng    JunXu Chen chenjunxu    Ke Zhang zhangke Humbertttttt   Ming Wen wenming    Sheng Wang wangsheng    Tomasz Pytel tompytel    Wei Hua alonelaval    Wei Jin kvn    Weijie Zou kdump RootShellExp   Weiyi Liu wayilau    Yanlong He heyanlong YanlongHe   Yuntao Li liyuntao    Zhusheng Xu aderm      Contributors 479  SkyWalking have hundreds of contributors, you could find them in our repositories' contribution list.   SkyWalking main repository  366    wu-sheng    peng-yongsheng    ascrutae    acurtain    kezhenxu94    hanahmily    JaredTan95    dmsolr    arugal    zhaoyuguang    lytscu    mrproliu    Fine0830    wingwong-knh    zifeihan    BFergerson    EvanLjp    IanCao    Ax1an    x22x22    wankai123    clevertension    xbkaishui    withlin    liuhaoyang    carlvine500    candyleer    zhangkewei    bai-yang    hailin0    TinyAllen    adermxzs    liqiangz    songzhendong    heyanlong    qxo    IluckySi    Jtrust    alonelaval    wendal    lujiajing1126    Humbertzhang    tristaZero    xuanyu66    J-Cod3r    stalary    jjtyro    heihaozi    yaojingguo    JohnNiang    tuohai666    honganan    darcydai    SataQiu    YunaiV    harvies    langyan1022    Liu-XinYuan    wallezhang    aiyanbo    FatihErdem    nisiyong    tom-pytel    cyberdak    dagmom    innerpeacez    codelipenghui    dominicqi    yu199195    LiWenGu    haoyann    chidaodezhongsheng    xinzhuxiansheng    Ahoo-Wang    a1vin-tian    dio    yanfch    fgksgf    devkanro    oflebbe    ScienJus    liu-junchi    Z-Beatles    WillemJiang    YczYanchengzhe    chenpengfei    gnr163    libinglong    neeuq    snakorse    xiaoy00    Indifer    huangyoje    s00373198    cyejing    vcjmhg    KangZhiDong    YunfengGao    ajanthan    AlexanderWert    willseeyou    codeglzhang    HarryFQ    potiuk    CalvinKirs    leizhiyuan    tsuilouis    buxingzhe    Qiliang    Shikugawa    yang-xiaodong    SummerOfServenteen    XhangUeiJong    zaunist    cheetah012    beckhampu    chenmudu    cngdkxw    coder-yqj    coki230    cuiweiwei    dengliming    amwyyyy    dsc6636926    elk-g    evanxuhe    jinlongwang    karott    klboke    makingtime    mgsheng    muyun12    novayoung    oatiz    osiriswd    scolia    terranhu    tzy1316106836    viswaramamoorthy    webb2019    wuguangkuo    xcaspar    yazong    zxbu    SoberChina    juzhiyuan    WildWolfBang    michaelsembwever    purgeyao    lkxiaolou    kuaikuai    seifeHu    1095071913    50168383    lunchboxav    AirTrioa    andyzzl    BZFYS    brucewu-fly    ychandu    shiluo34    CharlesMaster    CommissarXia    Cvimer    sdanzo    devon-ye    qqeasonchen    qijianbo010    efekaptan    FingerLiu    bootsrc    FrankyXu    Gallardot    tankilo    HendSame    DeadLion    JoeKerouac    zouyx    jbampton    augustowebd    zhentaoJin    Jargon96    olzhy    LazyLei    coolbeevip    liuhaoXD    maolie    TheRealHaui    zeaposs    mikkeschiren    ZhuoSiChen    nickwongwong    nikitap492    O-ll-O    HackerRookie    Patrick0308    QHWG67    RandyAbernethy    rlenferink    ruibaby    yymoth    zhangsean    compilerduck    witchc    kun-song    stevehu    Technoboy-    TerrellChen    gitter-badger    TomMD    trustin    Videl    viktoryi    moonming    wilsonwu    ViberW    Wooo0    yanickxia    ycoe    Miss-you    yuqichou    panniyuyu    ZS-Oliver    chenbeitang    ZhHong    adamni135    aix3    alexkarezin    amogege    beiwangnull    jy00464346    buzuotaxuan    c1ay    wbpcode    crystaldust    cui-liqiang    cutePanda123    divyakumarjain    donbing007    dvsv2    dzx2018    echooymxq    eoeac    fuhuo    geektcp    GerryYuan    ggndnn    guodongq    gonedays    kylixs    gy09535    guyukou    haotian2015    hardzhang    Heguoya    hi-sb    Hen1ng    hsoftxl    huliangdream    jjlu521016    jialong121    zhangjianweibj    jsbxyyx    justeene    aeolusheath    kayleyang    kevinyyyy    kikupotter    killGC    ksewen    landonzeng    langke93    lazycathome    leemove    lijial    linliaoy    liuyanggithup    louis-zhou    lpcy    lxliuxuankb    maxiaoguang64    mantuliu    margauxcabrera    Yebemeto    momo0313    Xlinlin    neatlife    nileblack    onecloud360    FeynmanZhou    carrypann    thanq    qiuyu-d    ralphgj    raybi-asus    mestarshine    sikelangya    simonlei    sk163    zhe1926    stenio2011    stone-wlg    surechen    hepyu    sxzaihua    tbdpmi    tianyuak    tincopper    tristan-tsl    weiqiang-w    lyzhang1999    web-xiaxia    wenjianzhang    whfjam    wind2008hxy    wqr2016    sonxy    wujun8    wuxingye    xdRight    a198720    xingren23    xuchangjunjx    yanbw    yangxb2010000    yanmingbi    yantaowu    yuyujulin    zaygrzx    zcai2    RedzRedz    zhousiliang163    zoidbergwill    zoumingzm    zshit    zygfengyuwuzu    liuzhengyang    tanjunchen    wengangJi    gzshilu    terrymanu    zhanghao001    pengweiqhca    lsyf    tzsword-2020          Rocketbot UI  59    TinyAllen    Fine0830    x22x22    wu-sheng    JaredTan95    kezhenxu94    bigflybrother    Jtrust    zhaoyuguang    heihaozi    dmsolr    alonelaval    hanahmily    tom-pytel    aeolusheath    arugal    horber    shiluo34    ruibaby    wilsonwu    constanine    leemove    liqiangz    whfjam    wuguangkuo    xuchangjunjx    Indifer    hailin0    aiyanbo    BFergerson    efekaptan    yanfch    grissom-grissom    grissomsh    Humbertzhang    liuhaoyang    tsuilouis    masterxxo    zeaposs    O-ll-O    QHWG67    Doublemine    zaunist    xiaoxiangmoe    c1ay    dagmom    fredster33    fuhuo    codelipenghui    lunamagic1978    novayoung    dominicqi    stone-wlg    surechen    xbkaishui    huangyoje    heyanlong    llissery    magic-akari          SkyWalking Website  61    wu-sheng    Jtrust    kezhenxu94    JaredTan95    hanahmily    innerpeacez    arugal    dmsolr    fgksgf    zhaoyuguang    TinyAllen    mrproliu    EvanLjp    rootsongjc    Fine0830    peng-yongsheng    BFergerson    yanmaipian    Humbertzhang    dependabot[bot]    gxthrj    thebouv    cheenursn    x22x22    nisiyong    alonelaval    libinglong    heyanlong    langyan1022    YunaiV    feelwing1314    CharlesMaster    devkanro    lilien1010    lucperkins    tom-pytel    moonming    withlin    wang-yeliang    YoungHu    agile6v    chopin-d    hailin0    jjlu521016    klboke    geomonlin    LiteSun    nic-chen    FeynmanZhou    tevahp    tristan-tsl    vcjmhg    wankai123    weiqiang333    xbkaishui    xdRight    yimeng    zhang98722            Nginx LUA Agent  18    wu-sheng    dmsolr    membphis    moonming    mrproliu    spacewander    yxudong    arugal    WALL-E    kezhenxu94    dingdongnigetou    JaredTan95    CalvinKirs    lilien1010    Jijun    Du-fei    wangrzneu          Kong Agent  4    dmsolr    wu-sheng    CalvinKirs    kezhenxu94          Python Agent  16    kezhenxu94    alonelaval    tom-pytel    Humbertzhang    langyizhao    wu-sheng    fgksgf    sungitly    TomMD    chestarss    c1ay    fuhuo    probeyang    taskmgr    dafu-wu    zkscpqm          NodeJS Agent  6    kezhenxu94    tom-pytel    tianyk    QuanjieDeng    zijin-m    wu-sheng          Client JavaScript  10    Fine0830    wu-sheng    arugal    kezhenxu94    tianyk    JaredTan95    Jtrust    givingwu    mage3k    qinhang3          SkyWalking Satellite  9    EvanLjp    kezhenxu94    gxthrj    mrproliu    wu-sheng    wangrzneu    CalvinKirs    nic-chen    arugal            SkyWalking CLI  9    kezhenxu94    fgksgf    wu-sheng    hanahmily    alonelaval    arugal    heyanlong    clk1st    innerpeacez          Kubernetes Helm  15    innerpeacez    wu-sheng    hanahmily    kezhenxu94    JaredTan95    chengshiwen    carllhw    CalvinKirs    glongzh    chenvista    swartz-k    tristan-tsl    vision-ken          SkyWalking Cloud on Kubernetes  6    hanahmily    kezhenxu94    wu-sheng    BFergerson    CalvinKirs    heyanlong          Docker Files  10    hanahmily    wu-sheng    JaredTan95    kezhenxu94    carlvine500    kkl129    tristan-tsl    arugal    heyanlong            Data Collect Protocol  18    wu-sheng    arugal    kezhenxu94    mrproliu    EvanLjp    Shikugawa    liuhaoyang    zifeihan    peng-yongsheng    dmsolr    hanahmily    nacx    yaojingguo    SataQiu    stalary    snakorse    heyanlong    Liu-XinYuan          Query Protocol  14    wu-sheng    arugal    mrproliu    peng-yongsheng    hanahmily    x22x22    kezhenxu94    JaredTan95    MiracleDx    liuhaoyang    Fine0830    chenmudu    liqiangz    heyanlong          Go API  6    kezhenxu94    wu-sheng    EvanLjp    CalvinKirs    mrproliu            skywalking-agent-test-tool  11    dmsolr    mrproliu    wu-sheng    kezhenxu94    EvanLjp    yaojingguo    CalvinKirs    Shikugawa    dagmom    harvies    alonelaval          SkyWalking Eyes  7    kezhenxu94    fgksgf    wu-sheng    fulmicoton    chengshiwen    zifeihan    heyanlong          Ecosystem Projects   SkyAPM-dotnet  27    liuhaoyang    snakorse    wu-sheng    ElderJames    yang-xiaodong    pengweiqhca    Ahoo-Wang    zeaposs    kaanid    ShaoHans    qq362220083    withlin    AlseinX    andyliyuze    ChaunceyLin5152    dimaaan    cnlangzi    WeihanLi    beckjin    dependabot[bot]    limfriend    linkinshi    zpf1989    refactor2    itsvse    misaya          Go2Sky  16    arugal    hanahmily    wu-sheng    mrproliu    nacx    fgksgf    Humbertzhang    JaredTan95    withlin    Just-maple    kuaikuai    zhuCheer    kezhenxu94    limfriend    xbkaishui    liweiv          go2sky-plugins  4    arugal    mrproliu    wu-sheng    zaunist          SkyAPM-php-sdk  22    heyanlong    bostin    lpf32    songzhian    jmjoy    wu-sheng    remicollet    mikkeschiren    xinfeingxia85    xonze    cyhii    MrYzys    rovast    tinyu0    xudianyang    huohuanhuan    kilingzhang    limfriend    qjgszzx    dickens7    yaowenqiang          SkyAPM Node.js  10    ascrutae    kezhenxu94    wu-sheng    zouyx    Jozdortraz    a526672351    rovast    Runrioter    jasper-zsh    TJ666          cpp2sky  3    Shikugawa    wu-sheng    makefriend8          SourceMarker  3    BFergerson    dependabot[bot]    chess-equality          java-plugin-extensions  5    ascrutae    wu-sheng    JaredTan95    raybi-asus    zifeihan          uranus  2    harvies    wu-sheng            Archived\n SkyWalking UI. Replaced by RocketBot UI. SkyWalking OAL tool  Contributor over time  Becoming a Committer SkyWalking follows the Apache way to build the community. Anyone can become a committer once they have contributed sufficiently to the project and earned the trust. Read Contributing Guides to take part in the community.\nThe SkyWalking community follows the Apache Community’s process on accepting a new committer.\n Start the discussion and vote in @private. Only current PMC member could nominate. If the vote passes, send an offer to become a committer with @private CC’ed. Add the committer to the team page Setup committer rights  ","excerpt":"SkyWalking Team The SkyWalking team is comprised of Members and Contributors. Members have direct …","ref":"/team/","title":"Team"},{"body":"","excerpt":"","ref":"/true/","title":"True"},{"body":"","excerpt":"","ref":"/zh/","title":"博客"}]